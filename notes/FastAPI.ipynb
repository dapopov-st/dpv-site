{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: FastAPI notes\n",
    "description: Notes on FastAPI\n",
    "date: 2024-09\n",
    "categories: [Programming]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build APIs based on standard Python type hints\n",
    "- Automatically generate interactive documentation\n",
    "- Fast to code, fewer bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install fastapi\n",
    "#! pip install \"uvicorn[standard]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the following contents are in main.py, can run via `uvicorn main:app --reload`.  \n",
    "    - `main` refers to main.py and app refers to the object inside main.py.\n",
    "    - `--reload` reloads the page upon changes, to be used during dev, not prod.\n",
    "- Can see documentation conforming to OpenAPI standard in `http://127.0.0.1:8000/docs`, from which you can use the endpoints!\n",
    "- `http://127.0.0.1:8000/redoc` returns documentation in alternative format.\n",
    "- Use `async def` to make the functions non-blocking, enabling other tasks to run concurrently. Useful when function performs I/O-bound operations, such as database queries, file I/O, or network requests, and when need to handle a large number of concurrent requests efficiently.\n",
    "- Type hints will be validated with Pydantic, so if use a non-int in `/items/{item_id}`, will get an error.\n",
    "- Order matters: If `read_user_current` is placed *after* `read_user`, will get an error since FastAPI will read functions top-down and try to validate input to be an integer.\n",
    "- Use `Enums` if path parameter must come from a certain list of values.  If improper parameter is passed, FastAPI will list available values!\n",
    "- To have paths be read correctly, use `:path` path converter, allowing the parameter to capture the entire path, including slashes.\n",
    "- `read_animal` without additional parameters will read off animals 0-10.  With additional parameters, can specify which ones we want via *query parameters*, as in http://127.0.0.1:8000/animals/?skip=0&limit=2.  Here, ? denotes start of query parameters and & separates them.  Can also pass optional parameter as http://127.0.0.1:8000/animals/?skip=0&limit=2&optional_param=3, just make sure to specify it as typing.Optional.\n",
    "- Can pass and use optional parameters as in `read_user_item`.\n",
    "- Request body is data sent by client to the API and response body is data sent from API to client.  Use Pydantic to specify request body with POST request type.\n",
    "    - To send a post request, could test it out in /docs or with curl -X POST \"http://127.0.0.1:8000/books/\" -H \"Content-Type: application/json\" -d '{\n",
    "    \"name\": \"The Great Gatsby\",\n",
    "    \"author\": \"F. Scott Fitzgerald\",\n",
    "    \"description\": \"A novel set in the 1920s\",\n",
    "    \"price\": 10.99\n",
    "}'\n",
    "    - Then can go to /books endpoint to see the books printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from enum import Enum\n",
    "import typing as t\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\") #route/endpoint\n",
    "def home_page():\n",
    "    return {\"message\":\"Hello World!\"}\n",
    "\n",
    "@app.get(\"/items/{item_id}\") #item_id is the path parameter\n",
    "async def read_item(item_id: int):\n",
    "    return {\"item_id\":item_id}\n",
    "\n",
    "@app.get(\"/users/me\") # will not work if placed after, must be before to be valid\n",
    "async def read_user_current():\n",
    "    return {\"user_id\":\"Current user\"}\n",
    "\n",
    "@app.get(\"/users/{user_id}\") \n",
    "async def read_user(user_id: int):\n",
    "    return {\"user_id\":user_id}\n",
    "\n",
    "class ModelName(str,Enum):\n",
    "    ALEXNET = 'ALEXNET'\n",
    "    RESNET = 'RESNET'\n",
    "    LENET = 'LENET'\n",
    "\n",
    "@app.get(\"/models/{model_name}\")\n",
    "async def get_model(model_name: ModelName):\n",
    "    if model_name == ModelName.ALEXNET:\n",
    "        return {'model_name':model_name}\n",
    "    elif model_name.value == \"LENET\":\n",
    "        return {'model_name': model_name}\n",
    "    else:\n",
    "        return {'model_name':f\"You have selected {model_name.value}\"}\n",
    "    \n",
    "@app.get(\"files/{file_path:path}\")\n",
    "async def read_file(file_path:str):\n",
    "    return {\"file_path\":file_path}\n",
    "\n",
    "animal_db = [{\"animal_name\":'cat'},{\"animal_name\":'llama'},{\"animal_name\":'alpaca'}]\n",
    "\n",
    "@app.get(\"/animals/\")\n",
    "async def read_animal(skip: int=0, limit: int=10, optional_param: t.Optional[int]=None):\n",
    "    return {\"animals\": animal_db[skip:skip+limit], \"optional_parameter\":optional_param}\n",
    "\n",
    "@app.get(\"/users/{user_id}/items/{item_id}\")\n",
    "async def read_user_item(\n",
    "    user_id: int, item_id: int, q: t.Optional[str]=None, short:bool=False\n",
    "):\n",
    "    item = {\"item_id\":item_id, \"owner_id\":user_id}\n",
    "    if q:\n",
    "        item.update({\"q\":q})\n",
    "    if not short:\n",
    "        item.update({'description':'great item with long description'})\n",
    "    return item\n",
    "\n",
    "books_db = []\n",
    "class Book(BaseModel):\n",
    "    name:str\n",
    "    author:str\n",
    "    description:t.Optional[str]\n",
    "    price:float\n",
    "\n",
    "@app.post(\"/books/\")\n",
    "async def create_item(book:Book):\n",
    "    books_db.append(book)\n",
    "    return book \n",
    "@app.get(\"/books/\")\n",
    "async def get_books():\n",
    "    return books_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes following \"Building Data Science Applications with FastAPI\" by François Voron Chapter 2: Python specificities -> asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What's the difference between WSGI and ASGI gateways as it pertains to Django and FastAPI?\n",
    "WSGI (Web Server Gateway Interface) and ASGI (Asynchronous Server Gateway Interface) are two different specifications for Python web servers and applications. They serve as interfaces between web servers and web applications or frameworks. Here’s a detailed comparison of WSGI and ASGI, particularly in the context of Django and FastAPI:\n",
    "\n",
    "- WSGI (Web Server Gateway Interface)\n",
    "Synchronous:\n",
    "\n",
    "WSGI is designed for synchronous web applications. It handles one request at a time per worker, which can lead to inefficiencies when dealing with I/O-bound operations like database queries or external API calls.\n",
    "Django:\n",
    "\n",
    "Django is traditionally a WSGI-based framework. It works well for most web applications but can struggle with real-time features like WebSockets or long-polling due to its synchronous nature.\n",
    "Common WSGI servers for Django include Gunicorn and uWSGI.\n",
    "Concurrency:\n",
    "\n",
    "WSGI applications handle concurrency by using multiple worker processes or threads. Each worker handles one request at a time.\n",
    "Deployment:\n",
    "\n",
    "WSGI applications are typically deployed using WSGI servers like Gunicorn, uWSGI, or mod_wsgi (for Apache).\n",
    "- ASGI (Asynchronous Server Gateway Interface)\n",
    "Asynchronous:\n",
    "\n",
    "ASGI is designed for asynchronous web applications. It supports both synchronous and asynchronous code, allowing for more efficient handling of I/O-bound operations and real-time features.\n",
    "FastAPI:\n",
    "\n",
    "FastAPI is an ASGI-based framework. It is built from the ground up to support asynchronous programming, making it ideal for applications that require high concurrency, real-time communication, or WebSockets.\n",
    "Common ASGI servers for FastAPI include Uvicorn and Daphne.\n",
    "Concurrency:\n",
    "\n",
    "ASGI applications can handle many requests concurrently using asynchronous I/O. This allows for more efficient use of resources, especially for I/O-bound tasks.\n",
    "Deployment:\n",
    "\n",
    "ASGI applications are typically deployed using ASGI servers like Uvicorn, Daphne, or Hypercorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nest_asyncio # run asyncio within Jupyter's already running even loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "A\n",
      "B\n",
      "A\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "# import asyncio\n",
    "# async def printer(name: str, times: int)->None:\n",
    "#     for i in range(times):\n",
    "#         print(name)\n",
    "#         await asyncio.sleep(1)\n",
    "# async def main():\n",
    "#     await asyncio.gather(\n",
    "#         printer(\"A\",3),\n",
    "#         printer(\"B\",3)\n",
    "#     )\n",
    "# asyncio.run(main())\n",
    "\n",
    "# adopting code since Jupyter has it's own event loop\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def printer(name: str, times: int) -> None:\n",
    "    for i in range(times):\n",
    "        print(name)\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def main():\n",
    "    await asyncio.gather(\n",
    "        printer(\"A\", 3),\n",
    "        printer(\"B\", 3)\n",
    "    )\n",
    "\n",
    "# Await the main coroutine directly\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- asyncio.sleep(1) was added since *writing code in a coroutine doesn't necessarily mean it will not block*.  Computations are blocking!  I/O opps will not block or we could use multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Path parameters and their validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6460/3090869963.py:5: DeprecationWarning: `regex` has been deprecated, please use `pattern` instead\n",
      "  async def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, Path\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get('/license-plates/{license}')\n",
    "async def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n",
    "    return {\"license\":license}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In FastAPI, ... above indicate that we don't want a default value.  RegEx validates French license plates like AB-123-CD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes by Key Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation, virtual environment (conda), running, first app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash\n",
    "conda create --name fastapi-env python=3.11\n",
    "conda activate fastapi-env \n",
    "pip install fastapi[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If FastAPI app is called app in main file, run as follows: \n",
    "`uvicorn main:app --reload`\n",
    "- Access interactive documentation using `http://127.0.0.1:8000/docs` (using Swagger UI) or `http://127.0.0.1:8000/redoc` (using ReDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining routes with path and query parameters (for user input) and validating requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining path parameters: user_id is a path parameter FastAPI will convert to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "app = FastAPI() \n",
    "\n",
    "@app.get(\"/users/{user_id}\")\n",
    "def read_user(user_id:int):\n",
    "    return {\"user_id\":user_id}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining query parameters via function parameters with default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/users/\")\n",
    "def read_user(skip:int=0,limit:int=10):\n",
    "    return {\"skip\":skip, \"limit\":limit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user accesses /users/?skip=5&limit=15, FastAPI will return `{\"skip\":5, \"limit\":15}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Request validation with Pydantic below.  Make the request as follows:\n",
    "- `curl -X POST \"http://127.0.0.1:8000/users/\" -H \"Content-Type: application/json\" -d '{\"id\":1, \"name\":\"John Smith\", \"email\":\"john@example.com\"}'\n",
    "    - `-X POST`: use HTTP method to post data\n",
    "    - `-H \"Content-Type: application/json\"`: add HTTP header to the request and specify that the data being sent is in JSON format\n",
    "    - -d '{\"id\":1, \"name\":\"John Smith\", \"email\":\"john@example.com\"}': send the specified data in the request body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "\n",
    "@app.post(\"/users/\")\n",
    "def create_user(user:User):\n",
    "    return {\"id\": user.id, \"name\": user.name, \"email\":user.email}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining path ahd query parameters with Pydantic: user_id is a path parameter and details is a query parameter that modifies the response.\n",
    "- Read simply as `curl \"http://127.0.0.1:8000/users/1\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/users/{user_id}\")\n",
    "def read_user(user_id: int, details: bool=False):\n",
    "    if details:\n",
    "        return {\"user_id\":user_id, \"details\":\"Detailed info\"}\n",
    "    return {\"user_id\":user_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request and response models\n",
    "- Request models define the structure of the data that your API expects to receive in the request body. They are used to validate and parse the incoming data.\n",
    "- Response models define the structure of the data that your API returns in response.  They ensure that the response data is correctly formatted and validated.\n",
    "- `curl -X POST \"http://127.0.0.1:8000/users/\" -H \"Content-Type: application/json\" -d '{\"id\":1, \"name\":\"John Smith\", \"email\":\"john@example.com\", \"age\":30}'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class UserCreate(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "    age: Optional[int] = None\n",
    "\n",
    "class UserResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "    age: Optional[int] = None\n",
    "    is_active:  bool\n",
    "\n",
    "@app.post(\"/users/\", response_model=UserResponse)\n",
    "async def create_user(user:UserCreate): #validate incoming data\n",
    "    user_response = UserResponse(       #validate outgoing data\n",
    "        id= user.id,\n",
    "        name=user.name,\n",
    "        email=user.email,\n",
    "        age=user.age,\n",
    "        is_active=True\n",
    "    )\n",
    "    return user_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Injection\n",
    "- Inject dependencies (database connections, configuration settings, other shared resources) into your functions or classes.\n",
    "- Separate concerns between the logic of the endpoint and the more generic logic for the pagination parameters. \n",
    "- Ideal for utility logic to retrieve or validate data, make security checks, or call external logic that will be needed several times across the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes following \"Building Data Science Applications with FastAPI\" by François Voron Chapter 2: Python specificities -> asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Depends, FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def pagination(skip:int=0,limit:int=10)->tuple[int,int]:\n",
    "    return (skip,limit)\n",
    "\n",
    "@app.get(\"/items\")\n",
    "async def list_items(p:tuple[int,int]=Depends(pagination)):\n",
    "    skip,limit = p\n",
    "    return {\"skip\":skip, \"limit\":limit}\n",
    "\n",
    "@app.get(\"/things\")\n",
    "async def list_things(p:tuple[int,int]=Depends(pagination)):\n",
    "    skip,limit = p\n",
    "    return {\"skip\":skip, \"limit\":limit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FastAPI limitation: `Depends` function is not able to forward the type of the dependency function, so we have to do this manually above.\n",
    "- Raising a 404 error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Depends, FastAPI, HTTPException, status\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Post(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class PostUpdate(BaseModel):\n",
    "    title: str | None\n",
    "    content: str | None\n",
    "\n",
    "\n",
    "class DummyDatabase:\n",
    "    posts: dict[int, Post] = {}\n",
    "\n",
    "\n",
    "db = DummyDatabase()\n",
    "db.posts = {\n",
    "    1: Post(id=1, title=\"Post 1\", content=\"Content 1\"),\n",
    "    2: Post(id=2, title=\"Post 2\", content=\"Content 2\"),\n",
    "    3: Post(id=3, title=\"Post 3\", content=\"Content 3\"),\n",
    "}\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def get_post_or_404(id: int) -> Post:\n",
    "    try:\n",
    "        return db.posts[id]\n",
    "    except KeyError:\n",
    "        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND)\n",
    "\n",
    "\n",
    "@app.get(\"/posts/{id}\")\n",
    "async def get(post: Post = Depends(get_post_or_404)):\n",
    "    return post\n",
    "\n",
    "\n",
    "@app.patch(\"/posts/{id}\")\n",
    "async def update(post_update: PostUpdate, post: Post = Depends(get_post_or_404)):\n",
    "    updated_post = post.copy(update=post_update.dict())\n",
    "    db.posts[post.id] = updated_post\n",
    "    return updated_post\n",
    "\n",
    "\n",
    "@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_CONTENT)\n",
    "async def delete(post: Post = Depends(get_post_or_404)):\n",
    "    db.posts.pop(post.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating and using a parametrized dependency with a class\n",
    "- Suppose we wanted to dynamically cap the limit value in the pagination example...-> would need to do this with a class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Depends, FastAPI, Query\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Pagination:\n",
    "    def __init__(self, maximum_limit:int = 100):\n",
    "        self.maximum_limit = maximum_limit\n",
    "    async def __call__(\n",
    "            self,\n",
    "            skip: int = Query(0, ge=0),\n",
    "            limit: int = Query(10,ge=0)\n",
    "    ) -> tuple[int,int]:\n",
    "        capped_limit = min(self.maximum_limit, limit)\n",
    "        return (skip, capped_limit)\n",
    "# hardcoded below, but could come from config file or env variable\n",
    "pagination = Pagination(maximum_limit=50)\n",
    "\n",
    "@app.get(\"/items\")\n",
    "async def list_items(p: tuple[int, int] = Depends(pagination)):\n",
    "    skip, limit = p\n",
    "    return {\"skip\": skip, \"limit\": limit}\n",
    "\n",
    "\n",
    "@app.get(\"/things\")\n",
    "async def list_things(p: tuple[int, int] = Depends(pagination)):\n",
    "    skip, limit = p\n",
    "    return {\"skip\": skip, \"limit\": limit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: n FastAPI, Query is used to define and validate query parameters for your API endpoints. Query parameters are the key-value pairs that appear after the ? in a URL. They are typically used to filter, sort, or paginate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Depends` simply expects a callable: in can be `__call__` or another function as below.  Note that the pattern below could be used to apply different preprocessing steps, depending on the data, in the ML context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import Depends, FastAPI, Query\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Pagination:\n",
    "    def __init__(self, maximum_limit: int = 100):\n",
    "        self.maximum_limit = maximum_limit\n",
    "\n",
    "    async def skip_limit(\n",
    "        self,\n",
    "        skip: int = Query(0, ge=0),\n",
    "        limit: int = Query(10, ge=0),\n",
    "    ) -> tuple[int, int]:\n",
    "        capped_limit = min(self.maximum_limit, limit)\n",
    "        return (skip, capped_limit)\n",
    "\n",
    "    async def page_size(\n",
    "        self,\n",
    "        page: int = Query(1, ge=1),\n",
    "        size: int = Query(10, ge=0),\n",
    "    ) -> tuple[int, int]:\n",
    "        capped_size = min(self.maximum_limit, size)\n",
    "        return (page, capped_size)\n",
    "\n",
    "\n",
    "pagination = Pagination(maximum_limit=50)\n",
    "\n",
    "\n",
    "@app.get(\"/items\")\n",
    "async def list_items(p: tuple[int, int] = Depends(pagination.skip_limit)):\n",
    "    skip, limit = p\n",
    "    return {\"skip\": skip, \"limit\": limit}\n",
    "\n",
    "\n",
    "@app.get(\"/things\")\n",
    "async def list_things(p: tuple[int, int] = Depends(pagination.page_size)):\n",
    "    page, size = p\n",
    "    return {\"page\": page, \"size\": size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using dependency injection to manage db connection, ensuring that each request gets a fresh connection and that connections are properly closed after use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Depends\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "DATABASE_URL = \"sqlite:///./test.db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define a User model\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, index=True)\n",
    "    email = Column(String, unique=True, index=True)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def get_db():\n",
    "    db = SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "# Endpoint to create a new user\n",
    "@app.post(\"/users/\")\n",
    "async def create_user(name: str, email: str, db: Session = Depends(get_db)):\n",
    "    user = User(name=name, email=email)\n",
    "    db.add(user)\n",
    "    db.commit()\n",
    "    db.refresh(user)\n",
    "    return user\n",
    "\n",
    "@app.get(\"/users/\")\n",
    "async def read_users(db: Session=Depends(get_db)):\n",
    "    users = db.query(User).all()\n",
    "    return users\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example in LLM context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Depends\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        # Load the pre-trained model and tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self, text: str, preprocess_steps: list):\n",
    "        # Preprocess the text\n",
    "        text = self.text_processor.preprocess(text, preprocess_steps)\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for k, v in inputs.items()}\n",
    "        # Perform prediction using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Get the predicted class\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        return predicted_class\n",
    "\n",
    "# Create a global LLM instance\n",
    "llm_instance = LLM()\n",
    "\n",
    "# Dependency to get the global LLM instance\n",
    "def get_llm():\n",
    "    return llm_instance\n",
    "\n",
    "# Request model for prediction\n",
    "class PredictionRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# Response model for prediction\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "\n",
    "# Use the LLM dependency in an endpoint\n",
    "@app.post(\"/predict/\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest, llm: LLM = Depends(get_llm)):\n",
    "    prediction = llm.predict(request.text)\n",
    "    return {\"prediction\": prediction}\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run as follows: `curl -X POST \"http://127.0.0.1:8000/predict/\" -H \"Content-Type: application/json\" -d '{\"text\": \"I love FastAPI!\"}'`\n",
    "- Will return something like the following: `{\n",
    "    \"prediction\": 1\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### async/await syntax for LLM calls that are I/O-bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "class OpenAIRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "class HuggingFaceRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "class OpenAIResponse(BaseModel):\n",
    "    id: str\n",
    "    object: str\n",
    "    created: int\n",
    "    model: str\n",
    "    choices: list\n",
    "\n",
    "class HuggingFaceResponse(BaseModel):\n",
    "    generated_text: str\n",
    "\n",
    "@app.post(\"/openai-generate/\", response_model=OpenAIResponse) #respone should conform to the given Pydantic model\n",
    "async def openai_generate(request: OpenAIRequest):\n",
    "    url = \"https://api.openai.com/v1/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"text-davinci-003\",\n",
    "        \"prompt\": request.prompt,\n",
    "        \"max_tokens\": 100,\n",
    "    }\n",
    "    async with httpx.AsyncClient() as client: #always async with context managers\n",
    "        try:\n",
    "            response = await client.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raises an error for 4xx/5xx responses\n",
    "            return response.json()\n",
    "        except httpx.HTTPStatusError as exc:\n",
    "            raise HTTPException(status_code=exc.response.status_code, detail=exc.response.text)\n",
    "        \n",
    "@app.post(\"/huggingface-generate\", response_model=HuggingFaceResponse)\n",
    "async def huggingface_generate(request: HuggingFaceRequest):\n",
    "    url = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"inputs\": request.prompt,\n",
    "        \"options\": {\"use_cache\": False},\n",
    "    }    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.post(url, headers = headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            return {\"generated_text\": response.json()[0]['generated_text']}\n",
    "        except httpx.HTTPStatusError as exc:\n",
    "            raise HTTPException(status_code=exc.response.status_code, detail=exc.response.text)\n",
    "# To run the app: uvicorn your_file_name:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "\n",
    "class LLMError(Exception):\n",
    "    def __init__(self, message: str):\n",
    "        self.message = message\n",
    "\n",
    "@app.exception_handler(LLMError)\n",
    "async def llm_exception_handler(request: Request, exc: LLMError):\n",
    "    return JSONResponse(\n",
    "        status_code=500,\n",
    "        content={\"message\":f\"LLM error occurred: {exc.message}\"}\n",
    "    )\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    generated_text: str\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerateResponse)\n",
    "async def generate(request: GenerateRequest):\n",
    "    if not request.prompt:\n",
    "        raise HTTPException(status_code=400, detail=\"Prompt cannot be empty!\")\n",
    "    url = \"https://api.llm.example/generate\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LLM_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\"prompt\": request.prompt}\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raise an error for 4xx/5xx responses\n",
    "            response_data = response.json()\n",
    "            return {\"generated_text\": response_data.get(\"generated_text\", \"\")}\n",
    "        except httpx.HTTPStatusError as exc:\n",
    "            raise LLMError(message = exc.response.text)\n",
    "        except Exception as exc:\n",
    "            raise LLMError(message=str(exc))\n",
    "\n",
    "# To run the app: uvicorn your_file_name:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving static files (like HTML, CSS, JS)  with FastAPI: Jinja2+JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the following file structure\n",
    "# /my_fastapi_app\n",
    "# ├── app.py               # Your FastAPI application\n",
    "# ├── static               # Directory for static files\n",
    "# │   ├── css\n",
    "# │   │   └── styles.css\n",
    "# │   ├── js\n",
    "# │   │   └── script.js\n",
    "# │   └── index.html\n",
    "# └── requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{{ title }}</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/css/styles.css\">\n",
    "</head>\n",
    "<body>\n",
    "    <h1>{{ title }}</h1>\n",
    "    <p>{{ description }}</p>\n",
    "    <form id=\"generate-form\">\n",
    "        <label for=\"prompt\">Enter your prompt:</label>\n",
    "        <input type=\"text\" id=\"prompt\" name=\"prompt\" required>\n",
    "        <button type=\"submit\">Generate</button>\n",
    "    </form>\n",
    "    <div id=\"result\"></div>\n",
    "    <script src=\"/static/js/scripts.js\"></script>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.js\n",
    "document.getElementById('generate-form').addEventListener('submit', async (event) => {\n",
    "    event.preventDefault();\n",
    "    const prompt = document.getElementById('prompt').value;\n",
    "    const response = await fetch('/generate', {\n",
    "        method: 'POST',\n",
    "        headers: {\n",
    "            'Content-Type': 'application/json'\n",
    "        },\n",
    "        body: JSON.stringify({ prompt })\n",
    "    });\n",
    "    const data = await response.json();\n",
    "    document.getElementById('result').innerText = data.generated_text;\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "#mount the static directory\n",
    "app.mount(\"/static\",StaticFiles(directory=\"static\"),name='static')\n",
    "\n",
    "# @app.get(\"/\",response_class=HTMLResponse)\n",
    "# async def read_index():\n",
    "#     with open(\"static/index.html\") as f:\n",
    "#         return f.read()\n",
    "    \n",
    "# Or better, serve HTML templates more dynamically:\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    context = {\n",
    "        \"request\": request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a large language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "# To run the app: uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you navigate to http://127.0.0.1:8000/, it will serve your index.html file.\n",
    "- `app.mount`: specify URL path+directory for static files, allowing FastAPI to handle requests to those files automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving static files (like HTML, CSS, JS)  with FastAPI: Jinja2+HTMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mount the static directory\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Serve HTML templates\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "@app.get('/', response_class=HTMLResponse)\n",
    "async def read_root(request:Request):\n",
    "    context = {\n",
    "        \"request\":request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "@app.post(\"/generate\",response_class=HTMLResponse)\n",
    "async def generate_text(request: Request, prompt: str = Form(...))\n",
    "    if not prompt:\n",
    "        return HTMLResponse(\"<div id='result'><p>Error: Error: Prompt cannot be empty!</p></div>\")\n",
    "        url = \"https://api.llm.example/generate\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('LLM_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    data = {\"prompt\": prompt}\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()  # Raise an error for 4xx/5xx responses\n",
    "            response_data = response.json()\n",
    "            generated_text = response_data.get(\"generated_text\", \"\")\n",
    "            return HTMLResponse(f\"<div id='result'><h2>Generated Text:</h2><p>{generated_text}</p></div>\") # update the #result endpoint\n",
    "        except httpx.HTTPStatusError as exc:\n",
    "            return HTMLResponse(f\"<div id='result'><p>Error: {exc.response.text}</p></div>\")\n",
    "        except Exception as exc:\n",
    "            return HTMLResponse(f\"<div id='result'><p>Error: {str(exc)}</p></div>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No longer any JavaScript found in the directory, all taken care of with HTMX as follows:\n",
    "\n",
    "    - HTML Template with HTMX:\n",
    "        - The index.html template includes the HTMX library by adding a `<script>` tag that loads HTMX from a CDN.\n",
    "        - The form uses HTMX attributes (hx-post, hx-target, hx-swap) to handle form submission and update the result dynamically:\n",
    "        - hx-post=\"/generate\": Sends a POST request to the /generate endpoint when the form is submitted.\n",
    "        - hx-target=\"#result\": Specifies the element (#result) to update with the server's response.\n",
    "        - hx-swap=\"innerHTML\": Replaces the inner HTML of the target element with the server's response.\n",
    "        \n",
    "    - FastAPI Endpoint:\n",
    "        - The /generate endpoint processes the form submission, interacts with the LLM API, and returns an HTML response with the generated text or an error message.\n",
    "        - The response is an HTMLResponse that updates the #result element in the HTML template.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .\n",
    "# ├── main.py\n",
    "# ├── static\n",
    "# │   ├── css\n",
    "# │   │   └── styles.css\n",
    "# └── templates\n",
    "#     └── index.html\n",
    "# templates/index.htmx\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>{{ title }}</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/css/styles.css\">\n",
    "    <script src=\"https://unpkg.com/htmx.org@1.6.1\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>{{ title }}</h1>\n",
    "    <p>{{ description }}</p>\n",
    "    <form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\">\n",
    "        <label for=\"prompt\">Enter your prompt:</label>\n",
    "        <input type=\"text\" id=\"prompt\" name=\"prompt\" required>\n",
    "        <button type=\"submit\">Generate</button>\n",
    "    </form>\n",
    "    <div id=\"result\"></div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging with Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".\n",
    "├── Dockerfile\n",
    "├── main.py\n",
    "├── requirements.txt\n",
    "├── static\n",
    "│   ├── css\n",
    "│   │   └── styles.css\n",
    "└── templates\n",
    "    └── index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- templates/index.html remains the same file compatible with HTMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "fastapi\n",
    "uvicorn\n",
    "torch==1.9.0+cu111  # Ensure this matches the CUDA version in the Docker image\n",
    "transformers\n",
    "python-dotenv\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For GPU support...\n",
    "\n",
    "Install NVIDIA Docker: Install the NVIDIA Docker runtime on your host machine.\n",
    "\n",
    "Modify the Dockerfile: Use a base image that includes CUDA and cuDNN libraries.\n",
    "\n",
    "Update the Docker Run Command: Use the --gpus flag to allocate GPU resources to the container.\n",
    "\n",
    "Ensure PyTorch is Installed with CUDA Support: Make sure the PyTorch version installed in the container supports CUDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the official NVIDIA CUDA runtime as a parent image\n",
    "FROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Expose port 80 to the outside world\n",
    "EXPOSE 80\n",
    "\n",
    "# Run the FastAPI application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mount the static directory\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Serve HTML templates\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Load the model and tokenizer from a shared storage location or model server\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    context = {\n",
    "        \"request\": request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a large language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "@app.post(\"/generate\", response_class=HTMLResponse)\n",
    "async def generate_text(request: Request, prompt: str = Form(...)):\n",
    "    if not prompt:\n",
    "        return HTMLResponse(\"<div id='result'><p>Error: Prompt cannot be empty!</p></div>\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return HTMLResponse(f\"<div id='result'><h2>Generated Text:</h2><p>{generated_text}</p></div>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% bash\n",
    "docker build -t my-fastapi-app .\n",
    "docker run --gpus all --env-file .env -d -p 80:80 --name my-fastapi-container my-fastapi-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying on GCP\n",
    "Step 1: Set Up GCP Project: Create a GCP project and enable necessary APIs.\n",
    "\n",
    "Step 2: Install and Configure Google Cloud SDK: Install the Google Cloud SDK and authenticate.\n",
    "\n",
    "Step 3:Build and Push Docker Image to GCR: Build the Docker image and push it to Google Container Registry.\n",
    "\n",
    "Step 4:Create a GKE Cluster: Create a Kubernetes cluster on GKE.\n",
    "\n",
    "Step 5:Deploy the Application on GKE: Deploy the FastAPI application on the GKE cluster.\n",
    "\n",
    "Step 6:Set Up Google Cloud Storage: Store the model in GCS and access it from the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Set Up GCP Project: Create a GCP project and enable necessary APIs.\n",
    "Create a GCP Project: \n",
    "Go to the Google Cloud Console.\n",
    "\n",
    "Create a new project.\n",
    "\n",
    "Enable the following APIs: Kubernetes Engine API, Container Registry API, Cloud Storage API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Install and Configure Google Cloud SDK\n",
    "Install Google Cloud SDK: https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "Authenticate with GCP: `gcloud init; gcloud auth login`\n",
    "\n",
    "Set the Project: `gcloud config set project YOUR_PROJECT_ID`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build and Push Docker Image to GCR\n",
    "`docker build -t gcr.io/YOUR_PROJECT_ID/my-fastapi-app .`\n",
    "\n",
    "`docker push gcr.io/YOUR_PROJECT_ID/my-fastapi-app`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4:Create a GKE Cluster.\n",
    "`gcloud container clusters create my-cluster --num-nodes=3`\n",
    "\n",
    "`gcloud container clusters get-credentials my-cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:Deploy the Application on GKE: Deploy the FastAPI application on the GKE cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Kubernetes Deployment `deployment.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-fastapi-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-fastapi-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-fastapi-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: my-fastapi-app\n",
    "        image: gcr.io/YOUR_PROJECT_ID/my-fastapi-app\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        env:\n",
    "        - name: MODEL_NAME\n",
    "          value: \"gs://YOUR_BUCKET_NAME/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Kubernetes Service `service.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: my-fastapi-app\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: my-fastapi-app\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploy to GKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl apply -f deployment.yaml`\n",
    "\n",
    "`kubectl apply -f service.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Set up Google Cloud Storage, upload model to bucket, update main.py to load model from GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gsutil mb gs://YOUR_BUCKET_NAME`\n",
    "\n",
    "`gsutil cp model/* gs://YOUR_BUCKET_NAME/model/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, Form\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mount the static directory\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Serve HTML templates\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Load the model and tokenizer from GCS\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "model_path = \"/app/model\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "download_blob(\"YOUR_BUCKET_NAME\", \"model/pytorch_model.bin\", f\"{model_path}/pytorch_model.bin\")\n",
    "download_blob(\"YOUR_BUCKET_NAME\", \"model/config.json\", f\"{model_path}/config.json\")\n",
    "download_blob(\"YOUR_BUCKET_NAME\", \"model/tokenizer_config.json\", f\"{model_path}/tokenizer_config.json\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    context = {\n",
    "        \"request\": request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a large language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "@app.post(\"/generate\", response_class=HTMLResponse)\n",
    "async def generate_text(request: Request, prompt: str = Form(...)):\n",
    "    if not prompt:\n",
    "        return HTMLResponse(\"<div id='result'><p>Error: Prompt cannot be empty!</p></div>\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return HTMLResponse(f\"<div id='result'><h2>Generated Text:</h2><p>{generated_text}</p></div>\")\n",
    "\n",
    "# To run the app: uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and making predictions with FastAPI app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, Form\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mount the static directory\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Serve HTML templates\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    context = {\n",
    "        \"request\": request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a large language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "@app.post(\"/generate\", response_class=HTMLResponse)\n",
    "async def generate_text(request: Request, prompt: str = Form(...)):\n",
    "    if not prompt:\n",
    "        return HTMLResponse(\"<div id='result'><p>Error: Prompt cannot be empty!</p></div>\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return HTMLResponse(f\"<div id='result'><h2>Generated Text:</h2><p>{generated_text}</p></div>\")\n",
    "\n",
    "@app.post(\"/api/generate\", response_class=JSONResponse)\n",
    "async def api_generate_text(prompt: str):\n",
    "    if not prompt:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Prompt cannot be empty!\"})\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return JSONResponse(content={\"generated_text\": generated_text})\n",
    "\n",
    "# To run the app: uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>LLM Text Generator</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/css/styles.css\">\n",
    "    <script src=\"https://unpkg.com/htmx.org@1.6.1\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>LLM Text Generator</h1>\n",
    "    <p>Generate text using a large language model.</p>\n",
    "    <form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\">\n",
    "        <label for=\"prompt\">Enter your prompt:</label>\n",
    "        <input type=\"text\" id=\"prompt\" name=\"prompt\" required>\n",
    "        <button type=\"submit\">Generate</button>\n",
    "    </form>\n",
    "    <div id=\"result\"></div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the user's perspective, the interaction with the FastAPI application will be straightforward and intuitive. Here's how each endpoint will be experienced by the user:\n",
    "\n",
    "#### User Experience\n",
    "\n",
    "1. **Accessing the Application (`/` Endpoint)**:\n",
    "   - When the user navigates to the root URL (`http://127.0.0.1:8000/`), they will see an HTML page with a form where they can input a prompt.\n",
    "   - The form will have a text input field for the prompt and a submit button.\n",
    "\n",
    "2. **Submitting the Form (`/generate` Endpoint)**:\n",
    "   - When the user types a prompt into the form and clicks the submit button, the form data is sent to the `/generate` endpoint.\n",
    "   - The `/generate` endpoint processes the form submission, queries the LLM, and returns the generated text as part of the HTML response.\n",
    "   - The user will see the generated text displayed on the same page below the form.\n",
    "\n",
    "3. **API Interaction (`/api/generate` Endpoint)**:\n",
    "   - The `/api/generate` endpoint is designed for programmatic access, such as from a frontend application or another service.\n",
    "   - Users or developers can send a JSON request to this endpoint with the prompt, and it will return the generated text as a JSON response.\n",
    "   - This endpoint is useful for integrating the LLM functionality into other applications or services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "@pytest.fixture\n",
    "def model_and_tokenizer():\n",
    "    model_name = os.getenv(\"MODEL_NAME\", \"gpt2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model_loading(model_and_tokenizer):\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    assert model is not None, \"Model should be loaded\"\n",
    "    assert tokenizer is not None, \"Tokenizer should be loaded\"\n",
    "    assert model.device.type in [\"cuda\", \"cpu\"], \"Model should be on CUDA or CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "from main import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_read_root():\n",
    "    response = client.get(\"/\")\n",
    "    assert response.status_code == 200\n",
    "    assert \"LLM Text Generator\" in response.text\n",
    "\n",
    "def test_generate_text():\n",
    "    response = client.post(\"/generate\", data={\"prompt\": \"Hello, world!\"})\n",
    "    assert response.status_code == 200\n",
    "    assert \"Generated Text:\" in response.text\n",
    "\n",
    "def test_api_generate_text():\n",
    "    response = client.post(\"/api/generate\", json={\"prompt\": \"Hello, world!\"})\n",
    "    assert response.status_code == 200\n",
    "    json_response = response.json()\n",
    "    assert \"generated_text\" in json_response\n",
    "    assert isinstance(json_response[\"generated_text\"], str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a database (Postresql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "fastapi\n",
    "uvicorn\n",
    "torch\n",
    "transformers\n",
    "python-dotenv\n",
    "passlib[bcrypt]\n",
    "pyjwt\n",
    "sqlalchemy\n",
    "databases\n",
    "asyncpg\n",
    "psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "from sqlalchemy import Column, Integer, String, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    username = Column(String, unique=True, index=True)\n",
    "    full_name = Column(String)\n",
    "    email = Column(String, unique=True, index=True)\n",
    "    hashed_password = Column(String)\n",
    "    disabled = Column(Boolean, default=False)\n",
    "\n",
    "\n",
    "class UserInDB(User):\n",
    "    hashed_password: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database.py\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from .models import Base\n",
    "from passlib.context import CryptContext\n",
    "\n",
    "pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "\n",
    "DATABASE_URL = \"postgresql+asyncpg://user:password@localhost/dbname\"\n",
    "\n",
    "engine = create_async_engine(DATABASE_URL, echo=True)  # echo to see SQL in terminal\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine, class_=AsyncSession)\n",
    "\n",
    "async def init_db():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "# Dependency to get the database session\n",
    "async def get_db():\n",
    "    async with SessionLocal() as session:\n",
    "        yield session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crud.py\n",
    "from sqlalchemy.orm import Session\n",
    "from .models import User\n",
    "from .schemas import UserCreate, UserUpdate\n",
    "from .auth import get_password_hash\n",
    "\n",
    "def get_user(db: Session, user_id: int):\n",
    "    return db.query(User).filter(User.id == user_id).first()\n",
    "\n",
    "def get_user_by_username(db: Session, username: str):\n",
    "    return db.query(User).filter(User.username == username).first()\n",
    "\n",
    "def create_user(db: Session, user: UserCreate):\n",
    "    hashed_password = get_password_hash(user.password)\n",
    "    db_user = User(\n",
    "        username=user.username,\n",
    "        full_name=user.full_name,\n",
    "        email=user.email,\n",
    "        hashed_password=hashed_password,\n",
    "        disabled=user.disabled,\n",
    "    )\n",
    "    db.add(db_user)\n",
    "    db.commit()\n",
    "    db.refresh(db_user)\n",
    "    return db_user\n",
    "\n",
    "def update_user(db: Session, user_id: int, user: UserUpdate):\n",
    "    db_user = get_user(db, user_id)\n",
    "    if db_user:\n",
    "        db_user.username = user.username\n",
    "        db_user.full_name = user.full_name\n",
    "        db_user.email = user.email\n",
    "        if user.password:\n",
    "            db_user.hashed_password = get_password_hash(user.password)\n",
    "        db_user.disabled = user.disabled\n",
    "        db.commit()\n",
    "        db.refresh(db_user)\n",
    "    return db_user\n",
    "\n",
    "def delete_user(db: Session, user_id: int):\n",
    "    db_user = get_user(db, user_id)\n",
    "    if db_user:\n",
    "        db.delete(db_user)\n",
    "        db.commit()\n",
    "    return db_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth.py\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "from fastapi import Depends, HTTPException, status\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n",
    "from jose import JWTError, jwt\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.future import select\n",
    "from .models import User\n",
    "from .database import get_db, pwd_context\n",
    "\n",
    "# Secret key to encode and decode JWT tokens\n",
    "SECRET_KEY = \"your_secret_key\"\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
    "\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n",
    "\n",
    "def verify_password(plain_password: str, hashed_password: str) -> bool:\n",
    "    return pwd_context.verify(plain_password, hashed_password)\n",
    "\n",
    "def get_password_hash(password: str) -> str:\n",
    "    return pwd_context.hash(password)\n",
    "\n",
    "async def authenticate_user(db: AsyncSession, username: str, password: str):\n",
    "    result = await db.execute(select(User).filter(User.username == username))\n",
    "    user = result.scalars().first()\n",
    "    if not user:\n",
    "        return False\n",
    "    if not verify_password(password, user.hashed_password):\n",
    "        return False\n",
    "    return user\n",
    "\n",
    "def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n",
    "    to_encode = data.copy()\n",
    "    if expires_delta:\n",
    "        expire = datetime.now() + expires_delta\n",
    "    else:\n",
    "        expire = datetime.now() + timedelta(minutes=15)\n",
    "    to_encode.update({\"exp\": expire})\n",
    "    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
    "    return encoded_jwt\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme), db: AsyncSession = Depends(get_db)):\n",
    "    credentials_exception = HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Could not validate credentials\",\n",
    "        headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "    )\n",
    "    try:\n",
    "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        username: str = payload.get(\"sub\")\n",
    "        if username is None:\n",
    "            raise credentials_exception\n",
    "    except JWTError:\n",
    "        raise credentials_exception\n",
    "    result = await db.execute(select(User).filter(User.username == username))\n",
    "    user = result.scalars().first()\n",
    "    if user is None:\n",
    "        raise credentials_exception\n",
    "    return user\n",
    "\n",
    "async def get_current_active_user(current_user: User = Depends(get_current_user)):\n",
    "    if current_user.disabled:\n",
    "        raise HTTPException(status_code=400, detail=\"Inactive user\")\n",
    "    return current_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schemas.py\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "class TokenData(BaseModel):\n",
    "    username: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from fastapi import FastAPI, Depends, HTTPException, Request, Form\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.security import OAuth2PasswordRequestForm\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from .auth import authenticate_user, create_access_token, get_current_active_user\n",
    "from .models import User\n",
    "from .database import get_db, init_db\n",
    "from .crud import get_user_by_username, create_user\n",
    "from .schemas import UserCreate, Token\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup event\n",
    "    await init_db()\n",
    "    yield\n",
    "    # Shutdown event\n",
    "    # Perform any necessary cleanup here\n",
    "# Mount the static directory\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n",
    "\n",
    "# Serve HTML templates\n",
    "templates = Jinja2Templates(directory=\"templates\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "@app.post(\"/token\", response_model=Token)\n",
    "async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: AsyncSession = Depends(get_db)):\n",
    "    user = await authenticate_user(db, form_data.username, form_data.password)\n",
    "    if not user:\n",
    "        raise HTTPException(\n",
    "            status_code=401,\n",
    "            detail=\"Incorrect username or password\",\n",
    "            headers={\"WWW-Authenticate\": \"Bearer\"},\n",
    "        )\n",
    "    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    access_token = create_access_token(\n",
    "        data={\"sub\": user.username}, expires_delta=access_token_expires\n",
    "    )\n",
    "    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n",
    "\n",
    "@app.post(\"/users/\", response_model=User)\n",
    "async def create_new_user(user: UserCreate, db: AsyncSession = Depends(get_db)):\n",
    "    db_user = await get_user_by_username(db, user.username)\n",
    "    if db_user:\n",
    "        raise HTTPException(status_code=400, detail=\"Username already registered\")\n",
    "    return await create_user(db, user)\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    context = {\n",
    "        \"request\": request,\n",
    "        \"title\": \"LLM Text Generator\",\n",
    "        \"description\": \"Generate text using a large language model.\"\n",
    "    }\n",
    "    return templates.TemplateResponse(\"index.html\", context)\n",
    "\n",
    "@app.post(\"/generate\", response_class=HTMLResponse)\n",
    "async def generate_text(request: Request, prompt: str = Form(...), current_user: User = Depends(get_current_active_user)):\n",
    "    if not prompt:\n",
    "        return HTMLResponse(\"<div id='result'><p>Error: Prompt cannot be empty!</p></div>\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return HTMLResponse(f\"<div id='result'><h2>Generated Text:</h2><p>{generated_text}</p></div>\")\n",
    "\n",
    "@app.post(\"/api/generate\", response_class=JSONResponse)\n",
    "async def api_generate_text(prompt: str, current_user: User = Depends(get_current_active_user)):\n",
    "    if not prompt:\n",
    "        return JSONResponse(status_code=400, content={\"error\": \"Prompt cannot be empty!\"})\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return JSONResponse(content={\"generated_text\": generated_text})\n",
    "\n",
    "# To run the app: uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>LLM Text Generator</title>\n",
    "    <link rel=\"stylesheet\" href=\"/static/css/styles.css\">\n",
    "    <script src=\"https://unpkg.com/htmx.org@1.6.1\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>LLM Text Generator</h1>\n",
    "    <p>Generate text using a large language model.</p>\n",
    "\n",
    "    <!-- User Registration Form -->\n",
    "    <div id=\"registration\">\n",
    "        <h2>Register</h2>\n",
    "        <form hx-post=\"/users/\" hx-target=\"#registration-result\" hx-swap=\"innerHTML\">\n",
    "            <label for=\"username\">Username:</label>\n",
    "            <input type=\"text\" id=\"username\" name=\"username\" required>\n",
    "            <label for=\"full_name\">Full Name:</label>\n",
    "            <input type=\"text\" id=\"full_name\" name=\"full_name\">\n",
    "            <label for=\"email\">Email:</label>\n",
    "            <input type=\"email\" id=\"email\" name=\"email\" required>\n",
    "            <label for=\"password\">Password:</label>\n",
    "            <input type=\"password\" id=\"password\" name=\"password\" required>\n",
    "            <button type=\"submit\">Register</button>\n",
    "        </form>\n",
    "        <div id=\"registration-result\"></div>\n",
    "    </div>\n",
    "\n",
    "    <!-- User Login Form -->\n",
    "    <div id=\"login\">\n",
    "        <h2>Login</h2>\n",
    "        <form hx-post=\"/token\" hx-target=\"#login-result\" hx-swap=\"innerHTML\">\n",
    "            <label for=\"login-username\">Username:</label>\n",
    "            <input type=\"text\" id=\"login-username\" name=\"username\" required>\n",
    "            <label for=\"login-password\">Password:</label>\n",
    "            <input type=\"password\" id=\"login-password\" name=\"password\" required>\n",
    "            <button type=\"submit\">Login</button>\n",
    "        </form>\n",
    "        <div id=\"login-result\"></div>\n",
    "    </div>\n",
    "\n",
    "    <!-- Text Generation Form -->\n",
    "    <div id=\"text-generation\">\n",
    "        <h2>Generate Text</h2>\n",
    "        <form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\">\n",
    "            <label for=\"prompt\">Enter your prompt:</label>\n",
    "            <input type=\"text\" id=\"prompt\" name=\"prompt\" required>\n",
    "            <button type=\"submit\">Generate</button>\n",
    "        </form>\n",
    "        <div id=\"result\"></div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above In progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintonano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
