{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: LLMs/PyTorch Basics and Recipes\n",
    "description: Key things to be able to do with LLMs (PyTorch/HF/Llamaindex)\n",
    "date: 2024-09\n",
    "categories: [PyTorch]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A custom Dataset class must have `__init__()`,`__len__()`, and `__getitem__()` methods to be used by the data loader.\n",
    "- Source: Machine Learning with PyTorch and Scikit-Learn by Raschka et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "t_x = torch.rand([4,3],dtype = torch.float32)\n",
    "t_y = torch.arange(4)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "joint_dataset = JointDataset(t_x,t_y)\n",
    "# alternatively, can create a joint dataset using TensorDataset\n",
    "# from torch.utils.data import TensorDataset\n",
    "# tensor_dataset = JointDataset(t_x,t_y)\n",
    "\n",
    "data_loader = DataLoader(dataset=joint_dataset,batch_size=2,shuffle=True) # will shuffle for every epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "seq_len = 40\n",
    "chunk_size = seq_len +1\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "        return text_chunks[:-1].long(), text_chunks[1:].long()\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))\n",
    "seq_dl = DataLoader(seq_dataset,batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing custom layers in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,input_size,output_size,noise_stddev=0.1):\n",
    "        super().__init__()\n",
    "        w = torch.Tensor(input_size, output_size)\n",
    "        self.w = nn.Parameter(w) # will be included in model.parameters() passed to the optimizer\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        b = torch.Tensor(output_size).fill_(0)\n",
    "        self.b = nn.Parameter(b)\n",
    "        self.noise_stddev = noise_stddev\n",
    "    def forward(self,x,training=False):\n",
    "        if training:\n",
    "            noise = torch.normal(0.0, self.noise_stddev, x.shape)\n",
    "            x_new = torch.add(x,noise)\n",
    "        else:\n",
    "            x_new = x\n",
    "        return torch.add(torch.mm(x_new,self.w),self.b)\n",
    "class NoisyModule(nn.Module):\n",
    "    def __init_(self):\n",
    "        super().__init__()\n",
    "        self.l1 = NoisyLinear(2,4,0.07)\n",
    "        self.a1 = nn.ReLU()\n",
    "        ...\n",
    "    def forward(self,x,training=False): \n",
    "        x = self.l1(x,training)\n",
    "        x = self.a1(x)\n",
    "        ...\n",
    "    def predict(self,x):\n",
    "        x = torch.tensor(x,dtype=torch.float32)\n",
    "        pred = self.forward(x)[:,0] # tra\n",
    "        return (pred>=0.5).float()\n",
    "# inside the training loop, use training = True\n",
    "...\n",
    "pred = model(x_batch,training=True)[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, PATH_TO_MODEL)\n",
    "model = torch.load(PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not recommended because not only does this save model parameters, it also saves model classes and directory structure of the source code.  If class signatures or directory structures change, may not be able to load the model in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Save the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),PATH_TO_MODEL)\n",
    "model = ConvNet()\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a basic model server and packaging with Docker (from Mastering PyTorch, Second Edition by Jha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inference pipeline (a) the data preprocessing component, (b) the model inference, and (c) the post-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server.py\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from flask import Flask, request\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op\n",
    "    \n",
    "model = ConvNet()\n",
    "PATH_TO_MODEL = \"./convnet.pth\"\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "def run_model(input_tensor):\n",
    "    model_input = input_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(model_input)[0]\n",
    "    model_prediction = model_output.detach().numpy().argmax()\n",
    "    return model_prediction\n",
    "\n",
    "def post_process(output):\n",
    "    return str(output)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/test\", methods=[\"POST\"])\n",
    "def test():\n",
    "    # 1. Preprocess\n",
    "    data = request.files['data'].read()\n",
    "    md = json.load(request.files['metadata'])\n",
    "    input_array = np.frombuffer(data, dtype=np.float32)\n",
    "    input_image_tensor = torch.from_numpy(input_array).view(md[\"dims\"])\n",
    "    # 2. Inference\n",
    "    output = run_model(input_image_tensor)\n",
    "    # 3. Postprocess\n",
    "    final_output = post_process(output)\n",
    "    return final_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a request as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_request.py\n",
    "import io\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "image = Image.open(\"./digit_image.jpg\")\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    gray_image = transforms.functional.to_grayscale(image)\n",
    "    resized_image = transforms.functional.resize(gray_image, (28, 28))\n",
    "    input_image_tensor = transforms.functional.to_tensor(resized_image)\n",
    "    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n",
    "    return input_image_tensor_norm\n",
    "\n",
    "image_tensor = image_to_tensor(image)\n",
    "\n",
    "dimensions = io.StringIO(json.dumps({'dims': list(image_tensor.shape)}))\n",
    "data = io.BytesIO(bytearray(image_tensor.numpy()))\n",
    "\n",
    "r = requests.post('http://localhost:8890/test',\n",
    "                  files={'metadata': dimensions, 'data' : data})\n",
    "\n",
    "response = json.loads(r.content)\n",
    "\n",
    "print(\"Predicted digit :\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Turn into a microservice with the following *Dockerfile*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.9-slim\n",
    "\n",
    "RUN apt-get -q update && apt-get -q install -y wget\n",
    "\n",
    "COPY ./server.py ./\n",
    "COPY ./requirements.txt ./\n",
    "\n",
    "RUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/convnet.pth\n",
    "RUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/digit_image.jpg\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "\n",
    "USER root\n",
    "ENTRYPOINT [\"python\", \"server.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build with digit_recognizer tag using current directory as context: `docker build -t digit_recognizer .`\n",
    "    - Note how server.py and requirements.txt are copied into Docker's directory.  Also USER root may give overly elevated privileges.\n",
    "- Forward the 8890 port on our machine to 8890 port on the container since app runs on port 8890: `docker run -p 8890:8890 digit_recognizer` \n",
    "- Python make_request.py will make a request to the Dockerized Flask model server and we'll get the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HF Chat Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Step 0: Prepare the data by finding the prompt and formatting a column ('text' for \n",
    "# regular SFT, 'messages' for instruction ft, etc), shuffling and splitting it ---\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_config,get_peft_model\n",
    "\n",
    "\n",
    "model_name = HF_MODEL_ID\n",
    "# ---Step 1: Initialize BitsAndBytesConfig and feed it to the model upon load---\n",
    "bnb_config = BitsAndBytesConfig( #Q in QLoRA\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# ---Step 2: Load the tokenizer---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# ---Step 3: Initialize LoraConfig and i.) peft.prepare_model_for_kbit_training and ii.) peft.get_peft_model---\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=128,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# ---Step 4: Define TrainingArguments, set up SFTTrainer, trainer.train()---\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs = 10.0,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "   # peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# NOTE: SFTTrainer will automatically send logs to wandb set up via\n",
    "#  import wandb; wandb.login(); %env WANDB_PROJECT=sql-fine-tuning\n",
    "\n",
    "# ---Step 5: Save QLoRA weights and merge---\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "# NOTE: In the future, can load this final merged model without knowing the QLoRA configurations\n",
    "\n",
    "\n",
    "# ---Step x: Can use the merged model to make predictions as follows---\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintonano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
