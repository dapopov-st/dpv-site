[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Dmitriy’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nSep 5, 2024\n\n\nBuilding the GPT-2 transformer in PyTorch\n\n\n\n\nAug 31, 2024\n\n\nHow to pass GCP PDE exam on the first attempt: Write up in progress\n\n\n\n\nMay 31, 2024\n\n\narXiv LLMs Assistant: Write up in progress\n\n\n\n\nOct 18, 2022\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\nAug 29, 2022\n\n\nInvesting in student housing\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Dmitriy’s Projects",
    "section": "",
    "text": "arXiv-llms-assistant\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\narXiv LLMs Assistant\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\nInvesting in student housing\n\n\n\n\n\n\n\n\nAug 29, 2022\n\n\n\n\n\n\n\nInventory classes with pytest and pdoc\n\n\n\n\n\n\n\n\nAug 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/indiv_projects/arxiv/arxiv-llm-assistant.html",
    "href": "projects/indiv_projects/arxiv/arxiv-llm-assistant.html",
    "title": "arXiv LLMs Assistant",
    "section": "",
    "text": "arXiv-llms-assistant: ‘Public’ version of a project submitted as part of an AI competition at LS Direct, demonstrating the use and evaluation of RAG with on-prem data as a way to capitalize on applications of gen AI to privacy-restricted domains.\n\nApplied exllamav2 6.0bpw quantization to run Mixtral-8x7B on personal GPUs with virtually no loss in quality and evaluate RAG pipeline configurations to find the best performing one (evaluation score ~90%).\nBuilt an assistant to study the LLM domain, compare out-of-repository papers with papers in a personal repo (Zotero) and recommend recent papers to read along with lists of question/answer pairs.\nOrchestrated a multi-faceted evaluation of RAG setups, implementing feedback mechanisms that led to the selection of the most effective configuration; this process improved question relevance, elevating overall project outcomes.\nBuilt a command line utility to ask questions about the contents of uploaded papers and generate question/answer sets based on these papers."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "",
    "text": "from dataclasses import dataclass\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#overall-tranformer-structure",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#overall-tranformer-structure",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Overall tranformer structure",
    "text": "Overall tranformer structure\n\nWe’ll be implementing the right hand side (the decoder) that composes GPT-2: each unit on the right will be a block in our transformer.\n\n\n\n\nTransformer Architecture from “Attention Is All You Need” by Vaswani et al.\n\n\n\nThe configuration below is the configuration for the entire tranformer, with each layer h pertaining to one of the blocks. We want to replicate the following structure from a GPT-2 model in Huggingface Transformers:\n\n\n\n\nHF Transformer\n\n\n\nThe code below is the skeleton on GPT2 config and main module that will allow us to replicate that structure:\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 256\n    vocab_size: int = 65\n    n_layer: int = 6\n    n_head: int = 8\n    n_embd: int = 384\n\nclass GPT(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        # With nn.ModuleDict() index into submodules just like a dictionary\n        self.tranformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n\nnn.ModuleDict allows you to index into submodules using keys, just like a dictionary.\nnn.ModuleList allows us to index into each individual layer using an index, just like with a list"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#transformer-block",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#transformer-block",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Transformer Block",
    "text": "Transformer Block\n\nNow let’s implement the Block,…\nUnlike the original GPT2 paper, establish a clean residual pathway by taking the layer norm of x and applying attention/multilayer perceptron layer to it then adding it to the input x. Since addition allows for an adulterated gradient flow during backpropagation, this pre-layer norm configuration is the better than the post-layer norm configuration where the norm is applied after the addition. More formally, Xiong et al. (2020) have shown that if post-layer norm is used, a warm-up stage is needed to avoid training instability whereas if pre-layer norm is used, the gradients are well-behaved at initialization. See the difference between original (Post-LN) GPT-2 implementation and the ‘corrected’ pre-LN implementation used here:\n\n\n\n\nSource: “On Layer Normalization in the Transformer Architecture” by Xiong et al. 2020\n\n\n\nFinally, onto the Block:\n\n\nclass Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config    \n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNote again how the layer norm is applied before the addition to the residual stream.\nAndrej notes that attention is a communication operation, where tokens communicate with each other and aggregate information. Thus attention can be thought of as a pooling function/weighted sum function/reduce operation. On the other hand, the multilayer perceptron (MLP) is applied to each token individually, with no information exchanged between the tokens. Thus attention is a reduce and MLP is the map operation and a transformer is a repeated application of MapReduce."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#multilayer-perceptron",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#multilayer-perceptron",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\n\nBriefly summarizing from Andrej’s previous video (Let’s build GPT: from scratch, in code, spelled out.), multilayer perceptron is implemented using a standard “bottleneck architecture” where the dimensions are first expanded to learn more complex representations, nonlinearity is applied to help the model learn more complex patterns, and finally the data is projected down again to keep the computational complexity in check.\n\n\nclass MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x    \n\n\nGPT-2 used an approximate version of GeLU because at the time of GPT-2’s creation, the erf function was very slow in TensorFlow and GPT-2 and the approximate version was used. Today there’s no reason to use the approximate version but Andrej is using the tanh approximation for veracity.\n\nAlso, GeLU is better than ReLU due to dead neuron problem since a local gradient is always present as seen below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#causal-self-attention",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#causal-self-attention",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Causal Self Attention",
    "text": "Causal Self Attention\n\nAndrej’s attention implementation is a more efficient implementation of the following simple one from “Lets build GPT: from scratch, in code, spelled out”:\n\n\n\n\nSource: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n\n\n\nNotice how in the implemenation above it is clear that the heads are parallel streams whose outputs are concatenated.\nThe idea of the more efficient implementation is to make another batch dimension with nh so that PyTorch effectively makes batches of dimension (B,nh) and applies all the operations on both B and nh in parallel.\n\nEach token emits query, key, and value. Queries and keys first multiply each other to deterimine “how interesting they find each other”.\n\nNext, we apply an autoregressive mask to make sure the tokens only attend to tokens before them.\n\nThe softmax normalizes the attention so it sums to 1.\nThe matrix multiply of attention with the values is a way, at every single token, to do a weigthed sum of the tokens each token finds intersting.\n\nTranspose, contiguous, and view reassembles everything in memory and performs what is equivalent of a concatenation operation.\n\ny.transpose(1,2): This line swaps the second and third dimensions of y. So the shape of y changes from (B, nh, T, hs) to (B, T, nh, hs).\n.contiguous(): This is used to ensure that the tensor is stored in a contiguous block of memory, which is required for some operations in PyTorch, including view.\n.view(B, T, C): This line reshapes the tensor y to have dimensions (B, T, C). Here, C is equal to nh*hs, which means that the last two dimensions of y (nh and hs) are flattened into a single dimension. This effectively concatenates the outputs of all the attention heads side by side.\n\nFinally, the output projection doesn’t change the dimension of y, but does introduce another learnable transformation so that the output can be projected in a way that is most useful for downstream tasks.\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        #key, qury, value projections for all heads in a batch\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        # mask to prevent attention to future tokens\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size)) # becomes available as self.bias\n    def forward(self, x):\n        B, T, C = x.size()\n        # calculate query, key, value for all heads in batch\n        qkv = self.c_attn(x) # (B,T, self.n_embd) x (self.n_embd,self.n_embd*3) = (B,T,self.n_embd*3)\n        q, k, v  = qkv.split(self.n_embd, dim=2) # (B,T,self.n_embd) x 3; make each split size self.n_embd by splitting dim 2\n        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # (B, nh, T, hs)\n        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # attention materializes a large (T,T) matrix fo each query and key\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # Change (B, nh, T, hs) to (B, T, nh, hs) with transpose, reassemle in memory, (B,T,C) makes nh*hs = n_embd (C)\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        # output projection: additional learnable transformation\n        y = self.c_proj(y) # (B, T, C)@(C, C) = (B, T, C)\n        return y"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Copy over the Hugging Face GPT-2 model parameters into our model",
    "text": "Copy over the Hugging Face GPT-2 model parameters into our model\n\nIgnore the attention mask buffers (these are not parameters)\nThe weights in Hugging Face version are transposed (as they are in the original TensorFlow implementation) from what PyTorch needs because they use Conv1D module. Since we want to use plan nn.Linear, we hardcode these and transpose them.\n\n\nclass GPT(nn.Module):\n    ...\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type) # HF GPT2LMHeadModel has .from_pretrained method, just like ours\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#forward",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#forward",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Forward",
    "text": "Forward\n\nThe nn.Embedding layer in PyTorch takes an input tensor of arbitrary shape with values in the range [0, vocab_size) and maps each integer in that range to a dense vector of size C (the embedding dimension). In our case, we have an input tensor idx of shape (B, T), where B is the batch size and T is the sequence length, then applying the embedding layer to idx will yield a tensor of shape (B, T, C). This is because each integer in idx is replaced with its corresponding embedding vector. Since the embedding vectors have C elements, this adds an extra dimension of size C to the output. So for every batch and every sequence position, an embedding of size C is constructed, resulting in an output of shape (B, T, C).\nAlso note that the position embedding is broadcast to the token embedding and the same position embedding vector is learned at (T,C) for every element in B. This works because the position embeddings are independent of the specific sequence it’s in, so it can be shared across all sequences in the batch.\n\n\nclass GPT2(nn.Module):\n    ...    \n    def forward(self, idx):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        return logits"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Generate next sequence predictions with weights from pretrained GPT2 model",
    "text": "Generate next sequence predictions with weights from pretrained GPT2 model\n\nThe goal is to get close to generations from the Hugging Face pipeline:\n\n\nfrom transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n\nTo generate next token predictions, get the next top 50 tokens so that the model does not deviate too much from likely tokens, and sample one token from this distribution.\nConcatenate the token obtained from sampling with input (or input plus previously sampled tokens) at each step.\nThe sampling will not match the Hugging Face generations exactly since there’s likely a parameter hiding in the pipeline that’s different, but will be sensible English.\n\n\nnum_return_sequences = 5\nmax_length = 30\nmodel = GPT.from_pretrained('gpt2')\nmodel.eval()\nmodel.to('cuda')\n\n# prefix tokens\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\ntokens = enc.encode(\"Hello I'm a language model, \")\nx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences,1).to('cuda') # (5,8) since sent. tokenized to 8 tokens\n\n# generate: with each loop iteration, generate one more token\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nwhile x.size(1) &lt; max_length:\n    with torch.no_grad():\n        logits = model(x) # (B,T,vocab_size)\n        logits = logits[:, -1, :]  # take the logits at the last position\n        probs = F.softmax(logits, dim=-1) # get the probabilities\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # get the top-50 tokens\n        ix = torch.multinomial(topk_probs, num_samples=1) # sample from the top 50\n        xcol = torch.gather(topk_indices, -1, ix) # select the indices of the sampled tokens\n        x = torch.cat((x, xcol), dim=1) # append the sampled token to the sequence\n\nfor i in range(num_return_sequences):\n    tokens = x[i,:max_length].tolist()\n    decoded = enc.decode(tokens)\n    print('&gt;',decoded)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#initialize-a-random-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#initialize-a-random-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Initialize a random model",
    "text": "Initialize a random model\n\nTo do this, simply replace the GPT model initialization as below:\n\n\n#model = GPT.from_pretrained('gpt2')\nmodel = GPT(GPTConfig())"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#autodetect-device",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#autodetect-device",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Autodetect device",
    "text": "Autodetect device\n\nDetect the most powerful device available and use it\n\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n    print(\"using GPU\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = 'mps'\n    print(\"using MPS\")\n\n\nNote that we guarded against device mismatch by initializing pos on the correct device.\n\n\npos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Load the Tiny Shakespeare data set for quick debugging and load a batch of data",
    "text": "Load the Tiny Shakespeare data set for quick debugging and load a batch of data\n\nAndrej’s favorite debugging dataset is Tiny Shakespeare which can be loaded and previewed as below:\n\n\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open(\"input.txt\", \"r\") as f:\n    text = f.read()\ndata = text[:1000]\nprint(data[:100])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\n\n\nTo get the corresponding GPT-2 tokens, load the tokenizer via tiktoken’s get_encoding method and encode the text data.\n\n\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(data)\nprint(tokens[:24])\n\n[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n\n\n\nAndrej’s favorite way to create a batch is to use view, which returns a new tensor with same data but different shape. It’s a view because the returned tensor shares the same underlying data with the original tensor and a change in one will affect the other.\nSince the desired outputs for every token in the (B,T) batch are just to the right of that token, extend the buffer by one element, take all but the last token as inputs and the first token onwards as outputs:\n\n\nimport torch\nbuf = torch.tensor(tokens[:24+1])\nx = buf[:-1].view(4,6)\ny = buf[1:].view(4,6)\nprint(x)\nprint(y)\n\ntensor([[ 5962, 22307,    25,   198,  8421,   356],\n        [ 5120,   597,  2252,    11,  3285,   502],\n        [ 2740,    13,   198,   198,  3237,    25],\n        [  198,  5248,   461,    11,  2740,    13]])\ntensor([[22307,    25,   198,  8421,   356,  5120],\n        [  597,  2252,    11,  3285,   502,  2740],\n        [   13,   198,   198,  3237,    25,   198],\n        [ 5248,   461,    11,  2740,    13,   198]])"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#update-the-forward-pass-to-calculate-the-loss",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#update-the-forward-pass-to-calculate-the-loss",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Update the forward pass to calculate the loss",
    "text": "Update the forward pass to calculate the loss\n\nPass in the optional targets and calculate cross entropy loss. Cross entropy loss in PyTorch expects (BT,vocab_size) logits and (BT,) targets, so reshape it with view.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def forward(self, idx, targets = None):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        # --- Added code ---\n        loss = None\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        if targets: \n            # F.cross_entropy expects (B, T, vocab_size)-&gt; (B*T, vocab_size) shapes for logits\n            # and (B*T,) shape for targets. \n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#optimizer-and-training-loop",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#optimizer-and-training-loop",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Optimizer and training loop",
    "text": "Optimizer and training loop\n\nSee the Andrew Ng’s videos for a review of momentum and RMSProp that compose the Adam optimizer.\nMake sure to zero the gradients since loss.backwards() always accumulates gradients.\n\noptimizer.step() will update the parameters to (ideally) decrease the loss.\n\nloss.item() will convert the loss to a float that’s placed on CPU.\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\nfor i in range(50):\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n\n\nRunning the training loop will yield the following output:\n\n\n\n\nimage.png\n\n\n\nNote that when the weights are randomly initialized, we expect each token in 0-50256 range to be equally likely. Thus we expect loss to be around -ln(1/50257) = 10.82. Currently, the loss starts around this value, which is a good sanity check.\nAlso, we can make sure that the training is set up correctly by overfitting on a single batch. Running the training loop for 500 iterations on the same batch yields: iteration 499, loss = 0.0008159472490660846"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#data-loader-lite",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#data-loader-lite",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Data Loader Lite",
    "text": "Data Loader Lite\n\nTo build a simple data loader, advance by batches of size BT, set the input x* and output y as before, and loop around to the start of our tokens if run out of them:\n\n\nclass DataLoaderLite:\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = 0\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T\n        # if run out of tokens, loop around to zero\n        if self.current_position + B*T &gt;= len(self.tokens):\n            self.current_position = 0\n        return x, y\n\n\nUsing this data loader, loading and training can now be done concisely as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nNote that even with different batches of text loaded at every step, the loss still goes down quickly because many of the tokens will not occur in out data set and GPT2 can, for example, drive down the biases (mask) of all the logits that don’t occur in our data to negative infinity."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#parameter-sharing-via-weight-tying",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#parameter-sharing-via-weight-tying",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Parameter sharing via weight tying",
    "text": "Parameter sharing via weight tying\n\nObserve that transformer.wte.weight and lm_head.weight are of the same shape (50257*768).\nIn addition, we can below that these point to the same tensor.\n\n\n(sd_hf['lm_head.weight']==sd_hf['transformer.wte.weight']).all() # tensor(True)\nsd_hf['lm_head.weight'].data_ptr()==sd_hf['transformer.wte.weight'].data_ptr()\n\n\nThis weight tying scheme comes from p.5 of Attention is All You Need paper, which in turn used the work of Press and Wolf (2017). The idea is that if two tokens are very similar semantically, they should be close in the token embedding space. For the same reason, we expect these tokens to have similar probabilities in the output of a transformer. Press and Wolf (2017) argue that tying these weights leads to better performance. The weight scheme can be implemented in code as\n\n\nself.transformer.wte.weight = self.lm_head.weight\n\n\nNote that this weight sharing scheme also entails a significant memory savings of 38,597,376 parameters (768*50257) for the 124M model, which amounts to about 30% of the weights."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#model-initialization",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#model-initialization",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Model initialization",
    "text": "Model initialization\n\nOpenAI initialized the weights with mean 0 and standard deviation with 0.02 and the bias with 0. They initialized the token embeddings with mean 0 and standard deviation with 0.02 and position embeddings with 0.01.\nFollowing Xavier initialization, the standard deviation should be 1/sqrt(in_features_for_layer), and with 768 to 1600 features used by GPT-2 models of different sizes, 0.02 is approximately correct. Ideally, though, one would want the standard deviation to scale down more precisely with the model size.\nIn code, this leads to\n\n\nclass GPT(nn.Module):\n    ...\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n    \n\n\nIn addition, observe that our Block’s forward method contains a residual stream:\n\n\nclass Block(nn.Module):\n    ...\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNot accounting for this in initialization would lead to exploding variance as seen from the following simple example:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += torch.randn(768)\nprint(x.std())\n\ntensor(10.0776)\n\n\n\nHowever, scaling it by 1/sqrt(N) approximately yields the original standard deviation of 1 as seen below:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += n**-.5*torch.randn(768)\nprint(x.std())\n\ntensor(0.9894)\n\n\n\nTo implement it in our transformer, add flags in the attention and mlp modules since these have residual connections:\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        ...\nclass MLP(nn.Module):\n    def __init__(self,config):\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n     \n\n\nThen in the GPT module, check if a module has this attribute and adjust the standard deviation accordingly. Note that 2 multiplies self.config.n_layers because the residual connection is used twice: once for attention and once for the mlp module.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                # number of residual layers is double self.config.n_layers\n                # one for attention, one for mlp\n                std *= (2*self.config.n_layers)**-0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std) \n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Build GPT2.html#references",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "References",
    "text": "References\nPress, O., & Wolf, L. (2017). Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859v3.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762 .\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., & Liu, T. (2020). On Layer Normalization in the Transformer Architecture. arXiv preprint arXiv:2002.04745."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist/machine learning engineer experienced with Python, GCP, and LLMs. I love to build with code and LLMs.\n\n\n\nI hold graduate degress in math and economics,and I’m passionate about machine learning, programming, and LLMs. Currenly, I’m working on number of machine learning projects. From November 2022 to August 2024, I worked as a Data Analyst at LS Direct, where I performed data analytics and data engineering tasks for some of the company’s largest clients, helping the company quintuple the original budgets in some cases. I have developed both internal and client-facing automations using Python, SQL, and GCP, saving our team hours of time weekly. Whenever possible, I deeply favor programmatic solutions that streamline daily processes.\nPreviously, I worked at Lake Mary High School as an AP Computer Science teacher for two years, restarting the school’s AP Computer Science A program (object-oriented and algorithmic programming with Java) and being the founding advisor for LMHS AI Club, introducing advanced students to deep learning with PyTorch and Linux. Prior to that, I taught algebra to disadvantaged communities at Vanden High School.\nBefore that, I’ve completed an undergrad in math and three graduate programs, one in math and yes, two in economics (applied and mathematical), the latest from UC Davis. As a graduate student, I got to work on computer vision applications and to teach statistical methods, econometrics, and graduate economic theory. I have always been drawn to linguistic and computational aspects of the world, and in the past, I’ve learned numerous languages (being reasonably fluent at roughly 7 at one point). Since then I have shifted my passion to deep learning and computer languages, especially LLMs and Python.\nWhen I’m not at work, I’m either working away at LLMs, reading arXiv papers, following along with the fastai community, climbing, or hiking."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist/machine learning engineer experienced with Python, GCP, and LLMs. I love to build with code and LLMs."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I hold graduate degress in math and economics,and I’m passionate about machine learning, programming, and LLMs. Currenly, I’m working on number of machine learning projects. From November 2022 to August 2024, I worked as a Data Analyst at LS Direct, where I performed data analytics and data engineering tasks for some of the company’s largest clients, helping the company quintuple the original budgets in some cases. I have developed both internal and client-facing automations using Python, SQL, and GCP, saving our team hours of time weekly. Whenever possible, I deeply favor programmatic solutions that streamline daily processes.\nPreviously, I worked at Lake Mary High School as an AP Computer Science teacher for two years, restarting the school’s AP Computer Science A program (object-oriented and algorithmic programming with Java) and being the founding advisor for LMHS AI Club, introducing advanced students to deep learning with PyTorch and Linux. Prior to that, I taught algebra to disadvantaged communities at Vanden High School.\nBefore that, I’ve completed an undergrad in math and three graduate programs, one in math and yes, two in economics (applied and mathematical), the latest from UC Davis. As a graduate student, I got to work on computer vision applications and to teach statistical methods, econometrics, and graduate economic theory. I have always been drawn to linguistic and computational aspects of the world, and in the past, I’ve learned numerous languages (being reasonably fluent at roughly 7 at one point). Since then I have shifted my passion to deep learning and computer languages, especially LLMs and Python.\nWhen I’m not at work, I’m either working away at LLMs, reading arXiv papers, following along with the fastai community, climbing, or hiking."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "You can email me at dpopovvelasco{at}gmail.com\nMy LinkedIn is https://www.linkedin.com/in/dapvelasco"
  }
]