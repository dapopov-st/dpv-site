[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Dmitriy’s Projects",
    "section": "",
    "text": "arXiv LLMs Assistant\n\n\n\n\n\narXiv-llms-assistant is a ‘public’ version of a project submitted as part of an AI competition at LS Direct, demonstrating the use and evaluation of RAG with on-prem data as a way to capitalize on applications of gen AI to privacy-restricted domains.\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\n\nDeveloping novel anomaly detection methods for the real estate market. \n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\nInvesting in student housing\n\n\n\n\n\nKaggle-style supervised learning \n\n\n\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\nInventory classes with pytest and pdoc\n\n\n\n\n\nInventory-classes-with-pytest-and-pdoc: Inventory application for a tech video channel featuring computer builds with inventory classes in object-oriented Python, 90%+ coverage tests with pytest, comprehensive API documentation with pdoc, and packaging inside Docker. \n\n\n\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCOMPARATIVE STATICS OF A MONOPOLISTIC FIRM FACING RATE-OF-RETURN AND COMMAND-AND-CONTROL POLLUTION CONSTRAINTS\n\n\n\n\n\nA paper I co-authored in mathematical economics/economic theory.\n\n\n\n\n\nMar 5, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Dmitriy’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nSep 14, 2024\n\n\nOptimize GPT-2\n\n\n\n\nSep 5, 2024\n\n\nBuilding the GPT-2 transformer in PyTorch\n\n\n\n\nAug 31, 2024\n\n\nHow to pass GCP PDE exam on the first attempt: Write up in progress\n\n\n\n\nMay 31, 2024\n\n\narXiv LLMs Assistant: Write up in progress\n\n\n\n\nOct 18, 2022\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\nAug 29, 2022\n\n\nInvesting in student housing\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html",
    "title": "Optimize GPT-2",
    "section": "",
    "text": "The following is Part 2 of my extended write-up on building and training GPT-2 from scratch following along with “Let’s reproduce GPT-2 (124M)” by Karpathy. In this part, I discuss the optimizations that speed up training first on one, then multiple GPUs, also adding more detail where I felt necessary for my own understanding (hoping this will be helpful to others)."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#establishing-a-timing-baseline",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#establishing-a-timing-baseline",
    "title": "Optimize GPT-2",
    "section": "Establishing a timing baseline",
    "text": "Establishing a timing baseline\n\n\n\nSource: GPU utilization on a 3090 with batch size of 10, image by the author\n\n\n\nEach iteration would take about 720ms with batch size of 10 on a 3090 (vs 1000 on A100 with double the batch size!), so I expect my single-GPU training to be a bit slower than Andrej’s. With Andrej’s A100, it’s possible to process one epoch in 20 batches of size 16 with about 1000ms/batch, whereas with a 3090, it’s possible to process one epoch in 33 batches of size 10 with 730ms/batch, so about 20% slower than Andrej’s time (33x730/20x1000).\nAlso, Andrej suggests using nice numbers that have lots of multiples of 2. I tried a batch size of 8, but in this specific use case, I found that using batch size of 10 vs 8 is slightly faster per epoch on a 3090.\nAlso, to properly time the code, it’s important to wait until the GPU processes a batch before the CPU times the work. It’s possible that the CPU has loaded a batch onto GPU, GPU is still processing it, and the CPU has moved on to record the end time for the operation. To prevent this from happening, use torch.cuda.synchronize():\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    #import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {i}, loss = {loss.item()}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nWe may change the batch size, so to get a more objective measure of training speed, look at tokens/second, which is 16.3K on an A100 and 14.1K on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#mixed-precision-logic",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#mixed-precision-logic",
    "title": "Optimize GPT-2",
    "section": "Mixed precision logic",
    "text": "Mixed precision logic\n\nFirst observe that we can check the type of the logits in our model by starting an interactive debugging session as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nWhich yields\n\n\n(InteractiveConsole)\n&gt;&gt;&gt; logits.dtype\ntorch.float32\n\n\nPyTorch supports up to FP64 precision, which is useful for scientific computing applications, but deep learning training can tolerate significantly lower precision than default FP32, however.\nFrom A100 spec sheet, for example, it’s possible to get 16x performance improvement by going down from FP32 to FP16 Tensor Core.\nFor deep learning, sparsity feature is not currently used, so disregard the second number in a cell when it’s present.\nWe do not use INT8 for training since it implies a uniform distribution and we need a normal distribution provided by the float data types. INT8 is used for inference, however.\nThe memory bandwidth of an A100 is 1935GB/s, and most well-tuned application are bound more by memory than by speed. Lowering precision means tensors will take less space in memory, making it easier to satisfy the memory bandwidth constraint.\n\nTo summarize, by lowering precision, “we can store more and access it faster” (Andrej).\n\n\n\n\nSource: https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#tensor-cores",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#tensor-cores",
    "title": "Optimize GPT-2",
    "section": "Tensor Cores",
    "text": "Tensor Cores\n\n\n\nSource: https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/\n\n\n\nA Tensor Core is an instruction in a given architecture, in the case above for a 4x4 multiply and add operations. Any time we have multiply (and less importantly add) operations, which compose the majority of our GPT-2 transformer, these operations will be performed using Tensor Cores. For example, the classifier head matrix multiply going from 768 to 50257 dominates the computations in GPT-2.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith FP32 (torch.float32 default of PyTorch), both the input operands and the intermediate add/multiplies that compose individual elements of the result matrix are done in FP32.\nWe could switch to TF32, however, which uses the full 32 bits of FP32 for accumulator but just 19 bits for input operands due to lower number of mantissa bits as seen below:\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith TF32, we get an 8x speedup without needing to modify the code. The outputs will still be in FP32 as seen below. If our application can tolerate a little bit of imprecision, TF32 is a great option. In practice,the difference between FP32 and TF32 is almost imperceptible.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nOn my 24GB 3090, I was able to use batch size of 8 (or 10 max) with sequence length of 1024 (Andrej got away with batch size of 16 on 40GB A100):"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#compute-the-speedup-with-tf32",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#compute-the-speedup-with-tf32",
    "title": "Optimize GPT-2",
    "section": "Compute the speedup with TF32",
    "text": "Compute the speedup with TF32\n\nTo enable TF32 in PyTorch, change the float32 matmul precision from it’s default ‘highest’ to ‘high’ with torch.set_float32_matmul_precision(‘high’):\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n...\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n   ...\n\n\nOn an A100, throughput increases from 16.1K tokens/second to about 49K tokens/second, so about a 3X speedup.\nOn 3090, this leads to only about 40% speedup from 14.1K tokens/second baseline to 19.6K tokens/second.\n\nWhile TF32 in principle offers an 8X speedup on an A100 (I couldn’t find reliable official estimates for 3090 TF32 tensor), a lot of these workloads are memory bound. Thus although a matrix multiplication could potentially happen 8X faster with TF32 compared to FP32, the output numbers are still FP32, and these get moved around through the memory system at a speed that’s much slower than the GPU’s ability to perform the calculations."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#bfloat16-to-reduce-memory-bandwidth-constraint",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#bfloat16-to-reduce-memory-bandwidth-constraint",
    "title": "Optimize GPT-2",
    "section": "Bfloat16 to reduce memory bandwidth constraint",
    "text": "Bfloat16 to reduce memory bandwidth constraint\n\nLet’s review the following once again\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nTo address the memory bandwidth constraint, we can use Bfloat16 (BF16) to more aggressively crop the mantissa without changing the sign and exponent (range) of the number.\n\nOriginally, FP16 was used, but this number format has a reduced range, causing issues that were patched by gradient scalers and similar solutions that introduced additional state and complexity. BF16 addresses these problems by preserving the original range of number.\nAndrej recommends studying the torch.autocast portion of mixed precision documentation, which has a context manager torch.autocast. This context manager is recommended around forward pass and loss calculation of the model only.\nWe thus only need to add one line of code as below:\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n    loss.backward()\n    ...\n\n\nUsing an interactive console breakpoint\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        import code; code.interact(local=locals())\n\n    loss.backward()\n    ...\n\n\nWe see that logits.dtype is indeed torch.bfloat16. Our weights remain in FP32, however, as model.transformer.wte.weight.dtype is torch.float32. This implies a mixed precision: PyTorch is keeping certain weights in full precision while converting others to bfloat16. What gets converted at what point is not exactly clear, but the general guidelines are below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/amp.html\n\n\n\nThus matrix multiplications, addition, etc. get converted while layer norms, softmax, etc. do not since they are less robust to precision changes.\nOn an A100, our previous benchmark is 50K tokens/second and it goes up to 55K tokens/second with bfloat16, about a 10% speedup.\nOn a 3090, our previous benchmark is 19.6K tokens/second and it goes up to 27.5K tokens/second with bfloat16, about a 40% speedup, suggesting that 3090 was perhaps more memory-bound than the A100."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#torch.compile-a-compiler-for-neural-networks",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#torch.compile-a-compiler-for-neural-networks",
    "title": "Optimize GPT-2",
    "section": "Torch.compile, a compiler for neural networks",
    "text": "Torch.compile, a compiler for neural networks\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# torch.compile only needs a single line\nmodel = torch.compile(model)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n\n\nThe first iteration with torch.compile will typically be slower since PyTorch takes time to compile the code at the start.\nOn an A100 with batch size of 16, Andrej observed a 2.3x improvement from using torch.compile.\nOn a 3090 with batch size of 10, no improvement was seen during the first run (27.4 K tokens/second -&gt; 26.8K tokens/second). However, switching down to batch size of 8 yielded a significant increase the first time around. However, when I repeated the experiment, torch.compile ended up being faster with batch size of 10 again! This could have been due to graph breaks or kernel launch overhead.\n\nTaking the best runtime with torch.compile and batch size of 10, 27.4 K -&gt; 41.7 K tokens/second, or about 48% speedup over mixed precision benchmark.\n\n\n\n\nSource: initial training dynamics, image by the author\n\n\n\ntorch.compile does the following:\n\nTake out the Python interpreter from the forward pass and compile the neural net as a single object\nReduce GPU read/writes as demonstrated in the following example. Suppose we didn’t use torch.compile and instead of nn.GELU in our MLP, we used our own custom implementation below:\n\nclass TanhGELU(nn.Module):\ndef forward(self, x):\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi)*(x + 0.044715 * torch.pow(x, 3.0))))\n\nLooking at the following diagram, the input would first need to be placed from the High-Bandwidth Memory (HBM) onto the GPU for the torch.pow(x, 3.0) computation. Once the results are computed, they would be sent back to HBM. Next the computed result would be sent back to be multiplied by 0.044715 and so on. This would results many reads and writes to and from GPU and HBM, which are the bottleneck in many modern system.\nSince all the operations in the custom TanhGELU are element-wise operations, torch.compile can move the input to GPU and, for every single element, perform all the operations while the memory is on the GPU, then write back a single time. This is an example of kernel fusion, a major way in which torch.compile creates a speedup.\n\n\n\n\n\ngpu-cpu\n\n\n\nIn addition, when the data is moved to GPU for element-wise operations, it will need intermediate memory, a small amount of which is found on the GPU. Andrej points out that on the GPU chip itself, there is L2 cache. On the streaming multiprocessors (SMs) that do the calculation, there’s L1 memory and registers. These use SRAM for fast access times and low power consumption vs transistors and capacitors implementation of HBM. Below is a typical diagram of memory and associated access speeds from Dao et al. (2022), Figure 1:\n\n\n\n\nmemory-hierarchy\n\n\n\nWith mixed precision and torch.compile optimizations, Andrej’s training on an A100 is about 3X faster than training on a 3090, with 125K tokens/second processed on an A100 and 41.5K tokens/second on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#flashattention",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#flashattention",
    "title": "Optimize GPT-2",
    "section": "FlashAttention",
    "text": "FlashAttention\n\nThe attention operation is currently composed of the four lines of code highlighted below:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nFlashAttention is a kernel fusion operation that torch.compile cannot find currently. This is because kernel fusion requires an algorithmic rewrite of how attention is currently implemented. While FlashAttention requires more FLOPs, it ends up being significantly faster because of its judicious use of the memory hierarchy, which leads to fewer read/writes between GPU and HBM.\n\nIn particular, the NxN attention matrix is never read from/written to HBM. It’s simple to perform the matrix multiplications in a streaming manner, but computing softmax this way is more of a challenge. The crux of the algorithmic rewrite is the online softmax trick, which incrementally computes the softmax without needing all the inputs as is customary for standard softmax normalization. The key insight is that the softmax function can be broken down into smaller chunks, computed independently on GPU, and then combined using a ‘softmax trick’ formula. This enables FlashAttention to process long sequences more efficiently, without having to materialize the entire attention matrix at once.\n\nIn more detail, from p.5 of Dao et al. (2022), the softmax trick uses intermediate m and l variables that combine the statistics from individual ‘attention submatrices’ to reconstruct the softmax for the entire attention matrix.\n\nWith FlashAttention in place, the four lines above become one line in the following code:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        y = F.scaled_dot_product_attention(q,k,v, is_causal=True)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nThe loss after 50 iterations is identical to what it was before since FlashAttention is exact attention, not an approximation, so identical operations are performed. However, on a 3090, with FlashAttention added, about 50K tokens/second can be processed compared 41.7K tokens/second with just mixed precision+torch.compile, hence about a 20% speedup. On an A100, Andrej obtained about a 27% improvement."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#nice-and-ugly-numbers-vocab-size",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#nice-and-ugly-numbers-vocab-size",
    "title": "Optimize GPT-2",
    "section": "Nice and ugly numbers, vocab size",
    "text": "Nice and ugly numbers, vocab size\n\nWe’re looking for the numbers in our transformer to have as many powers of two as possible since GPU kernels work best with these. In particular, vocab_size is 50257, which is not odd; one simple way to change this is to round it up to 50304, which divides by 128 (2**7). This adds ‘fake’ tokens and increases the amount of computation, yet by working better with the GPU, it would generally lead to a speedup in training.\nNote that the vocab_size appears in the embedding and the classifier layer, and the extra tokens added there would never be used. The weight sharing implies that the network has to learn that the logits associated with the extra added rows need to be driven to zero, which is similar to how it is already learning to drive down weights for tokens not encountered in our training set.\nOn an A100, 4% improvement was observed; on a 3090, the training speed increased from 50K tokens/second to about 53 tokens/second, so about a 6% increase.\nAndrej elaborates on the reason for the speedup. Many of the CUDA kernels use block tiles, which are usually ‘nice numbers’ with many powers of 6, so calculations are done in powers of 64 or 32, for example. When the desired calculation does not neatly fit into these block tiles, less efficient boundary kernels come in to do the last part at the second pass.\nOptimizations targeted at GPT-2 specifically follow next."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#gradient-clipping",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#gradient-clipping",
    "title": "Optimize GPT-2",
    "section": "Gradient clipping",
    "text": "Gradient clipping\n\nThe idea with gradient clipping is that it’s possible to get a particularly ‘unlucky’ batch during training, which would lead to high loss and gradient, leading to training instability (or ‘shock’, as Andrej puts it).\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n\n\n\n\nSource: training dynamics with grad norm clipping, image by the author\n\n\n\nNote that the norm is still high at the start of the training. Since the weights are randomly initialized, a lot of learning happens to (generally) drive down the biases of the output tokens. The norm then stabilizes to be around 1.0."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#learning-rate-scheduler",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#learning-rate-scheduler",
    "title": "Optimize GPT-2",
    "section": "Learning rate scheduler",
    "text": "Learning rate scheduler\n\nThe learning rate scheduler used is cosine decay learning rate scheduler.\n\nThere is a linear warmup step as it’s beneficial to have a low learning rate at the start since the network is randomly initialized.\nAfter that, the cosine decay takes place and the learning rate is gradually reduced to 10% of its original value.\nTaking the values for warmup steps and maximum number of steps from the GPT-3 paper (GPT-2 was scarce on training details), we see the following graph of learning rates:\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 375\nmax_steps = 2600\ndef get_lr(it):\n    # 1.) linear warmup for warmup_iters steps\n    if it &lt; warmup_steps:\n        return max_lr * (it+1)/warmup_steps\n    # 2.) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; max_steps:\n        return min_lr\n    # 3.) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps)/(max_steps-warmup_steps)\n    assert 0&lt;=decay_ratio&lt;=1\n    coeff = 0.5*(1.0 + math.cos(math.pi * decay_ratio)) # coef starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr-min_lr)\n\nits = np.arange(0,max_steps)\nget_lr_vectorized = np.vectorize(get_lr)\nlrs = get_lr_vectorized(its)\nplt.plot(its,lrs)\nplt.title('Cosine Learning Rate Decay')\nplt.xlabel(\"Number of tokens processed by GPT-2 in millions\")\n\nText(0.5, 0, 'Number of tokens processed by GPT-2 in millions')\n\n\n\n\n\n\n\n\n\n\nI chose to express the x-axis in tokens rather than steps to reduce the focus on hardware-related configurations used to train GPT-2 (eg, batch size), but the shape would be identical.\n\nThe learning rate scheduling can be added to the training loop in the code below:\n\n\n# ----------------- Training loop -----------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor step in range(50):\n    ...\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    ...\n\n\nNote that as part of setting the learning rate, we need to iterate through the parameter groups (of which there’s only one) and set the ‘lr’ parameter to our learning rate. This is a bit clunky, but Andrej’s impression is that it’s the way to do this in PyTorch currently. An alternative would be to use an off-the-shelf learning rates scheduler from PyTorch, but that’s an additional abstraction, and get_lr is composed of only a few lines of highly readable code.\nAndrej points out that the choice of a learning rate scheduler is up to the user and determining the ‘best’ one is an active area or research.\nIn addition, Andrej skips the gradual batch size increase used by GPT-2 since it complicates the arithmetic of the number of tokens used at each step in the optimization and it’s not a major improvement."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#weight-decay-and-fused-optimizer",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#weight-decay-and-fused-optimizer",
    "title": "Optimize GPT-2",
    "section": "Weight decay and fused optimizer",
    "text": "Weight decay and fused optimizer\n\nDecaying weights prevents any weight from getting too large, forcing the network to distribute the work.\nInstead of iterating over all the parameter updates, the kernels (computations) used for the AdamW update can be fused into a single kernel in more recent versions of PyTorch.\n\n\nclass GPT(nn.Module):\n    ...\n    \n    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n        # start with all of the candidate parameters that require grad\n        param_dict = {pn: p for pn,p in self.named_parameters()}\n        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n        # create optim groups: any 2D parameter weight decays, others don't:\n        # weight tensors in matmuls + embeddings will decay, biases and layernorms will not\n        decay_params = [p for n,p in param_dict.items() if p.dim()&gt;=2]\n        nodecay_params = [p for n,p in param_dict.items() if p.dim()&lt;2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decay param tensors: {len(decay_params)} with {num_decay_params} parameters\")\n        print(f\"num non-decay param tensors: {len(nodecay_params)} with {num_nodecay_params} parameters\")\n\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        print(f'using fused AdamW: {use_fused}')\n        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas=(0.9, 0.95), eps = 1e-8, fused=use_fused)\n        return optimizer\n# In the training loop replace the optimizer below:\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    ...\n\n\nAmong the output, we will see num decay param tensors: 50 with 124354560 parameters, num non-decay param tensors: 98 with 121344 parameters. Hence most of the parameters will be weight decayed.\nOn an A100, Andrej observed about a 3% speedup due to the fused optimizer, while I saw about a 2% speedup on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#gradient-accumulation",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#gradient-accumulation",
    "title": "Optimize GPT-2",
    "section": "Gradient accumulation",
    "text": "Gradient accumulation\n\nThe relationship between weight decay, batch size, and learning rate is quite complex. In GPT-2, smaller batch size and bigger learning rate was likely used for smaller neural networks and larger batch size with smaller learning rate for larger ones. For the smallest network size, OpenAI used a batch size of 0.5M tokens. With 1024 tokens/batch, this implies a batch size of about 500. However, we would still like to use the effective batch size of 0.5M since it’s related to other hyperparameters determined by OpenAI. A solution to this is gradient accumulation, which allows us to simulate in a serial way any arbitrary batch size of our choice.\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090; switching down 10-&gt;8 since speeds are comparable and it factors evenly into total_batch_size above\nT = 1024\nassert total_batch_size % (B*T) == 0, 'make sure total_batch_size is divisible by B*T'\ngrad_accum_steps = total_batch_size//(B*T)\nprint(f\"total desired batch size: {total_batch_size}\")\nprint(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\ntotal desired batch size: 524288\n=&gt; calculated gradient accumulation: 64\n\n\n\nThus we’ll need to do 64 forward backward steps followed by a single gradient update. For context, our current training loop is\n\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        #import code; code.interact(local=locals())\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {step}, loss = {loss.item()}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nTo replicate the effective batch size of around 0.5M, we can accumulate gradients using an inner for loop, track accumulated loss to print it correctly, and adjust the tokens per second calculation to account for the number of gradient accumulation steps:\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    optimizer.zero_grad()\n    loss_accum = 0.0 # need this  to correctly print the accumulated loss\n    for micro_step in range(grad_accum_steps):\n        x,y = train_loader.next_batch()\n        x,y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n            logits, loss = model(x,y)\n        loss = loss/grad_accum_steps\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        loss.backward() # deposits gradients by default\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps)/(t1-t0) # don't forget to adjust the tokens/second calculation \n    print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nNote in particular that the loss needs to be divided by grad_accum_steps. This is because the accumulator in F.cross_entropy loss is mean, whereas the loss within each grad_accum_steps loop does not account for averaging with the grad_accum_steps. To make this more concrete, without the gradient accumulation, the loss in the following simple regression example would include the division by 4:\n\n\nimport torch\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(16,32),\n    torch.nn.GELU(),\n    torch.nn.Linear(32,1)\n)\ntorch.random.manual_seed(42)\nx = torch.randn(4,16)\ny = torch.randn(4,1)\nnet.zero_grad()\nyhat = net(x)\nloss = torch.nn.functional.mse_loss(yhat,y)\nloss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n\n# The loss objective, due to reduction='mean' is\n# L = 1/4*[\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nIf we compute the loss by adding the losses of the individual examples, we omit division by 4:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0598,  0.0042,  0.0167, -0.0161,  0.0235, -0.0320, -0.0311, -0.0550,\n        -0.0410, -0.0536])\n\n\n\nClearly, the gradients are not the same. To fix this, divide the loss by number of steps:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss = loss/4\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (1/4)*(y[0]-yhat[0])**2 +\n#     (1/4)*(y[1]-yhat[1])**2 +\n#     (1/4)*(y[2]-yhat[2])**2 +\n#     (1/4)*(y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nAlso note that casting our training loop in terms of effective batch size, we can adjust the size of our micro batch without changing the training dynamics: micro batch size is purely a performance optimization setting."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#distributed-data-parallel-ddp",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#distributed-data-parallel-ddp",
    "title": "Optimize GPT-2",
    "section": "Distributed data parallel (DDP)",
    "text": "Distributed data parallel (DDP)\n\nWith DDP, we’ll have one process per GPU available. These will have the same model loaded but will be processing slightly different parts of the data. They’ll contribute their own gradients that will be averaged. DDP can be run with torchrun, which will correctly set the necessary environmental variables and launch distributed training.\n\n\nInitialize DDP\n\nWhere we were before initializing device, we now also initialize distributed training as follows:\n\n\n# ----------------- Run distributed training -----------------\nfrom torch.distributed import init_process_group, destroy_process_group\nddp = int(os.environ.get('RANK',-1)) != -1 # is this a ddp run?\nif ddp:\n    assert torch.cuda.is_available()\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ.get(\"RANK\"))\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank==0\n    print(\"RANKS: \",ddp_rank,ddp_local_rank,ddp_world_size,device)\nelse:\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    device = f\"cuda: {ddp_local_rank}\"\n    master_process = True\n    device = 'cpu'\n    if torch.cuda.is_available():\n        device = 'cuda'\n        print(\"using GPU\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        device = 'mps'\n        print(\"using MPS\")\n\n\nThere is a RANK variable assigned to each GPU (GPU 0 will have RANK of 0, GPU 1 will have RANK of 1, etc). Each of these processes will run on different parts of the data. LOCAL_RANK is only used in multi-node setting and represents the rank of a GPU on a single node. WORLD_SIZE is the total number of GPUs available.\n\n\n\nBatch size adjustments\n\nSince each GPU will need to run on a slightly different subset of our data, adjust the batch size and gradient accumulation as follows:\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090\nT = 1024\nassert total_batch_size % (B*T*ddp_world_size) == 0, 'make sure total_batch_size is divisible by B*T*ddp_world_size'\ngrad_accum_steps = total_batch_size//(B*T*ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\n\nAlso, print on master process only to avoid duplicates prints arising from other processes.\n\n\n\nRunning DDP with torchrun\nTo run on 2 GPUs, we can use torchrun –standalone –nproc_per_node=2 train_gpt2.py  (adjust 2 to the number of available GPUs).\n\n\nDataLoader modifications\n\n# ----------------- Data loader lite -----------------\nimport tiktoken\nclass DataLoaderLite:\n    def __init__(self, B, T, process_rank, num_processes): #1\n        self.B = B\n        self.T = T\n        self.process_rank = process_rank                   #2\n        self.num_processes = num_processes                 #3\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = self.B*self.T*self.process_rank\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T*self.num_processes                            #4\n        # if run out of tokens, loop around to zero\n        if self.current_position + (B*T*self.num_processes+1) &gt;= len(self.tokens): #5\n            self.current_position = self.B*self.T*self.process_rank                #6\n        return x, y\n...\ntrain_loader = DataLoaderLite(B=B,T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n\n\nThe dataloader should now advance by B x T x num_processes so that each process (GPU) gets its own batch. To accomplish this, modify/add lines commented with #1-#6.\n\n\n\nModel modifications\n\nCurrently, identical model copies are initialized and compiled on each of the GPUs since each GPU is passes the same seed. In addition wrap the compiled model in a DDP container as follows:\n\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n...\nmodel = torch.compile(model)\nif ddp:\n    model = DDP(model,device_ids=[ddp_local_rank])\n\n\nIn essence, DDP allows communication between the GPUs by averaging and synching the gradients. The forward pass will be identical, but once the backward pass is over, DDP will call allreduce to get the average across all the GPUs and deposit the average on every single GPU. In addition, DDP efficiently communicates the gradients while the backward pass is happening.\nIn addition, we should configure our optimizer using the ‘raw’ model that’s not wrapped in the DDP process:\n\n\nraw_model = model.module if ddp else model # always contains the unwrapped model\n...\noptimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\n\n\n\nTraining loop modifications\n\nIn addition, we only need the average of the gradients at the last of the gradient accumulation steps. The sanctioned way to do this is with ddp.no_sync context manager. Andrej notes that the context manager simply toggles the require_backward_grad_sync variable, noting that this is a risky move in case this variable disappears/gets renamed in future PyTorch versions.\n\n\nimport torch.distributed as dist\n...\n# ----------------- Training loop -----------------\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(max_steps):\n    ...\n    for micro_step in range(grad_accum_steps):\n        ...\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        if ddp: \n            model.require_backward_grad_sync = (micro_step==grad_accum_steps-1)\n        loss.backward() # deposits gradients by default\n    if ddp:\n        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps*ddp_world_size)/(t1-t0)\n    if master_process:\n        print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\nif ddp:\n    destroy_process_group()\n\n\nAlso, we want the average of all the losses from all the GPUs to be printed by the master process. The loss_accum tensor exists on all the ranks. Calling dist.all_reduce after the inner micro batch loop computes the average of loss_accums and deposits it on all the ranks. When we then print on the master process, the loss_accum that’s identical on all the ranks is printed.\n\nWe should also destroy the process group after the training is done to free up resources.\n\n\n\nOther tweaks, training dynamics\n\nAlso, I got an error regarding device_type first time I ran the training. Andrej noted in is code “added after the video, pytorch can be serious about it’s device vs. device_type distinction”. This can be addressed as follows:\n\n\n# add after ddp init code\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n# set device type to device_type in the training loop\nwith torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n    logits, loss = model(x,y)\n\n\nWe get the following training dynamics on two 3090s:\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 8, image by the author.png\n\n\n\nThe loss is going down smoothly and about 100K tokens/second are processed. This is far short of 1.5M tokens/second that can be processed on 8xA100’s, of course, but it still looks like training GPT-2 locally should be feasible. Andrej later mentions that the ‘full’ trained GPT-2 can be reproduced in his setup in about an hour, so it would take around 15 hours on two 3090s.\n\nIn addition, Andrej mentioned that our current setup fixes the effective batch size, leaving the size of the micro batch as a tunable parameter. While the following choice slightly violates the “nice numbers” principle, I think it’s worth considering: Adjusting the effective batch to 512K and batch size to 10 so that micro batches with sequence length of 1024 evenly fit into the effective batch size, I could get the training to speed up to ~110K tokens/second, implying that the training time can be shortened to around 14 hours.\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 10, image by the author.png"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT2.html#references",
    "title": "Optimize GPT-2",
    "section": "References",
    "text": "References\nDao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv preprint arXiv:2205.14135v2 (2022)."
  },
  {
    "objectID": "projects/indiv_projects/arxiv/arxiv-llms-assistant.html",
    "href": "projects/indiv_projects/arxiv/arxiv-llms-assistant.html",
    "title": "arXiv LLMs Assistant",
    "section": "",
    "text": "Applied exllamav2 6.0bpw quantization to run Mixtral-8x7B on personal GPUs with virtually no loss in quality and evaluate RAG pipeline configurations to find the best performing one (evaluation score ~90%).\nBuilt an assistant to study the LLM domain, compare out-of-repository papers with papers in a personal repo (Zotero) and recommend recent papers to read along with lists of question/answer pairs.\nOrchestrated a multi-faceted evaluation of RAG setups, implementing feedback mechanisms that led to the selection of the most effective configuration; this process improved question relevance, elevating overall project outcomes.\nBuilt a command line utility to ask questions about the contents of uploaded papers and generate question/answer sets based on these papers.\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/publication/bulletin-publication.html",
    "href": "projects/indiv_projects/publication/bulletin-publication.html",
    "title": "COMPARATIVE STATICS OF A MONOPOLISTIC FIRM FACING RATE-OF-RETURN AND COMMAND-AND-CONTROL POLLUTION CONSTRAINTS",
    "section": "",
    "text": "The intrinsic comparative statics properties of a general rate-of-return regulated, profit-maximizing model of a monopolist facing a command-and-control pollution constraint are derived. Recent advances in the theory of comparative statics are used to derive the basic comparative statics of the model, which are contained in an observable negative semi-definite matrix and possess the form of Slutsky-like expressions. We consider several command-and-control pollution constraints that are commonly implemented in practice, and conclude that the intrinsic comparative statics properties of the model are qualitatively invariant to the type of command-and-control pollution constraint imposed. We compare our results with those extant, and find that several basic results from the standard A–J model no longer hold in our model.\nPublication link"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "You can email me at dpopovvelasco{at}gmail.com\nMy LinkedIn is https://www.linkedin.com/in/dapvelasco"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "",
    "text": "The following project/write-up arises from my interest in understanding the under-the-hood details of training transformers. I’ve studied Andrej Karphaty’s video “Let’s build GPT: from scratch, in code, spelled out.” when it came out in 2023, and started working on describing parts he didn’t yet explain. Then “Let’s reproduce GPT-2 (124M)” came out, which answered a lot of my questions and compelled me to finish up the write up and make it available to others. My hope is that it will serve as a good review to those studying Andrej’s work, or those who do not have time to carefully work through a 4H+ video (which is highly, highly recommended).\n\nPart 1 focuses on building the transformer, Part 2 on optimizing it, and in the parts that follow, I hope to describe my own tokenization and training experiments.\nfrom dataclasses import dataclass\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#overall-tranformer-structure",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#overall-tranformer-structure",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Overall tranformer structure",
    "text": "Overall tranformer structure\n\nWe’ll be implementing the right hand side (the decoder) that composes GPT-2: each unit on the right will be a block in our transformer.\n\n\n\n\nTransformer Architecture from “Attention Is All You Need” by Vaswani et al.\n\n\n\nThe configuration below is the configuration for the entire tranformer, with each layer h pertaining to one of the blocks. We want to replicate the following structure from a GPT-2 model in Huggingface Transformers:\n\n\n\n\nHF Transformer\n\n\n\nThe code below is the skeleton on GPT2 config and main module that will allow us to replicate that structure:\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 256\n    vocab_size: int = 65\n    n_layer: int = 6\n    n_head: int = 8\n    n_embd: int = 384\n\nclass GPT(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        # With nn.ModuleDict() index into submodules just like a dictionary\n        self.tranformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n\nnn.ModuleDict allows you to index into submodules using keys, just like a dictionary.\nnn.ModuleList allows us to index into each individual layer using an index, just like with a list"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#transformer-block",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#transformer-block",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Transformer Block",
    "text": "Transformer Block\n\nNow let’s implement the Block,…\nUnlike the original GPT2 paper, establish a clean residual pathway by taking the layer norm of x and applying attention/multilayer perceptron layer to it then adding it to the input x. Since addition allows for an adulterated gradient flow during backpropagation, this pre-layer norm configuration is the better than the post-layer norm configuration where the norm is applied after the addition. More formally, Xiong et al. (2020) have shown that if post-layer norm is used, a warm-up stage is needed to avoid training instability whereas if pre-layer norm is used, the gradients are well-behaved at initialization. See the difference between original (Post-LN) GPT-2 implementation and the ‘corrected’ pre-LN implementation used here:\n\n\n\n\nSource: “On Layer Normalization in the Transformer Architecture” by Xiong et al. 2020\n\n\n\nFinally, onto the Block:\n\n\nclass Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config    \n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNote again how the layer norm is applied before the addition to the residual stream.\nAndrej notes that attention is a communication operation, where tokens communicate with each other and aggregate information. Thus attention can be thought of as a pooling function/weighted sum function/reduce operation. On the other hand, the multilayer perceptron (MLP) is applied to each token individually, with no information exchanged between the tokens. Thus attention is a reduce and MLP is the map operation and a transformer is a repeated application of MapReduce."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#multilayer-perceptron",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#multilayer-perceptron",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\n\nBriefly summarizing from Andrej’s previous video (Let’s build GPT: from scratch, in code, spelled out.), multilayer perceptron is implemented using a standard “bottleneck architecture” where the dimensions are first expanded to learn more complex representations, nonlinearity is applied to help the model learn more complex patterns, and finally the data is projected down again to keep the computational complexity in check.\n\n\nclass MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x    \n\n\nGPT-2 used an approximate version of GeLU because at the time of GPT-2’s creation, the erf function was very slow in TensorFlow and GPT-2 and the approximate version was used. Today there’s no reason to use the approximate version but Andrej is using the tanh approximation for veracity.\n\nAlso, GeLU is better than ReLU due to dead neuron problem since a local gradient is always present as seen below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#causal-self-attention",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#causal-self-attention",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Causal Self Attention",
    "text": "Causal Self Attention\n\nAndrej’s attention implementation is a more efficient implementation of the following simple one from “Lets build GPT: from scratch, in code, spelled out”:\n\n\n\n\nSource: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n\n\n\nNotice how in the implemenation above it is clear that the heads are parallel streams whose outputs are concatenated.\nThe idea of the more efficient implementation is to make another batch dimension with nh so that PyTorch effectively makes batches of dimension (B,nh) and applies all the operations on both B and nh in parallel.\n\nEach token emits query, key, and value. Queries and keys first multiply each other to deterimine “how interesting they find each other”.\n\nNext, we apply an autoregressive mask to make sure the tokens only attend to tokens before them.\n\nThe softmax normalizes the attention so it sums to 1.\nThe matrix multiply of attention with the values is a way, at every single token, to do a weigthed sum of the tokens each token finds intersting.\n\nTranspose, contiguous, and view reassembles everything in memory and performs what is equivalent of a concatenation operation.\n\ny.transpose(1,2): This line swaps the second and third dimensions of y. So the shape of y changes from (B, nh, T, hs) to (B, T, nh, hs).\n.contiguous(): This is used to ensure that the tensor is stored in a contiguous block of memory, which is required for some operations in PyTorch, including view.\n.view(B, T, C): This line reshapes the tensor y to have dimensions (B, T, C). Here, C is equal to nh*hs, which means that the last two dimensions of y (nh and hs) are flattened into a single dimension. This effectively concatenates the outputs of all the attention heads side by side.\n\nFinally, the output projection doesn’t change the dimension of y, but does introduce another learnable transformation so that the output can be projected in a way that is most useful for downstream tasks.\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        #key, qury, value projections for all heads in a batch\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        # mask to prevent attention to future tokens\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size)) # becomes available as self.bias\n    def forward(self, x):\n        B, T, C = x.size()\n        # calculate query, key, value for all heads in batch\n        qkv = self.c_attn(x) # (B,T, self.n_embd) x (self.n_embd,self.n_embd*3) = (B,T,self.n_embd*3)\n        q, k, v  = qkv.split(self.n_embd, dim=2) # (B,T,self.n_embd) x 3; make each split size self.n_embd by splitting dim 2\n        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # (B, nh, T, hs)\n        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # attention materializes a large (T,T) matrix fo each query and key\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # Change (B, nh, T, hs) to (B, T, nh, hs) with transpose, reassemle in memory, (B,T,C) makes nh*hs = n_embd (C)\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        # output projection: additional learnable transformation\n        y = self.c_proj(y) # (B, T, C)@(C, C) = (B, T, C)\n        return y"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Copy over the Hugging Face GPT-2 model parameters into our model",
    "text": "Copy over the Hugging Face GPT-2 model parameters into our model\n\nIgnore the attention mask buffers (these are not parameters)\nThe weights in Hugging Face version are transposed (as they are in the original TensorFlow implementation) from what PyTorch needs because they use Conv1D module. Since we want to use plan nn.Linear, we hardcode these and transpose them.\n\n\nclass GPT(nn.Module):\n    ...\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type) # HF GPT2LMHeadModel has .from_pretrained method, just like ours\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#forward",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#forward",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Forward",
    "text": "Forward\n\nThe nn.Embedding layer in PyTorch takes an input tensor of arbitrary shape with values in the range [0, vocab_size) and maps each integer in that range to a dense vector of size C (the embedding dimension). In our case, we have an input tensor idx of shape (B, T), where B is the batch size and T is the sequence length, then applying the embedding layer to idx will yield a tensor of shape (B, T, C). This is because each integer in idx is replaced with its corresponding embedding vector. Since the embedding vectors have C elements, this adds an extra dimension of size C to the output. So for every batch and every sequence position, an embedding of size C is constructed, resulting in an output of shape (B, T, C).\nAlso note that the position embedding is broadcast to the token embedding and the same position embedding vector is learned at (T,C) for every element in B. This works because the position embeddings are independent of the specific sequence it’s in, so it can be shared across all sequences in the batch.\n\n\nclass GPT2(nn.Module):\n    ...    \n    def forward(self, idx):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        return logits"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Generate next sequence predictions with weights from pretrained GPT2 model",
    "text": "Generate next sequence predictions with weights from pretrained GPT2 model\n\nThe goal is to get close to generations from the Hugging Face pipeline:\n\n\nfrom transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n\nTo generate next token predictions, get the next top 50 tokens so that the model does not deviate too much from likely tokens, and sample one token from this distribution.\nConcatenate the token obtained from sampling with input (or input plus previously sampled tokens) at each step.\nThe sampling will not match the Hugging Face generations exactly since there’s likely a parameter hiding in the pipeline that’s different, but will be sensible English.\n\n\nnum_return_sequences = 5\nmax_length = 30\nmodel = GPT.from_pretrained('gpt2')\nmodel.eval()\nmodel.to('cuda')\n\n# prefix tokens\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\ntokens = enc.encode(\"Hello I'm a language model, \")\nx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences,1).to('cuda') # (5,8) since sent. tokenized to 8 tokens\n\n# generate: with each loop iteration, generate one more token\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nwhile x.size(1) &lt; max_length:\n    with torch.no_grad():\n        logits = model(x) # (B,T,vocab_size)\n        logits = logits[:, -1, :]  # take the logits at the last position\n        probs = F.softmax(logits, dim=-1) # get the probabilities\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # get the top-50 tokens\n        ix = torch.multinomial(topk_probs, num_samples=1) # sample from the top 50\n        xcol = torch.gather(topk_indices, -1, ix) # select the indices of the sampled tokens\n        x = torch.cat((x, xcol), dim=1) # append the sampled token to the sequence\n\nfor i in range(num_return_sequences):\n    tokens = x[i,:max_length].tolist()\n    decoded = enc.decode(tokens)\n    print('&gt;',decoded)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#initialize-a-random-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#initialize-a-random-model",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Initialize a random model",
    "text": "Initialize a random model\n\nTo do this, simply replace the GPT model initialization as below:\n\n\n#model = GPT.from_pretrained('gpt2')\nmodel = GPT(GPTConfig())"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#autodetect-device",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#autodetect-device",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Autodetect device",
    "text": "Autodetect device\n\nDetect the most powerful device available and use it\n\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n    print(\"using GPU\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = 'mps'\n    print(\"using MPS\")\n\n\nNote that we guarded against device mismatch by initializing pos on the correct device.\n\n\npos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Load the Tiny Shakespeare data set for quick debugging and load a batch of data",
    "text": "Load the Tiny Shakespeare data set for quick debugging and load a batch of data\n\nAndrej’s favorite debugging dataset is Tiny Shakespeare which can be loaded and previewed as below:\n\n\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open(\"input.txt\", \"r\") as f:\n    text = f.read()\ndata = text[:1000]\nprint(data[:100])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\n\n\nTo get the corresponding GPT-2 tokens, load the tokenizer via tiktoken’s get_encoding method and encode the text data.\n\n\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(data)\nprint(tokens[:24])\n\n[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n\n\n\nAndrej’s favorite way to create a batch is to use view, which returns a new tensor with same data but different shape. It’s a view because the returned tensor shares the same underlying data with the original tensor and a change in one will affect the other.\nSince the desired outputs for every token in the (B,T) batch are just to the right of that token, extend the buffer by one element, take all but the last token as inputs and the first token onwards as outputs:\n\n\nimport torch\nbuf = torch.tensor(tokens[:24+1])\nx = buf[:-1].view(4,6)\ny = buf[1:].view(4,6)\nprint(x)\nprint(y)\n\ntensor([[ 5962, 22307,    25,   198,  8421,   356],\n        [ 5120,   597,  2252,    11,  3285,   502],\n        [ 2740,    13,   198,   198,  3237,    25],\n        [  198,  5248,   461,    11,  2740,    13]])\ntensor([[22307,    25,   198,  8421,   356,  5120],\n        [  597,  2252,    11,  3285,   502,  2740],\n        [   13,   198,   198,  3237,    25,   198],\n        [ 5248,   461,    11,  2740,    13,   198]])"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#update-the-forward-pass-to-calculate-the-loss",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#update-the-forward-pass-to-calculate-the-loss",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Update the forward pass to calculate the loss",
    "text": "Update the forward pass to calculate the loss\n\nPass in the optional targets and calculate cross entropy loss. Cross entropy loss in PyTorch expects (BT,vocab_size) logits and (BT,) targets, so reshape it with view.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def forward(self, idx, targets = None):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        # --- Added code ---\n        loss = None\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        if targets: \n            # F.cross_entropy expects (B, T, vocab_size)-&gt; (B*T, vocab_size) shapes for logits\n            # and (B*T,) shape for targets. \n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#optimizer-and-training-loop",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#optimizer-and-training-loop",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Optimizer and training loop",
    "text": "Optimizer and training loop\n\nSee the Andrew Ng’s videos for a review of momentum and RMSProp that compose the Adam optimizer.\nMake sure to zero the gradients since loss.backwards() always accumulates gradients.\n\noptimizer.step() will update the parameters to (ideally) decrease the loss.\n\nloss.item() will convert the loss to a float that’s placed on CPU.\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\nfor i in range(50):\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n\n\nRunning the training loop will yield the following output:\n\n\n\n\nSource: initial training, image by the author\n\n\n\nNote that when the weights are randomly initialized, we expect each token in 0-50256 range to be equally likely. Thus we expect loss to be around -ln(1/50257) = 10.82. Currently, the loss starts around this value, which is a good sanity check.\nAlso, we can make sure that the training is set up correctly by overfitting on a single batch. Running the training loop for 500 iterations on the same batch yields: iteration 499, loss = 0.0008159472490660846"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#data-loader-lite",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#data-loader-lite",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Data Loader Lite",
    "text": "Data Loader Lite\n\nTo build a simple data loader, advance by batches of size BT, set the input x* and output y as before, and loop around to the start of our tokens if run out of them:\n\n\nclass DataLoaderLite:\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = 0\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T\n        # if run out of tokens, loop around to zero\n        if self.current_position + B*T &gt;= len(self.tokens):\n            self.current_position = 0\n        return x, y\n\n\nUsing this data loader, loading and training can now be done concisely as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nNote that even with different batches of text loaded at every step, the loss still goes down quickly because many of the tokens will not occur in out data set and GPT2 can, for example, drive down the biases (mask) of all the logits that don’t occur in our data to negative infinity."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#parameter-sharing-via-weight-tying",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#parameter-sharing-via-weight-tying",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Parameter sharing via weight tying",
    "text": "Parameter sharing via weight tying\n\nObserve that transformer.wte.weight and lm_head.weight are of the same shape (50257*768).\nIn addition, we can below that these point to the same tensor.\n\n\n(sd_hf['lm_head.weight']==sd_hf['transformer.wte.weight']).all() # tensor(True)\nsd_hf['lm_head.weight'].data_ptr()==sd_hf['transformer.wte.weight'].data_ptr()\n\n\nThis weight tying scheme comes from p.5 of Attention is All You Need paper, which in turn used the work of Press and Wolf (2017). The idea is that if two tokens are very similar semantically, they should be close in the token embedding space. For the same reason, we expect these tokens to have similar probabilities in the output of a transformer. Press and Wolf (2017) argue that tying these weights leads to better performance. The weight scheme can be implemented in code as\n\n\nself.transformer.wte.weight = self.lm_head.weight\n\n\nNote that this weight sharing scheme also entails a significant memory savings of 38,597,376 parameters (768*50257) for the 124M model, which amounts to about 30% of the weights."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#model-initialization",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#model-initialization",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "Model initialization",
    "text": "Model initialization\n\nOpenAI initialized the weights with mean 0 and standard deviation with 0.02 and the bias with 0. They initialized the token embeddings with mean 0 and standard deviation with 0.02 and position embeddings with 0.01.\nFollowing Xavier initialization, the standard deviation should be 1/sqrt(in_features_for_layer), and with 768 to 1600 features used by GPT-2 models of different sizes, 0.02 is approximately correct. Ideally, though, one would want the standard deviation to scale down more precisely with the model size.\nIn code, this leads to\n\n\nclass GPT(nn.Module):\n    ...\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n    \n\n\nIn addition, observe that our Block’s forward method contains a residual stream:\n\n\nclass Block(nn.Module):\n    ...\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNot accounting for this in initialization would lead to exploding variance as seen from the following simple example:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += torch.randn(768)\nprint(x.std())\n\ntensor(10.0776)\n\n\n\nHowever, scaling it by 1/sqrt(N) approximately yields the original standard deviation of 1 as seen below:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += n**-.5*torch.randn(768)\nprint(x.std())\n\ntensor(0.9894)\n\n\n\nTo implement it in our transformer, add flags in the attention and mlp modules since these have residual connections:\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        ...\nclass MLP(nn.Module):\n    def __init__(self,config):\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n     \n\n\nThen in the GPT module, check if a module has this attribute and adjust the standard deviation accordingly. Note that 2 multiplies self.config.n_layers because the residual connection is used twice: once for attention and once for the mlp module.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                # number of residual layers is double self.config.n_layers\n                # one for attention, one for mlp\n                std *= (2*self.config.n_layers)**-0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std) \n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT2.html#references",
    "title": "Building the GPT-2 transformer in PyTorch",
    "section": "References",
    "text": "References\nPress, O., & Wolf, L. (2017). Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859v3.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762 .\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., & Liu, T. (2020). On Layer Normalization in the Transformer Architecture. arXiv preprint arXiv:2002.04745."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist/machine learning engineer experienced with Python, GCP, and LLMs. I love to build with code and LLMs.\n\n\n\nI hold graduate degress in math and economics,and I’m passionate about machine learning, programming, and LLMs. Currenly, I’m working on number of machine learning projects. From November 2022 to August 2024, I worked as a Data Analyst at LS Direct, where I performed data analytics and data engineering tasks for some of the company’s largest clients, helping the company quintuple the original budgets in some cases. I have developed both internal and client-facing automations using Python, SQL, and GCP, saving our team hours of time weekly. Whenever possible, I deeply favor programmatic solutions that streamline daily processes.\nPreviously, I worked at Lake Mary High School as an AP Computer Science teacher for two years, restarting the school’s AP Computer Science A program (object-oriented and algorithmic programming with Java) and being the founding advisor for LMHS AI Club, introducing advanced students to deep learning with PyTorch and Linux. Prior to that, I taught algebra to disadvantaged communities at Vanden High School.\nBefore that, I’ve completed an undergrad in math and three graduate programs, one in math and yes, two in economics (applied and mathematical), the latest from UC Davis. As a graduate student, I got to work on computer vision applications and to teach statistical methods, econometrics, and graduate economic theory. I have always been drawn to linguistic and computational aspects of the world, and in the past, I’ve learned numerous languages (being reasonably fluent at roughly 7 at one point). Since then I have shifted my passion to deep learning and computer languages, especially LLMs and Python.\nWhen I’m not at work, I’m either working away at LLMs, reading arXiv papers, following along with the fastai community, climbing, or hiking."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist/machine learning engineer experienced with Python, GCP, and LLMs. I love to build with code and LLMs."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I hold graduate degress in math and economics,and I’m passionate about machine learning, programming, and LLMs. Currenly, I’m working on number of machine learning projects. From November 2022 to August 2024, I worked as a Data Analyst at LS Direct, where I performed data analytics and data engineering tasks for some of the company’s largest clients, helping the company quintuple the original budgets in some cases. I have developed both internal and client-facing automations using Python, SQL, and GCP, saving our team hours of time weekly. Whenever possible, I deeply favor programmatic solutions that streamline daily processes.\nPreviously, I worked at Lake Mary High School as an AP Computer Science teacher for two years, restarting the school’s AP Computer Science A program (object-oriented and algorithmic programming with Java) and being the founding advisor for LMHS AI Club, introducing advanced students to deep learning with PyTorch and Linux. Prior to that, I taught algebra to disadvantaged communities at Vanden High School.\nBefore that, I’ve completed an undergrad in math and three graduate programs, one in math and yes, two in economics (applied and mathematical), the latest from UC Davis. As a graduate student, I got to work on computer vision applications and to teach statistical methods, econometrics, and graduate economic theory. I have always been drawn to linguistic and computational aspects of the world, and in the past, I’ve learned numerous languages (being reasonably fluent at roughly 7 at one point). Since then I have shifted my passion to deep learning and computer languages, especially LLMs and Python.\nWhen I’m not at work, I’m either working away at LLMs, reading arXiv papers, following along with the fastai community, climbing, or hiking."
  }
]