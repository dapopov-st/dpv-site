[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Dmitriy’s Projects",
    "section": "",
    "text": "Sample-efficient SQL fine-tuning\n\n\n\n\n\nSample-efficient LLM fine-tuning on a SQL database.\n\n\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-2 From Scratch to Tiny Stories\n\n\n\n\n\nBuilding the GPT-2 transformer from scratch\n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\narXiv LLMs Assistant\n\n\n\n\n\narXiv-llms-assistant is a ‘public’ version of a project submitted as part of an AI competition at LS Direct, demonstrating the use and evaluation of RAG with on-prem data as a way to capitalize on applications of gen AI to privacy-restricted domains.\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProject Bird\n\n\n\n\n\nGCP ETL project to pull data from eBird API 2.0 and display a map of recent bird observations in a given area - a small side project to practice GCP skills and see more birds.\n\n\n\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\n\nUnsupervised learning (TDA) project to detect housing market anomalies built with Zach Stone for Haystacks.ai.\n\n\n\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInvesting in student housing with ML\n\n\n\n\n\nKaggle-style supervised learning \n\n\n\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInventory classes with pytest and pdoc\n\n\n\n\n\nInventory application for a tech video channel featuring computer builds with inventory classes in object-oriented Python, 90%+ coverage tests with pytest, comprehensive API documentation with pdoc, and packaging inside Docker.\n\n\n\n\n\nMar 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCOMPARATIVE STATICS OF A MONOPOLISTIC FIRM FACING RATE-OF-RETURN AND COMMAND-AND-CONTROL POLLUTION CONSTRAINTS\n\n\n\n\n\nA paper I co-authored in mathematical economics/economic theory.\n\n\n\n\n\nMar 5, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "backups/investing-in-student-housing/investing-in-student-housing.html",
    "href": "backups/investing-in-student-housing/investing-in-student-housing.html",
    "title": "Investing in student housing with ML",
    "section": "",
    "text": "Source: Image by the author\n\n\n\nGitHub link"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "The following is Part 2 of my extended write-up on building and training GPT-2 from scratch following along with “Let’s reproduce GPT-2 (124M)” by Karpathy. In this part, I discuss the optimizations that speed up training first on one, then multiple GPUs, also adding more detail where I felt necessary for my own understanding (hoping this will be helpful to others).\n\n\n\n\n\n\nSource: GPU utilization on a 3090 with batch size of 10, image by the author\n\n\n\nEach iteration would take about 720ms with batch size of 10 on a 3090 (vs 1000 on A100 with double the batch size!), so I expect my single-GPU training to be a bit slower than Andrej’s. With Andrej’s A100, it’s possible to process one epoch in 20 batches of size 16 with about 1000ms/batch, whereas with a 3090, it’s possible to process one epoch in 33 batches of size 10 with 730ms/batch, so about 20% slower than Andrej’s time (33x730/20x1000).\nAlso, Andrej suggests using nice numbers that have lots of multiples of 2. I tried a batch size of 8, but in this specific use case, I found that using batch size of 10 vs 8 is slightly faster per epoch on a 3090.\nAlso, to properly time the code, it’s important to wait until the GPU processes a batch before the CPU times the work. It’s possible that the CPU has loaded a batch onto GPU, GPU is still processing it, and the CPU has moved on to record the end time for the operation. To prevent this from happening, use torch.cuda.synchronize():\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    #import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {i}, loss = {loss.item()}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nWe may change the batch size, so to get a more objective measure of training speed, look at tokens/second, which is 16.3K on an A100 and 14.1K on a 3090.\n\n\n\n\n\nFirst observe that we can check the type of the logits in our model by starting an interactive debugging session as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nWhich yields\n\n\n(InteractiveConsole)\n&gt;&gt;&gt; logits.dtype\ntorch.float32\n\n\nPyTorch supports up to FP64 precision, which is useful for scientific computing applications, but deep learning training can tolerate significantly lower precision than default FP32, however.\nFrom A100 spec sheet, for example, it’s possible to get 16x performance improvement by going down from FP32 to FP16 Tensor Core.\nFor deep learning, sparsity feature is not currently used, so disregard the second number in a cell when it’s present.\nWe do not use INT8 for training since it implies a uniform distribution and we need a normal distribution provided by the float data types. INT8 is used for inference, however.\nThe memory bandwidth of an A100 is 1935GB/s, and most well-tuned application are bound more by memory than by speed. Lowering precision means tensors will take less space in memory, making it easier to satisfy the memory bandwidth constraint.\n\nTo summarize, by lowering precision, “we can store more and access it faster” (Andrej).\n\n\n\n\nSource: https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\n\n\n\n\n\n\n\n\nSource: https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/\n\n\n\nA Tensor Core is an instruction in a given architecture, in the case above for a 4x4 multiply and add operations. Any time we have multiply (and less importantly add) operations, which compose the majority of our GPT-2 transformer, these operations will be performed using Tensor Cores. For example, the classifier head matrix multiply going from 768 to 50257 dominates the computations in GPT-2.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith FP32 (torch.float32 default of PyTorch), both the input operands and the intermediate add/multiplies that compose individual elements of the result matrix are done in FP32.\nWe could switch to TF32, however, which uses the full 32 bits of FP32 for accumulator but just 19 bits for input operands due to lower number of mantissa bits as seen below:\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith TF32, we get an 8x speedup without needing to modify the code. The outputs will still be in FP32 as seen below. If our application can tolerate a little bit of imprecision, TF32 is a great option. In practice,the difference between FP32 and TF32 is almost imperceptible.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nOn my 24GB 3090, I was able to use batch size of 8 (or 10 max) with sequence length of 1024 (Andrej got away with batch size of 16 on 40GB A100):\n\n\n\n\n\nTo enable TF32 in PyTorch, change the float32 matmul precision from it’s default ‘highest’ to ‘high’ with torch.set_float32_matmul_precision(‘high’):\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n...\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n   ...\n\n\nOn an A100, throughput increases from 16.1K tokens/second to about 49K tokens/second, so about a 3X speedup.\nOn 3090, this leads to only about 40% speedup from 14.1K tokens/second baseline to 19.6K tokens/second.\n\nWhile TF32 in principle offers an 8X speedup on an A100 (I couldn’t find reliable official estimates for 3090 TF32 tensor), a lot of these workloads are memory bound. Thus although a matrix multiplication could potentially happen 8X faster with TF32 compared to FP32, the output numbers are still FP32, and these get moved around through the memory system at a speed that’s much slower than the GPU’s ability to perform the calculations.\n\n\n\n\n\nLet’s review the following once again\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nTo address the memory bandwidth constraint, we can use Bfloat16 (BF16) to more aggressively crop the mantissa without changing the sign and exponent (range) of the number.\n\nOriginally, FP16 was used, but this number format has a reduced range, causing issues that were patched by gradient scalers and similar solutions that introduced additional state and complexity. BF16 addresses these problems by preserving the original range of number.\nAndrej recommends studying the torch.autocast portion of mixed precision documentation, which has a context manager torch.autocast. This context manager is recommended around forward pass and loss calculation of the model only.\nWe thus only need to add one line of code as below:\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n    loss.backward()\n    ...\n\n\nUsing an interactive console breakpoint\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        import code; code.interact(local=locals())\n\n    loss.backward()\n    ...\n\n\nWe see that logits.dtype is indeed torch.bfloat16. Our weights remain in FP32, however, as model.transformer.wte.weight.dtype is torch.float32. This implies a mixed precision: PyTorch is keeping certain weights in full precision while converting others to bfloat16. What gets converted at what point is not exactly clear, but the general guidelines are below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/amp.html\n\n\n\nThus matrix multiplications, addition, etc. get converted while layer norms, softmax, etc. do not since they are less robust to precision changes.\nOn an A100, our previous benchmark is 50K tokens/second and it goes up to 55K tokens/second with bfloat16, about a 10% speedup.\nOn a 3090, our previous benchmark is 19.6K tokens/second and it goes up to 27.5K tokens/second with bfloat16, about a 40% speedup, suggesting that 3090 was perhaps more memory-bound than the A100.\n\n\n\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# torch.compile only needs a single line\nmodel = torch.compile(model)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n\n\nThe first iteration with torch.compile will typically be slower since PyTorch takes time to compile the code at the start.\nOn an A100 with batch size of 16, Andrej observed a 2.3x improvement from using torch.compile.\nOn a 3090 with batch size of 10, no improvement was seen during the first run (27.4 K tokens/second -&gt; 26.8K tokens/second). However, switching down to batch size of 8 yielded a significant increase the first time around. However, when I repeated the experiment, torch.compile ended up being faster with batch size of 10 again! This could have been due to graph breaks or kernel launch overhead.\n\nTaking the best runtime with torch.compile and batch size of 10, 27.4 K -&gt; 41.7 K tokens/second, or about 48% speedup over mixed precision benchmark.\n\n\n\n\nSource: initial training dynamics, image by the author\n\n\n\ntorch.compile does the following:\n\nTake out the Python interpreter from the forward pass and compile the neural net as a single object\nReduce GPU read/writes as demonstrated in the following example. Suppose we didn’t use torch.compile and instead of nn.GELU in our MLP, we used our own custom implementation below:\n\nclass TanhGELU(nn.Module):\ndef forward(self, x):\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi)*(x + 0.044715 * torch.pow(x, 3.0))))\n\nLooking at the following diagram, the input would first need to be placed from the High-Bandwidth Memory (HBM) onto the GPU for the torch.pow(x, 3.0) computation. Once the results are computed, they would be sent back to HBM. Next the computed result would be sent back to be multiplied by 0.044715 and so on. This would results many reads and writes to and from GPU and HBM, which are the bottleneck in many modern system.\nSince all the operations in the custom TanhGELU are element-wise operations, torch.compile can move the input to GPU and, for every single element, perform all the operations while the memory is on the GPU, then write back a single time. This is an example of kernel fusion, a major way in which torch.compile creates a speedup.\n\n\n\n\n\ngpu-cpu\n\n\n\nIn addition, when the data is moved to GPU for element-wise operations, it will need intermediate memory, a small amount of which is found on the GPU. Andrej points out that on the GPU chip itself, there is L2 cache. On the streaming multiprocessors (SMs) that do the calculation, there’s L1 memory and registers. These use SRAM for fast access times and low power consumption vs transistors and capacitors implementation of HBM. Below is a typical diagram of memory and associated access speeds from Dao et al. (2022), Figure 1:\n\n\n\n\nmemory-hierarchy\n\n\n\nWith mixed precision and torch.compile optimizations, Andrej’s training on an A100 is about 3X faster than training on a 3090, with 125K tokens/second processed on an A100 and 41.5K tokens/second on a 3090.\n\n\n\n\n\nThe attention operation is currently composed of the four lines of code highlighted below:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nFlashAttention is a kernel fusion operation that torch.compile cannot find currently. This is because kernel fusion requires an algorithmic rewrite of how attention is currently implemented. While FlashAttention requires more FLOPs, it ends up being significantly faster because of its judicious use of the memory hierarchy, which leads to fewer read/writes between GPU and HBM.\n\nIn particular, the NxN attention matrix is never read from/written to HBM. It’s simple to perform the matrix multiplications in a streaming manner, but computing softmax this way is more of a challenge. The crux of the algorithmic rewrite is the online softmax trick, which incrementally computes the softmax without needing all the inputs as is customary for standard softmax normalization. The key insight is that the softmax function can be broken down into smaller chunks, computed independently on GPU, and then combined using a ‘softmax trick’ formula. This enables FlashAttention to process long sequences more efficiently, without having to materialize the entire attention matrix at once.\n\nIn more detail, from p.5 of Dao et al. (2022), the softmax trick uses intermediate m and l variables that combine the statistics from individual ‘attention submatrices’ to reconstruct the softmax for the entire attention matrix.\n\nWith FlashAttention in place, the four lines above become one line in the following code:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        y = F.scaled_dot_product_attention(q,k,v, is_causal=True)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nThe loss after 50 iterations is identical to what it was before since FlashAttention is exact attention, not an approximation, so identical operations are performed. However, on a 3090, with FlashAttention added, about 50K tokens/second can be processed compared 41.7K tokens/second with just mixed precision+torch.compile, hence about a 20% speedup. On an A100, Andrej obtained about a 27% improvement.\n\n\n\n\n\nWe’re looking for the numbers in our transformer to have as many powers of two as possible since GPU kernels work best with these. In particular, vocab_size is 50257, which is not odd; one simple way to change this is to round it up to 50304, which divides by 128 (2**7). This adds ‘fake’ tokens and increases the amount of computation, yet by working better with the GPU, it would generally lead to a speedup in training.\nNote that the vocab_size appears in the embedding and the classifier layer, and the extra tokens added there would never be used. The weight sharing implies that the network has to learn that the logits associated with the extra added rows need to be driven to zero, which is similar to how it is already learning to drive down weights for tokens not encountered in our training set.\nOn an A100, 4% improvement was observed; on a 3090, the training speed increased from 50K tokens/second to about 53 tokens/second, so about a 6% increase.\nAndrej elaborates on the reason for the speedup. Many of the CUDA kernels use block tiles, which are usually ‘nice numbers’ with many powers of 6, so calculations are done in powers of 64 or 32, for example. When the desired calculation does not neatly fit into these block tiles, less efficient boundary kernels come in to do the last part at the second pass.\nOptimizations targeted at GPT-2 specifically follow next.\n\n\n\n\n\nThe idea with gradient clipping is that it’s possible to get a particularly ‘unlucky’ batch during training, which would lead to high loss and gradient, leading to training instability (or ‘shock’, as Andrej puts it).\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n\n\n\n\nSource: training dynamics with grad norm clipping, image by the author\n\n\n\nNote that the norm is still high at the start of the training. Since the weights are randomly initialized, a lot of learning happens to (generally) drive down the biases of the output tokens. The norm then stabilizes to be around 1.0.\n\n\n\n\n\nThe learning rate scheduler used is cosine decay learning rate scheduler.\n\nThere is a linear warmup step as it’s beneficial to have a low learning rate at the start since the network is randomly initialized.\nAfter that, the cosine decay takes place and the learning rate is gradually reduced to 10% of its original value.\nTaking the values for warmup steps and maximum number of steps from the GPT-3 paper (GPT-2 was scarce on training details), we see the following graph of learning rates:\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 375\nmax_steps = 2600\ndef get_lr(it):\n    # 1.) linear warmup for warmup_iters steps\n    if it &lt; warmup_steps:\n        return max_lr * (it+1)/warmup_steps\n    # 2.) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; max_steps:\n        return min_lr\n    # 3.) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps)/(max_steps-warmup_steps)\n    assert 0&lt;=decay_ratio&lt;=1\n    coeff = 0.5*(1.0 + math.cos(math.pi * decay_ratio)) # coef starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr-min_lr)\n\nits = np.arange(0,max_steps)\nget_lr_vectorized = np.vectorize(get_lr)\nlrs = get_lr_vectorized(its)\nplt.plot(its,lrs)\nplt.title('Cosine Learning Rate Decay')\nplt.xlabel(\"Number of tokens processed by GPT-2 in millions\")\n\nText(0.5, 0, 'Number of tokens processed by GPT-2 in millions')\n\n\n\n\n\n\n\n\n\n\nI chose to express the x-axis in tokens rather than steps to reduce the focus on hardware-related configurations used to train GPT-2 (eg, batch size), but the shape would be identical.\n\nThe learning rate scheduling can be added to the training loop in the code below:\n\n\n# ----------------- Training loop -----------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor step in range(50):\n    ...\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    ...\n\n\nNote that as part of setting the learning rate, we need to iterate through the parameter groups (of which there’s only one) and set the ‘lr’ parameter to our learning rate. This is a bit clunky, but Andrej’s impression is that it’s the way to do this in PyTorch currently. An alternative would be to use an off-the-shelf learning rates scheduler from PyTorch, but that’s an additional abstraction, and get_lr is composed of only a few lines of highly readable code.\nAndrej points out that the choice of a learning rate scheduler is up to the user and determining the ‘best’ one is an active area or research.\nIn addition, Andrej skips the gradual batch size increase used by GPT-2 since it complicates the arithmetic of the number of tokens used at each step in the optimization and it’s not a major improvement.\n\n\n\n\n\nDecaying weights prevents any weight from getting too large, forcing the network to distribute the work.\nInstead of iterating over all the parameter updates, the kernels (computations) used for the AdamW update can be fused into a single kernel in more recent versions of PyTorch.\n\n\nclass GPT(nn.Module):\n    ...\n    \n    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n        # start with all of the candidate parameters that require grad\n        param_dict = {pn: p for pn,p in self.named_parameters()}\n        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n        # create optim groups: any 2D parameter weight decays, others don't:\n        # weight tensors in matmuls + embeddings will decay, biases and layernorms will not\n        decay_params = [p for n,p in param_dict.items() if p.dim()&gt;=2]\n        nodecay_params = [p for n,p in param_dict.items() if p.dim()&lt;2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decay param tensors: {len(decay_params)} with {num_decay_params} parameters\")\n        print(f\"num non-decay param tensors: {len(nodecay_params)} with {num_nodecay_params} parameters\")\n\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        print(f'using fused AdamW: {use_fused}')\n        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas=(0.9, 0.95), eps = 1e-8, fused=use_fused)\n        return optimizer\n# In the training loop replace the optimizer below:\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    ...\n\n\nAmong the output, we will see num decay param tensors: 50 with 124354560 parameters, num non-decay param tensors: 98 with 121344 parameters. Hence most of the parameters will be weight decayed.\nOn an A100, Andrej observed about a 3% speedup due to the fused optimizer, while I saw about a 2% speedup on a 3090.\n\n\n\n\n\nThe relationship between weight decay, batch size, and learning rate is quite complex. In GPT-2, smaller batch size and bigger learning rate was likely used for smaller neural networks and larger batch size with smaller learning rate for larger ones. For the smallest network size, OpenAI used a batch size of 0.5M tokens. With 1024 tokens/batch, this implies a batch size of about 500. However, we would still like to use the effective batch size of 0.5M since it’s related to other hyperparameters determined by OpenAI. A solution to this is gradient accumulation, which allows us to simulate in a serial way any arbitrary batch size of our choice.\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090; switching down 10-&gt;8 since speeds are comparable and it factors evenly into total_batch_size above\nT = 1024\nassert total_batch_size % (B*T) == 0, 'make sure total_batch_size is divisible by B*T'\ngrad_accum_steps = total_batch_size//(B*T)\nprint(f\"total desired batch size: {total_batch_size}\")\nprint(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\ntotal desired batch size: 524288\n=&gt; calculated gradient accumulation: 64\n\n\n\nThus we’ll need to do 64 forward backward steps followed by a single gradient update. For context, our current training loop is\n\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        #import code; code.interact(local=locals())\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {step}, loss = {loss.item()}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nTo replicate the effective batch size of around 0.5M, we can accumulate gradients using an inner for loop, track accumulated loss to print it correctly, and adjust the tokens per second calculation to account for the number of gradient accumulation steps:\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    optimizer.zero_grad()\n    loss_accum = 0.0 # need this  to correctly print the accumulated loss\n    for micro_step in range(grad_accum_steps):\n        x,y = train_loader.next_batch()\n        x,y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n            logits, loss = model(x,y)\n        loss = loss/grad_accum_steps\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        loss.backward() # deposits gradients by default\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps)/(t1-t0) # don't forget to adjust the tokens/second calculation \n    print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nNote in particular that the loss needs to be divided by grad_accum_steps. This is because the accumulator in F.cross_entropy loss is mean, whereas the loss within each grad_accum_steps loop does not account for averaging with the grad_accum_steps. To make this more concrete, without the gradient accumulation, the loss in the following simple regression example would include the division by 4:\n\n\nimport torch\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(16,32),\n    torch.nn.GELU(),\n    torch.nn.Linear(32,1)\n)\ntorch.random.manual_seed(42)\nx = torch.randn(4,16)\ny = torch.randn(4,1)\nnet.zero_grad()\nyhat = net(x)\nloss = torch.nn.functional.mse_loss(yhat,y)\nloss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n\n# The loss objective, due to reduction='mean' is\n# L = 1/4*[\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nIf we compute the loss by adding the losses of the individual examples, we omit division by 4:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0598,  0.0042,  0.0167, -0.0161,  0.0235, -0.0320, -0.0311, -0.0550,\n        -0.0410, -0.0536])\n\n\n\nClearly, the gradients are not the same. To fix this, divide the loss by number of steps:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss = loss/4\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (1/4)*(y[0]-yhat[0])**2 +\n#     (1/4)*(y[1]-yhat[1])**2 +\n#     (1/4)*(y[2]-yhat[2])**2 +\n#     (1/4)*(y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nAlso note that casting our training loop in terms of effective batch size, we can adjust the size of our micro batch without changing the training dynamics: micro batch size is purely a performance optimization setting.\n\n\n\n\n\nWith DDP, we’ll have one process per GPU available. These will have the same model loaded but will be processing slightly different parts of the data. They’ll contribute their own gradients that will be averaged. DDP can be run with torchrun, which will correctly set the necessary environmental variables and launch distributed training.\n\n\n\n\nWhere we were before initializing device, we now also initialize distributed training as follows:\n\n\n# ----------------- Run distributed training -----------------\nfrom torch.distributed import init_process_group, destroy_process_group\nddp = int(os.environ.get('RANK',-1)) != -1 # is this a ddp run?\nif ddp:\n    assert torch.cuda.is_available()\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ.get(\"RANK\"))\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank==0\n    print(\"RANKS: \",ddp_rank,ddp_local_rank,ddp_world_size,device)\nelse:\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    device = f\"cuda: {ddp_local_rank}\"\n    master_process = True\n    device = 'cpu'\n    if torch.cuda.is_available():\n        device = 'cuda'\n        print(\"using GPU\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        device = 'mps'\n        print(\"using MPS\")\n\n\nThere is a RANK variable assigned to each GPU (GPU 0 will have RANK of 0, GPU 1 will have RANK of 1, etc). Each of these processes will run on different parts of the data. LOCAL_RANK is only used in multi-node setting and represents the rank of a GPU on a single node. WORLD_SIZE is the total number of GPUs available.\n\n\n\n\n\nSince each GPU will need to run on a slightly different subset of our data, adjust the batch size and gradient accumulation as follows:\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090\nT = 1024\nassert total_batch_size % (B*T*ddp_world_size) == 0, 'make sure total_batch_size is divisible by B*T*ddp_world_size'\ngrad_accum_steps = total_batch_size//(B*T*ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\n\nAlso, print on master process only to avoid duplicates prints arising from other processes.\n\n\n\n\nTo run on 2 GPUs, we can use torchrun –standalone –nproc_per_node=2 train_gpt2.py  (adjust 2 to the number of available GPUs).\n\n\n\n\n# ----------------- Data loader lite -----------------\nimport tiktoken\nclass DataLoaderLite:\n    def __init__(self, B, T, process_rank, num_processes): #1\n        self.B = B\n        self.T = T\n        self.process_rank = process_rank                   #2\n        self.num_processes = num_processes                 #3\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = self.B*self.T*self.process_rank\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T*self.num_processes                            #4\n        # if run out of tokens, loop around to zero\n        if self.current_position + (B*T*self.num_processes+1) &gt;= len(self.tokens): #5\n            self.current_position = self.B*self.T*self.process_rank                #6\n        return x, y\n...\ntrain_loader = DataLoaderLite(B=B,T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n\n\nThe dataloader should now advance by B x T x num_processes so that each process (GPU) gets its own batch. To accomplish this, modify/add lines commented with #1-#6.\n\n\n\n\n\nCurrently, identical model copies are initialized and compiled on each of the GPUs since each GPU is passes the same seed. In addition wrap the compiled model in a DDP container as follows:\n\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n...\nmodel = torch.compile(model)\nif ddp:\n    model = DDP(model,device_ids=[ddp_local_rank])\n\n\nIn essence, DDP allows communication between the GPUs by averaging and synching the gradients. The forward pass will be identical, but once the backward pass is over, DDP will call allreduce to get the average across all the GPUs and deposit the average on every single GPU. In addition, DDP efficiently communicates the gradients while the backward pass is happening.\nIn addition, we should configure our optimizer using the ‘raw’ model that’s not wrapped in the DDP process:\n\n\nraw_model = model.module if ddp else model # always contains the unwrapped model\n...\noptimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\n\n\n\n\n\nIn addition, we only need the average of the gradients at the last of the gradient accumulation steps. The sanctioned way to do this is with ddp.no_sync context manager. Andrej notes that the context manager simply toggles the require_backward_grad_sync variable, noting that this is a risky move in case this variable disappears/gets renamed in future PyTorch versions.\n\n\nimport torch.distributed as dist\n...\n# ----------------- Training loop -----------------\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(max_steps):\n    ...\n    for micro_step in range(grad_accum_steps):\n        ...\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        if ddp: \n            model.require_backward_grad_sync = (micro_step==grad_accum_steps-1)\n        loss.backward() # deposits gradients by default\n    if ddp:\n        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps*ddp_world_size)/(t1-t0)\n    if master_process:\n        print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\nif ddp:\n    destroy_process_group()\n\n\nAlso, we want the average of all the losses from all the GPUs to be printed by the master process. The loss_accum tensor exists on all the ranks. Calling dist.all_reduce after the inner micro batch loop computes the average of loss_accums and deposits it on all the ranks. When we then print on the master process, the loss_accum that’s identical on all the ranks is printed.\n\nWe should also destroy the process group after the training is done to free up resources.\n\n\n\n\n\nAlso, I got an error regarding device_type first time I ran the training. Andrej noted in is code “added after the video, pytorch can be serious about it’s device vs. device_type distinction”. This can be addressed as follows:\n\n\n# add after ddp init code\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n# set device type to device_type in the training loop\nwith torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n    logits, loss = model(x,y)\n\n\nWe get the following training dynamics on two 3090s:\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 8, image by the author.png\n\n\n\nThe loss is going down smoothly and about 100K tokens/second are processed. This is far short of 1.5M tokens/second that can be processed on 8xA100’s, of course, but it still looks like training GPT-2 locally should be feasible. Andrej later mentions that the ‘full’ trained GPT-2 can be reproduced in his setup in about an hour, so it would take around 15 hours on two 3090s.\n\nIn addition, Andrej mentioned that our current setup fixes the effective batch size, leaving the size of the micro batch as a tunable parameter. While the following choice slightly violates the “nice numbers” principle, I think it’s worth considering: Adjusting the effective batch to 512K and batch size to 10 so that micro batches with sequence length of 1024 evenly fit into the effective batch size, I could get the training to speed up to ~110K tokens/second, implying that the training time can be shortened to around 14 hours.\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 10, image by the author.png\n\n\n\n\n\n\nDao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv preprint arXiv:2205.14135v2 (2022)."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#establishing-a-timing-baseline",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#establishing-a-timing-baseline",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "Source: GPU utilization on a 3090 with batch size of 10, image by the author\n\n\n\nEach iteration would take about 720ms with batch size of 10 on a 3090 (vs 1000 on A100 with double the batch size!), so I expect my single-GPU training to be a bit slower than Andrej’s. With Andrej’s A100, it’s possible to process one epoch in 20 batches of size 16 with about 1000ms/batch, whereas with a 3090, it’s possible to process one epoch in 33 batches of size 10 with 730ms/batch, so about 20% slower than Andrej’s time (33x730/20x1000).\nAlso, Andrej suggests using nice numbers that have lots of multiples of 2. I tried a batch size of 8, but in this specific use case, I found that using batch size of 10 vs 8 is slightly faster per epoch on a 3090.\nAlso, to properly time the code, it’s important to wait until the GPU processes a batch before the CPU times the work. It’s possible that the CPU has loaded a batch onto GPU, GPU is still processing it, and the CPU has moved on to record the end time for the operation. To prevent this from happening, use torch.cuda.synchronize():\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    #import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {i}, loss = {loss.item()}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nWe may change the batch size, so to get a more objective measure of training speed, look at tokens/second, which is 16.3K on an A100 and 14.1K on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#mixed-precision-logic",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#mixed-precision-logic",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "First observe that we can check the type of the logits in our model by starting an interactive debugging session as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    import code; code.interact(local=locals())\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nWhich yields\n\n\n(InteractiveConsole)\n&gt;&gt;&gt; logits.dtype\ntorch.float32\n\n\nPyTorch supports up to FP64 precision, which is useful for scientific computing applications, but deep learning training can tolerate significantly lower precision than default FP32, however.\nFrom A100 spec sheet, for example, it’s possible to get 16x performance improvement by going down from FP32 to FP16 Tensor Core.\nFor deep learning, sparsity feature is not currently used, so disregard the second number in a cell when it’s present.\nWe do not use INT8 for training since it implies a uniform distribution and we need a normal distribution provided by the float data types. INT8 is used for inference, however.\nThe memory bandwidth of an A100 is 1935GB/s, and most well-tuned application are bound more by memory than by speed. Lowering precision means tensors will take less space in memory, making it easier to satisfy the memory bandwidth constraint.\n\nTo summarize, by lowering precision, “we can store more and access it faster” (Andrej).\n\n\n\n\nSource: https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#tensor-cores",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#tensor-cores",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "Source: https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/\n\n\n\nA Tensor Core is an instruction in a given architecture, in the case above for a 4x4 multiply and add operations. Any time we have multiply (and less importantly add) operations, which compose the majority of our GPT-2 transformer, these operations will be performed using Tensor Cores. For example, the classifier head matrix multiply going from 768 to 50257 dominates the computations in GPT-2.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith FP32 (torch.float32 default of PyTorch), both the input operands and the intermediate add/multiplies that compose individual elements of the result matrix are done in FP32.\nWe could switch to TF32, however, which uses the full 32 bits of FP32 for accumulator but just 19 bits for input operands due to lower number of mantissa bits as seen below:\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nWith TF32, we get an 8x speedup without needing to modify the code. The outputs will still be in FP32 as seen below. If our application can tolerate a little bit of imprecision, TF32 is a great option. In practice,the difference between FP32 and TF32 is almost imperceptible.\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nOn my 24GB 3090, I was able to use batch size of 8 (or 10 max) with sequence length of 1024 (Andrej got away with batch size of 16 on 40GB A100):"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#compute-the-speedup-with-tf32",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#compute-the-speedup-with-tf32",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "To enable TF32 in PyTorch, change the float32 matmul precision from it’s default ‘highest’ to ‘high’ with torch.set_float32_matmul_precision(‘high’):\n\n\ntrain_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n...\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n   ...\n\n\nOn an A100, throughput increases from 16.1K tokens/second to about 49K tokens/second, so about a 3X speedup.\nOn 3090, this leads to only about 40% speedup from 14.1K tokens/second baseline to 19.6K tokens/second.\n\nWhile TF32 in principle offers an 8X speedup on an A100 (I couldn’t find reliable official estimates for 3090 TF32 tensor), a lot of these workloads are memory bound. Thus although a matrix multiplication could potentially happen 8X faster with TF32 compared to FP32, the output numbers are still FP32, and these get moved around through the memory system at a speed that’s much slower than the GPU’s ability to perform the calculations."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#bfloat16-to-reduce-memory-bandwidth-constraint",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#bfloat16-to-reduce-memory-bandwidth-constraint",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "Let’s review the following once again\n\n\n\n\nSource: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n\n\n\nTo address the memory bandwidth constraint, we can use Bfloat16 (BF16) to more aggressively crop the mantissa without changing the sign and exponent (range) of the number.\n\nOriginally, FP16 was used, but this number format has a reduced range, causing issues that were patched by gradient scalers and similar solutions that introduced additional state and complexity. BF16 addresses these problems by preserving the original range of number.\nAndrej recommends studying the torch.autocast portion of mixed precision documentation, which has a context manager torch.autocast. This context manager is recommended around forward pass and loss calculation of the model only.\nWe thus only need to add one line of code as below:\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n    loss.backward()\n    ...\n\n\nUsing an interactive console breakpoint\n\n\nfor i in range(50):\n    ...\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        import code; code.interact(local=locals())\n\n    loss.backward()\n    ...\n\n\nWe see that logits.dtype is indeed torch.bfloat16. Our weights remain in FP32, however, as model.transformer.wte.weight.dtype is torch.float32. This implies a mixed precision: PyTorch is keeping certain weights in full precision while converting others to bfloat16. What gets converted at what point is not exactly clear, but the general guidelines are below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/amp.html\n\n\n\nThus matrix multiplications, addition, etc. get converted while layer norms, softmax, etc. do not since they are less robust to precision changes.\nOn an A100, our previous benchmark is 50K tokens/second and it goes up to 55K tokens/second with bfloat16, about a 10% speedup.\nOn a 3090, our previous benchmark is 19.6K tokens/second and it goes up to 27.5K tokens/second with bfloat16, about a 40% speedup, suggesting that 3090 was perhaps more memory-bound than the A100."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#torch.compile-a-compiler-for-neural-networks",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#torch.compile-a-compiler-for-neural-networks",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "train_loader = DataLoaderLite(B=10,T=1024)\n\ntorch.set_float32_matmul_precision('high')\n\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# torch.compile only needs a single line\nmodel = torch.compile(model)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n\n\nThe first iteration with torch.compile will typically be slower since PyTorch takes time to compile the code at the start.\nOn an A100 with batch size of 16, Andrej observed a 2.3x improvement from using torch.compile.\nOn a 3090 with batch size of 10, no improvement was seen during the first run (27.4 K tokens/second -&gt; 26.8K tokens/second). However, switching down to batch size of 8 yielded a significant increase the first time around. However, when I repeated the experiment, torch.compile ended up being faster with batch size of 10 again! This could have been due to graph breaks or kernel launch overhead.\n\nTaking the best runtime with torch.compile and batch size of 10, 27.4 K -&gt; 41.7 K tokens/second, or about 48% speedup over mixed precision benchmark.\n\n\n\n\nSource: initial training dynamics, image by the author\n\n\n\ntorch.compile does the following:\n\nTake out the Python interpreter from the forward pass and compile the neural net as a single object\nReduce GPU read/writes as demonstrated in the following example. Suppose we didn’t use torch.compile and instead of nn.GELU in our MLP, we used our own custom implementation below:\n\nclass TanhGELU(nn.Module):\ndef forward(self, x):\n    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi)*(x + 0.044715 * torch.pow(x, 3.0))))\n\nLooking at the following diagram, the input would first need to be placed from the High-Bandwidth Memory (HBM) onto the GPU for the torch.pow(x, 3.0) computation. Once the results are computed, they would be sent back to HBM. Next the computed result would be sent back to be multiplied by 0.044715 and so on. This would results many reads and writes to and from GPU and HBM, which are the bottleneck in many modern system.\nSince all the operations in the custom TanhGELU are element-wise operations, torch.compile can move the input to GPU and, for every single element, perform all the operations while the memory is on the GPU, then write back a single time. This is an example of kernel fusion, a major way in which torch.compile creates a speedup.\n\n\n\n\n\ngpu-cpu\n\n\n\nIn addition, when the data is moved to GPU for element-wise operations, it will need intermediate memory, a small amount of which is found on the GPU. Andrej points out that on the GPU chip itself, there is L2 cache. On the streaming multiprocessors (SMs) that do the calculation, there’s L1 memory and registers. These use SRAM for fast access times and low power consumption vs transistors and capacitors implementation of HBM. Below is a typical diagram of memory and associated access speeds from Dao et al. (2022), Figure 1:\n\n\n\n\nmemory-hierarchy\n\n\n\nWith mixed precision and torch.compile optimizations, Andrej’s training on an A100 is about 3X faster than training on a 3090, with 125K tokens/second processed on an A100 and 41.5K tokens/second on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#flashattention",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#flashattention",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "The attention operation is currently composed of the four lines of code highlighted below:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nFlashAttention is a kernel fusion operation that torch.compile cannot find currently. This is because kernel fusion requires an algorithmic rewrite of how attention is currently implemented. While FlashAttention requires more FLOPs, it ends up being significantly faster because of its judicious use of the memory hierarchy, which leads to fewer read/writes between GPU and HBM.\n\nIn particular, the NxN attention matrix is never read from/written to HBM. It’s simple to perform the matrix multiplications in a streaming manner, but computing softmax this way is more of a challenge. The crux of the algorithmic rewrite is the online softmax trick, which incrementally computes the softmax without needing all the inputs as is customary for standard softmax normalization. The key insight is that the softmax function can be broken down into smaller chunks, computed independently on GPU, and then combined using a ‘softmax trick’ formula. This enables FlashAttention to process long sequences more efficiently, without having to materialize the entire attention matrix at once.\n\nIn more detail, from p.5 of Dao et al. (2022), the softmax trick uses intermediate m and l variables that combine the statistics from individual ‘attention submatrices’ to reconstruct the softmax for the entire attention matrix.\n\nWith FlashAttention in place, the four lines above become one line in the following code:\n\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n    def forward(self, x):\n        ...\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # -----Start: attention operation-----\n        y = F.scaled_dot_product_attention(q,k,v, is_causal=True)\n        # -----End: attention operation-----\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        ...\n\n\nThe loss after 50 iterations is identical to what it was before since FlashAttention is exact attention, not an approximation, so identical operations are performed. However, on a 3090, with FlashAttention added, about 50K tokens/second can be processed compared 41.7K tokens/second with just mixed precision+torch.compile, hence about a 20% speedup. On an A100, Andrej obtained about a 27% improvement."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#nice-and-ugly-numbers-vocab-size",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#nice-and-ugly-numbers-vocab-size",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "We’re looking for the numbers in our transformer to have as many powers of two as possible since GPU kernels work best with these. In particular, vocab_size is 50257, which is not odd; one simple way to change this is to round it up to 50304, which divides by 128 (2**7). This adds ‘fake’ tokens and increases the amount of computation, yet by working better with the GPU, it would generally lead to a speedup in training.\nNote that the vocab_size appears in the embedding and the classifier layer, and the extra tokens added there would never be used. The weight sharing implies that the network has to learn that the logits associated with the extra added rows need to be driven to zero, which is similar to how it is already learning to drive down weights for tokens not encountered in our training set.\nOn an A100, 4% improvement was observed; on a 3090, the training speed increased from 50K tokens/second to about 53 tokens/second, so about a 6% increase.\nAndrej elaborates on the reason for the speedup. Many of the CUDA kernels use block tiles, which are usually ‘nice numbers’ with many powers of 6, so calculations are done in powers of 64 or 32, for example. When the desired calculation does not neatly fit into these block tiles, less efficient boundary kernels come in to do the last part at the second pass.\nOptimizations targeted at GPT-2 specifically follow next."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#gradient-clipping",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#gradient-clipping",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "The idea with gradient clipping is that it’s possible to get a particularly ‘unlucky’ batch during training, which would lead to high loss and gradient, leading to training instability (or ‘shock’, as Andrej puts it).\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    ...\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n\n\n\n\nSource: training dynamics with grad norm clipping, image by the author\n\n\n\nNote that the norm is still high at the start of the training. Since the weights are randomly initialized, a lot of learning happens to (generally) drive down the biases of the output tokens. The norm then stabilizes to be around 1.0."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#learning-rate-scheduler",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#learning-rate-scheduler",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "The learning rate scheduler used is cosine decay learning rate scheduler.\n\nThere is a linear warmup step as it’s beneficial to have a low learning rate at the start since the network is randomly initialized.\nAfter that, the cosine decay takes place and the learning rate is gradually reduced to 10% of its original value.\nTaking the values for warmup steps and maximum number of steps from the GPT-3 paper (GPT-2 was scarce on training details), we see the following graph of learning rates:\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 375\nmax_steps = 2600\ndef get_lr(it):\n    # 1.) linear warmup for warmup_iters steps\n    if it &lt; warmup_steps:\n        return max_lr * (it+1)/warmup_steps\n    # 2.) if it &gt; lr_decay_iters, return min learning rate\n    if it &gt; max_steps:\n        return min_lr\n    # 3.) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps)/(max_steps-warmup_steps)\n    assert 0&lt;=decay_ratio&lt;=1\n    coeff = 0.5*(1.0 + math.cos(math.pi * decay_ratio)) # coef starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr-min_lr)\n\nits = np.arange(0,max_steps)\nget_lr_vectorized = np.vectorize(get_lr)\nlrs = get_lr_vectorized(its)\nplt.plot(its,lrs)\nplt.title('Cosine Learning Rate Decay')\nplt.xlabel(\"Number of tokens processed by GPT-2 in millions\")\n\nText(0.5, 0, 'Number of tokens processed by GPT-2 in millions')\n\n\n\n\n\n\n\n\n\n\nI chose to express the x-axis in tokens rather than steps to reduce the focus on hardware-related configurations used to train GPT-2 (eg, batch size), but the shape would be identical.\n\nThe learning rate scheduling can be added to the training loop in the code below:\n\n\n# ----------------- Training loop -----------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor step in range(50):\n    ...\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    ...\n\n\nNote that as part of setting the learning rate, we need to iterate through the parameter groups (of which there’s only one) and set the ‘lr’ parameter to our learning rate. This is a bit clunky, but Andrej’s impression is that it’s the way to do this in PyTorch currently. An alternative would be to use an off-the-shelf learning rates scheduler from PyTorch, but that’s an additional abstraction, and get_lr is composed of only a few lines of highly readable code.\nAndrej points out that the choice of a learning rate scheduler is up to the user and determining the ‘best’ one is an active area or research.\nIn addition, Andrej skips the gradual batch size increase used by GPT-2 since it complicates the arithmetic of the number of tokens used at each step in the optimization and it’s not a major improvement."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#weight-decay-and-fused-optimizer",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#weight-decay-and-fused-optimizer",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "Decaying weights prevents any weight from getting too large, forcing the network to distribute the work.\nInstead of iterating over all the parameter updates, the kernels (computations) used for the AdamW update can be fused into a single kernel in more recent versions of PyTorch.\n\n\nclass GPT(nn.Module):\n    ...\n    \n    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n        # start with all of the candidate parameters that require grad\n        param_dict = {pn: p for pn,p in self.named_parameters()}\n        param_dict = {pn: p for pn,p in param_dict.items() if p.requires_grad}\n        # create optim groups: any 2D parameter weight decays, others don't:\n        # weight tensors in matmuls + embeddings will decay, biases and layernorms will not\n        decay_params = [p for n,p in param_dict.items() if p.dim()&gt;=2]\n        nodecay_params = [p for n,p in param_dict.items() if p.dim()&lt;2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decay param tensors: {len(decay_params)} with {num_decay_params} parameters\")\n        print(f\"num non-decay param tensors: {len(nodecay_params)} with {num_nodecay_params} parameters\")\n\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        print(f'using fused AdamW: {use_fused}')\n        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas=(0.9, 0.95), eps = 1e-8, fused=use_fused)\n        return optimizer\n# In the training loop replace the optimizer below:\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    ...\n\n\nAmong the output, we will see num decay param tensors: 50 with 124354560 parameters, num non-decay param tensors: 98 with 121344 parameters. Hence most of the parameters will be weight decayed.\nOn an A100, Andrej observed about a 3% speedup due to the fused optimizer, while I saw about a 2% speedup on a 3090."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#gradient-accumulation",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#gradient-accumulation",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "The relationship between weight decay, batch size, and learning rate is quite complex. In GPT-2, smaller batch size and bigger learning rate was likely used for smaller neural networks and larger batch size with smaller learning rate for larger ones. For the smallest network size, OpenAI used a batch size of 0.5M tokens. With 1024 tokens/batch, this implies a batch size of about 500. However, we would still like to use the effective batch size of 0.5M since it’s related to other hyperparameters determined by OpenAI. A solution to this is gradient accumulation, which allows us to simulate in a serial way any arbitrary batch size of our choice.\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090; switching down 10-&gt;8 since speeds are comparable and it factors evenly into total_batch_size above\nT = 1024\nassert total_batch_size % (B*T) == 0, 'make sure total_batch_size is divisible by B*T'\ngrad_accum_steps = total_batch_size//(B*T)\nprint(f\"total desired batch size: {total_batch_size}\")\nprint(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\ntotal desired batch size: 524288\n=&gt; calculated gradient accumulation: 64\n\n\n\nThus we’ll need to do 64 forward backward steps followed by a single gradient update. For context, our current training loop is\n\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x,y)\n        #import code; code.interact(local=locals())\n    loss.backward()\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    # determine and set the learning rate for this iteration\n    lr = get_lr(step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    optimizer.step()\n    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n    t1= time.time()\n    dt = (t1-t0)*1000 # time diff in seconds\n    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n    print(f'iteration {step}, loss = {loss.item()}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nTo replicate the effective batch size of around 0.5M, we can accumulate gradients using an inner for loop, track accumulated loss to print it correctly, and adjust the tokens per second calculation to account for the number of gradient accumulation steps:\n\n\n# ----------------- Training loop -----------------\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(50):\n    t0 = time.time()\n    optimizer.zero_grad()\n    loss_accum = 0.0 # need this  to correctly print the accumulated loss\n    for micro_step in range(grad_accum_steps):\n        x,y = train_loader.next_batch()\n        x,y = x.to(device), y.to(device)\n        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n            logits, loss = model(x,y)\n        loss = loss/grad_accum_steps\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        loss.backward() # deposits gradients by default\n    norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps)/(t1-t0) # don't forget to adjust the tokens/second calculation \n    print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\n\n\nNote in particular that the loss needs to be divided by grad_accum_steps. This is because the accumulator in F.cross_entropy loss is mean, whereas the loss within each grad_accum_steps loop does not account for averaging with the grad_accum_steps. To make this more concrete, without the gradient accumulation, the loss in the following simple regression example would include the division by 4:\n\n\nimport torch\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(16,32),\n    torch.nn.GELU(),\n    torch.nn.Linear(32,1)\n)\ntorch.random.manual_seed(42)\nx = torch.randn(4,16)\ny = torch.randn(4,1)\nnet.zero_grad()\nyhat = net(x)\nloss = torch.nn.functional.mse_loss(yhat,y)\nloss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n\n# The loss objective, due to reduction='mean' is\n# L = 1/4*[\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nIf we compute the loss by adding the losses of the individual examples, we omit division by 4:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (y[0]-yhat[0])**2 +\n#     (y[1]-yhat[1])**2 +\n#     (y[2]-yhat[2])**2 +\n#     (y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0598,  0.0042,  0.0167, -0.0161,  0.0235, -0.0320, -0.0311, -0.0550,\n        -0.0410, -0.0536])\n\n\n\nClearly, the gradients are not the same. To fix this, divide the loss by number of steps:\n\n\nnet.zero_grad()\nfor i in range(4):\n    yhat = net(x[i])\n    loss = torch.nn.functional.mse_loss(yhat,y[i])\n    loss = loss/4\n    loss.backward()\nprint(net[0].weight.grad.view(-1)[:10])\n# The loss objective without reduction='mean' is\n# L = [\n#     (1/4)*(y[0]-yhat[0])**2 +\n#     (1/4)*(y[1]-yhat[1])**2 +\n#     (1/4)*(y[2]-yhat[2])**2 +\n#     (1/4)*(y[3]-yhat[3])**2 \n# ]\n\ntensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n        -0.0103, -0.0134])\n\n\n\nAlso note that casting our training loop in terms of effective batch size, we can adjust the size of our micro batch without changing the training dynamics: micro batch size is purely a performance optimization setting."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#distributed-data-parallel-ddp",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#distributed-data-parallel-ddp",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "With DDP, we’ll have one process per GPU available. These will have the same model loaded but will be processing slightly different parts of the data. They’ll contribute their own gradients that will be averaged. DDP can be run with torchrun, which will correctly set the necessary environmental variables and launch distributed training.\n\n\n\n\nWhere we were before initializing device, we now also initialize distributed training as follows:\n\n\n# ----------------- Run distributed training -----------------\nfrom torch.distributed import init_process_group, destroy_process_group\nddp = int(os.environ.get('RANK',-1)) != -1 # is this a ddp run?\nif ddp:\n    assert torch.cuda.is_available()\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ.get(\"RANK\"))\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f\"cuda:{ddp_local_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank==0\n    print(\"RANKS: \",ddp_rank,ddp_local_rank,ddp_world_size,device)\nelse:\n    ddp_rank = 0\n    ddp_local_rank = 0\n    ddp_world_size = 1\n    device = f\"cuda: {ddp_local_rank}\"\n    master_process = True\n    device = 'cpu'\n    if torch.cuda.is_available():\n        device = 'cuda'\n        print(\"using GPU\")\n    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n        device = 'mps'\n        print(\"using MPS\")\n\n\nThere is a RANK variable assigned to each GPU (GPU 0 will have RANK of 0, GPU 1 will have RANK of 1, etc). Each of these processes will run on different parts of the data. LOCAL_RANK is only used in multi-node setting and represents the rank of a GPU on a single node. WORLD_SIZE is the total number of GPUs available.\n\n\n\n\n\nSince each GPU will need to run on a slightly different subset of our data, adjust the batch size and gradient accumulation as follows:\n\n\n# ----------------- Batch size -----------------\ntotal_batch_size = 524288 # 2**19 ~ 0.5M\nB = 8 # micro batch size on a 3090\nT = 1024\nassert total_batch_size % (B*T*ddp_world_size) == 0, 'make sure total_batch_size is divisible by B*T*ddp_world_size'\ngrad_accum_steps = total_batch_size//(B*T*ddp_world_size)\nif master_process:\n    print(f\"total desired batch size: {total_batch_size}\")\n    print(f\"=&gt; calculated gradient accumulation: {grad_accum_steps}\")\n\n\nAlso, print on master process only to avoid duplicates prints arising from other processes.\n\n\n\n\nTo run on 2 GPUs, we can use torchrun –standalone –nproc_per_node=2 train_gpt2.py  (adjust 2 to the number of available GPUs).\n\n\n\n\n# ----------------- Data loader lite -----------------\nimport tiktoken\nclass DataLoaderLite:\n    def __init__(self, B, T, process_rank, num_processes): #1\n        self.B = B\n        self.T = T\n        self.process_rank = process_rank                   #2\n        self.num_processes = num_processes                 #3\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = self.B*self.T*self.process_rank\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T*self.num_processes                            #4\n        # if run out of tokens, loop around to zero\n        if self.current_position + (B*T*self.num_processes+1) &gt;= len(self.tokens): #5\n            self.current_position = self.B*self.T*self.process_rank                #6\n        return x, y\n...\ntrain_loader = DataLoaderLite(B=B,T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n\n\nThe dataloader should now advance by B x T x num_processes so that each process (GPU) gets its own batch. To accomplish this, modify/add lines commented with #1-#6.\n\n\n\n\n\nCurrently, identical model copies are initialized and compiled on each of the GPUs since each GPU is passes the same seed. In addition wrap the compiled model in a DDP container as follows:\n\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n...\nmodel = torch.compile(model)\nif ddp:\n    model = DDP(model,device_ids=[ddp_local_rank])\n\n\nIn essence, DDP allows communication between the GPUs by averaging and synching the gradients. The forward pass will be identical, but once the backward pass is over, DDP will call allreduce to get the average across all the GPUs and deposit the average on every single GPU. In addition, DDP efficiently communicates the gradients while the backward pass is happening.\nIn addition, we should configure our optimizer using the ‘raw’ model that’s not wrapped in the DDP process:\n\n\nraw_model = model.module if ddp else model # always contains the unwrapped model\n...\noptimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\n\n\n\n\n\nIn addition, we only need the average of the gradients at the last of the gradient accumulation steps. The sanctioned way to do this is with ddp.no_sync context manager. Andrej notes that the context manager simply toggles the require_backward_grad_sync variable, noting that this is a risky move in case this variable disappears/gets renamed in future PyTorch versions.\n\n\nimport torch.distributed as dist\n...\n# ----------------- Training loop -----------------\n#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\noptimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device)\nfor step in range(max_steps):\n    ...\n    for micro_step in range(grad_accum_steps):\n        ...\n        loss_accum += loss.detach() # don't need to backpropagate through accumulation process\n        if ddp: \n            model.require_backward_grad_sync = (micro_step==grad_accum_steps-1)\n        loss.backward() # deposits gradients by default\n    if ddp:\n        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n    ...\n    tokens_per_sec = (train_loader.B*train_loader.T*grad_accum_steps*ddp_world_size)/(t1-t0)\n    if master_process:\n        print(f'iteration {step}, loss = {loss_accum.item():.6f}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')\nif ddp:\n    destroy_process_group()\n\n\nAlso, we want the average of all the losses from all the GPUs to be printed by the master process. The loss_accum tensor exists on all the ranks. Calling dist.all_reduce after the inner micro batch loop computes the average of loss_accums and deposits it on all the ranks. When we then print on the master process, the loss_accum that’s identical on all the ranks is printed.\n\nWe should also destroy the process group after the training is done to free up resources.\n\n\n\n\n\nAlso, I got an error regarding device_type first time I ran the training. Andrej noted in is code “added after the video, pytorch can be serious about it’s device vs. device_type distinction”. This can be addressed as follows:\n\n\n# add after ddp init code\ndevice_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n# set device type to device_type in the training loop\nwith torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n    logits, loss = model(x,y)\n\n\nWe get the following training dynamics on two 3090s:\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 8, image by the author.png\n\n\n\nThe loss is going down smoothly and about 100K tokens/second are processed. This is far short of 1.5M tokens/second that can be processed on 8xA100’s, of course, but it still looks like training GPT-2 locally should be feasible. Andrej later mentions that the ‘full’ trained GPT-2 can be reproduced in his setup in about an hour, so it would take around 15 hours on two 3090s.\n\nIn addition, Andrej mentioned that our current setup fixes the effective batch size, leaving the size of the micro batch as a tunable parameter. While the following choice slightly violates the “nice numbers” principle, I think it’s worth considering: Adjusting the effective batch to 512K and batch size to 10 so that micro batches with sequence length of 1024 evenly fit into the effective batch size, I could get the training to speed up to ~110K tokens/second, implying that the training time can be shortened to around 14 hours.\n\n\n\n\nDDP training dynamics on two 3090’s with micro-batch size of 10, image by the author.png"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 2: Optimize GPT-2.html#references",
    "title": "Part 2 - Optimizing GPT-2",
    "section": "",
    "text": "Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.” arXiv preprint arXiv:2205.14135v2 (2022)."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html",
    "title": "Part 3 - Training on TinyStories",
    "section": "",
    "text": "This is Part 3 of building, optimizing, and training GPT-2 from scratch. In this part, I’ll train GPT-2-style transformer on the Tiny Stories dataset, which is a synthetic dataset of children’s stories produced by Microsoft Research. Since this dataset has shorter sequences and limited vocabulary, this is an opportunity to adjust both the tokenizer and the GPT-2 transformer to further speed up training. To this end, I’ll first train a Hugging Face tokenizer since OpenAI’s original tiktoken tokenizer cannot be trained. I’ll also show how to adjust Andrej’s code to be used with the Hugging Face tokenizer.\n\n\n\n\nI’ll first train the tokenizer using the method recommended by Hugging Face. I may loop back to this to replicate it from scratch in a later notebook.\n\n\nfrom datasets import load_dataset\ntiny_stories_ds=load_dataset(\"roneneldan/TinyStories\")\n\ndef get_training_corpus(dataset,split='train',field='text'):\n    dataset = dataset[split]\n    for start_idx in range(0, len(dataset), 1024): #using a power of 2, why not...\n        yield dataset[start_idx:start_idx+1024][field]\ntraining_corpus = get_training_corpus(tiny_stories_ds)\n\nfrom transformers import AutoTokenizer\nold_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus,vocab_size=6400) # don't forget to adjust the training script with this number!\ntokenizer.save_pretrained('./tokenizer/tiny_stories_tokenizer')\n\n\nMost common words get encoded in a single token\n\ntokenizer.encode(' dog'),tokenizer.encode(' girl'), tokenizer.encode(' bird'),tokenizer.encode(' dad'),tokenizer.encode(' mom'),tokenizer.encode(' cat'),tokenizer.encode(' Lily') yields\n([634], [446], [537], [648], [365], [701], [364]) - Note the presence of space before the common words: this is how GPT-2 encodes words by default.\n\nRunning some experiments on the tokenizer length, here are some statistics on a random subset of size 10000 tokenized with tokenizers with various vocabulary sizes:\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nSource: graph made by the author\n\n\n\nAs expected, as tokenizer size decreases, the average number of tokens per story increases and the distribution of tokens needed per story shifts to the rights slightly. Regardless, almost 97% of the stories can be tokenized in their entirety by 512 tokens or less. I thus choose to shrink the sequence length from 1024 to 512: critically, this will reduce the amount of memory taken by the embedding matrices and the activations, allowing me to use much bigger batch sizes per GPU (64 instead of 8 or 10) during training. This leads to further memory speed-ups.\n\nNote that 512 is still a multiple of 32, agreeing with recommendations by both Andrej and Anthony et al (2024).\nAlso, the model would be ~91M parameters rather than 126M parameters of the original GPT-2 small.\nShrinking just the vocab size but not the sequence length increases the training speed from 110K tokens/second with default GPT-2 setup from Part 2 to about 150K tokens/second.\nReducing both the vocab size and the sequence length further speeds up the training to 162K tokens/second, about 47% training speed-up overall.\n\nThe tinystories dataset has 2.12M stories. At about 200 tokens/story at vocab size of 6400, this makes for about 420M tokens. With vocab size shrunk to 6400, I’m able to process batches of size 32 with 1024 tokens/batch per each of the two GPUs, I’m able to process 150K tokens/second, leading to about an 8-hour training run for a full pass through the data.\nIn addition, 96.75% of the stories are under 512 tokens in length. Reducing the sequence length to 512 allows us to increase the batch size to 64 and speeds up training to 162K tokens/second.\n\n\ntorchrun --standalone --nproc_per_node=2 train_gpt2.py \n\n\ntorchrun shows the following warning: “Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.” I have 24 threads on my machine (can find this out with nproc), so setting export OMP_NUM_THREADS=8 to let each GPU access 8 threads.\nNow 405M tokens (2.12M stories at 191 tokens/story on average) at 131072 (64 batch size/GPU x 512 sequence length x 2 GPUs) tokens processed/step would take at most 6200 steps for a full epoch.\n\n\n\n\n\nInside fineweb.py, Andrej’s setup and tokenize function are as follows:\n\n\nimport tiktoken\nfw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n# init the tokenizer\nenc = tiktoken.get_encoding(\"gpt2\")\neot = enc._special_tokens['&lt;|endoftext|&gt;'] # end of text token\ndef tokenize(doc):\n    # tokenizes a single document and returns a numpy array of uint16 tokens\n    tokens = [eot] # the special &lt;|endoftext|&gt; token delimits all documents\n    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n    tokens_np = np.array(tokens)\n    assert (0 &lt;= tokens_np).all() and (tokens_np &lt; 2**16).all(), \"token dictionary too large for uint16\"\n    tokens_np_uint16 = tokens_np.astype(np.uint16)\n    return tokens_np_uint16\n\n\nI made an analogous script called tinystories.py to load and use the custom Hugging Face tokenizer trained above:\n\n\nfrom datasets import load_dataset, concatenate_datasets # pip install datasets\n\ntrain_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\nval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n\n# download the dataset\nfw = concatenate_datasets([train_dataset, val_dataset])\n\nfrom transformers import AutoTokenizer\nenc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\")\n\n# init the tokenizer\neot = enc.encode(enc.eos_token)[0]# end of text token\nMAX_LEN = 512\nprint(f'Using max len of {MAX_LEN}')\ndef tokenize(doc):\n    # tokenizes a single document and returns a numpy array of uint16 tokens\n    tokens = [eot] # the special &lt;|endoftext|&gt; token delimits all documents\n    tokens.extend(enc.encode(doc['text']))\n    #print(f\"tokens: {tokens}\")\n    if len(tokens)&gt;MAX_LEN: return None\n    tokens_np = np.array(tokens)\n    assert (0 &lt;= tokens_np).all() and (tokens_np &lt; 2**16).all(), \"token dictionary too large for uint16\"\n    tokens_np_uint16 = tokens_np.astype(np.uint16)\n    return tokens_np_uint16\n\n\nInside train_gpt2.py, I also loaded this tokenizer, adjusted the GPTConfig class, adjusted the batch size and the sequence length.\nI also commented out all parts pertaining to HellaSwag eval: I’m not interested in training the transformer to do great on HellaSwag as its main purpose is to generate coherent children’s stories.\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 512 #1024 # max sequence length\n    vocab_size: int = 6400 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 &lt;|endoftext|&gt; token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 #738 #768 # embedding/hidden dimension\n\n\nenc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\") #tiktoken.get_encoding(\"gpt2\")\n\ntotal_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\nB = 64 #32 # micro batch size\nT = 512 #1024 # sequence length\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n\n\n\n\n\nI trained for 6200 steps. Validation loss went down consistently from 8.84 at the start to 1.3003 after 6000 steps, then stopped decreasing, increasing slightly to 1.3026 after 6500 steps. As a full pass through the data has already been conducted, it is possible that the model started overfitting slightly to the training data. This suggests that the model with the current settings and size has been fully trained. I expect that perhaps further reducing the model size and training for more epochs could both reduce the potential for overfitting and produce a model of comparable quality in fewer parameters. In fact, TinyStories research project by Eldan, R., & Li, Y. (2023) suggests this possibility.\nSome sample generations to show the model’s learning progression are below.\n\n\n\nvalidation loss: 2.1717\nrank 1 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. When she arrived, she found a berry and took a rank 1 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. She decided to jump over the trees and jump over the rank 1 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He spent every day looking for something to eat, but rank 1 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\nOne day the carrot was so filthy that it rank 0 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\nAs the rabbit was running, he began to rank 0 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He felt so happy to have the special treat he took rank 0 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. The rabbit stopped to gather lots of juicy apples of apples rank 0 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. But something was different and it started to feel bored.\n\n\n\nvalidation loss: 1.5746\nrank 1 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He was so excited about his reward!\nAs rank 1 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. Every morning the rabbit would go out searching for something to rank 1 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. The rabbit had to find a big, juicy carrot and rank 1 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He hopped around all the sights until he found a big rank 0 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He hopped off the trail and went back to his den rank 0 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. One day, the rabbit came across a big tree with rank 0 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. One day, he came across a pond and had a rank 0 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. As he was hopping around, the rabbit saw a big\n\nNext, let’s load the model trained for 200 steps and generate from it.\n\n\nimport torch\nfrom train_gpt2 import GPT, GPTConfig\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f\"using device: {device}\")\n\n# Load the checkpoint\n#checkpoint = torch.load('log/model_00999.pt', map_location=device)\ncheckpoint = torch.load('log/model_06000.pt', map_location=device)\n\n# Extract the config from the checkpoint\nconfig = checkpoint['config']\n\n# Initialize the model with the saved config\nmodel = GPT(config)\n\n# Load the state dict into the model\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\nmodel.to(device)\n\nprint(\"Model loaded successfully and set to evaluation mode.\")\n\nusing device: cuda:0\nusing device: cuda:0\nModel loaded successfully and set to evaluation mode.\n\n\n\ncheckpoint = torch.load('log/model_01000.pt', map_location=device)\n\n# Extract the config from the checkpoin\ncheckpoint['val_loss']\n\n1.6329686641693115\n\n\n\ncheckpoint['val_loss']\n\n1.3002820014953613\n\n\n\nBest checkpoint above, started to overfit slightly, validation loss at last step 6500 was 1.3026, a bit larger than at step 6000.\n\n\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nnum_return_sequences = 4\nmax_length = 250\ndevice_type = device\ntokenizer = enc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\")\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ntokenizer.pad_token = tokenizer.eos_token\n#starter_txt=\"Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\"\nstarter_txt=\"Once upon a time in a land far, far away\"\n#starter_txt = \"Once upon a time\"\ntokens = enc.encode(starter_txt)\ntokens = torch.tensor(tokens, dtype=torch.long)\ntokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\nxgen = tokens.to(device)\nsample_rng = torch.Generator(device=device)\nsample_rng.manual_seed(42)\n\nwhile xgen.size(1) &lt; max_length:\n    # forward the model to get the logits\n    with torch.no_grad():\n        #with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n        logits, loss = model(xgen) # (B, T, vocab_size)\n        # take the logits at the last position\n        logits = logits[:, -1, :] # (B, vocab_size)\n        # get the probabilities\n        probs = F.softmax(logits, dim=-1)\n        # do top-k sampling of 50 (huggingface pipeline default)\n        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        # select a token from the top-k probabilities\n        # note: multinomial does not demand the input to sum to 1\n        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n        # gather the corresponding indices\n        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n        # append to the sequence\n        xgen = torch.cat((xgen, xcol), dim=1)\n# print the generated text\nfor i in range(num_return_sequences):\n    tokens = xgen[i, :max_length].tolist()\n    decoded = enc.decode(tokens)\n    print(f\"sample {i}: {decoded}\")\n\nsample 0: Once upon a time in a land far, far away there was a little boy named Joe. Joe was very anxious, as he didn't know how he was going to be successful. One day, he got an idea. He decided to march to a faraway village so he could get back at his family.\n\nJoe marched and marched, until his legs got tired. He stopped to rest at a small river.\n\nSuddenly, a bird landed beside him. Joe looked around and saw the bird was singing a song. The bird told him to keep going.\n\nJoe marched, and he eventually reached the village. His family was happy to see him and they hugged him tight.\n\nJoe smiled and said, \"I'm glad I marched. Now I'm home.\"\n\nThe bird replied, \"I'm glad too. Good luck!\"\n\nJoe waved goodbye to the bird and marched back&lt;|endoftext|&gt;Once upon a time, there was a boy. He loved to whistle. One day, he was walking in the park and he saw a very big hill. He was excited to see it, so he started to whistle his favorite tune.\n\nOn top of the hill, he saw something very special. It was a\nsample 1: Once upon a time in a land far, far away, there lived a kind and honest wolf. Every day, he would do kind things for the animals in the forest, and the animals would appreciate him.\n\nOne day, the wolf was walking through the forest and he saw a tiny house by the side of the road. Inside the house was a little girl, all alone. The honest wolf smiled and said \"Hello! Would you like to come in and help me?\".\n\nThe little girl smiled back and said \"Yes, please!\". The wolf then went inside the house to help her. He quickly pulled out a big box from under the table, which he could open to reveal. Inside the box was a special treat for the little girl. She thanked the kind and honest wolf, and happily ate the treat.\n\nAs the deer finished, he said goodbye to the little girl, and she waved goodbye to the honest wolf before leaving.\n\nThe honest wolf watched her go, smiled a happy smile, and went on his way, happy that he had been able to help.&lt;|endoftext|&gt;Once upon a time there was a boy named Tom. Tom loved to run around in the garden and had lots of fun. One day\nsample 2: Once upon a time in a land far, far away, there lived a special place. It was home to a beautiful garden, full of green plants and colorful flowers. Every day, the garden was filled with the most incredible things. The flowers bloomed for the garden and the grass grew bigger and bigger, until all of the garden was full of colour!\n\nEvery day, the sun shone brightly, and the birds sang as they sang. The garden was a vibrant, peaceful place to be.\n\nBut after a while, something strange happened. Every day, the plants in the garden started to decrease and grow, leavingting the ground completely dry leaves. So, the gardeners decided they had to stop it by taking it back into the forest to try and make it better.\n\nThe gardeners thought and thought, but they had no idea what to do. Then all of a sudden, one of the gardeners had an idea.\n\nâ€œLetâ€™s work together!â€ said the gardener. So the gardeners asked all their friends and family to help out. Soon, the garden was back to normal and the flowers were blooming brighter than ever before!\n\nThe gardeners were\nsample 3: Once upon a time in a land far, far away there lived a little girl named Lucy. Every day, Lucy would go out in her garden and pick berries.\n\nOne morning, Lucy saw something very special. In the middle of the garden was a big pot of jam! Lucy was so excited that she ran to get her mom to show her the jam.\n\n\"Mom, look!\" she said. \"What is this?\"\n\nHer mom smiled and said, \"This is a special jam, Lucy! It's something very fragile and sweet!\"\n\nLucy was so excited that she couldn't wait to try the jam and she went outside every morning to pick two jars of jam for her breakfast.\n\nThe jam always made Lucy so happy, because when she came home from breakfast, she was able to eat two jars of jam. She was so grateful that she owned two jars of jam, and was the happiest little girl in the world!&lt;|endoftext|&gt;Once there was a dog called Max. He was a very playful pup who loved to explore the world. One day Max went outside and saw a deep stream. He wanted to get to the other side, but it had gotten dark and he was scared.\n\n\n\nOnce upon a time, there was a little girl called Alice who lived in a big house. Every day, Alice’s mummy and daddy and her two siblings wanted to explain something very important to Alice.\n\nOne day it was Alice’s birthday, and her mummy and daddy said that she was getting a present! Alice couldn’t believe it, as she had never seen such a present before.\nMummy and Daddy explained that a present is a big, expensive box filled with toys. Alice was excited and couldn’t wait to open it.\nSuddenly, a loud noise came from outside. Daddy and Mummy and Alice ran out of the house and saw a truck full of presents on the street. Everyone in the truck had come out for Alice - it was all because of the expensive box!\nAlice was so excited. She opened the present and carefully chose her very own present, a teddy bear with a big, red bow. She hugged the toy, smiled, and thanked her mummy and daddy and the other men from the truck.\nAlice and her family left the house, excited about the presents. She couldn’t wait to show her friends at preschool the next day!\n\nOnce upon a time in a land far, far away… seems to produce good stories!\nOnce upon a time in a land far, far away there was a little boy named Joe. Joe was very anxious, as he didn’t know how he was going to be successful. One day, he got an idea. He decided to march to a faraway village so he could get back at his family.\n\nJoe marched and marched, until his legs got tired. He stopped to rest at a small river.\nSuddenly, a bird landed beside him. Joe looked around and saw the bird was singing a song. The bird told him to keep going.\nJoe marched, and he eventually reached the village. His family was happy to see him and they hugged him tight.\nJoe smiled and said, “I’m glad I marched. Now I’m home.”\nThe bird replied, “I’m glad too. Good luck!”\nJoe waved goodbye to the bird and marched back - Once upon a time in a land far, far away there lived a little girl named Lucy. Every day, Lucy would go out in her garden and pick berries.\nOne morning, Lucy saw something very special. In the middle of the garden was a big pot of jam! Lucy was so excited that she ran to get her mom to show her the jam.\n“Mom, look!” she said. “What is this?”\nHer mom smiled and said, “This is a special jam, Lucy! It’s something very fragile and sweet!”\nLucy was so excited that she couldn’t wait to try the jam and she went outside every morning to pick two jars of jam for her breakfast.\nThe jam always made Lucy so happy, because when she came home from breakfast, she was able to eat two jars of jam. She was so grateful that she owned two jars of jam, and was the happiest little girl in the world!\n\nOnce upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. As he hopped through the forest, he noticed a big, old, rusty well. He hopped closer and closer until he could see inside.\n\nâ€œWhat do you think is in there?â€ asked the loyal rabbit.\nSuddenly, a small frog poked his head out of the well. â€œI can see a secret of whatâ€™s insideâ€ he said.\nThe rabbit was excited, so he stepped into the well and started to dive. As he dove in, he could hear the stream gurgling and the water running. Suddenly, he heard a voice calling his name.\nâ€œI made it,â€ he said. It was a frog, who lived in the well. â€œCome this way, little one, itâ€™s so much fun here!â€\nThe rabbit thanked the frog and hopped away. Everytime he visits the well, he remembers that loyal and kind heart he made. - Not a bad story but perhaps has tokenization issues? Could consider training for 2000 steps with fully custom tokenizer or understand what’s going on.\n\nLooks like these unusual tokens come directly from the tinystories dataset hosted on Hugging Face. See the following, for example,\n\n\n\n\nSource: https://huggingface.co/datasets/roneneldan/TinyStories\n\n\n\nUpon further research, I found that the issue was brought up in the past and is likely a consequence of an encoding issue with UTF-8. Nonetheless, I plan to dig deeper into tokenization to round off this study.\n\n\n\n\n\nAnthony, Q., Hatef, J., Narayanan, D., Biderman, S., Bekman, S., Yin, J., Shafi, A., Subramoni, H., & Panda, D. (2024). The Case for Co-Designing Model Architectures with Hardware. arXiv preprint arXiv:2401.14489. https://doi.org/10.48550/arXiv.2401.14489\nEldan, R., & Li, Y. (2023). TinyStories: How Small Can Language Models Be and Still Speak Coherent English? arXiv preprint arXiv:2305.07759."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#training-the-tokenizer",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#training-the-tokenizer",
    "title": "Part 3 - Training on TinyStories",
    "section": "",
    "text": "I’ll first train the tokenizer using the method recommended by Hugging Face. I may loop back to this to replicate it from scratch in a later notebook.\n\n\nfrom datasets import load_dataset\ntiny_stories_ds=load_dataset(\"roneneldan/TinyStories\")\n\ndef get_training_corpus(dataset,split='train',field='text'):\n    dataset = dataset[split]\n    for start_idx in range(0, len(dataset), 1024): #using a power of 2, why not...\n        yield dataset[start_idx:start_idx+1024][field]\ntraining_corpus = get_training_corpus(tiny_stories_ds)\n\nfrom transformers import AutoTokenizer\nold_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus,vocab_size=6400) # don't forget to adjust the training script with this number!\ntokenizer.save_pretrained('./tokenizer/tiny_stories_tokenizer')\n\n\nMost common words get encoded in a single token\n\ntokenizer.encode(' dog'),tokenizer.encode(' girl'), tokenizer.encode(' bird'),tokenizer.encode(' dad'),tokenizer.encode(' mom'),tokenizer.encode(' cat'),tokenizer.encode(' Lily') yields\n([634], [446], [537], [648], [365], [701], [364]) - Note the presence of space before the common words: this is how GPT-2 encodes words by default.\n\nRunning some experiments on the tokenizer length, here are some statistics on a random subset of size 10000 tokenized with tokenizers with various vocabulary sizes:\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nSource: graph made by the author\n\n\n\nAs expected, as tokenizer size decreases, the average number of tokens per story increases and the distribution of tokens needed per story shifts to the rights slightly. Regardless, almost 97% of the stories can be tokenized in their entirety by 512 tokens or less. I thus choose to shrink the sequence length from 1024 to 512: critically, this will reduce the amount of memory taken by the embedding matrices and the activations, allowing me to use much bigger batch sizes per GPU (64 instead of 8 or 10) during training. This leads to further memory speed-ups.\n\nNote that 512 is still a multiple of 32, agreeing with recommendations by both Andrej and Anthony et al (2024).\nAlso, the model would be ~91M parameters rather than 126M parameters of the original GPT-2 small.\nShrinking just the vocab size but not the sequence length increases the training speed from 110K tokens/second with default GPT-2 setup from Part 2 to about 150K tokens/second.\nReducing both the vocab size and the sequence length further speeds up the training to 162K tokens/second, about 47% training speed-up overall.\n\nThe tinystories dataset has 2.12M stories. At about 200 tokens/story at vocab size of 6400, this makes for about 420M tokens. With vocab size shrunk to 6400, I’m able to process batches of size 32 with 1024 tokens/batch per each of the two GPUs, I’m able to process 150K tokens/second, leading to about an 8-hour training run for a full pass through the data.\nIn addition, 96.75% of the stories are under 512 tokens in length. Reducing the sequence length to 512 allows us to increase the batch size to 64 and speeds up training to 162K tokens/second.\n\n\ntorchrun --standalone --nproc_per_node=2 train_gpt2.py \n\n\ntorchrun shows the following warning: “Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.” I have 24 threads on my machine (can find this out with nproc), so setting export OMP_NUM_THREADS=8 to let each GPU access 8 threads.\nNow 405M tokens (2.12M stories at 191 tokens/story on average) at 131072 (64 batch size/GPU x 512 sequence length x 2 GPUs) tokens processed/step would take at most 6200 steps for a full epoch."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#integrating-the-hugging-face-tokenizer-with-andrejs-code",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#integrating-the-hugging-face-tokenizer-with-andrejs-code",
    "title": "Part 3 - Training on TinyStories",
    "section": "",
    "text": "Inside fineweb.py, Andrej’s setup and tokenize function are as follows:\n\n\nimport tiktoken\nfw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n# init the tokenizer\nenc = tiktoken.get_encoding(\"gpt2\")\neot = enc._special_tokens['&lt;|endoftext|&gt;'] # end of text token\ndef tokenize(doc):\n    # tokenizes a single document and returns a numpy array of uint16 tokens\n    tokens = [eot] # the special &lt;|endoftext|&gt; token delimits all documents\n    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n    tokens_np = np.array(tokens)\n    assert (0 &lt;= tokens_np).all() and (tokens_np &lt; 2**16).all(), \"token dictionary too large for uint16\"\n    tokens_np_uint16 = tokens_np.astype(np.uint16)\n    return tokens_np_uint16\n\n\nI made an analogous script called tinystories.py to load and use the custom Hugging Face tokenizer trained above:\n\n\nfrom datasets import load_dataset, concatenate_datasets # pip install datasets\n\ntrain_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\nval_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n\n# download the dataset\nfw = concatenate_datasets([train_dataset, val_dataset])\n\nfrom transformers import AutoTokenizer\nenc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\")\n\n# init the tokenizer\neot = enc.encode(enc.eos_token)[0]# end of text token\nMAX_LEN = 512\nprint(f'Using max len of {MAX_LEN}')\ndef tokenize(doc):\n    # tokenizes a single document and returns a numpy array of uint16 tokens\n    tokens = [eot] # the special &lt;|endoftext|&gt; token delimits all documents\n    tokens.extend(enc.encode(doc['text']))\n    #print(f\"tokens: {tokens}\")\n    if len(tokens)&gt;MAX_LEN: return None\n    tokens_np = np.array(tokens)\n    assert (0 &lt;= tokens_np).all() and (tokens_np &lt; 2**16).all(), \"token dictionary too large for uint16\"\n    tokens_np_uint16 = tokens_np.astype(np.uint16)\n    return tokens_np_uint16\n\n\nInside train_gpt2.py, I also loaded this tokenizer, adjusted the GPTConfig class, adjusted the batch size and the sequence length.\nI also commented out all parts pertaining to HellaSwag eval: I’m not interested in training the transformer to do great on HellaSwag as its main purpose is to generate coherent children’s stories.\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 512 #1024 # max sequence length\n    vocab_size: int = 6400 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 &lt;|endoftext|&gt; token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 #738 #768 # embedding/hidden dimension\n\n\nenc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\") #tiktoken.get_encoding(\"gpt2\")\n\ntotal_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\nB = 64 #32 # micro batch size\nT = 512 #1024 # sequence length\nassert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\""
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#generate-from-the-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#generate-from-the-model",
    "title": "Part 3 - Training on TinyStories",
    "section": "",
    "text": "I trained for 6200 steps. Validation loss went down consistently from 8.84 at the start to 1.3003 after 6000 steps, then stopped decreasing, increasing slightly to 1.3026 after 6500 steps. As a full pass through the data has already been conducted, it is possible that the model started overfitting slightly to the training data. This suggests that the model with the current settings and size has been fully trained. I expect that perhaps further reducing the model size and training for more epochs could both reduce the potential for overfitting and produce a model of comparable quality in fewer parameters. In fact, TinyStories research project by Eldan, R., & Li, Y. (2023) suggests this possibility.\nSome sample generations to show the model’s learning progression are below.\n\n\n\nvalidation loss: 2.1717\nrank 1 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. When she arrived, she found a berry and took a rank 1 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. She decided to jump over the trees and jump over the rank 1 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He spent every day looking for something to eat, but rank 1 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\nOne day the carrot was so filthy that it rank 0 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\nAs the rabbit was running, he began to rank 0 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He felt so happy to have the special treat he took rank 0 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. The rabbit stopped to gather lots of juicy apples of apples rank 0 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. But something was different and it started to feel bored.\n\n\n\nvalidation loss: 1.5746\nrank 1 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He was so excited about his reward!\nAs rank 1 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. Every morning the rabbit would go out searching for something to rank 1 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. The rabbit had to find a big, juicy carrot and rank 1 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He hopped around all the sights until he found a big rank 0 sample 0: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. He hopped off the trail and went back to his den rank 0 sample 1: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. One day, the rabbit came across a big tree with rank 0 sample 2: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. One day, he came across a pond and had a rank 0 sample 3: Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. As he was hopping around, the rabbit saw a big\n\nNext, let’s load the model trained for 200 steps and generate from it.\n\n\nimport torch\nfrom train_gpt2 import GPT, GPTConfig\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f\"using device: {device}\")\n\n# Load the checkpoint\n#checkpoint = torch.load('log/model_00999.pt', map_location=device)\ncheckpoint = torch.load('log/model_06000.pt', map_location=device)\n\n# Extract the config from the checkpoint\nconfig = checkpoint['config']\n\n# Initialize the model with the saved config\nmodel = GPT(config)\n\n# Load the state dict into the model\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\nmodel.to(device)\n\nprint(\"Model loaded successfully and set to evaluation mode.\")\n\nusing device: cuda:0\nusing device: cuda:0\nModel loaded successfully and set to evaluation mode.\n\n\n\ncheckpoint = torch.load('log/model_01000.pt', map_location=device)\n\n# Extract the config from the checkpoin\ncheckpoint['val_loss']\n\n1.6329686641693115\n\n\n\ncheckpoint['val_loss']\n\n1.3002820014953613\n\n\n\nBest checkpoint above, started to overfit slightly, validation loss at last step 6500 was 1.3026, a bit larger than at step 6000.\n\n\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nnum_return_sequences = 4\nmax_length = 250\ndevice_type = device\ntokenizer = enc = AutoTokenizer.from_pretrained(\"tokenizer/tiny_stories_tokenizer\")\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ntokenizer.pad_token = tokenizer.eos_token\n#starter_txt=\"Once upon a time there was a loyal rabbit who left the forest to look for something delicious to eat.\"\nstarter_txt=\"Once upon a time in a land far, far away\"\n#starter_txt = \"Once upon a time\"\ntokens = enc.encode(starter_txt)\ntokens = torch.tensor(tokens, dtype=torch.long)\ntokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\nxgen = tokens.to(device)\nsample_rng = torch.Generator(device=device)\nsample_rng.manual_seed(42)\n\nwhile xgen.size(1) &lt; max_length:\n    # forward the model to get the logits\n    with torch.no_grad():\n        #with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n        logits, loss = model(xgen) # (B, T, vocab_size)\n        # take the logits at the last position\n        logits = logits[:, -1, :] # (B, vocab_size)\n        # get the probabilities\n        probs = F.softmax(logits, dim=-1)\n        # do top-k sampling of 50 (huggingface pipeline default)\n        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        # select a token from the top-k probabilities\n        # note: multinomial does not demand the input to sum to 1\n        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n        # gather the corresponding indices\n        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n        # append to the sequence\n        xgen = torch.cat((xgen, xcol), dim=1)\n# print the generated text\nfor i in range(num_return_sequences):\n    tokens = xgen[i, :max_length].tolist()\n    decoded = enc.decode(tokens)\n    print(f\"sample {i}: {decoded}\")\n\nsample 0: Once upon a time in a land far, far away there was a little boy named Joe. Joe was very anxious, as he didn't know how he was going to be successful. One day, he got an idea. He decided to march to a faraway village so he could get back at his family.\n\nJoe marched and marched, until his legs got tired. He stopped to rest at a small river.\n\nSuddenly, a bird landed beside him. Joe looked around and saw the bird was singing a song. The bird told him to keep going.\n\nJoe marched, and he eventually reached the village. His family was happy to see him and they hugged him tight.\n\nJoe smiled and said, \"I'm glad I marched. Now I'm home.\"\n\nThe bird replied, \"I'm glad too. Good luck!\"\n\nJoe waved goodbye to the bird and marched back&lt;|endoftext|&gt;Once upon a time, there was a boy. He loved to whistle. One day, he was walking in the park and he saw a very big hill. He was excited to see it, so he started to whistle his favorite tune.\n\nOn top of the hill, he saw something very special. It was a\nsample 1: Once upon a time in a land far, far away, there lived a kind and honest wolf. Every day, he would do kind things for the animals in the forest, and the animals would appreciate him.\n\nOne day, the wolf was walking through the forest and he saw a tiny house by the side of the road. Inside the house was a little girl, all alone. The honest wolf smiled and said \"Hello! Would you like to come in and help me?\".\n\nThe little girl smiled back and said \"Yes, please!\". The wolf then went inside the house to help her. He quickly pulled out a big box from under the table, which he could open to reveal. Inside the box was a special treat for the little girl. She thanked the kind and honest wolf, and happily ate the treat.\n\nAs the deer finished, he said goodbye to the little girl, and she waved goodbye to the honest wolf before leaving.\n\nThe honest wolf watched her go, smiled a happy smile, and went on his way, happy that he had been able to help.&lt;|endoftext|&gt;Once upon a time there was a boy named Tom. Tom loved to run around in the garden and had lots of fun. One day\nsample 2: Once upon a time in a land far, far away, there lived a special place. It was home to a beautiful garden, full of green plants and colorful flowers. Every day, the garden was filled with the most incredible things. The flowers bloomed for the garden and the grass grew bigger and bigger, until all of the garden was full of colour!\n\nEvery day, the sun shone brightly, and the birds sang as they sang. The garden was a vibrant, peaceful place to be.\n\nBut after a while, something strange happened. Every day, the plants in the garden started to decrease and grow, leavingting the ground completely dry leaves. So, the gardeners decided they had to stop it by taking it back into the forest to try and make it better.\n\nThe gardeners thought and thought, but they had no idea what to do. Then all of a sudden, one of the gardeners had an idea.\n\nâ€œLetâ€™s work together!â€ said the gardener. So the gardeners asked all their friends and family to help out. Soon, the garden was back to normal and the flowers were blooming brighter than ever before!\n\nThe gardeners were\nsample 3: Once upon a time in a land far, far away there lived a little girl named Lucy. Every day, Lucy would go out in her garden and pick berries.\n\nOne morning, Lucy saw something very special. In the middle of the garden was a big pot of jam! Lucy was so excited that she ran to get her mom to show her the jam.\n\n\"Mom, look!\" she said. \"What is this?\"\n\nHer mom smiled and said, \"This is a special jam, Lucy! It's something very fragile and sweet!\"\n\nLucy was so excited that she couldn't wait to try the jam and she went outside every morning to pick two jars of jam for her breakfast.\n\nThe jam always made Lucy so happy, because when she came home from breakfast, she was able to eat two jars of jam. She was so grateful that she owned two jars of jam, and was the happiest little girl in the world!&lt;|endoftext|&gt;Once there was a dog called Max. He was a very playful pup who loved to explore the world. One day Max went outside and saw a deep stream. He wanted to get to the other side, but it had gotten dark and he was scared.\n\n\n\nOnce upon a time, there was a little girl called Alice who lived in a big house. Every day, Alice’s mummy and daddy and her two siblings wanted to explain something very important to Alice.\n\nOne day it was Alice’s birthday, and her mummy and daddy said that she was getting a present! Alice couldn’t believe it, as she had never seen such a present before.\nMummy and Daddy explained that a present is a big, expensive box filled with toys. Alice was excited and couldn’t wait to open it.\nSuddenly, a loud noise came from outside. Daddy and Mummy and Alice ran out of the house and saw a truck full of presents on the street. Everyone in the truck had come out for Alice - it was all because of the expensive box!\nAlice was so excited. She opened the present and carefully chose her very own present, a teddy bear with a big, red bow. She hugged the toy, smiled, and thanked her mummy and daddy and the other men from the truck.\nAlice and her family left the house, excited about the presents. She couldn’t wait to show her friends at preschool the next day!\n\nOnce upon a time in a land far, far away… seems to produce good stories!\nOnce upon a time in a land far, far away there was a little boy named Joe. Joe was very anxious, as he didn’t know how he was going to be successful. One day, he got an idea. He decided to march to a faraway village so he could get back at his family.\n\nJoe marched and marched, until his legs got tired. He stopped to rest at a small river.\nSuddenly, a bird landed beside him. Joe looked around and saw the bird was singing a song. The bird told him to keep going.\nJoe marched, and he eventually reached the village. His family was happy to see him and they hugged him tight.\nJoe smiled and said, “I’m glad I marched. Now I’m home.”\nThe bird replied, “I’m glad too. Good luck!”\nJoe waved goodbye to the bird and marched back - Once upon a time in a land far, far away there lived a little girl named Lucy. Every day, Lucy would go out in her garden and pick berries.\nOne morning, Lucy saw something very special. In the middle of the garden was a big pot of jam! Lucy was so excited that she ran to get her mom to show her the jam.\n“Mom, look!” she said. “What is this?”\nHer mom smiled and said, “This is a special jam, Lucy! It’s something very fragile and sweet!”\nLucy was so excited that she couldn’t wait to try the jam and she went outside every morning to pick two jars of jam for her breakfast.\nThe jam always made Lucy so happy, because when she came home from breakfast, she was able to eat two jars of jam. She was so grateful that she owned two jars of jam, and was the happiest little girl in the world!\n\nOnce upon a time there was a loyal rabbit who left the forest to look for something delicious to eat. As he hopped through the forest, he noticed a big, old, rusty well. He hopped closer and closer until he could see inside.\n\nâ€œWhat do you think is in there?â€ asked the loyal rabbit.\nSuddenly, a small frog poked his head out of the well. â€œI can see a secret of whatâ€™s insideâ€ he said.\nThe rabbit was excited, so he stepped into the well and started to dive. As he dove in, he could hear the stream gurgling and the water running. Suddenly, he heard a voice calling his name.\nâ€œI made it,â€ he said. It was a frog, who lived in the well. â€œCome this way, little one, itâ€™s so much fun here!â€\nThe rabbit thanked the frog and hopped away. Everytime he visits the well, he remembers that loyal and kind heart he made. - Not a bad story but perhaps has tokenization issues? Could consider training for 2000 steps with fully custom tokenizer or understand what’s going on.\n\nLooks like these unusual tokens come directly from the tinystories dataset hosted on Hugging Face. See the following, for example,\n\n\n\n\nSource: https://huggingface.co/datasets/roneneldan/TinyStories\n\n\n\nUpon further research, I found that the issue was brought up in the past and is likely a consequence of an encoding issue with UTF-8. Nonetheless, I plan to dig deeper into tokenization to round off this study."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 3: Train GPT-2.html#references",
    "title": "Part 3 - Training on TinyStories",
    "section": "",
    "text": "Anthony, Q., Hatef, J., Narayanan, D., Biderman, S., Bekman, S., Yin, J., Shafi, A., Subramoni, H., & Panda, D. (2024). The Case for Co-Designing Model Architectures with Hardware. arXiv preprint arXiv:2401.14489. https://doi.org/10.48550/arXiv.2401.14489\nEldan, R., & Li, Y. (2023). TinyStories: How Small Can Language Models Be and Still Speak Coherent English? arXiv preprint arXiv:2305.07759."
  },
  {
    "objectID": "blog/posts/linkedin/favorite_books_and_resources_2024.html",
    "href": "blog/posts/linkedin/favorite_books_and_resources_2024.html",
    "title": "Favorite Books and Resources of 2024",
    "section": "",
    "text": "Favorite Books of 2024.png"
  },
  {
    "objectID": "blog/posts/linkedin/favorite_books_and_resources_2024.html#books",
    "href": "blog/posts/linkedin/favorite_books_and_resources_2024.html#books",
    "title": "Favorite Books and Resources of 2024",
    "section": "Books",
    "text": "Books\n\nLearn Docker in a Month of Lunches by Elton Stoneman\n\nThe answer to a question I’ve had over the years, namely, “How to learn Docker properly”. Not all of the chapters may be applicable to you (at least that was my case), but I think Parts I and IV of the book do a terrific job explaining the logic of Docker in just the right amount of depth, while the other parts dive deeper into various aspects that may be needed for certain projects. I’ve been feeling a lot more confident working with Docker since I’ve started spending time with this book.\n\n\n\nOfficial Google Cloud Certified Professional Machine Learning Engineer Study Guide by Mona Mona and Pratap Ramamurthy\n\nThis has been quite helpful for preparing for the exam along with lots of tinkering/projects on GCP. If you’re studying for the exam, I would suggest working through this book along with the accompanying practice exams and experimenting with whatever it covers that you don’t regularly use. The book is not updated to include LLMs (there were maybe 4 or 5 questions on these on the exam), which in my case was OK since I spend lots of time with them, but if you don’t, you’ll probably get most of what you need from Google’s official study path. Travis Webb’s post was also spot on regarding how to prepare for non-LLM topics.\n\n\n\nDesigning Machine Learning Systems by Chip Huyen\n\nA great book for all things related to ML in production as well as an enjoyable read.\n\n\n\nDeep Learning for Coders with fastai & PyTorch by Jeremy Howard and Sylvain Gugger\n\nProbably a classic on pre-transformers deep learning by now. While I’ve had this book for years, I found Part IV quite handy while working through fastai part 2 (see below). The questions at the end of each chapter are a phenomenal way to actively rehearse deep learning fundamentals.\n\n\n\nThe Joy of Clojure by Michael Fogus and Chris Houser\n\nA great diversion if you’re interested in functional programming and are looking for a fun read. Certainly haven’t had as much time to spend with this book as I wished, but got it because it’s also available on Audible, making it easier to occasionally tune in during road trips and such."
  },
  {
    "objectID": "blog/posts/linkedin/favorite_books_and_resources_2024.html#courses",
    "href": "blog/posts/linkedin/favorite_books_and_resources_2024.html#courses",
    "title": "Favorite Books and Resources of 2024",
    "section": "Courses",
    "text": "Courses\n\nHere are some of my favorite courses from ~2024. All of these courses/videos are now offered freely:\n\nfastai part 2 has been a great course for stable diffusion and brushing up on deep learning foundations.\n\nAndrej Karpathy’s GPT videos are phenomenal.\nParlance Labs’ course has been highly instructive for more advanced LLM topics.\n\nHarrison Chase has a great series of courses on LangChain offered freely through deeplearning.ai."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "You can email me at dpopovvelasco{at}gmail.com\nMy LinkedIn is https://www.linkedin.com/in/dapvelasco"
  },
  {
    "objectID": "projects/indiv_projects/gpt2-from-scratch-to-tinystories/sql-finetune.html",
    "href": "projects/indiv_projects/gpt2-from-scratch-to-tinystories/sql-finetune.html",
    "title": "GPT-2 From Scratch to Tiny Stories",
    "section": "",
    "text": "Source: Attention is all you need by Vaswani et al.\n\n\nThe goal of this project is to build a GPT-2 transformer from scratch in PyTorch, making speed and memory optimizations, training a custom tokenizer (and discussing it’s mechanics), further tuning the model parameters to work best for TinyStories training on two 3090s, and finally generating stories from the trained model.\n\nPart 1: Build GPT2.ipynb builds the foundational GPT-2 architecture, mostly following Andrej Karpathy.\nPart 2: Optimize GPT-2.ipynb optimizes GPT-2 training process, again following Andrej Karpathy, with adjustments made for dual-3090 setup.\nPart 3: Train GPT-2.ipynb trains a custom Hugging Face tokenizer and performs custom training on Tiny Stories dataset roneneldan/TinyStories. Once trained from scratch, GPT-2 is able to generate coherent stories for children.\nPart 4: Tokenization mechanics overview.ipynb describes tokenization mechanics and challenges.\nEach part is described in a corresponding blog post here.\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/detecting-anomalies/detecting-anomalies.html",
    "href": "projects/indiv_projects/detecting-anomalies/detecting-anomalies.html",
    "title": "Detecting anomalies in a statewide housing market with alternative data",
    "section": "",
    "text": "Source: Image by the authors\n\n\n\nNYC DSA project with Zach Stone. Detected housing market anomalies with returns of over $100K/property in Georgia for Haystacks.ai by building a robust web scraping solution using Python/GCP and co-developing novel agglomerative clustering techniques (topological data analysis).\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/publication/bulletin-publication.html",
    "href": "projects/indiv_projects/publication/bulletin-publication.html",
    "title": "COMPARATIVE STATICS OF A MONOPOLISTIC FIRM FACING RATE-OF-RETURN AND COMMAND-AND-CONTROL POLLUTION CONSTRAINTS",
    "section": "",
    "text": "Source: COMPARATIVE STATICS OF A MONOPOLISTIC FIRM FACING RATE-OF-RETURN AND COMMAND-AND-CONTROL POLLUTION CONSTRAINTS\n\n\n\n\nThe intrinsic comparative statics properties of a general rate-of-return regulated, profit-maximizing model of a monopolist facing a command-and-control pollution constraint are derived. Recent advances in the theory of comparative statics are used to derive the basic comparative statics of the model, which are contained in an observable negative semi-definite matrix and possess the form of Slutsky-like expressions. We consider several command-and-control pollution constraints that are commonly implemented in practice, and conclude that the intrinsic comparative statics properties of the model are qualitatively invariant to the type of command-and-control pollution constraint imposed. We compare our results with those extant, and find that several basic results from the standard A–J model no longer hold in our model.\nPublication link"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html",
    "href": "notes/AcceleratingMatrixMultiplication.html",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "",
    "text": "from pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\nimport torch\nimport numpy as np\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=140)\ntorch.manual_seed(1)\nweights = torch.randn(100,500) #flattenned out mnist digit * 10 possible digits\nbias = torch.zeros(500)\nA = torch.randn(5,1000)\nB = torch.randn(1000,500)\nA.shape,B.shape\n\n(torch.Size([5, 1000]), torch.Size([1000, 500]))\nAr,Ac = A.shape # n_rows * n_cols\nBr,Bc = B.shape\n(Ar,Ac),(Br,Bc)\n\n((5, 1000), (1000, 500))\nC = torch.zeros(Ar, Bc) # will store product of A and B\nC.shape\n\ntorch.Size([5, 500])"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#matmul-with-numba-for-speeding-up-the-dot-product",
    "href": "notes/AcceleratingMatrixMultiplication.html#matmul-with-numba-for-speeding-up-the-dot-product",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Matmul with numba for speeding up the dot product",
    "text": "Matmul with numba for speeding up the dot product\n\nfrom numba import njit\nfrom numpy import array\n\n\n@njit\ndef dot(a,b):\n  res = 0.\n  for i in range(len(a)): res += a[i]*b[i]\n  return res\n\n\ndot(array([1,2,3]),array([2,0,1]))\n\n5.0\n\n\n\ndef matmul_numba(A,B):\n  \"\"\"\n  Perform matrix multiplication matrices of A and B with\n  inner product optimized with numba\n  \"\"\"\n  Ar, Ac = A.shape\n  Br, Bc = B.shape\n  C = torch.zeros(Ar, Bc)\n  for i in range(Ar):\n    for j in range(Bc):\n      C[i,j] = dot(A[i,:],B[:,j])\n  return C\n\n\nmatmul_numba(A.numpy(),B.numpy())\n\ntensor([[ -47.17,  -54.96,  -10.88,  ...,   21.26,    4.28,   -4.78],\n        [  77.24,   23.64,  -20.62,  ...,   13.62,  -26.03,   22.42],\n        [-108.64,   27.16,   49.40,  ...,   27.56,    9.35,   16.46],\n        [ -13.44,   45.17,   -2.30,  ...,  -79.52,  -58.32,  -13.37],\n        [ -21.50,  -12.12,   55.95,  ...,   28.55,  -32.96,  -35.81]])\n\n\n\nnp.allclose(matmul_numba(A.numpy(),B.numpy()), reference.numpy())\n\nFalse\n\n\n\nAnp, Bnp = A.numpy(), B.numpy()\n%timeit -n 50 matmul_numba(Anp,Bnp)\n\n7.14 ms ± 31.3 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/344:.0f}\")\n\nSpeedup factor over naive matmul: 2151"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#matmul-with-pytorch-inner-product",
    "href": "notes/AcceleratingMatrixMultiplication.html#matmul-with-pytorch-inner-product",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Matmul with PyTorch inner product",
    "text": "Matmul with PyTorch inner product\n\ndef matmul_innertorch(A,B):\n  Ar, Ac = A.shape\n  Br, Bc = B.shape\n  C = torch.zeros(Ar, Bc)\n  for i in range(Ar):\n    for j in range(Bc):\n      C[i,j] = (A[i,:]*B[:,j]).sum()\n  return C\n\n\nmatmul_innertorch(A,B)\n\ntensor([[ -47.17,  -54.96,  -10.88,  ...,   21.26,    4.28,   -4.78],\n        [  77.24,   23.64,  -20.62,  ...,   13.62,  -26.03,   22.42],\n        [-108.64,   27.16,   49.40,  ...,   27.56,    9.35,   16.46],\n        [ -13.44,   45.17,   -2.30,  ...,  -79.52,  -58.32,  -13.37],\n        [ -21.50,  -12.12,   55.95,  ...,   28.55,  -32.96,  -35.81]])\n\n\n\ntorch.allclose(reference,matmul_innertorch(A, B),atol=1e-04, rtol=1e-04)\n\nTrue\n\n\n\n%timeit -n 50 _=matmul_innertorch(A, B)\n\n18 ms ± 575 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/882:.0f}\")\n\nSpeedup factor over naive matmul: 839"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#matmul-with-broadcasting",
    "href": "notes/AcceleratingMatrixMultiplication.html#matmul-with-broadcasting",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Matmul with broadcasting",
    "text": "Matmul with broadcasting\n\nWe can multiply each row of A by all columns of B simultaneously.\n\nA[i,:] is [1000] of size while B is of size [1000, 10].\n\nBy adding an extra dimension to A[i,:] via A[i,:,None] (or A[i,:].unsqueeze(1)), we get shape [1000, 1] and are able to broadcast along the column dimension.\n-To put it losely, each row of A is stretched out into a column and multiplied by B; when summed, this yields a row of the final product. We then need to iterate along the rows only, reducing the number of for loops from three with naive matmul to one with broadcasting.\n\n\nprint(f\"\"\"B.shape: {B.shape} \\n\\nA[i,:].shape: {A[i,:].shape} \\n\nA[i,:].unsqueeze(1).shape: {A[i,:].unsqueeze(1).shape} \\n\nA[i,:,None].shape: {A[i,:,None].shape} \\n\n(A[i,:,None]*B).sum(dim=0).shape: {(A[i,:,None]*B).sum(dim=0).shape}\"\"\")\n\nB.shape: torch.Size([1000, 500]) \n\nA[i,:].shape: torch.Size([1000]) \n\nA[i,:].unsqueeze(1).shape: torch.Size([1000, 1]) \n\nA[i,:,None].shape: torch.Size([1000, 1]) \n\n(A[i,:,None]*B).sum(dim=0).shape: torch.Size([500])\n\n\n\nTo be convinced, uncomment the line below and experiment with A[i,:].shape; A[i,:,None].shape, (A[i,:,None]*B).shape; (A[i,:,None]*B).sum(dim=0), keeping in mind that final shape is 10*5 here\n\n\ndef matmul_broadcast(A,B):\n  Ar, Ac = A.shape\n  Br, Bc = B.shape\n  C = torch.zeros(Ar, Bc)\n  for i in range(Ar):\n    C[i] = (A[i,:,None]*B).sum(dim=0)\n    #import pdb; pdb.set_trace()\n  return C\n\n\nmatmul_broadcast(A,B)\n\ntensor([[ -47.17,  -54.96,  -10.88,  ...,   21.26,    4.28,   -4.78],\n        [  77.24,   23.64,  -20.62,  ...,   13.62,  -26.03,   22.42],\n        [-108.64,   27.16,   49.40,  ...,   27.56,    9.35,   16.46],\n        [ -13.44,   45.17,   -2.30,  ...,  -79.52,  -58.32,  -13.37],\n        [ -21.50,  -12.12,   55.95,  ...,   28.55,  -32.96,  -35.81]])\n\n\n\ntorch.allclose(reference,matmul_broadcast(A, B),atol=1e-04, rtol=1e-04)\n\nTrue\n\n\n\n%timeit -n 50 _=matmul_broadcast(A, B)\n\n358 μs ± 36 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/180:.0f}\")\n\nSpeedup factor over naive matmul: 4111"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#matmul-via-einstein-summation",
    "href": "notes/AcceleratingMatrixMultiplication.html#matmul-via-einstein-summation",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Matmul via Einstein summation",
    "text": "Matmul via Einstein summation\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. The key rules are:\n\nRepeating letters between input arrays means that values along those axes will be multiplied together.\nOmitting a letter from the output means that values along that axis will be summed.\n\n\ndef matmul_einsum(A,B):\n  return torch.einsum('ik,kj-&gt;ij',A,B)\n\n\n%timeit -n 50 _=matmul_einsum(A,B)\n\nThe slowest run took 8.89 times longer than the fastest. This could mean that an intermediate result is being cached.\n45.8 μs ± 49.8 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/77:.0f}\")\n\nSpeedup factor over naive matmul: 9610"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#default-pytorch-matmul-on-cpu",
    "href": "notes/AcceleratingMatrixMultiplication.html#default-pytorch-matmul-on-cpu",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Default PyTorch matmul on CPU",
    "text": "Default PyTorch matmul on CPU\nWe can use pytorch’s function or operator directly for matrix multiplication.\n\ntorch.allclose(reference, A.to('cpu')@B.to('cpu'))\n\nTrue\n\n\n\n# Warm-up run\nAcpu, Bcpu = A.to('cpu'),B.to('cpu')\n%timeit -n 50 _=Acpu@Bcpu\n\n12.5 μs ± 1.02 μs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/65.6:.0f}\")\n\nSpeedup factor over naive matmul: 11280\n\n\n\ntorch.cuda.is_available()\n\nTrue"
  },
  {
    "objectID": "notes/AcceleratingMatrixMultiplication.html#default-pytorch-matmul-on-gpu",
    "href": "notes/AcceleratingMatrixMultiplication.html#default-pytorch-matmul-on-gpu",
    "title": "Speeding up matrix multiplication ~ 5 million times",
    "section": "Default PyTorch matmul on GPU",
    "text": "Default PyTorch matmul on GPU\n\n# Warm-up run\n%timeit -n 50 _=A@B\n\n17.9 µs ± 6.4 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n\n\n\nprint(f\"Speedup factor over naive matmul: {740000/18.3:.0f}\")\n\nSpeedup factor over naive matmul: 40437\n\n\n\nC.shape\n\ntorch.Size([5, 10])\n\n\n\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n\n(1, 1)\n\n\n\n# matmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\n# r = rg.copy_to_host()\n# torch.allclose(C, r)\n\n\ntype(C),type(r)\n\n(torch.Tensor, numpy.ndarray)\n\n\n\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n\n/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n  warn(NumbaPerformanceWarning(msg))\n\n\nThe slowest run took 136.47 times longer than the fastest. This could mean that an intermediate result is being cached.\n8.26 ms ± 19.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nr=(m1c@m2c).cpu()\n\n\n%timeit -n 10 r=(m1c@m2c).cpu()\n\nOur broadcasting version was &gt;500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!"
  },
  {
    "objectID": "notes/Docker/Docker.html",
    "href": "notes/Docker/Docker.html",
    "title": "Docker notes",
    "section": "",
    "text": "Containers are isolated environments using the kernel of your host OS\n\nThey are running instances of images which are “blueprints” or snapshots for containers\nThey are build in layers, which can be used across multiple images\n\nImages ~ Classes: Specified via Dockerfile\nContainers ~ Objects: instantiations of images\nDockerfile: Text document containing commands user would call on the command line to assemble an image\nBasic Dockerfile launching a shell with Python 3.8 available:"
  },
  {
    "objectID": "notes/Docker/Docker.html#docker-abcs-running-a-simple-application-with-docker-volume-to-persist-data",
    "href": "notes/Docker/Docker.html#docker-abcs-running-a-simple-application-with-docker-volume-to-persist-data",
    "title": "Docker notes",
    "section": "Docker ABCs: Running a simple application with Docker volume to persist data",
    "text": "Docker ABCs: Running a simple application with Docker volume to persist data\n\nStep 1: Create Dockerfile\n\n\n\n\nimage.png\n\n\n\nStep 2: Create simple Python application\n\n\n# app.py\nimport os\nimport time\n\n# Create a file and write some content to it\nwith open(\"output.txt\", \"a\") as f:\n    f.write(f\"Hello from Docker!\\nTime: {time.time()}\\n\")\n\nprint(\"File written successfully.\")\n\n\nStep 3: Create requirements.txt to specify Python dependencies.\nStep 4: Build the Docker image using Dockerfile and name it (here my-python-app)\n\n\n%%bash\ndocker build -t my-python-app .\n\n\nStep 5: Run the Docker image with a volume\n\n\n%%bash\ndocker run -v $(pwd):/app my-python-app\n\n\nCan also run interactively and run from shell from inside a container:\n\nWhen run the container, application will write message + time\nCan run python app.py from inside the container, which will keep appending to the file\nexit and the changes will persist\n\n\n\n%%bash\ndocker run -it --name my-python-app-container -v $(pwd):/app my-python-app /bin/bash\n\n\nDon’t forget to remove the container with docker rm my-python-app-container, or better yet, run with rm flag as below:\n\n\n%%bash\ndocker run --rm -it --name my-python-app-container -v $(pwd):/app my-python-app /bin/bash"
  },
  {
    "objectID": "notes/Docker/Docker.html#following-learn-docker-in-a-month-of-lunches-by-elton-stoneman",
    "href": "notes/Docker/Docker.html#following-learn-docker-in-a-month-of-lunches-by-elton-stoneman",
    "title": "Docker notes",
    "section": "Following Learn Docker in a Month of Lunches by Elton Stoneman",
    "text": "Following Learn Docker in a Month of Lunches by Elton Stoneman\n\nCh.2: Understanding Docker and Hello World\n\n\n\nimage.png\n\n\n\nNote that each container shares the operating system of the host computer!\ndocker container ls: lists running containers; add –all to see exited containers\ndocker container logs {start_of_container_id}: displays log entries collected by container\ndocker container stats {start_of_container_id}: displays stats about CPU, memory, network, and disk the container is using\ndocker container inspect {start_of_container_id}: displays low level details of the container\nNotes on ‘Exited’ containers: i.) They no longer consume CPU time or memory, ii.) They still take up disk space and can be restarted, can also check logs and copy files to/from container’s filesystem.\ndocker container run --detach --publish 8088:80 diamol/ch02-hello-diamol-web:\n\n–detach starts the container in the background and shows container id\n–publish publishes a port from the container to the computer. When Docker is installed, it injects itself into computer’s networking layer. Publishing means Docker will listen to network traffic on computer port and send it to the container.\n\n\ndocker container rm --force $(docker container ls --all --quiet): remove all containers, use with caution since it does not ask for confirmation.\nCLI sends requests to Docker API, which then interacts with the Docker Engine.\nExercise: replace index.html inside the container to change website front matter.\n\nUsed docker container to find cp from among the available commands.\nUsed docker exec jovial_meninsky ls -R to list container’s file system info.\nHad to find absolute path since relative paths would not work -&gt; docker exec jovial_meninsky pwd for path prefix\nSolution: docker container cp diamol/ch02/exercises/hello-diamol-web/html/index.html jovial_meninsky:/usr/local/apache2/htdocs/index.html Successfully copied 2.05kB to jovial_meninsky:/usr/local/apache2/htdocs/index.html\n\n\n\n\nCh. 3: Building your own docker images with Dockerfile\n\ndocker container run will download image locally if it’s not on the machine because software distribution is built into the Docker platform. Or can use docker image pull.\nDocker images may be packaged with default set of config values, but you should be able to provide different config settings when running a container (using environment variables for example, which are just key/value pairs provided by the operating system). For example, replace TARGET environment variable with google.com below:\n\ndocker container run –env TARGET=google.com diamol/ch03-web-ping\n\nFrom a directory containing a Dockerfile, build an image with current directory as context via:\n\ndocker image build --tag web-ping .\n\nProcess for running apps with Docker:\n\nWrite Dockerfile with steps to package the app\nCollect the resources that need to go into Docker image\nDecide how you want users of image to configure app behavior\n\nImage layers can be shared between different images and different containers. \nNotice how each gcr and mintonano image are listed at 17.4GB yet docker system df lists total usage at less than 18GB: This is because the layers between these two large images are shared!\n\n\n\n\nimage.png\n\n\n\nBecause image layers can be shared, they can’t be edited (read-only).\nIf contents/commands of a given step change, the following steps will be re-executed; previous layers are used from cache. Docker knows that previous layers didn’t change because their hash hasn’t changed (hash is made from instructions and files being copied).\nOptimize the file by moving parts unlikely to change towards the top and combining commands:\n\n\n\n\nimage-2.png\n\n\n\nLab 3: Modifying contents of Docker container without modifying Dockerfile. \nNote that I ran docker commit while the diamol/ch03-lab was running, before typing exit. Then can go to the newly created container that now has new contents.\n\n\n\nCh. 4: Packaging applications from source code into Docker images\nIn the provided Dockerfile, the build tools and dependencies are excluded from the final image by using a multi-stage build. Here’s how it works step-by-step:\n\n\nMulti-Stage Build Explanation\n\nBuilder Stage: ```Dockerfile FROM diamol/maven AS builder\nWORKDIR /usr/src/iotd COPY pom.xml . RUN mvn -B dependency:go-offline\nCOPY . . RUN mvn package ```\n\nBase Image: diamol/maven is used as the base image, which includes Maven and all necessary build tools.\nWorking Directory: Sets the working directory to /usr/src/iotd.\nCopy pom.xml: Copies the pom.xml file to the container.\nDownload Dependencies: Runs mvn -B dependency:go-offline to download all dependencies specified in the pom.xml file.\nCopy Source Code: Copies the entire source code to the container.\nBuild the Project: Runs mvn package to compile the source code and package it into a JAR file.\n\nApplication Stage: ```Dockerfile FROM diamol/openjdk\nWORKDIR /app COPY –from=builder /usr/src/iotd/target/iotd-service-0.1.0.jar .\nEXPOSE 80 ENTRYPOINT [“java”, “-jar”, “/app/iotd-service-0.1.0.jar”] ```\n\nBase Image: diamol/openjdk is used as the base image, which includes only the JDK runtime environment.\nWorking Directory: Sets the working directory to /app.\nCopy JAR File: Copies the JAR file from the builder stage to the /app directory in the application stage using COPY --from=builder.\nExpose Port: Informs Docker that the container will listen on port 80.\nSet Entry Point: Sets the command to run the JAR file using java -jar.\n\n\n\n\nHow Build Tools and Dependencies are Excluded\n\nBuilder Stage: The builder stage uses the diamol/maven image, which includes Maven and all necessary build tools to compile the Java application. This stage performs all the build-related tasks, such as downloading dependencies and packaging the application into a JAR file.\nApplication Stage: The app stage uses the diamol/openjdk image, which only includes the JDK runtime environment. It does not include Maven or any other build tools.\n\nThe COPY --from=builder command copies only the compiled JAR file from the builder stage to the app stage. This means that none of the build tools, source code, or dependencies downloaded during the build process are included in the final image.\nAs a result, the final image is smaller and only contains the runtime environment and the packaged application, making it more efficient and secure.\n\n\nBy using multi-stage builds, you ensure that the final Docker image contains only what is necessary to run the application, excluding all build tools and intermediate files used during the build process.\n\ndocker network create nat: creates a Docker network, allowing any containers on the network to reach each other using the container names via a bridge network. Else containers can communicate via host’s networking space.\nTo run, use docker run --name iotd -p 800:80 --network nat image-of-the-day. Will run NASA’s image of the day REST API so you can make repeated calls to this application without hitting NASA’s service.\n\n\nExample in interpreted application\n\nLet’s break down the purpose of the final WORKDIR and COPY commands in the context of the provided Dockerfile:\n\nFROM diamol/node AS builder\n\nWORKDIR /src\nCOPY src/package.json .\nRUN npm install\n\n# app\nFROM diamol/node\n\nEXPOSE 80\nCMD [\"node\", \"server.js\"]\n\nWORKDIR /app\nCOPY --from=builder /src/node_modules/ /app/node_modules/\n\n\n\nApplication Stage\n\nFROM diamol/node\nFROM diamol/node\n\nPurpose: Uses the diamol/node image as the base image for the application stage.\nEffect: This stage will be used to run the application.\n\n\n\nEXPOSE 80\nEXPOSE 80\n\nPurpose: Informs Docker that the container will listen on port 80 at runtime.\nEffect: This is a documentation feature and does not actually publish the port.\n\n\n\nCMD [“node”, “server.js”]\nCMD [\"node\", \"server.js\"]\n\nPurpose: Sets the default command to run when the container starts.\nEffect: The container will run node server.js when it starts.\n\n\n\nWORKDIR /app\nWORKDIR /app\n\nPurpose: Sets the working directory to /app for subsequent instructions in the application stage.\nEffect: Any relative paths in subsequent commands will be relative to /app.\n\n\n\nCOPY –from=builder /src/node_modules/ /app/node_modules/\nCOPY --from=builder /src/node_modules/ /app/node_modules/\n\nPurpose: Copies the node_modules directory from the builder stage to the /app directory in the application stage.\nEffect: This ensures that the application has access to the installed dependencies without including the entire build context.\n\n\n\n\nSummary\n\nWORKDIR /app: Sets the working directory to /app for subsequent instructions. This helps in organizing the application files within the container.\nCOPY –from=builder /src/node_modules/ /app/node_modules/: Copies the node_modules directory from the builder stage to the /app directory in the final image. This ensures that the final image contains the necessary runtime dependencies without including the entire build context.\n\nBy using these commands, the Dockerfile ensures that the final image is clean and contains only the necessary files to run the application, while keeping the build process and dependencies separate.\n\nExample of ‘refactoring’ Dockerfile from ~500MB -&gt; ~17MB on Linux.\n\n\nFROM diamol/golang \n\nWORKDIR web\nCOPY index.html .\nCOPY main.go .\n\nRUN go build -o /web/server\nRUN chmod +x /web/server\n\nCMD [\"/web/server\"]\nENV USER=sixeyed\nEXPOSE 80\n\n\nThe image built with this file includes the go compiler and other unnecessary build tools. In addition, any time we change index.html or anything prior to the RUN commands, the entire application will be rebuilt since RUN commands are in layers following these changes. The final-stage second image will not contain the build tools and if we change index.html, only the final layer will be rebuilt since others will come from cache.\n\n\nFROM diamol/golang AS builder\n\nCOPY main.go .\nRUN go build -o /server\nRUN chmod +x /server\n\n# app\nFROM diamol/base\n\nEXPOSE 80\nCMD [\"/web/server\"]\nENV USER=\"sixeyed\"\n\nWORKDIR web\nCOPY --from=builder /server .\nCOPY index.html .\n\n\n\nCh. 5: Sharing images with Docker Hub and other registries\n\n\n\nimage.png\n\n\n\nPushing images to Docker Hub:\n\nSet dockerId environment variable: export dockerId=\"my-docker-id\" (used to log in to Docker Hub)\ndocker login --username $dockerId\ndocker image tag image-gallery $dockerId/image-gallery:v1 (note that tagging an image built locally will not rebuild the image, will just make another reference to the same image)\ndocker image push $dockerId/image-gallery:v1\n\nNote that free version of Docker Hub only supports public repositories.\n\n\n\nOther Notes\n\nRestarting docker service installed with snap:\n\n\n\n\nimage.png\n\n\n\nDocker under the hood: containerd -&gt; Kernel (Namespaces, Cgroups, system processes)\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nUnderstanding Namespaces and cgroups in containerd\ncontainerd leverages Linux kernel features, specifically Namespaces and cgroups (control groups), to provide container isolation and resource management. These technologies are fundamental to how containers operate securely and efficiently on a host system.\n\n\n1. Namespaces\nNamespaces provide isolation for various system resources, ensuring that processes within a container are separated from those in other containers and the host system. Each namespace type isolates a specific resource.\n\nTypes of Namespaces Used by containerd:\n\n\n\n\n\n\n\nNamespace Type\nDescription\n\n\n\n\nPID Namespace\nIsolates the process IDs, so processes inside the container cannot see or interact with processes outside.\n\n\nNET Namespace\nIsolates networking resources like network interfaces, IP addresses, and routing tables.\n\n\nIPC Namespace\nIsolates inter-process communication resources, such as shared memory and semaphores.\n\n\nUTS Namespace\nIsolates hostname and domain name settings, allowing containers to have their own hostname.\n\n\nMNT Namespace\nIsolates mount points, ensuring the container has its own filesystem hierarchy.\n\n\nUSER Namespace\nIsolates user and group IDs, allowing containers to have a different set of user privileges.\n\n\n\n\n\nHow containerd Uses Namespaces:\n\nProcess Isolation: Ensures that container processes run independently without interfering with host or other container processes.\nNetwork Isolation: Allows containers to have separate networking stacks, enabling features like port mapping and virtual networks.\nFilesystem Isolation: Provides containers with their own root filesystem, preventing unauthorized access to the host filesystem.\nSecurity: Enhances security by limiting the visibility and accessibility of system resources to containers.\n\n\n\nExample: PID Namespace Isolation\n# On the host\nps aux\n\n# Inside a container\nps aux\n# The container only sees its own processes, not those on the host.\n\n\n\n\n2. cgroups (Control Groups)\ncgroups manage and limit the resources (CPU, memory, disk I/O, network) that containers can consume. They ensure that containers operate within defined resource boundaries, preventing any single container from monopolizing system resources.\n\nKey Features of cgroups:\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nResource Limiting\nSets maximum resource usage (e.g., CPU shares, memory limits) for containers.\n\n\nPrioritization\nAllocates resources based on priority, ensuring critical containers receive necessary resources.\n\n\nAccounting\nMonitors and tracks resource usage by containers for reporting and analysis.\n\n\nControl\nProvides mechanisms to freeze, resume, or modify resource allocations of running containers.\n\n\n\n\n\nHow containerd Uses cgroups:\n\nResource Allocation: Defines limits on CPU, memory, and other resources for each container to ensure fair distribution.\nPerformance Management: Prevents resource overuse by containers, maintaining overall system stability and performance.\nScalability: Enables efficient scaling of containers by managing resource distribution dynamically based on container demands.\n\n\n\nExample: Memory Limiting with cgroups\n# Define a memory limit for a container\ndocker run -d --name limited-container --memory=\"256m\" nginx\n\n# Verify the memory limit using cgroups\ncat /sys/fs/cgroup/memory/docker/&lt;container-id&gt;/memory.limit_in_bytes\n# Output: 268435456 (256MB)\n\n\n\n\n3. Integration of Namespaces and cgroups in containerd\ncontainerd orchestrates the creation and management of both Namespaces and cgroups to ensure containers are isolated and resource-constrained appropriately.\n\nWorkflow Overview:\n\nContainer Creation:\n\ncontainerd initiates the creation of a container using the specified image.\n\nNamespace Setup:\n\nAllocates and configures appropriate namespaces to isolate the container’s environment.\n\ncgroups Configuration:\n\nAssigns the container to specific cgroups with defined resource limits.\n\nProcess Execution:\n\nLaunches the container’s main process within the isolated namespaces and under the resource constraints of the cgroups.\n\nMonitoring and Management:\n\nContinuously monitors resource usage and enforces cgroup limits.\nManages the lifecycle of the container, including starting, stopping, and deletion.\n\n\n\n\n\n4. Benefits of Using Namespaces and cgroups with containerd\n\nIsolation: Ensures containers run securely and independently without affecting each other or the host system.\nResource Efficiency: Optimizes resource usage by enforcing limits and preventing overconsumption.\nScalability: Facilitates the deployment of multiple containers on a single host by managing resource distribution effectively.\nSecurity: Enhances security by restricting containers’ access to system resources and isolating their environments.\nPerformance: Maintains system stability and performance by preventing resource contention among containers.\n\n\n\n\n\nSummary\n\nNamespaces: Provide isolation for container processes by segregating various system resources, ensuring containers operate independently and securely.\ncgroups: Manage and limit resource usage for containers, maintaining system stability and efficient resource distribution.\ncontainerd: Utilizes both Namespaces and cgroups to orchestrate container lifecycles, ensuring isolation, security, and resource management."
  },
  {
    "objectID": "notes/LlmBasicsAndRecipes.html",
    "href": "notes/LlmBasicsAndRecipes.html",
    "title": "LLMs/PyTorch Basics and Recipes",
    "section": "",
    "text": "image.png\n\n\n\n\n\n\nIf affine, initialize gamma and beta\nInitialize running mean and variance buffers. Note: For variance, set unbiased=False. This is because the biased estimator (dividing by N instead of N-1 in the variance formula) is MLE estimator of variance under iid and normally distributed assumptions. Also, the data will be normalized consistently even if batch size is 1.\nGet mean/var over nonchannel dimensions\nUpdate running mean and variance statistics using momentum running average without tracking gradients\nIf training, use the computed mean/var; else use running mean/var. Thus the running mean/var are computed on the fly for inference only.\nNormalize the input\nApply gamma and beta if they’re there (affine)\nReturn normalized input\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass BatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(BatchNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # Learnable affine parameters: gamma (scale) and beta (shift)\n        self.gamma, self.beta = None, None\n        if affine:\n            self.gamma = nn.Parameter(torch.ones(num_features))  # scale parameter\n            self.beta = nn.Parameter(torch.zeros(num_features))  # shift parameter\n\n        # Running statistics for mean and variance\n        self.running_mean = torch.zeros(num_features) # mean 0\n        self.running_var = torch.ones(num_features)   # var 1\n\n    def forward(self, x):\n        # Calculate mean and variance across the batch and spatial dimensions (H, W)\n        batch_mean = x.mean(dim=(0, 2, 3), keepdim=True)  # mean across N, H, W for each channel\n        batch_var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True)  # variance across N, H, W for each channel\n\n        # Update running statistics (mean and variance): See batch_mean dimension note below\n        with torch.no_grad(): # running mean/var are updated via moving avg approach, not SGD\n          self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.squeeze()\n          self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.squeeze()\n\n        # During training, we use the batch statistics\n        if self.training:\n            mean = batch_mean\n            var = batch_var\n        else:\n            # During inference, we use the running statistics\n            mean = self.running_mean.view(1, self.num_features, 1, 1)\n            var = self.running_var.view(1, self.num_features, 1, 1)\n            # mean = self.running_mean[None, :, None, None] # Alternatively...\n            # var = self.running_var[None, :, None, None]\n\n        # Normalize the input\n        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n\n        # Apply the affine transformation (scaling and shifting)\n        if self.gamma is not None and self.beta is not None:\n            x_normalized = self.gamma.view(1, self.num_features, 1, 1) * x_normalized + self.beta.view(1, self.num_features, 1, 1)\n            #x_normalized = self.gamma[None, :, None, None] * x_normalized + self.beta[None,:, None, None] # Alternatively\n\n        return x_normalized\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create random input with shape (batch_size, num_features, height, width)\n    x = torch.randn(32, 64, 28, 28)  # batch_size=32, num_features=64 (channels), height=28, width=28\n\n    # Instantiate the BatchNorm2d layer\n    batch_norm = BatchNorm2d(num_features=64)\n\n    # Forward pass through the batch norm layer\n    output = batch_norm(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n\n\nBatch mean dimension: Input is of dimension (batch_size, channels, height, width). When PyTorch computes the mean over dim=(0,2,3), it keeps dim=1 (channel dimension) intact, collapses the batch dimension into 1 and height+width spacial dimensions into one. So with 64 channels, the shape of mean over dim=(0,2,3) will be (64=uncollapsed channel,1=collapsed bs,1=collapsed hxw). Will want to squeeze out the 1’s to get (64,)-shaped tensor.\nAK: In batch norm, examples in the batch are coupled mathematically. Activations will change depending on the given batch since mean and variance statistics would change. Interestingly, this turns out to be good due to a ‘regularizer’ effect: each individual example is ‘padded out’, making it harder for neural net to overfit on any specific example. Still, bugs can arise and other types of normalization have been proposed. Nonetheless, batch norm is often the most effective.\n\nFor example, must be careful to always correctly place the model into training and eval mode with model.train() and model.eval().\nDuring transfer learning, will likely want to unfreeze the batch norm layers to allow them to adapt (requires_grad=True)\nIf using distributed training, each replica computes BatchNorm statistics independently, possibly leading to discrepancies between single-device and multi-device training. Use nn.SyncBatchNorm.\n\n\n\n\n\n\nimport torch\n\nclass AdamOptimizer:\n    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0):\n        \"\"\"\n        Adam optimizer from scratch for PyTorch.\n\n        Args:\n            params (iterable): The parameters to optimize (typically model.parameters()).\n            lr (float): Learning rate (default 1e-3).\n            beta1 (float): Exponential decay rate for first moment estimate (default 0.9).\n            beta2 (float): Exponential decay rate for second moment estimate (default 0.999).\n            epsilon (float): Term added to the denominator to avoid division by zero (default 1e-8).\n            weight_decay (float): Weight decay (L2 penalty) (default 0).\n        \"\"\"\n        self.params = list(params)\n        self.lr = lr\n        self.beta1 = beta1 # weight on past param mean\n        self.beta2 = beta2 # weight on past param var\n        self.epsilon = epsilon \n        self.weight_decay = weight_decay # how much of param to include in the grad\n\n        # Initialize moment estimates\n        self.m = [torch.zeros_like(param) for param in self.params]  # First moment (m)\n        self.v = [torch.zeros_like(param) for param in self.params]  # Second moment (v)\n\n        # Time step counter\n        self.t = 0\n\n    def step(self):\n        \"\"\"\n        Perform a single optimization step.\n        \"\"\"\n        self.t += 1 # important for bias correction below\n        with torch.no_grad():\n            for i, param in enumerate(self.params):\n                if param.grad is None: continue\n                # Get the gradients for the current parameter\n                grad = param.grad\n                # Apply weight decay (L2 penalty): weighting grad linearly ~ weighting weight quadratically w/ L2\n                if self.weight_decay != 0: grad = grad + self.weight_decay * param\n                # Update biased first and second moment estimates\n                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n\n                # Bias correction\n                m_hat = self.m[i] / (1 - self.beta1**self.t)\n                v_hat = self.v[i] / (1 - self.beta2**self.t)\n\n                # Update parameters\n                param -= self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n    def zero_grad(self):\n        \"\"\"\n        Reset gradients of all parameters.\n        \"\"\"\n        for param in self.params:\n            if param.grad is not None:\n                param.grad.zero_()\n\n\n\n\nimage.png\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n# Define a simple neural network\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 256)\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)  # Flatten the input\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Set up data loaders for MNIST\ntransform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n# Initialize the network and optimizer\nmodel = SimpleNet()\noptimizer = AdamOptimizer(model.parameters(), lr=1e-3)\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(1, 6):  # Run for 5 epochs\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Optimization step\n        optimizer.step()\n\n        # Print progress occasionally\n        if batch_idx % 200 == 0:\n            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n\n    # Evaluation on test data\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += criterion(output, target).item()  # Sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)      # Get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader)\n    accuracy = correct / len(test_loader.dataset)\n\n    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy * 100:.2f}%)\\n')\n\nEpoch 1 [0/60000] Loss: 2.307041\nEpoch 1 [12800/60000] Loss: 0.193796\nEpoch 1 [25600/60000] Loss: 0.254285\nEpoch 1 [38400/60000] Loss: 0.195109\nEpoch 1 [51200/60000] Loss: 0.142228\n\nTest set: Average loss: 0.1497, Accuracy: 9551/10000 (95.51%)\n\nEpoch 2 [0/60000] Loss: 0.056716\nEpoch 2 [12800/60000] Loss: 0.116285\nEpoch 2 [25600/60000] Loss: 0.286116\nEpoch 2 [38400/60000] Loss: 0.097069\nEpoch 2 [51200/60000] Loss: 0.040939\n\nTest set: Average loss: 0.1021, Accuracy: 9698/10000 (96.98%)\n\nEpoch 3 [0/60000] Loss: 0.152332\nEpoch 3 [12800/60000] Loss: 0.059672\nEpoch 3 [25600/60000] Loss: 0.074841\nEpoch 3 [38400/60000] Loss: 0.116336\nEpoch 3 [51200/60000] Loss: 0.092772\n\nTest set: Average loss: 0.0838, Accuracy: 9746/10000 (97.46%)\n\nEpoch 4 [0/60000] Loss: 0.078431\nEpoch 4 [12800/60000] Loss: 0.036434\nEpoch 4 [25600/60000] Loss: 0.119966\nEpoch 4 [38400/60000] Loss: 0.120010\nEpoch 4 [51200/60000] Loss: 0.054376\n\nTest set: Average loss: 0.0735, Accuracy: 9771/10000 (97.71%)\n\nEpoch 5 [0/60000] Loss: 0.042105\nEpoch 5 [12800/60000] Loss: 0.036648\nEpoch 5 [25600/60000] Loss: 0.033848\nEpoch 5 [38400/60000] Loss: 0.043520\nEpoch 5 [51200/60000] Loss: 0.071161\n\nTest set: Average loss: 0.0677, Accuracy: 9792/10000 (97.92%)\n\n\n\n\n\n\n\nThe major architectural improvement used in transformers according to Andrej.\nInstead adding a vector to the token embeddings vector, apply a rotation. Take the token embedding and rotate it by position*theta (ex, 4xtheta if there are 4 tokens appearing before the token in question).\n\nKV-Cacheable, like absolute positional embeddings and unlike relative positional embeddings. If add more tokens at the end of the sentence, the tokens at the beginning stay the same.\n\nUnlike absolute positional embeddings and like relative positional embeddings, relative positions of the words are preserved. Looking at words ‘pig’ and ‘dog’ in ‘The pig chased the dog’, ‘Once upon a time, the pig chased the dog’, they will have the same rotation angle in both sentences.\n\n\n\n\n\nSource: https://math.stackexchange.com/questions/1404484/how-is-that-a-rotation-by-an-angle-%CE%B8-about-the-origin-can-be-represented-by-this\n\n\n\n\n\nSource: RoFormer\n\n\n\nApply the linear transformations to get the query and key vectors before applying the rotation matrix: want embeddings for relative positions between tokens.\nIn terms of notation, note that frequencies are represented as:\n\n\n\n\nSource: RoFormer paper, eq.4\n\n\n\nRange of Frequencies: By using a range of frequencies, the model can capture positional information at different scales. Lower frequencies capture long-range dependencies, while higher frequencies capture short-range dependencies.\nAn inefficient implementation would rely on applying matrix multiplication to each pair of coordinates as below:\n\n\n\n\nSource: RoFormer\n\n\n\nA more efficient implementation would use the outer product.\n\n\n\n\nOuter product review\n\n\n\n\n\nSource: RoFormer\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.dim, self.base = dim, base\n        # precompute the sinusoidal embeddings: needed for eq. 4 in paper, \n        # position for denominator derived in forward once know seq_len\n        inv_freq = 1.0/(self.base**(torch.arange(0,dim,2).float()/dim)) \n        self.register_buffer('inv_freq',inv_freq)\n\n    def forward(self, x):\n        # Compute positional embeddings\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n        sinusoid_inp = torch.outer(pos, self.inv_freq) # see equation (34) above; [seq_len, dim // 2]\n        sin_emb = sinusoid_inp.sin()[None, :, :]  # Shape: [1, seq_len, dim // 2]\n        cos_emb = sinusoid_inp.cos()[None, :, :]  # Shape: [1, seq_len, dim // 2]\n\n        # Split the input tensor into even and odd parts\n        x_even = x[:, :, ::2]  # Even indices\n        x_odd = x[:, :, 1::2]  # Odd indices\n\n        # Apply rotary embeddings: best to follow simple 2D setup for intuition\n        x_rotated = torch.zeros_like(x)\n        x_rotated[:, :, ::2] = x_even * cos_emb - x_odd * sin_emb\n        x_rotated[:, :, 1::2] = x_even * sin_emb + x_odd * cos_emb\n\n        return x_rotated\nbatch_size, seq_len, dim = 2, 10, 64\nx = torch.randn(batch_size, seq_len, dim)\n\nrotary_pos_emb = RotaryPositionalEmbedding(dim)\nx_rotary = rotary_pos_emb(x)\n\nprint(\"Input shape:\", x.shape)\nprint(\"Output shape:\", x_rotary.shape)\n\nInput shape: torch.Size([2, 10, 64])\nOutput shape: torch.Size([2, 10, 64])\n\n\n\nWords close together have a larger dot product:\n\n\n\n\nSource: RoFormer\n\n\n\n\n\n\nKVCache itself is a simple class with update and get methods. Update will either create new tensors for k and v at the start or torch.cat new k/v’s with existing ones.\nInside MultiheadSelfAttention, we only pass in the last token to forward so x is of size [batch_size, 1, embed_dim]. This is crucial since the main attn = q @ k, then (post masking and softmax), attn @ v products need to compute only the last token’s attention/attention output.\n\nIf cache is nonempty, we set k to torch.cat([cached_k, k_new], dim=2), ditto for v. Also, update the kv_cache with new values from the batch.\nThe attention scores computation attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.head_dim ** -0.5 looks the same as always BUT AGAIN, q’s seq_len==1 in [batch_size, num_heads, seq_len, seq_len_total], and matmul occurs with full k constructed using the cache: there’s no need to recompute full k @ v, just get the next token since the previous computations are cached.\nSame holds for the attention output computation attn_output = torch.matmul(attn_probs, v).\n\nWithout the cache, each call to forward would only have access to the current token’s keys and values. This would mean the model couldn’t attend to previous tokens, leading to incorrect outputs.\n\n\nimport torch\nimport torch.nn as nn\n\nclass KVCache:\n    def __init__(self):\n        self.k = None  # [batch_size, num_heads, seq_len_cache, head_dim]\n        self.v = None\n\n    def update(self, k_new, v_new):\n        # Append new keys and values to cache\n        if self.k is None:\n            self.k = k_new\n            self.v = v_new\n        else:\n            self.k = torch.cat([self.k, k_new], dim=2)\n            self.v = torch.cat([self.v, v_new], dim=2)\n\n    def get(self):\n        return self.k, self.v\n\nclass MultiheadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n            \n    def forward(self, x, kv_cache=None):\n        batch_size, seq_len, embed_dim = x.size()\n        \n        # Project input to Q, K, V\n        # IMPORTANT: note seq_len == 1, see generation loop below, the main point of the cache\n        qkv = self.qkv_proj(x)  # [batch_size, seq_len, 3 * embed_dim] \n        q, k_new, v_new = qkv.chunk(3, dim=-1)\n        \n        # Reshape and transpose for multi-head attention\n        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, 1, head_dim]\n        k_new = k_new.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v_new = v_new.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        ### --- Start: KV Cache code modifications --- ###\n        if kv_cache is not None:\n            cached_k, cached_v = kv_cache.get()\n            if cached_k is not None: # 1/2 update for the computations\n                k = torch.cat([cached_k, k_new], dim=2)\n                v = torch.cat([cached_v, v_new], dim=2)\n            else:\n                k = k_new\n                v = v_new\n            kv_cache.update(k_new,v_new) # 2/2 update for the cache\n        else:\n            k = k_new\n            v = v_new\n\n        # Compute attention scores: same as always BUT AGAIN, q's seq_len==1 in  [batch_size, num_heads, seq_len, seq_len_total]\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.head_dim ** -0.5   \n        \n        # Create causal mask: note that unlike in a vanilla transformer with a registered buffer of size seq_len by seq_len, it's seq_len by seq_len_total\n        seq_len_total = k.size(2)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len_total, device=x.device)).bool()  # [seq_len, seq_len_total]\n        attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        ### --- End: KV Cache code modifications --- ###\n        \n        # Compute attention probabilities\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        \n        # Compute attention output\n        attn_output = torch.matmul(attn_probs, v)  # [batch_size, num_heads, seq_len, head_dim]\n        \n        # Reshape and project output\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        output = self.out_proj(attn_output)\n        \n        return output\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.self_attn = MultiheadSelfAttention(embed_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.GELU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x, kv_cache=None):\n        # Self-attention with KV cache\n        attn_output = self.self_attn(x, kv_cache=kv_cache)\n        x = x + attn_output\n        x = self.norm1(x)\n    \n        # Feed-forward network\n        ffn_output = self.ffn(x)\n        x = x + ffn_output\n        x = self.norm2(x)\n        return x\n\n# Example usage\nembed_dim = 512\nnum_heads = 8\ndecoder_layer = TransformerDecoderLayer(embed_dim, num_heads)\nkv_cache = KVCache()\n\n# Initial input token\ninput_token = torch.randn(1, 1, embed_dim)  # [batch_size, seq_len=1, embed_dim]\n\n# Autoregressive generation loop\noutput_tokens = []\nfor _ in range(10):\n    output = decoder_layer(input_token, kv_cache=kv_cache) \n    output_tokens.append(output)\n    input_token = output[:, -1:, :]  # Use the last token as the next input; MUST provide kv_cache above for this to work\n\noutput_sequence = torch.cat(output_tokens, dim=1)\nprint(\"Generated sequence shape:\", output_sequence.shape)\n\n\n\n\n\nA custom Dataset class must have __init__(),__len__(), and __getitem__() methods to be used by the data loader.\nSource: Machine Learning with PyTorch and Scikit-Learn by Raschka et al.\n\n\nimport torch\ntorch.manual_seed(42)\nt_x = torch.rand([4,3],dtype = torch.float32)\nt_y = torch.arange(4)\n\nfrom torch.utils.data import Dataset, DataLoader\nclass JointDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = x\n        self.y = y\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\njoint_dataset = JointDataset(t_x,t_y)\n# alternatively, can create a joint dataset using TensorDataset\n# from torch.utils.data import TensorDataset\n# tensor_dataset = JointDataset(t_x,t_y)\n\ndata_loader = DataLoader(dataset=joint_dataset,batch_size=2,shuffle=True) # will shuffle for every epoch!\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nseq_len = 40\nchunk_size = seq_len +1\ntext_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]\n\nclass TextDataset(Dataset):\n    def __init__(self, text_chunks):\n        self.text_chunks = text_chunks\n    def __len__(self):\n        return len(self.text_chunks)\n    def __getitem__(self, index):\n        text_chunk = self.text_chunks[index]\n        return text_chunks[:-1].long(), text_chunks[1:].long()\nseq_dataset = TextDataset(torch.tensor(text_chunks))\nseq_dl = DataLoader(seq_dataset,batch_size=batch_size, shuffle=True, drop_last=True)\n\n\n\n\n\nBy default, the DataLoader will try to stack everything into tensors of the same size, which doesn’t work well for variable-length sequences. Use a custom collate_fn to pad these sequences to a common length, ensuring your batches are properly structured for the model.\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer\nfrom torch.nn.utils.rnn import pad_sequence\n\nPAD_TOKEN = 0\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nclass LanguageModelingDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=512):\n        \"\"\"\n        Args:\n            texts (List[str]): List of text samples.\n            tokenizer (transformers.PreTrainedTokenizer): Tokenizer for encoding text.\n            max_length (int): Maximum sequence length.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.encodings = self.tokenizer(texts, truncation=True, padding=False)  # No padding here\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long)\n\ndef custom_collate_fn(batch):\n    # Same as before\n    input_sequences = [sample[:-1] for sample in batch]\n    target_sequences = [sample[1:] for sample in batch]\n    padded_inputs = pad_sequence(input_sequences, batch_first=True, padding_value=PAD_TOKEN)\n    padded_targets = pad_sequence(target_sequences, batch_first=True, padding_value=PAD_TOKEN)\n    attention_masks = (padded_inputs != PAD_TOKEN).long()\n    return padded_inputs, padded_targets, attention_masks\n\n# Example texts\ntexts = [\n    \"Hello, how are you?\",\n    \"I am fine, thank you!\",\n    \"What are you doing today?\",\n    \"I am working on a language model.\"\n]\n\n# Create Dataset and DataLoader\ndataset = LanguageModelingDataset(texts, tokenizer)\ndataloader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn, shuffle=True)\n\n# Iterate through DataLoader\nfor batch_idx, (inputs, targets, masks) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Inputs:\\n\", inputs)\n    print(\"Targets:\\n\", targets)\n    print(\"Attention Masks:\\n\", masks)\n    print(\"-\" * 50)\n\nBatch 1\nInputs:\n tensor([[ 101, 2054, 2024, 2017, 2725, 2651, 1029,    0,    0],\n        [ 101, 1045, 2572, 2551, 2006, 1037, 2653, 2944, 1012]])\nTargets:\n tensor([[2054, 2024, 2017, 2725, 2651, 1029,  102,    0,    0],\n        [1045, 2572, 2551, 2006, 1037, 2653, 2944, 1012,  102]])\nAttention Masks:\n tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\nBatch 2\nInputs:\n tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,    0],\n        [ 101, 1045, 2572, 2986, 1010, 4067, 2017,  999]])\nTargets:\n tensor([[7592, 1010, 2129, 2024, 2017, 1029,  102,    0],\n        [1045, 2572, 2986, 1010, 4067, 2017,  999,  102]])\nAttention Masks:\n tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\n\n\n\n\n\n\nimport torch.nn as nn\nclass NoisyLinear(nn.Module):\n    def __init__(self,input_size,output_size,noise_stddev=0.1):\n        super().__init__()\n        w = torch.Tensor(input_size, output_size)\n        self.w = nn.Parameter(w) # will be included in model.parameters() passed to the optimizer\n        nn.init.xavier_uniform_(self.w)\n        b = torch.Tensor(output_size).fill_(0)\n        self.b = nn.Parameter(b)\n        self.noise_stddev = noise_stddev\n    def forward(self,x,training=False):\n        if training:\n            noise = torch.normal(0.0, self.noise_stddev, x.shape)\n            x_new = torch.add(x,noise)\n        else:\n            x_new = x\n        return torch.add(torch.mm(x_new,self.w),self.b)\nclass NoisyModule(nn.Module):\n    def __init_(self):\n        super().__init__()\n        self.l1 = NoisyLinear(2,4,0.07)\n        self.a1 = nn.ReLU()\n        ...\n    def forward(self,x,training=False): \n        x = self.l1(x,training)\n        x = self.a1(x)\n        ...\n    def predict(self,x):\n        x = torch.tensor(x,dtype=torch.float32)\n        pred = self.forward(x)[:,0] # tra\n        return (pred&gt;=0.5).float()\n# inside the training loop, use training = True\n...\npred = model(x_batch,training=True)[:,0]\n\n\n\n\n\nFor simple functions, can just write a simple function that works like torch.nn.functional\n\n\nimport torch\n\ndef custom_mse_loss(y_pred, y_true):\n    return torch.mean((y_pred - y_true) ** 2)\n\ny_true = torch.tensor([1.0, 2.0, 3.0])\ny_pred = torch.tensor([1.5, 2.5, 3.5])\n\nloss = custom_mse_loss(y_pred, y_true)\nprint(loss)  # Output: 0.25\n\ntensor(0.2500)\n\n\n\nFor more complex functions requiring parameters, use nn.Module, as in the following ElasticNet example:\n\n\nimport torch\nimport torch.nn as nn\n\nclass ElasticNetLoss(nn.Module):\n    def __init__(self, alpha=1.0, beta=1.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for L1 loss\n        self.beta = beta    # Weight for L2 loss\n\n    def forward(self, y_pred, y_true):\n        l1_loss = torch.sum(torch.abs(y_pred - y_true))\n        l2_loss = torch.sum((y_pred - y_true) ** 2)\n        return self.alpha * l1_loss + self.beta * l2_loss\nloss_fn = ElasticNetLoss(alpha=0.5, beta=0.5)\n\ny_true = torch.tensor([1.0, 2.0, 3.0])\ny_pred = torch.tensor([1.5, 2.5, 3.5])\n\nloss = loss_fn(y_pred, y_true)\nprint(loss)  # Output: Weighted combination of L1 and L2 losses\n\ntensor(1.1250)\n\n\n\n\n\n\nRemember that during training, dropout drops units, while during evaluation, it scales the weights by the dropout factor instead of altering the activations. Thus must be very careful to use model.train() and model.eval() as in\n\n\n# Training phase\nmodel.train()\nfor data, target in train_loader:\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# Evaluation phase\nmodel.eval()\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        # Compute metrics\n\n# Or after loading a model:\ntorch.save(model.state_dict(), 'model.pth')\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()  # Set to evaluation mode after loading\n\n\nDropout is typically applied after activation functions:\n\n\nclass CNNNet(nn.Module):\n    def __init__(self):\n        super(CNNNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.dropout = nn.Dropout2d(p=0.25)\n        self.fc1 = nn.Linear(32 * 26 * 26, 128)\n        self.fc2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\nWhen using with BatchNorm, be careful to not overregularize and place BatchNorm before Dropout: BatchNorm normalizes the activations first, ensuring stable statistics, and Dropout is applied afterward without affecting the normalization process.\n\n\n\n\n\n\n\ntorch.save(model, PATH_TO_MODEL) # example path: \"model.pth\"\nmodel = torch.load(PATH_TO_MODEL)\n\n\nNot recommended because not only does this save model parameters, it also saves model classes and directory structure of the source code. If class signatures or directory structures change, may not be able to load the model in the future.\n\n\n\n\n\ntorch.save(model.state_dict(),PATH_TO_MODEL)\nmodel = ConvNet()\nmodel.load_state_dict(torch.load(PATH_TO_MODEL))\nmodel.eval()\n\n\n\n\n\n\nInference pipeline (a) the data preprocessing component, (b) the model inference, and (c) the post-processing step.\n\n\n# server.py\nimport os\nimport json\nimport numpy as np\nfrom flask import Flask, request\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n        self.dp1 = nn.Dropout2d(0.10)\n        self.dp2 = nn.Dropout2d(0.25)\n        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n        self.fc2 = nn.Linear(64, 10)\n \n    def forward(self, x):\n        x = self.cn1(x)\n        x = F.relu(x)\n        x = self.cn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dp1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dp2(x)\n        x = self.fc2(x)\n        op = F.log_softmax(x, dim=1)\n        return op\n    \nmodel = ConvNet()\nPATH_TO_MODEL = \"./convnet.pth\"\nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\nmodel.eval()\n\ndef run_model(input_tensor):\n    model_input = input_tensor.unsqueeze(0)\n    with torch.no_grad():\n        model_output = model(model_input)[0]\n    model_prediction = model_output.detach().numpy().argmax()\n    return model_prediction\n\ndef post_process(output):\n    return str(output)\n\napp = Flask(__name__)\n\n@app.route(\"/test\", methods=[\"POST\"])\ndef test():\n    # 1. Preprocess\n    data = request.files['data'].read()\n    md = json.load(request.files['metadata'])\n    input_array = np.frombuffer(data, dtype=np.float32)\n    input_image_tensor = torch.from_numpy(input_array).view(md[\"dims\"])\n    # 2. Inference\n    output = run_model(input_image_tensor)\n    # 3. Postprocess\n    final_output = post_process(output)\n    return final_output\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8890)\n\n\nMake a request as follows:\n\n\n# make_request.py\nimport io\nimport json\nimport requests\nfrom PIL import Image\n\nfrom torchvision import transforms\n\n\nimage = Image.open(\"./digit_image.jpg\")\n\ndef image_to_tensor(image):\n    gray_image = transforms.functional.to_grayscale(image)\n    resized_image = transforms.functional.resize(gray_image, (28, 28))\n    input_image_tensor = transforms.functional.to_tensor(resized_image)\n    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n    return input_image_tensor_norm\n\nimage_tensor = image_to_tensor(image)\n\ndimensions = io.StringIO(json.dumps({'dims': list(image_tensor.shape)}))\ndata = io.BytesIO(bytearray(image_tensor.numpy()))\n\nr = requests.post('http://localhost:8890/test',\n                  files={'metadata': dimensions, 'data' : data})\n\nresponse = json.loads(r.content)\n\nprint(\"Predicted digit :\", response)\n\n\nTurn into a microservice with the following Dockerfile:\n\n\nFROM python:3.9-slim\n\nRUN apt-get -q update && apt-get -q install -y wget\n\nCOPY ./server.py ./\nCOPY ./requirements.txt ./\n\nRUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/convnet.pth\nRUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/digit_image.jpg\n\nRUN pip install -r requirements.txt\n\n\nUSER root\nENTRYPOINT [\"python\", \"server.py\"]\n\n\nBuild with digit_recognizer tag using current directory as context: docker build -t digit_recognizer .\n\nNote how server.py and requirements.txt are copied into Docker’s directory. Also USER root may give overly elevated privileges.\n\nForward the 8890 port on our machine to 8890 port on the container since app runs on port 8890: docker run -p 8890:8890 digit_recognizer\nPython make_request.py will make a request to the Dockerized Flask model server and we’ll get the response.\n\n\n\n\n\nTorchScript allows exporting optimized and serialized PyTorch models to Intermediate Representation that can be ran outside of Python (say in C++ with TorchScript compiler).\nTracing: TorchScript with tracing requires an input to trace through the computation graph, implying that only one of the possible paths will be rendered.\n\n\n# Step 1: Define and instantiate the model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n        self.dp1 = nn.Dropout2d(0.10)\n        self.dp2 = nn.Dropout2d(0.25)\n        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n        self.fc2 = nn.Linear(64, 10)\n \n    def forward(self, x):\n        x = self.cn1(x)\n        x = F.relu(x)\n        x = self.cn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dp1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dp2(x)\n        x = self.fc2(x)\n        op = F.log_softmax(x, dim=1)\n        return op\n    \nmodel = ConvNet()\n\n# Step 2: Load the model's state_dict and put model in eval mode\nPATH_TO_MODEL = \"./convnet.pth\" \nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\")) \nmodel.eval()\n\n# Step 3: Turn off gradient tracking unless already loaded the model with torch.no_grad\nfor p in model.parameters():    p.requires_grad_(False)\n\n# Step 4: Trace the model\ndemo_input = torch.ones(1, 1, 28, 28) \ntraced_model = torch.jit.trace(model, demo_input)\nprint(traced_model.graph)\nprint(traced_model.code)\n\n# Step 5: Save the traced model\ntorch.jit.save(traced_model, 'traced_convnet.pt')\n\n# Step 6: Load the model and use it for inference on a preprocessed image\nloaded_traced_model = torch.jit.load('traced_convnet.pt')\nimage = Image.open(\"./digit_image.jpg\")\ndef image_to_tensor(image):    \n    ... \n    return input_image_tensor_norm\ninput_tensor = image_to_tensor(image)\n\nloaded_traced_model(input_tensor.unsqueeze(0))\n# Should produce the same output as\nmodel(input_tensor.unsqueeze(0))\n\n\nFor scripted model, there’s no need to provide dummy input and it suffices to simply use torch.jit.script instead of torch.jit.trace.\n\n\nscripted_model = torch.jit.script(model)\n\n\n\n\ntracing vs scripting from Mastering PyTorch, Second Edition by Jha.png\n\n\n\n\n\n\nUsing Jeremy’s book (Ch. 18) as a starting point\nPyTorch’s equivalent of fastai Learner callback except that they allow you to inject code into forward and backward calculations rather than training loop.\nBest to have the Hook class be a context manager to avoid a memory leak if hook.remove() is not called.\nCan register_forward_hook and register_backward_hook.\nUse cases:\n\nDebugging: inspecting intermediate outputs and gradients.\nFeature extraction: collect activations of a particular layer (see below) for visualization or transfer learning.\nCustom modifications: modify inputs/outputs or their grads (custom regularization or grad manipulation, for example).\n\n\n\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self # 'just give me the hook'\n    def __exit__(self, *args): self.hook.remove() # remove the hook to avoid memory leak\n# use as follows\n\nwith Hook(learn.model[0]) as hook: # will register forward hook on learn.model[0]\n    with torch.no_grad(): output = learn.model.eval()(x.cuda())\n    act = hook.stored\n\n# This will allow you to get dot product of weight matrix with the activations (2,k activations) @ (k activations, rows, cols)\n# Can then look which pixels contributed to model's prediction by plotting this map\ncam_map = torch.einsum('ck,kij-&gt;cij',learn.model[1][-1].weight,act)\n\n\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)\n    def hook_func(self, m, gi, go):\n        \"\"\"Will access gradients of the input gi and output go of the m module\"\"\"\n        self.stored = go[0].detach().clone\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\nCan’t simply call output.backward because output is a rank-2 tensor and we can only take derivatives wrt scalars. Can call output[0,cls].backward, though:\n\n\ncls = 1\nwith HookBwd(learn.model[0]) as hookg:\n    with Hook(learn.model[0]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n# can then get product of gradients and activations\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\nExamples of use with HuggingFace:\n\n\nfrom transformers import Trainer, BertForSequenceClassification, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\nclass Hook():\n    def __init__(self, m, print_every=100):\n        self.hook = m.register_forward_hook(self.hook_func)\n        self.counter = 0\n        self.print_every = print_every\n\n    def hook_func(self,m,i,o):\n        if self.counter % self.print_every == 0:\n            input_shape = [tensor.shape for tensor in i]\n            output_shape = output.shape if isinstance(output, torch.Tensor) else [tensor.shape for tensor in output]\n            print(f\"Layer: {m}, Input shape: {input_shape}, Output shape: {output_shape}\")\n        self.counter += 1\n\n    def __enter__(self, *args): return self\n\n    def __exit__(self, *args): self.hook.remove()\n\ntext = \"...\"\ninputs = tokenizer(text, return_tensors='pt')\nwith Hook(model.bert.encoder.layer[0].attention) as hookf:\n    outputs = model(**inputs)\n\n# Then use Trainer or SFTTrainer as always ...\ntrainer = Trainer(...)\ntrainer.train()\n# Will print input/output shapes every print_every iterations. Can log these, of course.\n\n\n\n\n\nPyTorch compares shapes of two tensors elementwise. It starts at the trailing dimensions and works its way backwards, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following holds:\n\nThey are equal\nOne of them is 1, in which case it’s broadcast to make it the same shape as the other.\n\n\n\n\n# Define tensors\na = torch.tensor([1, 2, 3])  # Shape: (3,)\nb = torch.tensor([4, 5])     # Shape: (2,)\n\ntry:\n    result_direct = a * b\nexcept RuntimeError as e:\n    print(\"Error without reshaping:\\n\", e)\n# Output:\n# RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n# Reshape tensors for broadcasting\na_reshaped = a.view(3, 1)  # Shape: (3, 1)\nb_reshaped = b.view(1, 2)  # Shape: (1, 2)\n\n# Multiply after reshaping\nresult_broadcast = a_reshaped * b_reshaped\nprint(\"Result after reshaping and multiplying:\\n\", result_broadcast)\n# Output:\n\nError without reshaping:\n The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\nResult after reshaping and multiplying:\n tensor([[ 4,  5],\n        [ 8, 10],\n        [12, 15]])\n\n\n\n\n\n\nRepeated indices are implicitly summed over.\nEach index can appear at most twice in any term.\nEach term must contain identical nonrepeated indices.\n\n\nimport torch\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij',a,b)\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784, 10)\n%timeit -n 10 t5 = matmul(m1,m2)\n\nThe slowest run took 130.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n164 μs ± 380 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nBatch matmul\n\n\ndef batch_matmul(a, b):\n    return torch.einsum('bik,bkj-&gt;bij', a, b)\n# Example usage\nbatch_size = 2\ni, k, j = 3, 4, 5\na = torch.randn(batch_size, i, k)\nb = torch.randn(batch_size, k, j)\nresult = batch_matmul(a, b)\nprint(\"Batch MatMul Result Shape:\", result.shape)\n\nBatch MatMul Result Shape: torch.Size([2, 3, 5])\n\n\n\nElement-wise Multiplication with Broadcasting (Outer Product). Note that PyTorch’s einsum allows implicit reshaping to conform to broadcasting semantics: Normally, could not multiply a and b since standard broadcasting rules would make them (3,1) and (2,1), which are incompatible.\n\n\ndef outer_product(a,b):\n    return torch.einsum('i,j-&gt;ij',a,b)\na = torch.tensor([1, 2, 3]) # shape (3,) -&gt; (3,1) \nb = torch.tensor([4, 5])    # shape (2,) -&gt; (1.2)\nresult = outer_product(a, b)\nprint(\"Outer Product Result:\\n\", result)\n\nOuter Product Result:\n tensor([[ 4,  5],\n        [ 8, 10],\n        [12, 15]])\n\n\n\nDot product. The absence of indices on the right side of the arrow (‘-&gt;’) indicates that the output is a scalar resulting from the summation.\n\n\ndef dot_product(a, b):\n    return torch.einsum('i,i-&gt;', a, b)\n\n# Define tensors\na = torch.tensor([1, 2, 3])  # Shape: (3,)\nb = torch.tensor([4, 5, 6])  # Shape: (3,)\n\n# Compute dot product using einsum\nresult = dot_product(a, b)\nprint(\"Dot Product Result:\", result)\n\nDot Product Result: tensor(32)\n\n\n\nTranspose of a Matrix\n\n\ndef transpose(a):\n    return torch.einsum('ij-&gt;ji', a)\n\n# Example usage\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nresult = transpose(a)\nprint(\"Transpose Result:\\n\", result)\n\nTranspose Result:\n tensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nSum Over a Specific Dimension\n\n\ndef sum_over_dim(a):\n    return torch.einsum('ij-&gt;i', a)\n\n# Example usage\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nresult = sum_over_dim(a)\nprint(\"Sum Over Rows:\", result)\n# Output: Sum Over Rows: tensor([ 6, 15])\n\nSum Over Rows: tensor([ 6, 15])\n\n\n\nAttention Score Calculation\n\n\ndef attention_scores(query, key):\n    return torch.einsum('bqd,bkd-&gt;bqk', query, key) # sum the products across the feature dimension d to compute attention scores\n\n# Example usage\nbatch_size, q, d, k = 2, 3, 4, 4\nquery = torch.randn(batch_size, q, d)\nkey = torch.randn(batch_size, k, d)\nscores = attention_scores(query, key)\nprint(query)\nprint(key)\nprint(scores)\nprint(\"Attention Scores Shape:\", scores.shape)\n# Output: Attention Scores Shape: torch.Size([2, 3, 4])\n\ntensor([[[ 1.9826,  0.6645, -0.9895, -0.0247],\n         [-1.5472,  0.6203, -0.9500, -0.3087],\n         [ 0.3845,  0.3481,  0.2347,  0.7434]],\n\n        [[ 1.0612,  1.7969, -0.4906, -0.1500],\n         [-0.3895,  1.5363, -0.4685, -0.4239],\n         [ 0.9783, -0.3329,  0.9564,  0.3776]]])\ntensor([[[-1.5022,  1.1593,  0.8890,  1.0148],\n         [ 1.1577,  0.4848, -0.1506,  1.7842],\n         [-1.4799,  0.6236,  0.7959,  0.1494],\n         [-1.7694, -1.7675,  0.2545,  0.0117]],\n\n        [[-1.1997,  1.1282, -0.8853, -0.8205],\n         [-0.4073, -1.8880,  0.1635,  0.8772],\n         [ 0.2545, -1.0884, -0.1758, -0.2735],\n         [-0.1619,  1.4902,  0.2765, -1.1585]]])\ntensor([[[-3.1126,  2.7223, -3.3110, -4.9348],\n         [ 1.8854, -1.8981,  1.8743,  1.3959],\n         [ 0.7889,  1.9051, -0.0542, -1.2273]],\n\n        [[ 1.3114, -4.0365, -1.5585,  2.5441],\n         [ 2.9630, -3.1904, -1.5730,  2.7140],\n         [-2.7057,  0.7177,  0.3398, -0.8274]]])\nAttention Scores Shape: torch.Size([2, 3, 4])\n\n\n\nimport numpy as np\nnp.dot([ 1.9826,  0.6645, -0.9895, -0.0247],[-1.5022,  1.1593,  0.8890,  1.0148])\n\n-3.11263793\n\n\n\nnp.dot([ 1.9826,  0.6645, -0.9895, -0.0247],[ 1.1577,  0.4848, -0.1506,  1.7842])\n\n2.7223545799999997\n\n\n\nWeighted Sum for Attention Output\n\n\n\n\n\ndef weighted_sum(attention_weights, value):\n    return torch.einsum('bqk,bvd-&gt;bqd', attention_weights, value)\n\n# Example usage\nbatch_size, q, k, d = 2, 3, 4, 5\nattention_weights = torch.randn(batch_size, q, k) # (2,3,4)\nvalue = torch.randn(batch_size, d, 5)             # (2,5,5)\nresult = weighted_sum(attention_weights, value)\nprint(\"Weighted Sum Result Shape:\", result.shape)\n# Output: Weighted Sum Result Shape: torch.Size([2, 3, 5])\n\n\nOuter Product for Rank-1 Updates\n\n\ndef outer_product(a, b):\n    return torch.einsum('i,j-&gt;ij', a, b)\n\n# Example usage\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nresult = outer_product(a, b)\nprint(\"Outer Product Result:\\n\", result)\n# Output:\n# Outer Product Result:\n# tensor([[ 4,  5,  6],\n#         [ 8, 10, 12],\n#         [12, 15, 18]])\n\n\n\n\n\nKey step 1/4: Initialize LoRA parameters\nKey step 2/4: Use LoRA parameters in matmuls\nKey step 3/4: Replace nn.Linear layers with nn.LoRALinear layers\nKey step 4/4: Only do backprop on the LoRA parameters\n\n\nimport torch\nimport torch.nn as nn\n\nclass LoRALinear(nn.Module):\n    \"\"\"\n    A LoRA (Low-Rank Adaptation) layer that wraps an existing nn.Linear layer.\n    It adds a low-rank update to the output of the original linear layer.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha = 1.0):\n        super().__init__()\n        self.linear =  original_linear\n        self.r = r\n        self.alpha = alpha\n        # Key step 1/4: Initialize LoRA parameters\n        if r &gt; 0:\n            in_features = original_linear.in_features\n            out_features = original_linear.out_features\n            # initialize A and B low-rank matrices\n            self.lora_A = nn.Parameter(torch.zeros(in_features,r))\n            self.lora_B = nn.Parameter(torch.zeros(r,out_features))\n            # Use normal init (mean 0, std 0.02)\n            nn.init.normal_(self.lora_A, std=0.02)\n            nn.init.normal_(self.lora_B, std=0.02)\n            # Scaling factor for LoRA update\n            self.scaling = self.alpha/self.r\n        else:\n            self.lora_A =  None\n            self.lora_B = None\n            self.scaling = 1.0\n\n    def forward(self, x):\n        result = self.linear(x)\n        # Key step 2/4: Use LoRA parameters in matmuls\n        if self.r &gt; 0:\n            # ((bs,in_features) @ (in_features,r)) @ (r,out_features) -&gt; (bs, out_features)\n            lora_update = (x @ self.lora_A) @ self.lora_B \n            lora_update *= self.scaling\n            result += lora_update\n        return result\n\n# Key step 3/4: Replace nn.Linear layers with nn.LoRALinear layers\ndef replace_linear_with_lora(model, r=4, alpha=1.0):\n    \"\"\"\n    Recursively replace nn.Linear layers with LoRALinear layers in the model.\n    \"\"\"\n    for name, module in model.named_children():\n        if isinstance(module,nn.Linear):\n            setattr(model,name,LoRALinear(module,r,alpha))\n        else: # recursively apply to child modules\n            replace_linear_with_lora(module, r, alpha)\n    return model\n\n\ndef get_lora_params(model):\n    \"\"\"\n    Retrieve all LoRA parameters from the model.\n    \"\"\"\n    lora_parameters = []\n    for module in model.modules():\n        if isinstance(module, LoRALinear):\n            lora_parameters.extend([module.lora_A, module.lora_B])\n    return lora_parameters\n\n# Example Usage\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out\n\nmodel = SimpleModel(input_dim=128, hidden_dim=64, output_dim=10)\nmodel = replace_linear_with_lora(model, r=4, alpha=1.0)\n\n# Key step 4/4: Only do backprop on the LoRA parameters\nlora_parameters = get_lora_params(model)\noptimizer = torch.optim.Adam(lora_parameters, lr=1e-3)\n\n# Training loop follows\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass QLoRALinear(nn.Module):\n    \"\"\"\n    A QLoRA (Quantized Low-Rank Adaptation) layer that wraps an existing quantized nn.Linear layer.\n    It adds a low-rank update to the output of the original quantized linear layer.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha=1.0):\n        super().__init__()\n        # Quantize the original linear layer's weights to 4-bit\n        ...\n        self.linear.weight.data = self.quantize_weights(self.linear.weight.data)\n        self.linear.bias = original_linear.bias  # Bias remains in full precision\n\n        ...\n        if r &gt; 0:\n            ...\n        else:\n            ...\n    def quantize_weights(self, weight, num_bits=4):\n        \"\"\"\n        Quantizes the weights to the specified number of bits.\n        \"\"\"\n        qmin = 0\n        qmax = 2**num_bits - 1\n\n        min_val, max_val = weight.min(),weight.max()\n        scale = (max_val-min_val)/(qmax-qmin)\n        zero_point = qmin - min_val/scale\n\n        q_weight = torch.clamp((weight/scale + zero_point).round(),qmin,qmax)\n        #q_weight = (q_weight - zero_point) * scale\n        return q_weight        \n\n    def forward(self, x):\n        ...\n\ndef replace_linear_with_qlora(model, r=4, alpha=1.0):\n    \"\"\"Swap LoRALinear with QLoRALinear\"\"\"\n    \n\ndef get_qlora_params(model):\n    \"\"\"Swap LoRALinear with QLoRALinear\"\"\"\n\n# Example Usage\nclass SimpleModel(nn.Module):\n    \"\"\"Same\"\"\"\n\nmodel = SimpleModel(input_dim=128, hidden_dim=64, output_dim=10)\nmodel = replace_linear_with_qlora(model, r=4, alpha=1.0)\n\n# Only do backprop on the QLoRA parameters\nqlora_parameters = get_qlora_params(model)\noptimizer = torch.optim.Adam(qlora_parameters, lr=1e-3)\n\n# Training loop follows\n\n\nNote on dequantization: Dequantization would happen via q_weight = (q_weight - zero_point) * scale. There will be quantization error, see below.\n\n\n\n\nimage.png\n\n\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\nclass QLoRALinear(nn.Module):\n    \"\"\"\n    A QLoRA (Quantized Low-Rank Adaptation) layer that wraps an existing nn.Linear layer.\n    It quantizes the original linear layer and adds a low-rank update.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha=1.0, num_bits=4):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n\n        if num_bits == 8:\n            self.linear = bnb.nn.Linear8bitLt(\n                original_linear.in_features,\n                original_linear.out_features,\n                bias=original_linear.bias is not None,\n                has_fp16_weights=False,\n                threshold=6.0\n            )\n        elif num_bits == 4:\n            self.linear = bnb.nn.Linear4bit(\n                original_linear.in_features,\n                original_linear.out_features,\n                bias = original_linear.bias is not None,\n                compute_dtype=torch.float16\n            )\n        else:\n            raise ValueError(f\"Unsupported num_bits: {num_bits}. Supported values are 4 and 8.\")\n\n        self.linear.weight.data = original_linear.weight.data.clone()\n        if original_linear.bias is not None:\n            self.linear.bias.data = original_linear.bias.data.clone()\n\n        if r &gt; 0: ... else:...\n        \n        # Freeze the quantized weights\n        for param in self.linear.parameters():\n            param.requires_grad = False\n\n\n\n        # Quantize the original linear layer\n        self.linear = quantization.QuantWrapper(original_linear)\n        # Dynamic quantization quantizes weights ahead of time and quantizes activations on-the-fly during forward pass\n        self.linear.qconfig = quantization.default_dynamic_qconfig\n        quantization.prepare(self.linear, inplace=True) # collect stats needed for quantization\n        quantization.convert(self.linear, inplace=True) # replace original operations with quantized counterparts\n\n\nPurpose of self.linear.weight.data = original_linear.weight.data.clone(); if original_linear.bias is not None: self.linear.bias.data = original_linear.bias.data.clone()\n\nIsolation: Avoid modifying original_linear and prevent unintended gradient updates.\nConsistency: Maintain parameter consistency between the original and quantized layers.\nStarting point for quantization\n\n\n\n\n\n\nimport torch\nnum_epochs = 10\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n\nT_max: This parameter defines the maximum number of iterations (typically epochs) for one cycle of the cosine annealing schedule. Essentially, it determines the period over which the learning rate decreases from the initial value down to the minimum learning rate (eta_min).\n\nSingle-Cycle Annealing: If you set T_max equal to the total number of training epochs, the learning rate will smoothly decrease from the initial learning rate to eta_min over the entire training process.\nMulti-Cycle Annealing: By setting T_max to a fraction of the total epochs, you can create multiple cycles of learning rate adjustments within the training process. For more advanced multi-cycle schedules, consider using CosineAnnealingWarmRestarts.\n\n\n\n\n\nAvailable LR Schedulers on https://pytorch.org/docs\n\n\n\n\n\n\nTorchServe a custom model handler, which should:\n\nInitialize the model\nPreprocess inputs from the client\nRun the preprocessed inputs through the model for inference\nPostprocess the model’s outputs before returning to client\n\n\n\nimport logging\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\nclass TransformersHandler(BaseHandler):\n    def initialize(self, ctx):\n        logger.info(\"Initializing the TransformersHandler.\")\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        model_dir = properties.get(\"model_dir\")\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n        \n        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n        self.model.to(self.device)\n        self.model.eval()\n        logger.info(\"Model and tokenizer loaded successfully.\")\n\n    def preprocess(self, data):\n        logger.info(\"Preprocessing input data.\")\n        logger.debug(f\"Raw data received: {data}\")\n        logger.info(f\"DATA: {data}\")\n        data_body = data[0]['body'] # should have probably figured out I needed to access this pre-Docker!\n        logger.info(f\"DATA_BODY: {data_body}\")\n\n        text = data_body.get(\"text\")\n        if isinstance(text, bytes):\n            text = text.decode('utf-8')\n            logger.debug(f\"Decoded text: {text}\")\n        \n        try:\n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=True\n            )\n            logger.debug(f\"Tokenized inputs: {inputs}\")\n        except Exception as e:\n            logger.error(f\"Error during tokenization: {e}\")\n            raise e\n        \n        return inputs\n\n    def inference(self, inputs):\n        logger.info(\"Performing inference.\")\n        try:\n            with torch.no_grad():\n                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n                outputs = self.model(**inputs)\n                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n                confidences, predictions = torch.max(probs, dim=1)\n                result = {\"confidence\": confidences.item(), \n                          \"prediction\": predictions.item()}\n                logger.debug(f\"Inference result: {result}\")\n                return result\n        except Exception as e:\n            logger.error(f\"Error during inference: {e}\")\n            raise e\n\n    def postprocess(self, inference_output):\n        logger.info(\"Postprocessing inference output.\")\n        logger.debug(f\"Postprocessing result: {inference_output}\")\n        return [inference_output]\n\n\nrequirements.txt file is below:\n\n\ntorchserve==0.6.0\ntorch-model-archiver==0.6.0\ntransformers==4.47.1\ntorch==2.4.0+cu121\n\n\nBelow is a multi stage Dockerfile for the build:\n\n\n# Stage 0: Fetch Model Files\nFROM alpine/git AS fetcher\nRUN apk add --no-cache git\nRUN git clone https://huggingface.co/bert-base-uncased /model\n\n# Stage 1: Build Stage\nFROM pytorch/torchserve:latest AS build\nRUN pip install transformers\nRUN mkdir -p /home/model-server/model-store /home/model-server/code\nCOPY requirements.txt /home/model-server/\nRUN pip install -r /home/model-server/requirements.txt\nCOPY --from=fetcher /model /home/model-server/model-store/bert\nCOPY handler.py /home/model-server/code/handler.py\n# Archive the model\nRUN torch-model-archiver \\\n    --model-name bert_seq_class \\\n    --version 1.0 \\\n    --serialized-file /home/model-server/model-store/bert/pytorch_model.bin \\\n    --handler /home/model-server/code/handler.py \\\n    --extra-files \"/home/model-server/model-store/bert/config.json,/home/model-server/model-store/bert/tokenizer.json,/home/model-server/model-store/bert/tokenizer_config.json,/home/model-server/model-store/bert/vocab.txt\" \\\n    --export-path /home/model-server/model-store\n\n# Stage 2: Runtime Stage\nFROM pytorch/torchserve:latest-gpu\nCOPY requirements.txt /home/model-server/\nRUN pip install -r /home/model-server/requirements.txt\n# Copy model archive and configuration from the build stage\nCOPY --from=build /home/model-server/model-store /home/model-server/model-store\nEXPOSE 8080\nEXPOSE 8081\n# Start TorchServe\nCMD [\"torchserve\", \"--start\", \"--model-store\", \"/home/model-server/model-store\", \"--models\", \"bert_seq_class=bert_seq_class.mar\"]\n\n\nBuild and run the above file as\n\n\ndocker build -t torchserve-bert-model .\ndocker run -p 8080:8080 -p 8081:8081 --name torchserve-bert-model torchserve-bert-model\n\n\nDebug by looking at the logs in\n\n\ndocker exec -it torchserve-bert-model bash\n\n\nSend and receive requests as\n\n\n%%bash\ncurl -X POST http://localhost:8080/predictions/bert_seq_class      -H \"Content-Type: application/json\"      -d '{\"text\": \"I love using TorchServe with Hugging Face models!\"}'\n{\n  \"confidence\": 0.5290595293045044,\n  \"prediction\": 1"
  },
  {
    "objectID": "notes/LlmBasicsAndRecipes.html#pytorch-basics",
    "href": "notes/LlmBasicsAndRecipes.html#pytorch-basics",
    "title": "LLMs/PyTorch Basics and Recipes",
    "section": "",
    "text": "image.png\n\n\n\n\n\n\nIf affine, initialize gamma and beta\nInitialize running mean and variance buffers. Note: For variance, set unbiased=False. This is because the biased estimator (dividing by N instead of N-1 in the variance formula) is MLE estimator of variance under iid and normally distributed assumptions. Also, the data will be normalized consistently even if batch size is 1.\nGet mean/var over nonchannel dimensions\nUpdate running mean and variance statistics using momentum running average without tracking gradients\nIf training, use the computed mean/var; else use running mean/var. Thus the running mean/var are computed on the fly for inference only.\nNormalize the input\nApply gamma and beta if they’re there (affine)\nReturn normalized input\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass BatchNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(BatchNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # Learnable affine parameters: gamma (scale) and beta (shift)\n        self.gamma, self.beta = None, None\n        if affine:\n            self.gamma = nn.Parameter(torch.ones(num_features))  # scale parameter\n            self.beta = nn.Parameter(torch.zeros(num_features))  # shift parameter\n\n        # Running statistics for mean and variance\n        self.running_mean = torch.zeros(num_features) # mean 0\n        self.running_var = torch.ones(num_features)   # var 1\n\n    def forward(self, x):\n        # Calculate mean and variance across the batch and spatial dimensions (H, W)\n        batch_mean = x.mean(dim=(0, 2, 3), keepdim=True)  # mean across N, H, W for each channel\n        batch_var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True)  # variance across N, H, W for each channel\n\n        # Update running statistics (mean and variance): See batch_mean dimension note below\n        with torch.no_grad(): # running mean/var are updated via moving avg approach, not SGD\n          self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.squeeze()\n          self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.squeeze()\n\n        # During training, we use the batch statistics\n        if self.training:\n            mean = batch_mean\n            var = batch_var\n        else:\n            # During inference, we use the running statistics\n            mean = self.running_mean.view(1, self.num_features, 1, 1)\n            var = self.running_var.view(1, self.num_features, 1, 1)\n            # mean = self.running_mean[None, :, None, None] # Alternatively...\n            # var = self.running_var[None, :, None, None]\n\n        # Normalize the input\n        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n\n        # Apply the affine transformation (scaling and shifting)\n        if self.gamma is not None and self.beta is not None:\n            x_normalized = self.gamma.view(1, self.num_features, 1, 1) * x_normalized + self.beta.view(1, self.num_features, 1, 1)\n            #x_normalized = self.gamma[None, :, None, None] * x_normalized + self.beta[None,:, None, None] # Alternatively\n\n        return x_normalized\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create random input with shape (batch_size, num_features, height, width)\n    x = torch.randn(32, 64, 28, 28)  # batch_size=32, num_features=64 (channels), height=28, width=28\n\n    # Instantiate the BatchNorm2d layer\n    batch_norm = BatchNorm2d(num_features=64)\n\n    # Forward pass through the batch norm layer\n    output = batch_norm(x)\n\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {output.shape}\")\n\n\nBatch mean dimension: Input is of dimension (batch_size, channels, height, width). When PyTorch computes the mean over dim=(0,2,3), it keeps dim=1 (channel dimension) intact, collapses the batch dimension into 1 and height+width spacial dimensions into one. So with 64 channels, the shape of mean over dim=(0,2,3) will be (64=uncollapsed channel,1=collapsed bs,1=collapsed hxw). Will want to squeeze out the 1’s to get (64,)-shaped tensor.\nAK: In batch norm, examples in the batch are coupled mathematically. Activations will change depending on the given batch since mean and variance statistics would change. Interestingly, this turns out to be good due to a ‘regularizer’ effect: each individual example is ‘padded out’, making it harder for neural net to overfit on any specific example. Still, bugs can arise and other types of normalization have been proposed. Nonetheless, batch norm is often the most effective.\n\nFor example, must be careful to always correctly place the model into training and eval mode with model.train() and model.eval().\nDuring transfer learning, will likely want to unfreeze the batch norm layers to allow them to adapt (requires_grad=True)\nIf using distributed training, each replica computes BatchNorm statistics independently, possibly leading to discrepancies between single-device and multi-device training. Use nn.SyncBatchNorm.\n\n\n\n\n\n\nimport torch\n\nclass AdamOptimizer:\n    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0):\n        \"\"\"\n        Adam optimizer from scratch for PyTorch.\n\n        Args:\n            params (iterable): The parameters to optimize (typically model.parameters()).\n            lr (float): Learning rate (default 1e-3).\n            beta1 (float): Exponential decay rate for first moment estimate (default 0.9).\n            beta2 (float): Exponential decay rate for second moment estimate (default 0.999).\n            epsilon (float): Term added to the denominator to avoid division by zero (default 1e-8).\n            weight_decay (float): Weight decay (L2 penalty) (default 0).\n        \"\"\"\n        self.params = list(params)\n        self.lr = lr\n        self.beta1 = beta1 # weight on past param mean\n        self.beta2 = beta2 # weight on past param var\n        self.epsilon = epsilon \n        self.weight_decay = weight_decay # how much of param to include in the grad\n\n        # Initialize moment estimates\n        self.m = [torch.zeros_like(param) for param in self.params]  # First moment (m)\n        self.v = [torch.zeros_like(param) for param in self.params]  # Second moment (v)\n\n        # Time step counter\n        self.t = 0\n\n    def step(self):\n        \"\"\"\n        Perform a single optimization step.\n        \"\"\"\n        self.t += 1 # important for bias correction below\n        with torch.no_grad():\n            for i, param in enumerate(self.params):\n                if param.grad is None: continue\n                # Get the gradients for the current parameter\n                grad = param.grad\n                # Apply weight decay (L2 penalty): weighting grad linearly ~ weighting weight quadratically w/ L2\n                if self.weight_decay != 0: grad = grad + self.weight_decay * param\n                # Update biased first and second moment estimates\n                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n\n                # Bias correction\n                m_hat = self.m[i] / (1 - self.beta1**self.t)\n                v_hat = self.v[i] / (1 - self.beta2**self.t)\n\n                # Update parameters\n                param -= self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n    def zero_grad(self):\n        \"\"\"\n        Reset gradients of all parameters.\n        \"\"\"\n        for param in self.params:\n            if param.grad is not None:\n                param.grad.zero_()\n\n\n\n\nimage.png\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n# Define a simple neural network\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 256)\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28 * 28)  # Flatten the input\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Set up data loaders for MNIST\ntransform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n\n# Initialize the network and optimizer\nmodel = SimpleNet()\noptimizer = AdamOptimizer(model.parameters(), lr=1e-3)\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(1, 6):  # Run for 5 epochs\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Optimization step\n        optimizer.step()\n\n        # Print progress occasionally\n        if batch_idx % 200 == 0:\n            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n\n    # Evaluation on test data\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            test_loss += criterion(output, target).item()  # Sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)      # Get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader)\n    accuracy = correct / len(test_loader.dataset)\n\n    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy * 100:.2f}%)\\n')\n\nEpoch 1 [0/60000] Loss: 2.307041\nEpoch 1 [12800/60000] Loss: 0.193796\nEpoch 1 [25600/60000] Loss: 0.254285\nEpoch 1 [38400/60000] Loss: 0.195109\nEpoch 1 [51200/60000] Loss: 0.142228\n\nTest set: Average loss: 0.1497, Accuracy: 9551/10000 (95.51%)\n\nEpoch 2 [0/60000] Loss: 0.056716\nEpoch 2 [12800/60000] Loss: 0.116285\nEpoch 2 [25600/60000] Loss: 0.286116\nEpoch 2 [38400/60000] Loss: 0.097069\nEpoch 2 [51200/60000] Loss: 0.040939\n\nTest set: Average loss: 0.1021, Accuracy: 9698/10000 (96.98%)\n\nEpoch 3 [0/60000] Loss: 0.152332\nEpoch 3 [12800/60000] Loss: 0.059672\nEpoch 3 [25600/60000] Loss: 0.074841\nEpoch 3 [38400/60000] Loss: 0.116336\nEpoch 3 [51200/60000] Loss: 0.092772\n\nTest set: Average loss: 0.0838, Accuracy: 9746/10000 (97.46%)\n\nEpoch 4 [0/60000] Loss: 0.078431\nEpoch 4 [12800/60000] Loss: 0.036434\nEpoch 4 [25600/60000] Loss: 0.119966\nEpoch 4 [38400/60000] Loss: 0.120010\nEpoch 4 [51200/60000] Loss: 0.054376\n\nTest set: Average loss: 0.0735, Accuracy: 9771/10000 (97.71%)\n\nEpoch 5 [0/60000] Loss: 0.042105\nEpoch 5 [12800/60000] Loss: 0.036648\nEpoch 5 [25600/60000] Loss: 0.033848\nEpoch 5 [38400/60000] Loss: 0.043520\nEpoch 5 [51200/60000] Loss: 0.071161\n\nTest set: Average loss: 0.0677, Accuracy: 9792/10000 (97.92%)\n\n\n\n\n\n\n\nThe major architectural improvement used in transformers according to Andrej.\nInstead adding a vector to the token embeddings vector, apply a rotation. Take the token embedding and rotate it by position*theta (ex, 4xtheta if there are 4 tokens appearing before the token in question).\n\nKV-Cacheable, like absolute positional embeddings and unlike relative positional embeddings. If add more tokens at the end of the sentence, the tokens at the beginning stay the same.\n\nUnlike absolute positional embeddings and like relative positional embeddings, relative positions of the words are preserved. Looking at words ‘pig’ and ‘dog’ in ‘The pig chased the dog’, ‘Once upon a time, the pig chased the dog’, they will have the same rotation angle in both sentences.\n\n\n\n\n\nSource: https://math.stackexchange.com/questions/1404484/how-is-that-a-rotation-by-an-angle-%CE%B8-about-the-origin-can-be-represented-by-this\n\n\n\n\n\nSource: RoFormer\n\n\n\nApply the linear transformations to get the query and key vectors before applying the rotation matrix: want embeddings for relative positions between tokens.\nIn terms of notation, note that frequencies are represented as:\n\n\n\n\nSource: RoFormer paper, eq.4\n\n\n\nRange of Frequencies: By using a range of frequencies, the model can capture positional information at different scales. Lower frequencies capture long-range dependencies, while higher frequencies capture short-range dependencies.\nAn inefficient implementation would rely on applying matrix multiplication to each pair of coordinates as below:\n\n\n\n\nSource: RoFormer\n\n\n\nA more efficient implementation would use the outer product.\n\n\n\n\nOuter product review\n\n\n\n\n\nSource: RoFormer\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.dim, self.base = dim, base\n        # precompute the sinusoidal embeddings: needed for eq. 4 in paper, \n        # position for denominator derived in forward once know seq_len\n        inv_freq = 1.0/(self.base**(torch.arange(0,dim,2).float()/dim)) \n        self.register_buffer('inv_freq',inv_freq)\n\n    def forward(self, x):\n        # Compute positional embeddings\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n        sinusoid_inp = torch.outer(pos, self.inv_freq) # see equation (34) above; [seq_len, dim // 2]\n        sin_emb = sinusoid_inp.sin()[None, :, :]  # Shape: [1, seq_len, dim // 2]\n        cos_emb = sinusoid_inp.cos()[None, :, :]  # Shape: [1, seq_len, dim // 2]\n\n        # Split the input tensor into even and odd parts\n        x_even = x[:, :, ::2]  # Even indices\n        x_odd = x[:, :, 1::2]  # Odd indices\n\n        # Apply rotary embeddings: best to follow simple 2D setup for intuition\n        x_rotated = torch.zeros_like(x)\n        x_rotated[:, :, ::2] = x_even * cos_emb - x_odd * sin_emb\n        x_rotated[:, :, 1::2] = x_even * sin_emb + x_odd * cos_emb\n\n        return x_rotated\nbatch_size, seq_len, dim = 2, 10, 64\nx = torch.randn(batch_size, seq_len, dim)\n\nrotary_pos_emb = RotaryPositionalEmbedding(dim)\nx_rotary = rotary_pos_emb(x)\n\nprint(\"Input shape:\", x.shape)\nprint(\"Output shape:\", x_rotary.shape)\n\nInput shape: torch.Size([2, 10, 64])\nOutput shape: torch.Size([2, 10, 64])\n\n\n\nWords close together have a larger dot product:\n\n\n\n\nSource: RoFormer\n\n\n\n\n\n\nKVCache itself is a simple class with update and get methods. Update will either create new tensors for k and v at the start or torch.cat new k/v’s with existing ones.\nInside MultiheadSelfAttention, we only pass in the last token to forward so x is of size [batch_size, 1, embed_dim]. This is crucial since the main attn = q @ k, then (post masking and softmax), attn @ v products need to compute only the last token’s attention/attention output.\n\nIf cache is nonempty, we set k to torch.cat([cached_k, k_new], dim=2), ditto for v. Also, update the kv_cache with new values from the batch.\nThe attention scores computation attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.head_dim ** -0.5 looks the same as always BUT AGAIN, q’s seq_len==1 in [batch_size, num_heads, seq_len, seq_len_total], and matmul occurs with full k constructed using the cache: there’s no need to recompute full k @ v, just get the next token since the previous computations are cached.\nSame holds for the attention output computation attn_output = torch.matmul(attn_probs, v).\n\nWithout the cache, each call to forward would only have access to the current token’s keys and values. This would mean the model couldn’t attend to previous tokens, leading to incorrect outputs.\n\n\nimport torch\nimport torch.nn as nn\n\nclass KVCache:\n    def __init__(self):\n        self.k = None  # [batch_size, num_heads, seq_len_cache, head_dim]\n        self.v = None\n\n    def update(self, k_new, v_new):\n        # Append new keys and values to cache\n        if self.k is None:\n            self.k = k_new\n            self.v = v_new\n        else:\n            self.k = torch.cat([self.k, k_new], dim=2)\n            self.v = torch.cat([self.v, v_new], dim=2)\n\n    def get(self):\n        return self.k, self.v\n\nclass MultiheadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n            \n    def forward(self, x, kv_cache=None):\n        batch_size, seq_len, embed_dim = x.size()\n        \n        # Project input to Q, K, V\n        # IMPORTANT: note seq_len == 1, see generation loop below, the main point of the cache\n        qkv = self.qkv_proj(x)  # [batch_size, seq_len, 3 * embed_dim] \n        q, k_new, v_new = qkv.chunk(3, dim=-1)\n        \n        # Reshape and transpose for multi-head attention\n        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch_size, num_heads, 1, head_dim]\n        k_new = k_new.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v_new = v_new.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        ### --- Start: KV Cache code modifications --- ###\n        if kv_cache is not None:\n            cached_k, cached_v = kv_cache.get()\n            if cached_k is not None: # 1/2 update for the computations\n                k = torch.cat([cached_k, k_new], dim=2)\n                v = torch.cat([cached_v, v_new], dim=2)\n            else:\n                k = k_new\n                v = v_new\n            kv_cache.update(k_new,v_new) # 2/2 update for the cache\n        else:\n            k = k_new\n            v = v_new\n\n        # Compute attention scores: same as always BUT AGAIN, q's seq_len==1 in  [batch_size, num_heads, seq_len, seq_len_total]\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.head_dim ** -0.5   \n        \n        # Create causal mask: note that unlike in a vanilla transformer with a registered buffer of size seq_len by seq_len, it's seq_len by seq_len_total\n        seq_len_total = k.size(2)\n        causal_mask = torch.tril(torch.ones(seq_len, seq_len_total, device=x.device)).bool()  # [seq_len, seq_len_total]\n        attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        ### --- End: KV Cache code modifications --- ###\n        \n        # Compute attention probabilities\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        \n        # Compute attention output\n        attn_output = torch.matmul(attn_probs, v)  # [batch_size, num_heads, seq_len, head_dim]\n        \n        # Reshape and project output\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n        output = self.out_proj(attn_output)\n        \n        return output\n\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.self_attn = MultiheadSelfAttention(embed_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, 4 * embed_dim),\n            nn.GELU(),\n            nn.Linear(4 * embed_dim, embed_dim),\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x, kv_cache=None):\n        # Self-attention with KV cache\n        attn_output = self.self_attn(x, kv_cache=kv_cache)\n        x = x + attn_output\n        x = self.norm1(x)\n    \n        # Feed-forward network\n        ffn_output = self.ffn(x)\n        x = x + ffn_output\n        x = self.norm2(x)\n        return x\n\n# Example usage\nembed_dim = 512\nnum_heads = 8\ndecoder_layer = TransformerDecoderLayer(embed_dim, num_heads)\nkv_cache = KVCache()\n\n# Initial input token\ninput_token = torch.randn(1, 1, embed_dim)  # [batch_size, seq_len=1, embed_dim]\n\n# Autoregressive generation loop\noutput_tokens = []\nfor _ in range(10):\n    output = decoder_layer(input_token, kv_cache=kv_cache) \n    output_tokens.append(output)\n    input_token = output[:, -1:, :]  # Use the last token as the next input; MUST provide kv_cache above for this to work\n\noutput_sequence = torch.cat(output_tokens, dim=1)\nprint(\"Generated sequence shape:\", output_sequence.shape)\n\n\n\n\n\nA custom Dataset class must have __init__(),__len__(), and __getitem__() methods to be used by the data loader.\nSource: Machine Learning with PyTorch and Scikit-Learn by Raschka et al.\n\n\nimport torch\ntorch.manual_seed(42)\nt_x = torch.rand([4,3],dtype = torch.float32)\nt_y = torch.arange(4)\n\nfrom torch.utils.data import Dataset, DataLoader\nclass JointDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = x\n        self.y = y\n    def __len__(self):\n        return len(self.x)\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\njoint_dataset = JointDataset(t_x,t_y)\n# alternatively, can create a joint dataset using TensorDataset\n# from torch.utils.data import TensorDataset\n# tensor_dataset = JointDataset(t_x,t_y)\n\ndata_loader = DataLoader(dataset=joint_dataset,batch_size=2,shuffle=True) # will shuffle for every epoch!\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nseq_len = 40\nchunk_size = seq_len +1\ntext_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded)-chunk_size+1)]\n\nclass TextDataset(Dataset):\n    def __init__(self, text_chunks):\n        self.text_chunks = text_chunks\n    def __len__(self):\n        return len(self.text_chunks)\n    def __getitem__(self, index):\n        text_chunk = self.text_chunks[index]\n        return text_chunks[:-1].long(), text_chunks[1:].long()\nseq_dataset = TextDataset(torch.tensor(text_chunks))\nseq_dl = DataLoader(seq_dataset,batch_size=batch_size, shuffle=True, drop_last=True)\n\n\n\n\n\nBy default, the DataLoader will try to stack everything into tensors of the same size, which doesn’t work well for variable-length sequences. Use a custom collate_fn to pad these sequences to a common length, ensuring your batches are properly structured for the model.\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"huggingface_hub\")\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer\nfrom torch.nn.utils.rnn import pad_sequence\n\nPAD_TOKEN = 0\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nclass LanguageModelingDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=512):\n        \"\"\"\n        Args:\n            texts (List[str]): List of text samples.\n            tokenizer (transformers.PreTrainedTokenizer): Tokenizer for encoding text.\n            max_length (int): Maximum sequence length.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.encodings = self.tokenizer(texts, truncation=True, padding=False)  # No padding here\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long)\n\ndef custom_collate_fn(batch):\n    # Same as before\n    input_sequences = [sample[:-1] for sample in batch]\n    target_sequences = [sample[1:] for sample in batch]\n    padded_inputs = pad_sequence(input_sequences, batch_first=True, padding_value=PAD_TOKEN)\n    padded_targets = pad_sequence(target_sequences, batch_first=True, padding_value=PAD_TOKEN)\n    attention_masks = (padded_inputs != PAD_TOKEN).long()\n    return padded_inputs, padded_targets, attention_masks\n\n# Example texts\ntexts = [\n    \"Hello, how are you?\",\n    \"I am fine, thank you!\",\n    \"What are you doing today?\",\n    \"I am working on a language model.\"\n]\n\n# Create Dataset and DataLoader\ndataset = LanguageModelingDataset(texts, tokenizer)\ndataloader = DataLoader(dataset, batch_size=2, collate_fn=custom_collate_fn, shuffle=True)\n\n# Iterate through DataLoader\nfor batch_idx, (inputs, targets, masks) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Inputs:\\n\", inputs)\n    print(\"Targets:\\n\", targets)\n    print(\"Attention Masks:\\n\", masks)\n    print(\"-\" * 50)\n\nBatch 1\nInputs:\n tensor([[ 101, 2054, 2024, 2017, 2725, 2651, 1029,    0,    0],\n        [ 101, 1045, 2572, 2551, 2006, 1037, 2653, 2944, 1012]])\nTargets:\n tensor([[2054, 2024, 2017, 2725, 2651, 1029,  102,    0,    0],\n        [1045, 2572, 2551, 2006, 1037, 2653, 2944, 1012,  102]])\nAttention Masks:\n tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\nBatch 2\nInputs:\n tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,    0],\n        [ 101, 1045, 2572, 2986, 1010, 4067, 2017,  999]])\nTargets:\n tensor([[7592, 1010, 2129, 2024, 2017, 1029,  102,    0],\n        [1045, 2572, 2986, 1010, 4067, 2017,  999,  102]])\nAttention Masks:\n tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1]])\n--------------------------------------------------\n\n\n\n\n\n\nimport torch.nn as nn\nclass NoisyLinear(nn.Module):\n    def __init__(self,input_size,output_size,noise_stddev=0.1):\n        super().__init__()\n        w = torch.Tensor(input_size, output_size)\n        self.w = nn.Parameter(w) # will be included in model.parameters() passed to the optimizer\n        nn.init.xavier_uniform_(self.w)\n        b = torch.Tensor(output_size).fill_(0)\n        self.b = nn.Parameter(b)\n        self.noise_stddev = noise_stddev\n    def forward(self,x,training=False):\n        if training:\n            noise = torch.normal(0.0, self.noise_stddev, x.shape)\n            x_new = torch.add(x,noise)\n        else:\n            x_new = x\n        return torch.add(torch.mm(x_new,self.w),self.b)\nclass NoisyModule(nn.Module):\n    def __init_(self):\n        super().__init__()\n        self.l1 = NoisyLinear(2,4,0.07)\n        self.a1 = nn.ReLU()\n        ...\n    def forward(self,x,training=False): \n        x = self.l1(x,training)\n        x = self.a1(x)\n        ...\n    def predict(self,x):\n        x = torch.tensor(x,dtype=torch.float32)\n        pred = self.forward(x)[:,0] # tra\n        return (pred&gt;=0.5).float()\n# inside the training loop, use training = True\n...\npred = model(x_batch,training=True)[:,0]\n\n\n\n\n\nFor simple functions, can just write a simple function that works like torch.nn.functional\n\n\nimport torch\n\ndef custom_mse_loss(y_pred, y_true):\n    return torch.mean((y_pred - y_true) ** 2)\n\ny_true = torch.tensor([1.0, 2.0, 3.0])\ny_pred = torch.tensor([1.5, 2.5, 3.5])\n\nloss = custom_mse_loss(y_pred, y_true)\nprint(loss)  # Output: 0.25\n\ntensor(0.2500)\n\n\n\nFor more complex functions requiring parameters, use nn.Module, as in the following ElasticNet example:\n\n\nimport torch\nimport torch.nn as nn\n\nclass ElasticNetLoss(nn.Module):\n    def __init__(self, alpha=1.0, beta=1.0):\n        super().__init__()\n        self.alpha = alpha  # Weight for L1 loss\n        self.beta = beta    # Weight for L2 loss\n\n    def forward(self, y_pred, y_true):\n        l1_loss = torch.sum(torch.abs(y_pred - y_true))\n        l2_loss = torch.sum((y_pred - y_true) ** 2)\n        return self.alpha * l1_loss + self.beta * l2_loss\nloss_fn = ElasticNetLoss(alpha=0.5, beta=0.5)\n\ny_true = torch.tensor([1.0, 2.0, 3.0])\ny_pred = torch.tensor([1.5, 2.5, 3.5])\n\nloss = loss_fn(y_pred, y_true)\nprint(loss)  # Output: Weighted combination of L1 and L2 losses\n\ntensor(1.1250)\n\n\n\n\n\n\nRemember that during training, dropout drops units, while during evaluation, it scales the weights by the dropout factor instead of altering the activations. Thus must be very careful to use model.train() and model.eval() as in\n\n\n# Training phase\nmodel.train()\nfor data, target in train_loader:\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n# Evaluation phase\nmodel.eval()\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        # Compute metrics\n\n# Or after loading a model:\ntorch.save(model.state_dict(), 'model.pth')\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()  # Set to evaluation mode after loading\n\n\nDropout is typically applied after activation functions:\n\n\nclass CNNNet(nn.Module):\n    def __init__(self):\n        super(CNNNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.dropout = nn.Dropout2d(p=0.25)\n        self.fc1 = nn.Linear(32 * 26 * 26, 128)\n        self.fc2 = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\nWhen using with BatchNorm, be careful to not overregularize and place BatchNorm before Dropout: BatchNorm normalizes the activations first, ensuring stable statistics, and Dropout is applied afterward without affecting the normalization process.\n\n\n\n\n\n\n\ntorch.save(model, PATH_TO_MODEL) # example path: \"model.pth\"\nmodel = torch.load(PATH_TO_MODEL)\n\n\nNot recommended because not only does this save model parameters, it also saves model classes and directory structure of the source code. If class signatures or directory structures change, may not be able to load the model in the future.\n\n\n\n\n\ntorch.save(model.state_dict(),PATH_TO_MODEL)\nmodel = ConvNet()\nmodel.load_state_dict(torch.load(PATH_TO_MODEL))\nmodel.eval()\n\n\n\n\n\n\nInference pipeline (a) the data preprocessing component, (b) the model inference, and (c) the post-processing step.\n\n\n# server.py\nimport os\nimport json\nimport numpy as np\nfrom flask import Flask, request\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n        self.dp1 = nn.Dropout2d(0.10)\n        self.dp2 = nn.Dropout2d(0.25)\n        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n        self.fc2 = nn.Linear(64, 10)\n \n    def forward(self, x):\n        x = self.cn1(x)\n        x = F.relu(x)\n        x = self.cn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dp1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dp2(x)\n        x = self.fc2(x)\n        op = F.log_softmax(x, dim=1)\n        return op\n    \nmodel = ConvNet()\nPATH_TO_MODEL = \"./convnet.pth\"\nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\nmodel.eval()\n\ndef run_model(input_tensor):\n    model_input = input_tensor.unsqueeze(0)\n    with torch.no_grad():\n        model_output = model(model_input)[0]\n    model_prediction = model_output.detach().numpy().argmax()\n    return model_prediction\n\ndef post_process(output):\n    return str(output)\n\napp = Flask(__name__)\n\n@app.route(\"/test\", methods=[\"POST\"])\ndef test():\n    # 1. Preprocess\n    data = request.files['data'].read()\n    md = json.load(request.files['metadata'])\n    input_array = np.frombuffer(data, dtype=np.float32)\n    input_image_tensor = torch.from_numpy(input_array).view(md[\"dims\"])\n    # 2. Inference\n    output = run_model(input_image_tensor)\n    # 3. Postprocess\n    final_output = post_process(output)\n    return final_output\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8890)\n\n\nMake a request as follows:\n\n\n# make_request.py\nimport io\nimport json\nimport requests\nfrom PIL import Image\n\nfrom torchvision import transforms\n\n\nimage = Image.open(\"./digit_image.jpg\")\n\ndef image_to_tensor(image):\n    gray_image = transforms.functional.to_grayscale(image)\n    resized_image = transforms.functional.resize(gray_image, (28, 28))\n    input_image_tensor = transforms.functional.to_tensor(resized_image)\n    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n    return input_image_tensor_norm\n\nimage_tensor = image_to_tensor(image)\n\ndimensions = io.StringIO(json.dumps({'dims': list(image_tensor.shape)}))\ndata = io.BytesIO(bytearray(image_tensor.numpy()))\n\nr = requests.post('http://localhost:8890/test',\n                  files={'metadata': dimensions, 'data' : data})\n\nresponse = json.loads(r.content)\n\nprint(\"Predicted digit :\", response)\n\n\nTurn into a microservice with the following Dockerfile:\n\n\nFROM python:3.9-slim\n\nRUN apt-get -q update && apt-get -q install -y wget\n\nCOPY ./server.py ./\nCOPY ./requirements.txt ./\n\nRUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/convnet.pth\nRUN wget -q https://github.com/PacktPublishing/Mastering-PyTorch/raw/master/Chapter10/digit_image.jpg\n\nRUN pip install -r requirements.txt\n\n\nUSER root\nENTRYPOINT [\"python\", \"server.py\"]\n\n\nBuild with digit_recognizer tag using current directory as context: docker build -t digit_recognizer .\n\nNote how server.py and requirements.txt are copied into Docker’s directory. Also USER root may give overly elevated privileges.\n\nForward the 8890 port on our machine to 8890 port on the container since app runs on port 8890: docker run -p 8890:8890 digit_recognizer\nPython make_request.py will make a request to the Dockerized Flask model server and we’ll get the response.\n\n\n\n\n\nTorchScript allows exporting optimized and serialized PyTorch models to Intermediate Representation that can be ran outside of Python (say in C++ with TorchScript compiler).\nTracing: TorchScript with tracing requires an input to trace through the computation graph, implying that only one of the possible paths will be rendered.\n\n\n# Step 1: Define and instantiate the model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n        self.dp1 = nn.Dropout2d(0.10)\n        self.dp2 = nn.Dropout2d(0.25)\n        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n        self.fc2 = nn.Linear(64, 10)\n \n    def forward(self, x):\n        x = self.cn1(x)\n        x = F.relu(x)\n        x = self.cn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dp1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dp2(x)\n        x = self.fc2(x)\n        op = F.log_softmax(x, dim=1)\n        return op\n    \nmodel = ConvNet()\n\n# Step 2: Load the model's state_dict and put model in eval mode\nPATH_TO_MODEL = \"./convnet.pth\" \nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\")) \nmodel.eval()\n\n# Step 3: Turn off gradient tracking unless already loaded the model with torch.no_grad\nfor p in model.parameters():    p.requires_grad_(False)\n\n# Step 4: Trace the model\ndemo_input = torch.ones(1, 1, 28, 28) \ntraced_model = torch.jit.trace(model, demo_input)\nprint(traced_model.graph)\nprint(traced_model.code)\n\n# Step 5: Save the traced model\ntorch.jit.save(traced_model, 'traced_convnet.pt')\n\n# Step 6: Load the model and use it for inference on a preprocessed image\nloaded_traced_model = torch.jit.load('traced_convnet.pt')\nimage = Image.open(\"./digit_image.jpg\")\ndef image_to_tensor(image):    \n    ... \n    return input_image_tensor_norm\ninput_tensor = image_to_tensor(image)\n\nloaded_traced_model(input_tensor.unsqueeze(0))\n# Should produce the same output as\nmodel(input_tensor.unsqueeze(0))\n\n\nFor scripted model, there’s no need to provide dummy input and it suffices to simply use torch.jit.script instead of torch.jit.trace.\n\n\nscripted_model = torch.jit.script(model)\n\n\n\n\ntracing vs scripting from Mastering PyTorch, Second Edition by Jha.png\n\n\n\n\n\n\nUsing Jeremy’s book (Ch. 18) as a starting point\nPyTorch’s equivalent of fastai Learner callback except that they allow you to inject code into forward and backward calculations rather than training loop.\nBest to have the Hook class be a context manager to avoid a memory leak if hook.remove() is not called.\nCan register_forward_hook and register_backward_hook.\nUse cases:\n\nDebugging: inspecting intermediate outputs and gradients.\nFeature extraction: collect activations of a particular layer (see below) for visualization or transfer learning.\nCustom modifications: modify inputs/outputs or their grads (custom regularization or grad manipulation, for example).\n\n\n\nclass Hook():\n    def __init__(self, m):\n        self.hook = m.register_forward_hook(self.hook_func)\n    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n    def __enter__(self, *args): return self # 'just give me the hook'\n    def __exit__(self, *args): self.hook.remove() # remove the hook to avoid memory leak\n# use as follows\n\nwith Hook(learn.model[0]) as hook: # will register forward hook on learn.model[0]\n    with torch.no_grad(): output = learn.model.eval()(x.cuda())\n    act = hook.stored\n\n# This will allow you to get dot product of weight matrix with the activations (2,k activations) @ (k activations, rows, cols)\n# Can then look which pixels contributed to model's prediction by plotting this map\ncam_map = torch.einsum('ck,kij-&gt;cij',learn.model[1][-1].weight,act)\n\n\nclass HookBwd():\n    def __init__(self, m):\n        self.hook = m.register_backward_hook(self.hook_func)\n    def hook_func(self, m, gi, go):\n        \"\"\"Will access gradients of the input gi and output go of the m module\"\"\"\n        self.stored = go[0].detach().clone\n    def __enter__(self, *args): return self\n    def __exit__(self, *args): self.hook.remove()\n\n\nCan’t simply call output.backward because output is a rank-2 tensor and we can only take derivatives wrt scalars. Can call output[0,cls].backward, though:\n\n\ncls = 1\nwith HookBwd(learn.model[0]) as hookg:\n    with Hook(learn.model[0]) as hook:\n        output = learn.model.eval()(x.cuda())\n        act = hook.stored\n    output[0,cls].backward()\n    grad = hookg.stored\n\n# can then get product of gradients and activations\nw = grad[0].mean(dim=[1,2], keepdim=True)\ncam_map = (w * act[0]).sum(0)\n\n\nExamples of use with HuggingFace:\n\n\nfrom transformers import Trainer, BertForSequenceClassification, BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\nclass Hook():\n    def __init__(self, m, print_every=100):\n        self.hook = m.register_forward_hook(self.hook_func)\n        self.counter = 0\n        self.print_every = print_every\n\n    def hook_func(self,m,i,o):\n        if self.counter % self.print_every == 0:\n            input_shape = [tensor.shape for tensor in i]\n            output_shape = output.shape if isinstance(output, torch.Tensor) else [tensor.shape for tensor in output]\n            print(f\"Layer: {m}, Input shape: {input_shape}, Output shape: {output_shape}\")\n        self.counter += 1\n\n    def __enter__(self, *args): return self\n\n    def __exit__(self, *args): self.hook.remove()\n\ntext = \"...\"\ninputs = tokenizer(text, return_tensors='pt')\nwith Hook(model.bert.encoder.layer[0].attention) as hookf:\n    outputs = model(**inputs)\n\n# Then use Trainer or SFTTrainer as always ...\ntrainer = Trainer(...)\ntrainer.train()\n# Will print input/output shapes every print_every iterations. Can log these, of course.\n\n\n\n\n\nPyTorch compares shapes of two tensors elementwise. It starts at the trailing dimensions and works its way backwards, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following holds:\n\nThey are equal\nOne of them is 1, in which case it’s broadcast to make it the same shape as the other.\n\n\n\n\n# Define tensors\na = torch.tensor([1, 2, 3])  # Shape: (3,)\nb = torch.tensor([4, 5])     # Shape: (2,)\n\ntry:\n    result_direct = a * b\nexcept RuntimeError as e:\n    print(\"Error without reshaping:\\n\", e)\n# Output:\n# RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n# Reshape tensors for broadcasting\na_reshaped = a.view(3, 1)  # Shape: (3, 1)\nb_reshaped = b.view(1, 2)  # Shape: (1, 2)\n\n# Multiply after reshaping\nresult_broadcast = a_reshaped * b_reshaped\nprint(\"Result after reshaping and multiplying:\\n\", result_broadcast)\n# Output:\n\nError without reshaping:\n The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\nResult after reshaping and multiplying:\n tensor([[ 4,  5],\n        [ 8, 10],\n        [12, 15]])\n\n\n\n\n\n\nRepeated indices are implicitly summed over.\nEach index can appear at most twice in any term.\nEach term must contain identical nonrepeated indices.\n\n\nimport torch\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij',a,b)\nm1 = torch.randn(5,28*28)\nm2 = torch.randn(784, 10)\n%timeit -n 10 t5 = matmul(m1,m2)\n\nThe slowest run took 130.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n164 μs ± 380 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nBatch matmul\n\n\ndef batch_matmul(a, b):\n    return torch.einsum('bik,bkj-&gt;bij', a, b)\n# Example usage\nbatch_size = 2\ni, k, j = 3, 4, 5\na = torch.randn(batch_size, i, k)\nb = torch.randn(batch_size, k, j)\nresult = batch_matmul(a, b)\nprint(\"Batch MatMul Result Shape:\", result.shape)\n\nBatch MatMul Result Shape: torch.Size([2, 3, 5])\n\n\n\nElement-wise Multiplication with Broadcasting (Outer Product). Note that PyTorch’s einsum allows implicit reshaping to conform to broadcasting semantics: Normally, could not multiply a and b since standard broadcasting rules would make them (3,1) and (2,1), which are incompatible.\n\n\ndef outer_product(a,b):\n    return torch.einsum('i,j-&gt;ij',a,b)\na = torch.tensor([1, 2, 3]) # shape (3,) -&gt; (3,1) \nb = torch.tensor([4, 5])    # shape (2,) -&gt; (1.2)\nresult = outer_product(a, b)\nprint(\"Outer Product Result:\\n\", result)\n\nOuter Product Result:\n tensor([[ 4,  5],\n        [ 8, 10],\n        [12, 15]])\n\n\n\nDot product. The absence of indices on the right side of the arrow (‘-&gt;’) indicates that the output is a scalar resulting from the summation.\n\n\ndef dot_product(a, b):\n    return torch.einsum('i,i-&gt;', a, b)\n\n# Define tensors\na = torch.tensor([1, 2, 3])  # Shape: (3,)\nb = torch.tensor([4, 5, 6])  # Shape: (3,)\n\n# Compute dot product using einsum\nresult = dot_product(a, b)\nprint(\"Dot Product Result:\", result)\n\nDot Product Result: tensor(32)\n\n\n\nTranspose of a Matrix\n\n\ndef transpose(a):\n    return torch.einsum('ij-&gt;ji', a)\n\n# Example usage\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nresult = transpose(a)\nprint(\"Transpose Result:\\n\", result)\n\nTranspose Result:\n tensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nSum Over a Specific Dimension\n\n\ndef sum_over_dim(a):\n    return torch.einsum('ij-&gt;i', a)\n\n# Example usage\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nresult = sum_over_dim(a)\nprint(\"Sum Over Rows:\", result)\n# Output: Sum Over Rows: tensor([ 6, 15])\n\nSum Over Rows: tensor([ 6, 15])\n\n\n\nAttention Score Calculation\n\n\ndef attention_scores(query, key):\n    return torch.einsum('bqd,bkd-&gt;bqk', query, key) # sum the products across the feature dimension d to compute attention scores\n\n# Example usage\nbatch_size, q, d, k = 2, 3, 4, 4\nquery = torch.randn(batch_size, q, d)\nkey = torch.randn(batch_size, k, d)\nscores = attention_scores(query, key)\nprint(query)\nprint(key)\nprint(scores)\nprint(\"Attention Scores Shape:\", scores.shape)\n# Output: Attention Scores Shape: torch.Size([2, 3, 4])\n\ntensor([[[ 1.9826,  0.6645, -0.9895, -0.0247],\n         [-1.5472,  0.6203, -0.9500, -0.3087],\n         [ 0.3845,  0.3481,  0.2347,  0.7434]],\n\n        [[ 1.0612,  1.7969, -0.4906, -0.1500],\n         [-0.3895,  1.5363, -0.4685, -0.4239],\n         [ 0.9783, -0.3329,  0.9564,  0.3776]]])\ntensor([[[-1.5022,  1.1593,  0.8890,  1.0148],\n         [ 1.1577,  0.4848, -0.1506,  1.7842],\n         [-1.4799,  0.6236,  0.7959,  0.1494],\n         [-1.7694, -1.7675,  0.2545,  0.0117]],\n\n        [[-1.1997,  1.1282, -0.8853, -0.8205],\n         [-0.4073, -1.8880,  0.1635,  0.8772],\n         [ 0.2545, -1.0884, -0.1758, -0.2735],\n         [-0.1619,  1.4902,  0.2765, -1.1585]]])\ntensor([[[-3.1126,  2.7223, -3.3110, -4.9348],\n         [ 1.8854, -1.8981,  1.8743,  1.3959],\n         [ 0.7889,  1.9051, -0.0542, -1.2273]],\n\n        [[ 1.3114, -4.0365, -1.5585,  2.5441],\n         [ 2.9630, -3.1904, -1.5730,  2.7140],\n         [-2.7057,  0.7177,  0.3398, -0.8274]]])\nAttention Scores Shape: torch.Size([2, 3, 4])\n\n\n\nimport numpy as np\nnp.dot([ 1.9826,  0.6645, -0.9895, -0.0247],[-1.5022,  1.1593,  0.8890,  1.0148])\n\n-3.11263793\n\n\n\nnp.dot([ 1.9826,  0.6645, -0.9895, -0.0247],[ 1.1577,  0.4848, -0.1506,  1.7842])\n\n2.7223545799999997\n\n\n\nWeighted Sum for Attention Output\n\n\n\n\n\ndef weighted_sum(attention_weights, value):\n    return torch.einsum('bqk,bvd-&gt;bqd', attention_weights, value)\n\n# Example usage\nbatch_size, q, k, d = 2, 3, 4, 5\nattention_weights = torch.randn(batch_size, q, k) # (2,3,4)\nvalue = torch.randn(batch_size, d, 5)             # (2,5,5)\nresult = weighted_sum(attention_weights, value)\nprint(\"Weighted Sum Result Shape:\", result.shape)\n# Output: Weighted Sum Result Shape: torch.Size([2, 3, 5])\n\n\nOuter Product for Rank-1 Updates\n\n\ndef outer_product(a, b):\n    return torch.einsum('i,j-&gt;ij', a, b)\n\n# Example usage\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nresult = outer_product(a, b)\nprint(\"Outer Product Result:\\n\", result)\n# Output:\n# Outer Product Result:\n# tensor([[ 4,  5,  6],\n#         [ 8, 10, 12],\n#         [12, 15, 18]])\n\n\n\n\n\nKey step 1/4: Initialize LoRA parameters\nKey step 2/4: Use LoRA parameters in matmuls\nKey step 3/4: Replace nn.Linear layers with nn.LoRALinear layers\nKey step 4/4: Only do backprop on the LoRA parameters\n\n\nimport torch\nimport torch.nn as nn\n\nclass LoRALinear(nn.Module):\n    \"\"\"\n    A LoRA (Low-Rank Adaptation) layer that wraps an existing nn.Linear layer.\n    It adds a low-rank update to the output of the original linear layer.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha = 1.0):\n        super().__init__()\n        self.linear =  original_linear\n        self.r = r\n        self.alpha = alpha\n        # Key step 1/4: Initialize LoRA parameters\n        if r &gt; 0:\n            in_features = original_linear.in_features\n            out_features = original_linear.out_features\n            # initialize A and B low-rank matrices\n            self.lora_A = nn.Parameter(torch.zeros(in_features,r))\n            self.lora_B = nn.Parameter(torch.zeros(r,out_features))\n            # Use normal init (mean 0, std 0.02)\n            nn.init.normal_(self.lora_A, std=0.02)\n            nn.init.normal_(self.lora_B, std=0.02)\n            # Scaling factor for LoRA update\n            self.scaling = self.alpha/self.r\n        else:\n            self.lora_A =  None\n            self.lora_B = None\n            self.scaling = 1.0\n\n    def forward(self, x):\n        result = self.linear(x)\n        # Key step 2/4: Use LoRA parameters in matmuls\n        if self.r &gt; 0:\n            # ((bs,in_features) @ (in_features,r)) @ (r,out_features) -&gt; (bs, out_features)\n            lora_update = (x @ self.lora_A) @ self.lora_B \n            lora_update *= self.scaling\n            result += lora_update\n        return result\n\n# Key step 3/4: Replace nn.Linear layers with nn.LoRALinear layers\ndef replace_linear_with_lora(model, r=4, alpha=1.0):\n    \"\"\"\n    Recursively replace nn.Linear layers with LoRALinear layers in the model.\n    \"\"\"\n    for name, module in model.named_children():\n        if isinstance(module,nn.Linear):\n            setattr(model,name,LoRALinear(module,r,alpha))\n        else: # recursively apply to child modules\n            replace_linear_with_lora(module, r, alpha)\n    return model\n\n\ndef get_lora_params(model):\n    \"\"\"\n    Retrieve all LoRA parameters from the model.\n    \"\"\"\n    lora_parameters = []\n    for module in model.modules():\n        if isinstance(module, LoRALinear):\n            lora_parameters.extend([module.lora_A, module.lora_B])\n    return lora_parameters\n\n# Example Usage\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out\n\nmodel = SimpleModel(input_dim=128, hidden_dim=64, output_dim=10)\nmodel = replace_linear_with_lora(model, r=4, alpha=1.0)\n\n# Key step 4/4: Only do backprop on the LoRA parameters\nlora_parameters = get_lora_params(model)\noptimizer = torch.optim.Adam(lora_parameters, lr=1e-3)\n\n# Training loop follows\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass QLoRALinear(nn.Module):\n    \"\"\"\n    A QLoRA (Quantized Low-Rank Adaptation) layer that wraps an existing quantized nn.Linear layer.\n    It adds a low-rank update to the output of the original quantized linear layer.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha=1.0):\n        super().__init__()\n        # Quantize the original linear layer's weights to 4-bit\n        ...\n        self.linear.weight.data = self.quantize_weights(self.linear.weight.data)\n        self.linear.bias = original_linear.bias  # Bias remains in full precision\n\n        ...\n        if r &gt; 0:\n            ...\n        else:\n            ...\n    def quantize_weights(self, weight, num_bits=4):\n        \"\"\"\n        Quantizes the weights to the specified number of bits.\n        \"\"\"\n        qmin = 0\n        qmax = 2**num_bits - 1\n\n        min_val, max_val = weight.min(),weight.max()\n        scale = (max_val-min_val)/(qmax-qmin)\n        zero_point = qmin - min_val/scale\n\n        q_weight = torch.clamp((weight/scale + zero_point).round(),qmin,qmax)\n        #q_weight = (q_weight - zero_point) * scale\n        return q_weight        \n\n    def forward(self, x):\n        ...\n\ndef replace_linear_with_qlora(model, r=4, alpha=1.0):\n    \"\"\"Swap LoRALinear with QLoRALinear\"\"\"\n    \n\ndef get_qlora_params(model):\n    \"\"\"Swap LoRALinear with QLoRALinear\"\"\"\n\n# Example Usage\nclass SimpleModel(nn.Module):\n    \"\"\"Same\"\"\"\n\nmodel = SimpleModel(input_dim=128, hidden_dim=64, output_dim=10)\nmodel = replace_linear_with_qlora(model, r=4, alpha=1.0)\n\n# Only do backprop on the QLoRA parameters\nqlora_parameters = get_qlora_params(model)\noptimizer = torch.optim.Adam(qlora_parameters, lr=1e-3)\n\n# Training loop follows\n\n\nNote on dequantization: Dequantization would happen via q_weight = (q_weight - zero_point) * scale. There will be quantization error, see below.\n\n\n\n\nimage.png\n\n\n\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\nclass QLoRALinear(nn.Module):\n    \"\"\"\n    A QLoRA (Quantized Low-Rank Adaptation) layer that wraps an existing nn.Linear layer.\n    It quantizes the original linear layer and adds a low-rank update.\n    \"\"\"\n    def __init__(self, original_linear, r=4, alpha=1.0, num_bits=4):\n        super().__init__()\n        self.r = r\n        self.alpha = alpha\n\n        if num_bits == 8:\n            self.linear = bnb.nn.Linear8bitLt(\n                original_linear.in_features,\n                original_linear.out_features,\n                bias=original_linear.bias is not None,\n                has_fp16_weights=False,\n                threshold=6.0\n            )\n        elif num_bits == 4:\n            self.linear = bnb.nn.Linear4bit(\n                original_linear.in_features,\n                original_linear.out_features,\n                bias = original_linear.bias is not None,\n                compute_dtype=torch.float16\n            )\n        else:\n            raise ValueError(f\"Unsupported num_bits: {num_bits}. Supported values are 4 and 8.\")\n\n        self.linear.weight.data = original_linear.weight.data.clone()\n        if original_linear.bias is not None:\n            self.linear.bias.data = original_linear.bias.data.clone()\n\n        if r &gt; 0: ... else:...\n        \n        # Freeze the quantized weights\n        for param in self.linear.parameters():\n            param.requires_grad = False\n\n\n\n        # Quantize the original linear layer\n        self.linear = quantization.QuantWrapper(original_linear)\n        # Dynamic quantization quantizes weights ahead of time and quantizes activations on-the-fly during forward pass\n        self.linear.qconfig = quantization.default_dynamic_qconfig\n        quantization.prepare(self.linear, inplace=True) # collect stats needed for quantization\n        quantization.convert(self.linear, inplace=True) # replace original operations with quantized counterparts\n\n\nPurpose of self.linear.weight.data = original_linear.weight.data.clone(); if original_linear.bias is not None: self.linear.bias.data = original_linear.bias.data.clone()\n\nIsolation: Avoid modifying original_linear and prevent unintended gradient updates.\nConsistency: Maintain parameter consistency between the original and quantized layers.\nStarting point for quantization\n\n\n\n\n\n\nimport torch\nnum_epochs = 10\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n\nT_max: This parameter defines the maximum number of iterations (typically epochs) for one cycle of the cosine annealing schedule. Essentially, it determines the period over which the learning rate decreases from the initial value down to the minimum learning rate (eta_min).\n\nSingle-Cycle Annealing: If you set T_max equal to the total number of training epochs, the learning rate will smoothly decrease from the initial learning rate to eta_min over the entire training process.\nMulti-Cycle Annealing: By setting T_max to a fraction of the total epochs, you can create multiple cycles of learning rate adjustments within the training process. For more advanced multi-cycle schedules, consider using CosineAnnealingWarmRestarts.\n\n\n\n\n\nAvailable LR Schedulers on https://pytorch.org/docs\n\n\n\n\n\n\nTorchServe a custom model handler, which should:\n\nInitialize the model\nPreprocess inputs from the client\nRun the preprocessed inputs through the model for inference\nPostprocess the model’s outputs before returning to client\n\n\n\nimport logging\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\n\n# Configure logging\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\nclass TransformersHandler(BaseHandler):\n    def initialize(self, ctx):\n        logger.info(\"Initializing the TransformersHandler.\")\n        self.manifest = ctx.manifest\n        properties = ctx.system_properties\n        model_dir = properties.get(\"model_dir\")\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Using device: {self.device}\")\n        \n        self.model = BertForSequenceClassification.from_pretrained(model_dir)\n        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n        self.model.to(self.device)\n        self.model.eval()\n        logger.info(\"Model and tokenizer loaded successfully.\")\n\n    def preprocess(self, data):\n        logger.info(\"Preprocessing input data.\")\n        logger.debug(f\"Raw data received: {data}\")\n        logger.info(f\"DATA: {data}\")\n        data_body = data[0]['body'] # should have probably figured out I needed to access this pre-Docker!\n        logger.info(f\"DATA_BODY: {data_body}\")\n\n        text = data_body.get(\"text\")\n        if isinstance(text, bytes):\n            text = text.decode('utf-8')\n            logger.debug(f\"Decoded text: {text}\")\n        \n        try:\n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                truncation=True,\n                padding=True\n            )\n            logger.debug(f\"Tokenized inputs: {inputs}\")\n        except Exception as e:\n            logger.error(f\"Error during tokenization: {e}\")\n            raise e\n        \n        return inputs\n\n    def inference(self, inputs):\n        logger.info(\"Performing inference.\")\n        try:\n            with torch.no_grad():\n                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n                outputs = self.model(**inputs)\n                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n                confidences, predictions = torch.max(probs, dim=1)\n                result = {\"confidence\": confidences.item(), \n                          \"prediction\": predictions.item()}\n                logger.debug(f\"Inference result: {result}\")\n                return result\n        except Exception as e:\n            logger.error(f\"Error during inference: {e}\")\n            raise e\n\n    def postprocess(self, inference_output):\n        logger.info(\"Postprocessing inference output.\")\n        logger.debug(f\"Postprocessing result: {inference_output}\")\n        return [inference_output]\n\n\nrequirements.txt file is below:\n\n\ntorchserve==0.6.0\ntorch-model-archiver==0.6.0\ntransformers==4.47.1\ntorch==2.4.0+cu121\n\n\nBelow is a multi stage Dockerfile for the build:\n\n\n# Stage 0: Fetch Model Files\nFROM alpine/git AS fetcher\nRUN apk add --no-cache git\nRUN git clone https://huggingface.co/bert-base-uncased /model\n\n# Stage 1: Build Stage\nFROM pytorch/torchserve:latest AS build\nRUN pip install transformers\nRUN mkdir -p /home/model-server/model-store /home/model-server/code\nCOPY requirements.txt /home/model-server/\nRUN pip install -r /home/model-server/requirements.txt\nCOPY --from=fetcher /model /home/model-server/model-store/bert\nCOPY handler.py /home/model-server/code/handler.py\n# Archive the model\nRUN torch-model-archiver \\\n    --model-name bert_seq_class \\\n    --version 1.0 \\\n    --serialized-file /home/model-server/model-store/bert/pytorch_model.bin \\\n    --handler /home/model-server/code/handler.py \\\n    --extra-files \"/home/model-server/model-store/bert/config.json,/home/model-server/model-store/bert/tokenizer.json,/home/model-server/model-store/bert/tokenizer_config.json,/home/model-server/model-store/bert/vocab.txt\" \\\n    --export-path /home/model-server/model-store\n\n# Stage 2: Runtime Stage\nFROM pytorch/torchserve:latest-gpu\nCOPY requirements.txt /home/model-server/\nRUN pip install -r /home/model-server/requirements.txt\n# Copy model archive and configuration from the build stage\nCOPY --from=build /home/model-server/model-store /home/model-server/model-store\nEXPOSE 8080\nEXPOSE 8081\n# Start TorchServe\nCMD [\"torchserve\", \"--start\", \"--model-store\", \"/home/model-server/model-store\", \"--models\", \"bert_seq_class=bert_seq_class.mar\"]\n\n\nBuild and run the above file as\n\n\ndocker build -t torchserve-bert-model .\ndocker run -p 8080:8080 -p 8081:8081 --name torchserve-bert-model torchserve-bert-model\n\n\nDebug by looking at the logs in\n\n\ndocker exec -it torchserve-bert-model bash\n\n\nSend and receive requests as\n\n\n%%bash\ncurl -X POST http://localhost:8080/predictions/bert_seq_class      -H \"Content-Type: application/json\"      -d '{\"text\": \"I love using TorchServe with Hugging Face models!\"}'\n{\n  \"confidence\": 0.5290595293045044,\n  \"prediction\": 1"
  },
  {
    "objectID": "notes/LlmBasicsAndRecipes.html#hugging-face",
    "href": "notes/LlmBasicsAndRecipes.html#hugging-face",
    "title": "LLMs/PyTorch Basics and Recipes",
    "section": "Hugging Face",
    "text": "Hugging Face\n\nTokenizer\n\nHF Chat Templates\n\n\n\nSFTTrainer\n\n# ---Step 0: Prepare the data by finding the prompt and formatting a column ('text' for \n# regular SFT, 'messages' for instruction ft, etc), shuffling and splitting it ---\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_config,get_peft_model\n\n\nmodel_name = HF_MODEL_ID\n# ---Step 1: Initialize BitsAndBytesConfig and feed it to the model upon load---\nbnb_config = BitsAndBytesConfig( #Q in QLoRA\n    load_in_4bit=True,  # Use 4-bit precision model loading\n    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    # Leave this out for regular SFT\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# ---Step 2: Load the tokenizer---\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = \"&lt;PAD&gt;\"\ntokenizer.padding_side = \"left\"\n\n# ---Step 3: Initialize LoraConfig and i.) peft.prepare_model_for_kbit_training and ii.) peft.get_peft_model---\npeft_config = LoraConfig(\n    lora_alpha=32,\n    lora_dropout=0.1,\n    r=128,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=  # Layers to target\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# ---Step 4: Define TrainingArguments, set up SFTTrainer, trainer.train()---\nfrom transformers import DataCollatorForLanguageModeling\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs = 10.0,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True\n)\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['valid'],\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n   # peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=256,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=True,\n)\n\ntrainer.train()\n# NOTE: SFTTrainer will automatically send logs to wandb set up via\n#  import wandb; wandb.login(); %env WANDB_PROJECT=sql-fine-tuning\n\n# ---Step 5: Save QLoRA weights and merge---\ntrainer.model.save_pretrained(output_dir)\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel = model.merge_and_unload()\n\noutput_merged_dir = os.path.join(output_dir, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)\n# NOTE: In the future, can load this final merged model without knowing the QLoRA configurations\n\n\n# ---Step x: Can use the merged model to make predictions as follows---\nfrom transformers import pipeline\n\n# Use our predefined prompt template\nprompt = \"\"\"&lt;|user|&gt;\nTell me something about Large Language Models.&lt;/s&gt;\n&lt;|assistant|&gt;\n\"\"\"\n\n# Run our instruction-tuned model\npipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\nprint(pipe(prompt)[0][\"generated_text\"])\n\n\n\nAccelerate and Distributed Training\n\nNotes below are based on Zack Mueller’s talk\nDistributed Data Parallelism (DDP): full copy of the model exists on every device, but data is chunked between each GPU.\nFully Sharded Data Parallelism (FSDP) & Deepspeed (DS): Model and optimizer states are split across GPUs -&gt; Two GPUs act as a ‘single GPU’ for model training.\n\n\n\n\nimage.png\n\n\n\nsharding_strategy\n\nFULL_SHARD: optimizer state, gradients, and parameters are sharded\nSHARD_GRAD_OP: shards optimizer states and gradients; model is split during forward pass, joined during backward pass. Reduces training vRAM a little.\nNO_SHARD: Normal DDP\nHYBRID_SHARD: includes optimizer states, gradients, and parameters but each node has the full model. Reduces multi-node communication.\n\noffload_params\n\nfsdp_offload_params allows gradients and model parameters to be offloaded into RAM. Can train much bigger model, but can be very slow.\n\nSee https://huggingface.co/docs/accelerate/en/usage_guides/explore for script and config pairings for various settings.\n\n\nMain commands in the CLI interface\n\naccelerate config: configure the environment\naccelerate estimate-memory: estimate vRAM requirements\naccelerate launch: launch the script\n\n\n\nMain basic script modifications:\n\n\n\nimage.png\n\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom accelerate import Accelerator #1 Import Accelerator\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm.auto import tqdm\n\naccelerator = Accelerator() #2 Initialize the Accelerator\n\ndataset = load_dataset(..., split='train')\nmodel_name = ...\ntokenizer = ...\nmodel = ...\ntokenized_dataset = ...\ntrain_dataloader = ...\n\n#3 Prepare everything with Accelerator\nmodel, optimizer, train_dataloader = accelerator.prepare(\n    model,\n    AdamW(model.parameters(), lr=5e-5),\n    train_dataloader\n)\n\n#4 Training loop with accelerator.backward\nmodel.train()\nfor epoch in range(1, 4):\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch}\"):\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss) #4\n        optimizer.step()\n        optimizer.zero_grad()\n\n\n\nRegister for checkpointing:\n\nfrom accelerate import Accelerator\nimport torch\nimport os\n\naccelerator = Accelerator(project_dir=\"my/save/path\")\n\nmy_scheduler = torch.optim.lr_scheduler.StepLR(my_optimizer, step_size=1, gamma=0.99)\nmy_model, my_optimizer, my_training_dataloader = accelerator.prepare(my_model, my_optimizer, my_training_dataloader)\n\n# Register the model, optimizer, and LR scheduler for checkpointing\naccelerator.register_for_checkpointing(my_model)\naccelerator.register_for_checkpointing(my_optimizer)\naccelerator.register_for_checkpointing(my_scheduler)\n\n# Check if a checkpoint exists and load it\ncheckpoint_dir = \"my/save/path/checkpointing/checkpoint_0\"\nif os.path.exists(checkpoint_dir):\n    accelerator.load_state(checkpoint_dir)\n    print(f\"Loaded checkpoint from {checkpoint_dir}\")\nelse:\n    print(\"No checkpoint found, starting from scratch\")\n\ndevice = accelerator.device\nmy_model.to(device)\n\n# Perform training\nfor epoch in range(num_epochs):\n    for batch in my_training_dataloader:\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n    my_scheduler.step()\n\n    # Save checkpoint at the end of each epoch\n    if accelerator.is_main_process:\n        accelerator.save_state()\n        print(f\"Saved checkpoint at end of epoch {epoch + 1}\")\n\n# Save the final model\nif accelerator.is_main_process:\n    unwrapped_model = accelerator.unwrap_model(my_model)\n    unwrapped_model.save_pretrained(\"trained_model\")\n    tokenizer.save_pretrained(\"trained_model\")\naccelerator.wait_for_everyone()\n\n\nBasic script is below. Run it with accelerate launch train.py.\n\n\nimport argparse\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    get_linear_schedule_with_warmup,\n    AdamW,\n)\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\n\n\ndef main():\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", help=\"Model name or path\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"wikitext\", help=\"Dataset name\")\n    parser.add_argument(\"--dataset_config\", type=str, default=\"wikitext-2-raw-v1\", help=\"Dataset config\")\n    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=4, help=\"Batch size per device\")\n    parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n    args = parser.parse_args()\n\n    # Initialize Accelerator\n    accelerator = Accelerator()\n    device = accelerator.device\n\n    # Load the tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token if not already set\n\n    model = AutoModelForCausalLM.from_pretrained(args.model_name)\n    model.to(device)\n\n    # Load and preprocess the dataset\n    raw_datasets = load_dataset(args.dataset_name, args.dataset_config, split=\"train\")\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n    tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\n    # Create DataLoader\n    train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=args.per_device_train_batch_size)\n\n    # Prepare optimizer and scheduler\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    num_update_steps_per_epoch = len(train_dataloader)\n    max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=max_train_steps\n    )\n\n    # Prepare everything with Accelerator\n    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, lr_scheduler\n    )\n\n    # Training loop\n    model.train()\n    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n    for epoch in range(args.num_train_epochs):\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch, labels=batch[\"input_ids\"])\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n            if step % 100 == 0 and accelerator.is_local_main_process:\n                print(f\"Epoch {epoch}, Step {step}, Loss: {loss.detach().item()}\")\n\n    # Save the model (only on the main process)\n    if accelerator.is_main_process:\n        model.save_pretrained(\"trained_model\")\n        tokenizer.save_pretrained(\"trained_model\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nUsing accelerate config:\n\n\n\nimage.png\n\n\n\nThe yaml config will end up looking as follows:\n\n\n\n\nimage.png\n\n\n\nBasic DDP vs FSDP configs, will be used by accelerate launch:\n\n\n\n\nimage.png\n\n\n\n\nLogging with accelerate:\n\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(log_with=\"all\") # log_with='wandb'\nconfig = {\n    \"num_iterations\": 5,\n    \"learning_rate\": 1e-2,\n    \"loss_function\": str(my_loss_function),\n}\n\naccelerator.init_trackers(\"example_project\", config=config)\n\nmy_model, my_optimizer, my_training_dataloader = accelerate.prepare(my_model, my_optimizer, my_training_dataloader)\ndevice = accelerator.device\nmy_model.to(device)\n\nfor iteration in config[\"num_iterations\"]:\n    for step, batch in my_training_dataloader:\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n        accelerator.log({\"training_loss\": loss}, step=step)\naccelerator.end_training()\n\n\n\nHandling CUDA Out-of-Memory with find_executable_batch_size\n\ndef training_function(args):\n    accelerator = Accelerator()\n\n+   @find_executable_batch_size(starting_batch_size=args.batch_size)\n+   def inner_training_loop(batch_size):\n+       nonlocal accelerator # Ensure they can be used in our context\n+       accelerator.free_memory() # Free all lingering references\n        model = get_model()\n        model.to(accelerator.device)\n        optimizer = get_optimizer()\n        train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)\n        lr_scheduler = get_scheduler(\n            optimizer, \n            num_training_steps=len(train_dataloader)*num_epochs\n        )\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n            model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n        )\n        train(model, optimizer, train_dataloader, lr_scheduler)\n        validate(model, eval_dataloader)\n+   inner_training_loop()\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "notes/Backprop.html",
    "href": "notes/Backprop.html",
    "title": "Backpropagation from scratch",
    "section": "",
    "text": "Rederive main backprop pieces after watching Karpathy’s https://www.youtube.com/watch?v=VMj-3S1tku0&ab_channel=AndrejKarpathy\n\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef f(x):\n  return 3*x**2 - 4*x + 5\n\n\nxs = np.arange(-5,5,0.25)\nys = f(xs)\nplt.plot(xs,ys)\n\n\n\n\n\n\n\n\n\nclass Value:\n  def __init__(self,data,_children=(),_op='',label=''):\n    self.data = data\n    self.grad = 0.0\n    self._backward = lambda: None # backward function does not do anything by default\n    self._prev = set(_children) #set for efficiency, children in the backpropagation sense\n    self._op = _op\n    self.label = label\n  def __repr__(self):\n    return f\"Value(data={self.data})\"\n  def __add__(self,other):\n    other = other if isinstance(other,Value) else Value(other)\n    out = Value(self.data+other.data,_children=(self,other),_op='+')\n    def _backward():\n      self.grad += 1.0*out.grad\n      other.grad += 1.0*out.grad\n    out._backward = _backward\n    return out\n  def __mul__(self,other):\n    other = other if isinstance(other,Value) else Value(other)\n    out = Value(self.data*other.data,_children=(self,other),_op='*')\n    def _backward():\n      # self.grad += other.grad*out.grad #HAD A BUG HERE COULD NOT PROPAGATE GRAD; FIXED BY INVESTIGATING PIECE-BY-PIECE AS SHOULD DO IN FUTURE!\n      # other.grad += self.grad*out.grad\n      self.grad += other.data*out.grad\n      other.grad += self.data*out.grad\n    out._backward = _backward\n    return out\n\n  def __pow__(self, other):\n    assert isinstance(other,(int,float))\n    out = Value(self.data**other,(self,),_op=f'**{other}')\n    def _backward():\n      self.grad += (other*self.data**(other-1)) *out.grad\n    out._backward = _backward\n    return out\n  def tanh(self):\n    x= self.data\n    t = (math.exp(2*x)-1)/(math.exp(2*x)+1)\n    out = Value(t,_children=(self,),_op='tanh')\n    def _backward():\n      self.grad += (1 - t**2)*out.grad\n    out._backward = _backward\n    return out\n\n  # exercise: ReLu\n  def relu(self):\n    out = Value( self.data if self.data &gt;=0 else 0, _children = (self,), _op='ReLU')\n    def _backward():\n      if self.data&gt;=0:\n        self.grad += out.grad\n      else:\n        self.grad = 0\n      # Karpathy summarized above as self.grad += (out.data &gt; 0) * out.grad\n    out._backward = _backward\n  def exp(self):\n    x= self.data\n    t = Value(math.exp(2*x), _children=(self,),_op='exp')\n    def _backward():\n      self.grad += t*t.grad\n    t._backward = _backward\n    return t\n\n  def __truediv__(self,other):\n    return self*other**-1\n\n  def backward(self):\n    topo = []\n    visited =set()\n    def build_topo(v):\n      if v not in visited:\n        visited.add(v)\n        for child in v._prev: # FIXED INDENTATION\n          build_topo(child)\n        topo.append(v) # add yourself to the list after your children get added to the list\n    build_topo(self)\n\n    self.grad = 1.0\n    for v in reversed(topo): # Propagating backward\n      v._backward()\n\n  def __neg__(self):\n    return -1*self\n\n  def __sub__(self,other):\n    return self + (-other)\n  #Python first check if self, other in __add__ can be added (self must be Value). If cannot, will flip self and other before passing them to __radd__\n  #Hence __radd__ already gets arguments in flipped order!!!\n  def __radd__(self,other):\n    return self+other\n  def __rsub__(self,other):\n      return self-other\n  def __rmul__(self,other):\n    return self*other\n\n\na = Value(2.0,label='a')\nb = Value(-3.0,label='b')\nc = Value(10.0,label='c')\ne=a*b; e.label = 'e'\nd=e+c; d.label = 'd'\nf = Value(-2.0, label='f')\nL = d*f; L.label ='Loss'; L.grad=1.0\nL.backward()\n\n\nd._prev\n\n{Value(data=-6.0), Value(data=10.0)}\n\n\n\nfrom graphviz import Digraph\n# ME: Note how Andrej used a closure instead of a helper, as I might\ndef trace(root):\n  # builds a set of all nodes and edges in a graph\n  nodes, edges = set(),set()\n  def build(v):\n    if v not in edges:\n      nodes.add(v)\n      for child in v._prev:\n        edges.add((child,v))\n        build(child)\n  build(root)\n  return nodes, edges\n\ndef draw_dot(root):\n  dot = Digraph(format='svg', graph_attr={'rankdir':'LR'})\n  nodes,edges = trace(root)\n  for n in nodes:\n    uid = str(id(n))\n    # for any value in the graph, create a rectangular ('record') node for it\n    dot.node(name=uid, label=\"{%s| data %.4f | grad %.4f}\" % (n.label,n.data,n.grad), shape='record')\n    if n._op:\n      # if this value is a result of some operation, create an op node for it\n      dot.node(name=uid+n._op, label=n._op)\n      # and connect this node for it\n      dot.edge(uid + n._op, uid)\n  for n1, n2 in edges:\n    # connect n1 to the op node of n2\n    dot.edge(str(id(n1)),str(id(n2))+n2._op)\n  return dot\n\n\ndraw_dot(L)\n\n\n\n\n\n\n\n\n\nL = d*f\n\n\nL.grad = 1.0\n\n\ndef locally_scope():\n  h=0.001\n  a = Value(2.0,label='a')\n  b = Value(-3.0,label='b')\n  c = Value(10.0,label='c')\n  e=a*b; e.label = 'e'\n  d=e+c; d.label = 'd'\n  f = Value(-2.0, label='f')\n  L = d*f; L.label ='Loss'\n  L1 = L.data\n\n  a = Value(2.0,label='a')\n  b = Value(-3.0,label='b')\n  c = Value(10.0,label='c')\n  e=a*b; e.label = 'e'\n  d=e+c; d.label = 'd'\n  f = Value(-2.0+h, label='f')\n  L = d*f; L.label ='Loss'\n  L2 = L.data\n  print((L2-L1)/h)\n\nlocally_scope()\n\n3.9999999999995595\n\n\n\n\n\nScreen Shot 2023-02-17 at 6.46.32 AM.png\n\n\n\nimport random\nclass Module:\n  def zero_grad(self):\n    for p in self.parameters():\n      p.grad = 0\n  def parameters(self):\n    return []\n\nclass Neuron(Module):\n  def __init__(self,nin): #nin: number of inputs/weights to the Neuron\n    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n    self.b = Value(random.uniform(-1,1))\n\n  def __call__(self,x):\n    #w*x + b\n    act = sum((wi*xi for wi,xi in zip(self.w,x)),self.b) #sum takes second argument as start of sum, so start at self.b\n    out = act.tanh()\n    return out\n\n  def parameters(self):\n    return self.w +[self.b]\nclass Layer(Module):\n  def __init__(self,nin,nout):\n    self.neurons = [Neuron(nin) for _ in range(nout)]\n\n  def __call__(self,x):\n    outs = [n(x) for n in self.neurons] # call each neuron on data\n    return outs[0] if len(outs) == 1 else outs\n  def parameters(self):\n    return [p for neuron in self.neurons for p in neuron.parameters()]\n\nclass MLP(Module):\n  def __init__(self,nin,nouts): # a list of nouts defining the sizes of all the layers in MLP\n    sz = [nin] + nouts # [3,4,4,1] in example from picture\n    self.layers = [Layer(sz[i],sz[i+1]) for i in range(len(nouts))] #sz is len(nouts) + 1!\n\n  def __call__(self,x):\n    for layer in self.layers:\n      x = layer(x) #going through and applying layer() to previous input\n      #print(x)\n    return x\n\n  def parameters(self):\n    return [p for layer in self.layers for p in layer.parameters()] #Notice how loops are ordered in same order as usual!!!\n\n\n\n\n\nx = [2.0, 3.0]\nn = Neuron(2)\nprint('n(x): ', n(x))\nl = Layer(2,3)\nprint('l(x): ', l(x))\n\nx = [2.0, 3.0, -1.0]\nm = MLP(3, [4,4,1])\nprint('m(x): ', m(x))\n\nn(x):  Value(data=-0.9830138790054755)\nl(x):  [Value(data=0.7705154949040606), Value(data=0.7657867069400349), Value(data=0.9990156183039722)]\nm(x):  Value(data=-0.3627764910030339)\n\n\n\ndraw_dot(m(x))\n\n\n\n\n\n\n\n\n\n\n\nnn_layer.jpg\n\n\n\nxs =[\n    [2.0, 3.0, -1.0],\n     [3.0,-1.0,0.5],\n     [0.5,1.0,1.0],\n     [1.0,1.0,-1.0]\n]\n\nys = [1.0, -1.0, -1.0, 1.0]\nypred = [m(x) for x in xs]\nypred\n\n[Value(data=-0.3627764910030339),\n Value(data=-0.37942407700946207),\n Value(data=-0.34471593548870183),\n Value(data=-0.2922952263934983)]\n\n\n\n#loss = sum([(yout-ygt)**2 for ygt,yout in zip(ys,ypred)])\nloss = sum([(ygt-yout)**2 for ygt,yout in zip(ys,ypred)])\n# ygt: y ground truth; order of subtraction important unless define __rsub__, as I did above\n# deleted Layer and reran class initialization for Neuron, etc, to make this change take effect\n\n\nloss\n\nValue(data=4.341698397987971)\n\n\n\nloss.grad=1.0\n\n\nloss.backward()\n\n\ndraw_dot(loss)\n\n\n\n\n\n\n\n\n\nm.layers[0].neurons[0].w[0].grad\n\n0.012225375750811174\n\n\nlen(m.parameters())\n\nfor p in m.parameters():\n  p.data += -0.01*p.grad\n\n\nm.layers[0].neurons[0].w[0].grad\n\n0.012225375750811174\n\n\n\nypred\n\n[Value(data=-0.3627764910030339),\n Value(data=-0.37942407700946207),\n Value(data=-0.34471593548870183),\n Value(data=-0.2922952263934983)]\n\n\n\n# Building a training loop\nxs =[\n    [2.0, 3.0, -1.0],\n     [3.0,-1.0,0.5],\n     [0.5,1.0,1.0],\n     [1.0,1.0,-1.0]\n]\nys = [1.0, -1.0, -1.0, 1.0]\n\n\n\nITERS = 100\nfor k in range(ITERS):\n  #forward pass\n  ypred = [m(x) for x in xs]\n  loss = sum((ygt-yout)**2 for ygt,yout in zip(ys,ypred))\n  if k == 0: loss.grad = 1.0\n\n  #backward pass\n  for p in m.parameters(): # Don't double accumulate; the backward pass is taking care of accumulation with += (counteracting overwriting derivs if same variable in multiple places)\n    p.grad = 0.0\n\n  loss.backward()\n\n  # update\n  for p in m.parameters(): # m.parameters!!! Had n.parameters. FOR FUTURE, KEEP SAME NOTATION AS PRESENTER TO SAVE MY TIME!!!\n    p.data += -0.05 * p.grad\n  #print(k,loss.data)\n\n\n#ypred\n\n\ndraw_dot(loss)"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Dmitriy’s Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nML Algorithms in Numpy and PyTorch\n\n\n\n\nLLMs/PyTorch Basics and Recipes\n\n\n\n\nLinux notes\n\n\n\n\nFastAPI notes\n\n\n\n\nDocker notes\n\n\n\n\nImplementing DBSCAN\n\n\n\n\nSpeeding up matrix multiplication ~ 5 million times\n\n\n\n\nBackpropagation from scratch\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/ImplementingDbscan.html",
    "href": "notes/ImplementingDbscan.html",
    "title": "Implementing DBSCAN",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nThis was inspired by a fastai Part 2 assignment by Jeremy Howard. The main idea is to practice implementing algorithms on a GPU for practice with broadcasting and other key techniques relevant to working on GPUs.\nThere are 4 Steps - CPU implementation - Slower simple GPU implementation - Somewhat faster simple GPU implementation - Much faster cuml GPU implementation with brief explanation of what makes it work - See what replacing only the distance calculation does in my slower GPU implementation"
  },
  {
    "objectID": "notes/ImplementingDbscan.html#dbscan",
    "href": "notes/ImplementingDbscan.html#dbscan",
    "title": "Implementing DBSCAN",
    "section": "DBSCAN",
    "text": "DBSCAN\n\nFirst, check look at the desired behavior using the default implementation\n\ndb = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n\ny_db = db.fit_predict(X)\nprint(y_db,len(y_db))\nprint(len(X[y_db == 0, 0]), len(X[y_db == 1, 1]))\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n[0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1\n 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1\n 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1\n 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1] 200\n100 100\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN custom implementation\n\nA point is a core point if at least a specified number min_samples of neighboring points fall within eps radius\nA border point is a point that has fewer than min_samples within eps, but lies within the radius eps of a core point\nAll other points that are neither core nor border points are noise points\n\nAlgorithm - Label the points as core, border, or noise. - Form a separate cluster for each core point or connected group of core points. (Core points are connected if they are no farther away than eps). - Assign each border point to the cluster of its corresponding core points.\n\nWant border point to have a reference to its core point for easy merging\nOnce merge cluster points, want to go find and update corresponding border points’ reference\n\n\nclass DbscanCustom: #(torch.nn.Module):\n    def __init__(self, eps, min_samples):\n        super().__init__()\n        self.eps = eps\n        self.min_samples = min_samples\n    def fit_predict(self, X):\n        # Label points as core or assign border to core, skip if noise\n        self.label_points(X)\n        # Form clusters of connected groups of core points\n        # Avoid RuntimeError: dictionary changed size during iteration,\n        # make a copy of the keys!!!\n        keys = list(self.mapping.keys())\n        #print(len(keys)+sum([len(v) for _,v in self.mapping.items()]))\n        merged = set()\n        for idx,i in enumerate(keys[:-1]): # iterating through a list now!\n          for j in keys[idx+1:]:\n            #if i == j: continue\n            if self.distance(i,j) &lt;= self.eps:\n              #print(self.mapping[i],self.mapping[j])\n              self.mapping[j].add(i) #IN-PLACE\n              self.mapping[j]=self.mapping[j].union(self.mapping[i]) #NOT IN-PLACE!!!!!!!!!!\n              print(f\"Deleting {self.mapping[i]} and i is {i} and self.mapping[j] is {self.mapping[j]}\")\n              del self.mapping[i]\n              merged.add(i)\n              break\n          #if i in merged: continue\n        # Relabel clusters\n        print(self.mapping)\n        final_map = {i:v for i, (k,v) in enumerate(self.mapping.items())}\n        len_pts = len(final_map)+sum([len(v) for _,v in final_map.items()])\n        print(final_map, \"\\n\", len_pts)\n        return final_map\n\n    def label_points(self, X):\n        \"\"\"1. Label the points as core, border, or noise.\"\"\"\n        self.mapping = {}\n        self.X  = X\n        self.get_core_points()\n        self.get_border_points()\n        # Ignore the noise points, don't need them later\n    def get_core_points(self):\n      for i,x in enumerate(self.X):\n            if self.is_core_point(i):\n                self.mapping[i] = set()\n\n    def get_border_points(self):\n       for i,x in enumerate(X):\n            if i not in self.mapping:\n                self.map_border(i)\n\n    def is_core_point(self, i):\n      # TODO: update to point to itself?\n        \"\"\"A point is a core point if at least a specified number min_samples of neighboring points fall within eps radius\"\"\"\n        num_within = 0\n        for j,x in enumerate(self.X): # TODO: Keys vs indices; straighten up!\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n              num_within += 1\n          if num_within &gt;= self.min_samples:\n            #print(X[i],X[j],self.distance(i,j),num_within,self.min_samples)\n            return True\n        return False\n    def map_border(self, i):\n      # TODO: update to point to point to its core idx\n        \"\"\"A *border point* is a point that has fewer than min_samples within eps, but lies within the radius eps of a core point\"\"\"\n        for j in self.mapping:\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n            self.mapping[j].add(i)\n            break\n\n\n    def distance(self,i,j):\n      \"\"\"Distance between points of self.X with indices i and j\"\"\"\n      #print(np.sqrt(np.sum((self.X[i]-self.X[j])**2)))\n      return np.sqrt(np.sum((self.X[i]-self.X[j])**2))\n\n\nclass DbscanCustom: #(torch.nn.Module):\n    def __init__(self, eps, min_samples):\n        super().__init__()\n        self.eps = eps\n        self.min_samples = min_samples\n    def fit_predict(self, X):\n        # Label points as core or assign border to core, skip if noise\n        self.label_points(X)\n        # Form clusters of connected groups of core points\n        # Avoid RuntimeError: dictionary changed size during iteration,\n        # make a copy of the keys!!!\n        keys = list(self.mapping.keys())\n        #print(keys)\n        #print(len(keys)+sum([len(v) for _,v in self.mapping.items()]))\n        merged = set()\n        for idx,i in enumerate(keys[:-1]): # iterating through a list now!\n          for j in keys[idx+1:]:\n            #if i == j: continue\n            if self.distance(i,j) &lt;= self.eps:\n              #print(self.mapping[i],self.mapping[j])\n              self.mapping[j].add(i) #IN-PLACE\n              self.mapping[j]=self.mapping[j].union(self.mapping[i]) #NOT IN-PLACE!!!!!!!!!!\n              merged.add(i)\n          if i in merged:\n            #print(f\"Deleting {self.mapping[i]} and i is {i} and self.mapping[j] is {self.mapping[j]}\")\n            del self.mapping[i]\n            #break\n          #if i in merged: continue\n        # Relabel clusters\n        print(self.mapping)\n        final_map = {i:v for i, (k,v) in enumerate(self.mapping.items())}\n        len_pts = len(final_map)+sum([len(v) for _,v in final_map.items()])\n        print(final_map, \"\\n\", len_pts)\n        return final_map\n\n    def label_points(self, X):\n        \"\"\"1. Label the points as core, border, or noise.\"\"\"\n        self.mapping = {}\n        self.X  = X\n        self.get_core_points()\n        self.get_border_points()\n        # Ignore the noise points, don't need them later\n    def get_core_points(self):\n      for i,x in enumerate(self.X):\n            if self.is_core_point(i):\n                self.mapping[i] = set()\n\n    def get_border_points(self):\n       for i,x in enumerate(X):\n            if i not in self.mapping:\n                self.map_border(i)\n\n    def is_core_point(self, i):\n      # TODO: update to point to itself?\n        \"\"\"A point is a core point if at least a specified number min_samples of neighboring points fall within eps radius\"\"\"\n        num_within = 0\n        for j,x in enumerate(self.X): # TODO: Keys vs indices; straighten up!\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n              num_within += 1\n          if num_within &gt;= self.min_samples:\n            #print(X[i],X[j],self.distance(i,j),num_within,self.min_samples)\n            return True\n        return False\n    def map_border(self, i):\n      # TODO: update to point to point to its core idx\n        \"\"\"A *border point* is a point that has fewer than min_samples within eps, but lies within the radius eps of a core point\"\"\"\n        for j in self.mapping:\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n            self.mapping[j].add(i)\n            break\n\n\n    def distance(self,i,j):\n      \"\"\"Distance between points of self.X with indices i and j\"\"\"\n      #print(np.sqrt(np.sum((self.X[i]-self.X[j])**2)))\n      #print(np.sqrt(np.sum((self.X[20]-self.X[199])**2)))\n      return np.sqrt(np.sum((self.X[i]-self.X[j])**2))\n\n\nNext, handle border points\nI think the three remaining points are probably border points and need to be mapped to their respective cluster!!!!\n\n\nfrom collections import Counter\nclass DbscanCustom: #(torch.nn.Module):\n    def __init__(self, eps, min_samples):\n        super().__init__()\n        self.eps = eps\n        self.min_samples = min_samples\n        self.edges = [] # for union find\n\n    def find(self,i):\n      res = i\n      while self.parents[res]!=res:\n        self.parents[res] = self.parents[self.parents[res]]\n        res = self.parents[res]\n      return res\n\n    def union(self, i,j):\n      pi, pj = self.find(i), self.find(j)\n      if pi == pj: return 0, None\n      if self.rank[pi] &gt;= self.rank[pj]:\n        self.parents[pj] = pi\n        self.rank[pi] += self.rank[pj]\n        return 1, i\n      else:\n        self.parents[pi] = pj\n        self.rank[pj] += self.rank[pi]\n        return 1, j\n\n\n\n    def fit_predict(self, X):\n        self.parents = list(range(len(X)))\n        self.rank = [1]*len(X)\n        self.mapping = {}\n        self.X = X\n        # Label points as core or assign border to core, skip if noise\n        self.label_points(X)\n        keys = list(self.mapping.keys())\n        n = len(keys)\n        for i, j in self.edges:\n          if i in self.mapping and j in self.mapping:\n            res,p = self.union(i,j) #0 or 1, idx\n            #if res: n -= 1\n            if res and p == i:\n              self.mapping[i].update(self.mapping[j])\n              #del self.mapping[j]\n            elif res and p == j:\n              self.mapping[j].update(self.mapping[i])\n              #del self.mapping[i]\n        print(\"SELF.MAPPING\")\n        print(self.mapping)\n        print(\"PARENTS\")\n        print(self.parents)\n        print(\"Parents len\")\n        print(len(self.parents))\n        print(\"CLUSTERS\")\n        print(set(self.parents))\n        print(\"RANKS\")\n        print(self.rank)\n        print(self.parents[3], self.parents[73], self.parents[103])\n        final_map = {}\n        counts = Counter(self.parents)\n        #cluster_rename = {i:v for i,(k,v) in enumerate(counts.items()) if counts[k]&gt;1}\n        # print(\"CLUSTER RENAME\")\n        # print(cluster_rename)\n        result = [np.nan]*len(X)\n        all_vals = self.mapping.values()\n        for idx,elt in enumerate(self.parents):\n          if elt not in final_map:\n              final_map[elt] = set()\n          if counts[elt]&gt; 1:\n            final_map[elt].add(idx)\n\n        v_to_k = {v1:k for k,v in self.mapping.items() for v1 in v if v}\n        print(\"v_to_k\")\n        print(v_to_k)\n        print(final_map)\n        print(len(final_map))\n        final_map = {i:v for i, (k,v) in enumerate(final_map.items())}\n        print(final_map)\n\n        for i,v in final_map.items():\n          for j in v:\n            result[j] = i\n        for v,k in v_to_k.items():\n          result[v]  = result[k]\n        print(result)\n        return np.array(result)\n\n        # final_map = {i:v for i, (k,v) in enumerate(self.mapping.items())}\n        # len_pts = len(final_map)+sum([len(v) for _,v in final_map.items()])\n        # print(final_map, \"\\n\", len_pts)\n        #return final_map\n\n    def label_points(self, X):\n        \"\"\"1. Label the points as core, border, or noise.\"\"\"\n        self.mapping = {}\n        self.X  = X\n        self.get_core_points()\n        self.get_border_points()\n        print(\"EDGES:\")\n        print(self.edges)\n        # Ignore the noise points, don't need them later\n    def get_core_points(self):\n      for i,x in enumerate(self.X):\n            if self.map_core(i):\n                self.mapping[i] = set()\n\n    def get_border_points(self):\n       for i,x in enumerate(X):\n            if i not in self.mapping:\n                self.map_border(i)\n\n    def map_core(self, i):\n      # TODO: update to point to itself?\n        \"\"\"A point is a core point if at least a specified number min_samples of neighboring points fall within eps radius\"\"\"\n        num_within = 0\n        for j,x in enumerate(self.X): # TODO: Keys vs indices; straighten up!\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n              self.edges.append([i,j])\n              num_within += 1\n        if num_within &gt;= self.min_samples:\n          return True\n        return False\n    def map_border(self, i):\n      # TODO: update to point to point to its core idx\n        \"\"\"A *border point* is a point that has fewer than min_samples within eps, but lies within the radius eps of a core point\"\"\"\n        min_dist = self.eps\n        min_idx = None\n        for j in self.mapping:\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n            if self.distance(i,j) &lt;= min_dist:\n              min_dist = self.distance(i,j)\n              min_idx = j\n        if min_idx: self.mapping[min_idx].add(i)\n\n\n    def distance(self,i,j):\n      \"\"\"Distance between points of self.X with indices i and j\"\"\"\n      #print(np.sqrt(np.sum((self.X[i]-self.X[j])**2)))\n      return np.sqrt(np.sum((self.X[i]-self.X[j])**2))\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\ndb_custom = DbscanCustom(eps=0.2, min_samples=5)\ny_db = db_custom.fit_predict(X)\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nEDGES:\n[[0, 8], [0, 30], [0, 34], [0, 55], [0, 63], [0, 77], [0, 114], [0, 119], [0, 120], [0, 139], [0, 152], [0, 192], [1, 44], [1, 67], [1, 96], [1, 108], [1, 151], [1, 163], [1, 168], [1, 180], [1, 183], [1, 199], [2, 33], [2, 39], [2, 76], [2, 84], [2, 105], [2, 122], [2, 124], [2, 141], [2, 149], [2, 194], [3, 53], [3, 106], [3, 171], [4, 5], [4, 12], [4, 26], [4, 62], [4, 95], [4, 96], [4, 135], [4, 151], [4, 165], [4, 183], [4, 199], [5, 4], [5, 12], [5, 20], [5, 26], [5, 27], [5, 52], [5, 62], [5, 80], [5, 95], [5, 117], [6, 8], [6, 15], [6, 37], [6, 46], [6, 86], [6, 91], [6, 114], [6, 126], [6, 130], [6, 139], [6, 145], [6, 193], [7, 21], [7, 32], [7, 49], [7, 54], [7, 74], [7, 79], [7, 167], [7, 169], [7, 175], [7, 187], [8, 0], [8, 6], [8, 15], [8, 30], [8, 63], [8, 77], [8, 91], [8, 114], [8, 120], [8, 130], [8, 139], [8, 152], [8, 192], [8, 193], [9, 13], [9, 32], [9, 54], [9, 57], [9, 64], [9, 100], [9, 134], [9, 136], [9, 156], [9, 161], [9, 169], [9, 175], [10, 31], [10, 41], [10, 45], [10, 85], [10, 87], [10, 107], [10, 131], [10, 143], [10, 144], [10, 195], [11, 17], [11, 25], [11, 28], [11, 36], [11, 48], [11, 51], [11, 60], [11, 97], [11, 133], [11, 157], [11, 164], [12, 4], [12, 5], [12, 20], [12, 26], [12, 27], [12, 62], [12, 95], [12, 96], [12, 165], [12, 183], [13, 9], [13, 23], [13, 54], [13, 57], [13, 64], [13, 100], [13, 128], [13, 134], [13, 136], [13, 147], [13, 156], [13, 161], [14, 19], [14, 38], [14, 47], [14, 93], [14, 121], [14, 125], [14, 172], [14, 173], [14, 177], [14, 178], [14, 181], [15, 6], [15, 8], [15, 37], [15, 46], [15, 86], [15, 91], [15, 114], [15, 126], [15, 130], [15, 145], [15, 193], [16, 18], [16, 56], [16, 78], [16, 142], [16, 153], [16, 158], [16, 159], [16, 162], [16, 166], [16, 170], [16, 179], [16, 191], [17, 11], [17, 36], [17, 51], [17, 60], [17, 97], [17, 133], [17, 150], [17, 157], [17, 164], [18, 16], [18, 22], [18, 58], [18, 90], [18, 109], [18, 142], [18, 148], [18, 153], [18, 158], [18, 166], [18, 185], [18, 191], [19, 14], [19, 47], [19, 50], [19, 93], [19, 110], [19, 125], [19, 177], [19, 189], [20, 5], [20, 12], [20, 26], [20, 27], [20, 52], [20, 62], [20, 80], [20, 95], [20, 117], [20, 182], [21, 7], [21, 32], [21, 49], [21, 54], [21, 74], [21, 79], [21, 167], [21, 169], [21, 175], [21, 187], [22, 18], [22, 29], [22, 58], [22, 68], [22, 90], [22, 102], [22, 109], [22, 111], [22, 148], [22, 153], [22, 185], [22, 198], [23, 13], [23, 57], [23, 67], [23, 115], [23, 128], [23, 134], [23, 136], [23, 147], [23, 156], [23, 163], [23, 180], [24, 31], [24, 45], [24, 59], [24, 66], [24, 85], [24, 87], [24, 143], [24, 155], [24, 186], [25, 11], [25, 28], [25, 43], [25, 48], [25, 51], [25, 127], [25, 133], [25, 140], [25, 146], [25, 157], [25, 160], [25, 174], [25, 197], [26, 4], [26, 5], [26, 12], [26, 20], [26, 27], [26, 52], [26, 62], [26, 95], [26, 96], [26, 165], [26, 183], [27, 5], [27, 12], [27, 20], [27, 26], [27, 35], [27, 52], [27, 62], [27, 80], [27, 95], [27, 117], [27, 182], [27, 196], [28, 11], [28, 25], [28, 43], [28, 48], [28, 51], [28, 99], [28, 127], [28, 133], [28, 140], [28, 146], [28, 157], [28, 160], [28, 174], [28, 197], [29, 22], [29, 58], [29, 68], [29, 81], [29, 90], [29, 109], [29, 111], [29, 123], [29, 148], [29, 198], [30, 0], [30, 8], [30, 34], [30, 55], [30, 63], [30, 77], [30, 114], [30, 119], [30, 120], [30, 139], [30, 152], [30, 192], [31, 10], [31, 24], [31, 45], [31, 59], [31, 66], [31, 87], [31, 143], [31, 155], [31, 186], [32, 7], [32, 9], [32, 21], [32, 49], [32, 54], [32, 64], [32, 74], [32, 136], [32, 161], [32, 169], [32, 175], [33, 2], [33, 39], [33, 43], [33, 76], [33, 84], [33, 99], [33, 105], [33, 122], [33, 149], [33, 160], [33, 174], [33, 194], [34, 0], [34, 30], [34, 42], [34, 55], [34, 63], [34, 78], [34, 116], [34, 119], [34, 120], [34, 159], [34, 179], [34, 192], [35, 27], [35, 52], [35, 80], [35, 89], [35, 92], [35, 94], [35, 101], [35, 117], [35, 182], [35, 184], [35, 196], [36, 11], [36, 17], [36, 51], [36, 60], [36, 97], [36, 150], [36, 154], [36, 157], [36, 164], [37, 6], [37, 15], [37, 46], [37, 86], [37, 103], [37, 126], [37, 130], [37, 145], [37, 193], [38, 14], [38, 47], [38, 69], [38, 71], [38, 75], [38, 113], [38, 121], [38, 125], [38, 132], [38, 172], [38, 173], [38, 178], [38, 181], [38, 188], [38, 190], [39, 2], [39, 33], [39, 73], [39, 76], [39, 84], [39, 105], [39, 122], [39, 124], [39, 141], [39, 149], [39, 194], [40, 70], [40, 72], [40, 79], [40, 82], [40, 88], [40, 118], [40, 150], [40, 154], [40, 164], [40, 167], [41, 10], [41, 45], [41, 85], [41, 87], [41, 107], [41, 110], [41, 131], [41, 143], [41, 144], [41, 195], [42, 34], [42, 55], [42, 56], [42, 78], [42, 116], [42, 159], [42, 162], [42, 170], [42, 179], [43, 25], [43, 28], [43, 33], [43, 48], [43, 76], [43, 84], [43, 99], [43, 127], [43, 140], [43, 146], [43, 160], [43, 174], [43, 197], [44, 1], [44, 67], [44, 108], [44, 115], [44, 135], [44, 151], [44, 163], [44, 168], [44, 180], [44, 199], [45, 10], [45, 24], [45, 31], [45, 41], [45, 59], [45, 85], [45, 87], [45, 107], [45, 143], [45, 144], [45, 155], [45, 195], [46, 6], [46, 15], [46, 37], [46, 86], [46, 91], [46, 126], [46, 130], [46, 145], [46, 193], [47, 14], [47, 19], [47, 38], [47, 50], [47, 93], [47, 121], [47, 125], [47, 173], [47, 177], [47, 178], [47, 189], [48, 11], [48, 25], [48, 28], [48, 43], [48, 51], [48, 127], [48, 133], [48, 140], [48, 146], [48, 157], [48, 160], [48, 174], [48, 197], [49, 7], [49, 21], [49, 32], [49, 54], [49, 74], [49, 79], [49, 167], [49, 169], [49, 175], [49, 187], [50, 19], [50, 47], [50, 93], [50, 110], [50, 125], [50, 131], [50, 177], [50, 189], [50, 195], [51, 11], [51, 17], [51, 25], [51, 28], [51, 36], [51, 48], [51, 60], [51, 97], [51, 133], [51, 157], [51, 164], [52, 5], [52, 20], [52, 26], [52, 27], [52, 35], [52, 80], [52, 117], [52, 182], [53, 3], [53, 61], [53, 106], [53, 137], [53, 171], [54, 7], [54, 9], [54, 13], [54, 21], [54, 32], [54, 49], [54, 57], [54, 64], [54, 134], [54, 136], [54, 161], [54, 169], [54, 175], [55, 0], [55, 30], [55, 34], [55, 42], [55, 63], [55, 78], [55, 116], [55, 119], [55, 120], [55, 159], [55, 179], [55, 192], [56, 16], [56, 42], [56, 78], [56, 116], [56, 159], [56, 162], [56, 166], [56, 170], [56, 179], [57, 9], [57, 13], [57, 23], [57, 54], [57, 64], [57, 100], [57, 115], [57, 128], [57, 134], [57, 136], [57, 147], [57, 156], [57, 161], [58, 18], [58, 22], [58, 29], [58, 90], [58, 109], [58, 142], [58, 148], [58, 153], [58, 185], [58, 191], [58, 198], [59, 24], [59, 31], [59, 45], [59, 66], [59, 81], [59, 85], [59, 87], [59, 123], [59, 143], [59, 155], [59, 186], [60, 11], [60, 17], [60, 36], [60, 51], [60, 97], [60, 133], [60, 150], [60, 157], [60, 164], [61, 53], [61, 65], [61, 98], [61, 104], [61, 106], [61, 137], [61, 171], [62, 4], [62, 5], [62, 12], [62, 20], [62, 26], [62, 27], [62, 95], [62, 96], [62, 135], [62, 165], [62, 183], [62, 199], [63, 0], [63, 8], [63, 30], [63, 34], [63, 55], [63, 77], [63, 114], [63, 119], [63, 120], [63, 139], [63, 152], [63, 192], [64, 9], [64, 13], [64, 32], [64, 54], [64, 57], [64, 134], [64, 136], [64, 156], [64, 161], [64, 169], [64, 175], [65, 61], [65, 71], [65, 75], [65, 98], [65, 104], [65, 106], [65, 137], [66, 24], [66, 31], [66, 59], [66, 68], [66, 81], [66, 87], [66, 111], [66, 112], [66, 123], [66, 155], [66, 186], [67, 1], [67, 23], [67, 44], [67, 108], [67, 115], [67, 128], [67, 147], [67, 163], [67, 168], [67, 180], [68, 22], [68, 29], [68, 66], [68, 81], [68, 102], [68, 111], [68, 112], [68, 123], [68, 198], [69, 38], [69, 71], [69, 75], [69, 104], [69, 113], [69, 132], [69, 172], [69, 173], [69, 181], [69, 188], [69, 190], [70, 40], [70, 72], [70, 74], [70, 79], [70, 82], [70, 88], [70, 118], [70, 154], [70, 167], [70, 187], [71, 38], [71, 65], [71, 69], [71, 75], [71, 98], [71, 104], [71, 113], [71, 132], [71, 137], [71, 172], [71, 181], [71, 188], [71, 190], [72, 40], [72, 70], [72, 82], [72, 88], [72, 118], [72, 150], [72, 154], [72, 164], [73, 39], [73, 124], [73, 141], [73, 194], [74, 7], [74, 21], [74, 32], [74, 49], [74, 70], [74, 79], [74, 82], [74, 167], [74, 169], [74, 175], [74, 187], [75, 38], [75, 65], [75, 69], [75, 71], [75, 98], [75, 104], [75, 113], [75, 132], [75, 137], [75, 172], [75, 181], [75, 188], [75, 190], [76, 2], [76, 33], [76, 39], [76, 43], [76, 84], [76, 99], [76, 105], [76, 122], [76, 127], [76, 149], [76, 160], [76, 174], [77, 0], [77, 8], [77, 30], [77, 63], [77, 91], [77, 114], [77, 120], [77, 130], [77, 139], [77, 152], [77, 192], [77, 193], [78, 16], [78, 34], [78, 42], [78, 55], [78, 56], [78, 116], [78, 158], [78, 159], [78, 162], [78, 166], [78, 170], [78, 179], [79, 7], [79, 21], [79, 40], [79, 49], [79, 70], [79, 74], [79, 82], [79, 88], [79, 118], [79, 154], [79, 167], [79, 187], [80, 5], [80, 20], [80, 27], [80, 35], [80, 52], [80, 92], [80, 101], [80, 117], [80, 182], [80, 184], [80, 196], [81, 29], [81, 59], [81, 66], [81, 68], [81, 102], [81, 111], [81, 112], [81, 123], [81, 155], [81, 186], [81, 198], [82, 40], [82, 70], [82, 72], [82, 74], [82, 79], [82, 88], [82, 118], [82, 154], [82, 167], [82, 187], [83, 89], [83, 92], [83, 94], [83, 101], [83, 129], [83, 138], [83, 176], [83, 184], [84, 2], [84, 33], [84, 39], [84, 43], [84, 76], [84, 99], [84, 105], [84, 122], [84, 149], [84, 194], [85, 10], [85, 24], [85, 41], [85, 45], [85, 59], [85, 87], [85, 107], [85, 143], [85, 144], [85, 155], [86, 6], [86, 15], [86, 37], [86, 46], [86, 103], [86, 126], [86, 145], [87, 10], [87, 24], [87, 31], [87, 41], [87, 45], [87, 59], [87, 66], [87, 85], [87, 107], [87, 143], [87, 144], [87, 155], [87, 186], [88, 40], [88, 70], [88, 72], [88, 79], [88, 82], [88, 118], [88, 150], [88, 154], [88, 167], [89, 35], [89, 83], [89, 92], [89, 94], [89, 101], [89, 129], [89, 138], [89, 176], [89, 184], [89, 196], [90, 18], [90, 22], [90, 29], [90, 58], [90, 109], [90, 142], [90, 148], [90, 153], [90, 158], [90, 166], [90, 185], [90, 191], [91, 6], [91, 8], [91, 15], [91, 46], [91, 77], [91, 114], [91, 130], [91, 139], [91, 152], [91, 193], [92, 35], [92, 80], [92, 83], [92, 89], [92, 94], [92, 101], [92, 117], [92, 182], [92, 184], [92, 196], [93, 14], [93, 19], [93, 47], [93, 50], [93, 110], [93, 121], [93, 125], [93, 131], [93, 177], [93, 178], [93, 189], [93, 195], [94, 35], [94, 83], [94, 89], [94, 92], [94, 101], [94, 129], [94, 138], [94, 176], [94, 182], [94, 184], [94, 196], [95, 4], [95, 5], [95, 12], [95, 20], [95, 26], [95, 27], [95, 62], [95, 96], [95, 135], [95, 165], [95, 183], [95, 199], [96, 1], [96, 4], [96, 12], [96, 26], [96, 62], [96, 95], [96, 135], [96, 151], [96, 165], [96, 183], [96, 199], [97, 11], [97, 17], [97, 36], [97, 51], [97, 60], [97, 133], [97, 150], [97, 157], [97, 164], [98, 61], [98, 65], [98, 71], [98, 75], [98, 104], [98, 106], [98, 137], [98, 188], [98, 190], [99, 28], [99, 33], [99, 43], [99, 76], [99, 84], [99, 127], [99, 140], [99, 146], [99, 149], [99, 160], [99, 174], [99, 197], [100, 9], [100, 13], [100, 57], [100, 115], [100, 128], [100, 134], [100, 147], [100, 156], [101, 35], [101, 80], [101, 83], [101, 89], [101, 92], [101, 94], [101, 117], [101, 138], [101, 176], [101, 182], [101, 184], [101, 196], [102, 22], [102, 68], [102, 81], [102, 111], [102, 112], [102, 123], [102, 198], [103, 37], [103, 86], [103, 126], [103, 145], [104, 61], [104, 65], [104, 69], [104, 71], [104, 75], [104, 98], [104, 113], [104, 132], [104, 137], [104, 188], [104, 190], [105, 2], [105, 33], [105, 39], [105, 76], [105, 84], [105, 122], [105, 124], [105, 149], [105, 194], [106, 3], [106, 53], [106, 61], [106, 65], [106, 98], [106, 137], [106, 171], [107, 10], [107, 41], [107, 45], [107, 85], [107, 87], [107, 110], [107, 143], [107, 144], [107, 195], [108, 1], [108, 44], [108, 67], [108, 115], [108, 135], [108, 151], [108, 163], [108, 168], [108, 180], [108, 199], [109, 18], [109, 22], [109, 29], [109, 58], [109, 90], [109, 148], [109, 153], [109, 185], [109, 191], [109, 198], [110, 19], [110, 41], [110, 50], [110, 93], [110, 107], [110, 131], [110, 144], [110, 177], [110, 189], [110, 195], [111, 22], [111, 29], [111, 66], [111, 68], [111, 81], [111, 102], [111, 112], [111, 123], [111, 198], [112, 66], [112, 68], [112, 81], [112, 102], [112, 111], [112, 123], [112, 155], [112, 198], [113, 38], [113, 69], [113, 71], [113, 75], [113, 104], [113, 132], [113, 172], [113, 173], [113, 181], [113, 188], [113, 190], [114, 0], [114, 6], [114, 8], [114, 15], [114, 30], [114, 63], [114, 77], [114, 91], [114, 120], [114, 130], [114, 139], [114, 152], [114, 192], [114, 193], [115, 23], [115, 44], [115, 57], [115, 67], [115, 100], [115, 108], [115, 128], [115, 147], [115, 156], [115, 163], [115, 180], [116, 34], [116, 42], [116, 55], [116, 56], [116, 78], [116, 159], [116, 162], [116, 170], [116, 179], [117, 5], [117, 20], [117, 27], [117, 35], [117, 52], [117, 80], [117, 92], [117, 101], [117, 182], [117, 184], [117, 196], [118, 40], [118, 70], [118, 72], [118, 79], [118, 82], [118, 88], [118, 150], [118, 154], [118, 164], [118, 167], [119, 0], [119, 30], [119, 34], [119, 55], [119, 63], [119, 120], [119, 139], [119, 152], [119, 192], [120, 0], [120, 8], [120, 30], [120, 34], [120, 55], [120, 63], [120, 77], [120, 114], [120, 119], [120, 139], [120, 152], [120, 192], [121, 14], [121, 38], [121, 47], [121, 93], [121, 125], [121, 173], [121, 177], [121, 178], [121, 181], [122, 2], [122, 33], [122, 39], [122, 76], [122, 84], [122, 105], [122, 124], [122, 141], [122, 149], [122, 194], [123, 29], [123, 59], [123, 66], [123, 68], [123, 81], [123, 102], [123, 111], [123, 112], [123, 155], [123, 186], [123, 198], [124, 2], [124, 39], [124, 73], [124, 105], [124, 122], [124, 141], [124, 149], [124, 194], [125, 14], [125, 19], [125, 38], [125, 47], [125, 50], [125, 93], [125, 121], [125, 172], [125, 173], [125, 177], [125, 178], [125, 181], [125, 189], [126, 6], [126, 15], [126, 37], [126, 46], [126, 86], [126, 103], [126, 145], [127, 25], [127, 28], [127, 43], [127, 48], [127, 76], [127, 99], [127, 140], [127, 146], [127, 160], [127, 174], [127, 197], [128, 13], [128, 23], [128, 57], [128, 67], [128, 100], [128, 115], [128, 134], [128, 136], [128, 147], [128, 156], [128, 163], [128, 180], [129, 83], [129, 89], [129, 94], [129, 138], [129, 176], [130, 6], [130, 8], [130, 15], [130, 37], [130, 46], [130, 77], [130, 91], [130, 114], [130, 139], [130, 193], [131, 10], [131, 41], [131, 50], [131, 93], [131, 110], [131, 144], [131, 177], [131, 189], [131, 195], [132, 38], [132, 69], [132, 71], [132, 75], [132, 104], [132, 113], [132, 172], [132, 181], [132, 188], [132, 190], [133, 11], [133, 17], [133, 25], [133, 28], [133, 48], [133, 51], [133, 60], [133, 97], [133, 140], [133, 146], [133, 157], [133, 197], [134, 9], [134, 13], [134, 23], [134, 54], [134, 57], [134, 64], [134, 100], [134, 128], [134, 136], [134, 147], [134, 156], [134, 161], [135, 4], [135, 44], [135, 62], [135, 95], [135, 96], [135, 108], [135, 151], [135, 163], [135, 165], [135, 168], [135, 199], [136, 9], [136, 13], [136, 23], [136, 32], [136, 54], [136, 57], [136, 64], [136, 128], [136, 134], [136, 147], [136, 156], [136, 161], [137, 53], [137, 61], [137, 65], [137, 71], [137, 75], [137, 98], [137, 104], [137, 106], [137, 171], [137, 188], [137, 190], [138, 83], [138, 89], [138, 94], [138, 101], [138, 129], [138, 176], [139, 0], [139, 6], [139, 8], [139, 30], [139, 63], [139, 77], [139, 91], [139, 114], [139, 119], [139, 120], [139, 130], [139, 152], [139, 192], [139, 193], [140, 25], [140, 28], [140, 43], [140, 48], [140, 99], [140, 127], [140, 133], [140, 146], [140, 160], [140, 174], [140, 197], [141, 2], [141, 39], [141, 73], [141, 122], [141, 124], [141, 194], [142, 16], [142, 18], [142, 58], [142, 90], [142, 153], [142, 158], [142, 162], [142, 166], [142, 170], [142, 185], [142, 191], [143, 10], [143, 24], [143, 31], [143, 41], [143, 45], [143, 59], [143, 85], [143, 87], [143, 107], [143, 144], [143, 155], [144, 10], [144, 41], [144, 45], [144, 85], [144, 87], [144, 107], [144, 110], [144, 131], [144, 143], [144, 195], [145, 6], [145, 15], [145, 37], [145, 46], [145, 86], [145, 103], [145, 126], [146, 25], [146, 28], [146, 43], [146, 48], [146, 99], [146, 127], [146, 133], [146, 140], [146, 160], [146, 174], [146, 197], [147, 13], [147, 23], [147, 57], [147, 67], [147, 100], [147, 115], [147, 128], [147, 134], [147, 136], [147, 156], [147, 163], [147, 180], [148, 18], [148, 22], [148, 29], [148, 58], [148, 90], [148, 109], [148, 153], [148, 185], [148, 191], [148, 198], [149, 2], [149, 33], [149, 39], [149, 76], [149, 84], [149, 99], [149, 105], [149, 122], [149, 124], [149, 194], [150, 17], [150, 36], [150, 40], [150, 60], [150, 72], [150, 88], [150, 97], [150, 118], [150, 154], [150, 164], [151, 1], [151, 4], [151, 44], [151, 96], [151, 108], [151, 135], [151, 163], [151, 168], [151, 183], [151, 199], [152, 0], [152, 8], [152, 30], [152, 63], [152, 77], [152, 91], [152, 114], [152, 119], [152, 120], [152, 139], [152, 192], [153, 16], [153, 18], [153, 22], [153, 58], [153, 90], [153, 109], [153, 142], [153, 148], [153, 158], [153, 166], [153, 185], [153, 191], [154, 36], [154, 40], [154, 70], [154, 72], [154, 79], [154, 82], [154, 88], [154, 118], [154, 150], [154, 164], [154, 167], [155, 24], [155, 31], [155, 45], [155, 59], [155, 66], [155, 81], [155, 85], [155, 87], [155, 112], [155, 123], [155, 143], [155, 186], [156, 9], [156, 13], [156, 23], [156, 57], [156, 64], [156, 100], [156, 115], [156, 128], [156, 134], [156, 136], [156, 147], [156, 161], [157, 11], [157, 17], [157, 25], [157, 28], [157, 36], [157, 48], [157, 51], [157, 60], [157, 97], [157, 133], [157, 164], [158, 16], [158, 18], [158, 78], [158, 90], [158, 142], [158, 153], [158, 162], [158, 166], [158, 170], [158, 185], [158, 191], [159, 16], [159, 34], [159, 42], [159, 55], [159, 56], [159, 78], [159, 116], [159, 162], [159, 166], [159, 170], [159, 179], [160, 25], [160, 28], [160, 33], [160, 43], [160, 48], [160, 76], [160, 99], [160, 127], [160, 140], [160, 146], [160, 174], [160, 197], [161, 9], [161, 13], [161, 32], [161, 54], [161, 57], [161, 64], [161, 134], [161, 136], [161, 156], [161, 169], [161, 175], [162, 16], [162, 42], [162, 56], [162, 78], [162, 116], [162, 142], [162, 158], [162, 159], [162, 166], [162, 170], [162, 179], [163, 1], [163, 23], [163, 44], [163, 67], [163, 108], [163, 115], [163, 128], [163, 135], [163, 147], [163, 151], [163, 168], [163, 180], [163, 199], [164, 11], [164, 17], [164, 36], [164, 40], [164, 51], [164, 60], [164, 72], [164, 97], [164, 118], [164, 150], [164, 154], [164, 157], [165, 4], [165, 12], [165, 26], [165, 62], [165, 95], [165, 96], [165, 135], [165, 199], [166, 16], [166, 18], [166, 56], [166, 78], [166, 90], [166, 142], [166, 153], [166, 158], [166, 159], [166, 162], [166, 170], [166, 179], [166, 185], [166, 191], [167, 7], [167, 21], [167, 40], [167, 49], [167, 70], [167, 74], [167, 79], [167, 82], [167, 88], [167, 118], [167, 154], [167, 187], [168, 1], [168, 44], [168, 67], [168, 108], [168, 135], [168, 151], [168, 163], [168, 180], [168, 199], [169, 7], [169, 9], [169, 21], [169, 32], [169, 49], [169, 54], [169, 64], [169, 74], [169, 161], [169, 175], [169, 187], [170, 16], [170, 42], [170, 56], [170, 78], [170, 116], [170, 142], [170, 158], [170, 159], [170, 162], [170, 166], [170, 179], [171, 3], [171, 53], [171, 61], [171, 106], [171, 137], [172, 14], [172, 38], [172, 69], [172, 71], [172, 75], [172, 113], [172, 125], [172, 132], [172, 173], [172, 178], [172, 181], [172, 188], [172, 190], [173, 14], [173, 38], [173, 47], [173, 69], [173, 113], [173, 121], [173, 125], [173, 172], [173, 177], [173, 178], [173, 181], [174, 25], [174, 28], [174, 33], [174, 43], [174, 48], [174, 76], [174, 99], [174, 127], [174, 140], [174, 146], [174, 160], [174, 197], [175, 7], [175, 9], [175, 21], [175, 32], [175, 49], [175, 54], [175, 64], [175, 74], [175, 161], [175, 169], [175, 187], [176, 83], [176, 89], [176, 94], [176, 101], [176, 129], [176, 138], [176, 184], [176, 196], [177, 14], [177, 19], [177, 47], [177, 50], [177, 93], [177, 110], [177, 121], [177, 125], [177, 131], [177, 173], [177, 178], [177, 189], [178, 14], [178, 38], [178, 47], [178, 93], [178, 121], [178, 125], [178, 172], [178, 173], [178, 177], [178, 181], [179, 16], [179, 34], [179, 42], [179, 55], [179, 56], [179, 78], [179, 116], [179, 159], [179, 162], [179, 166], [179, 170], [180, 1], [180, 23], [180, 44], [180, 67], [180, 108], [180, 115], [180, 128], [180, 147], [180, 163], [180, 168], [181, 14], [181, 38], [181, 69], [181, 71], [181, 75], [181, 113], [181, 121], [181, 125], [181, 132], [181, 172], [181, 173], [181, 178], [181, 188], [181, 190], [182, 20], [182, 27], [182, 35], [182, 52], [182, 80], [182, 92], [182, 94], [182, 101], [182, 117], [182, 184], [182, 196], [183, 1], [183, 4], [183, 12], [183, 26], [183, 62], [183, 95], [183, 96], [183, 151], [183, 199], [184, 35], [184, 80], [184, 83], [184, 89], [184, 92], [184, 94], [184, 101], [184, 117], [184, 176], [184, 182], [184, 196], [185, 18], [185, 22], [185, 58], [185, 90], [185, 109], [185, 142], [185, 148], [185, 153], [185, 158], [185, 166], [185, 191], [186, 24], [186, 31], [186, 59], [186, 66], [186, 81], [186, 87], [186, 123], [186, 155], [187, 7], [187, 21], [187, 49], [187, 70], [187, 74], [187, 79], [187, 82], [187, 167], [187, 169], [187, 175], [188, 38], [188, 69], [188, 71], [188, 75], [188, 98], [188, 104], [188, 113], [188, 132], [188, 137], [188, 172], [188, 181], [188, 190], [189, 19], [189, 47], [189, 50], [189, 93], [189, 110], [189, 125], [189, 131], [189, 177], [189, 195], [190, 38], [190, 69], [190, 71], [190, 75], [190, 98], [190, 104], [190, 113], [190, 132], [190, 137], [190, 172], [190, 181], [190, 188], [191, 16], [191, 18], [191, 58], [191, 90], [191, 109], [191, 142], [191, 148], [191, 153], [191, 158], [191, 166], [191, 185], [192, 0], [192, 8], [192, 30], [192, 34], [192, 55], [192, 63], [192, 77], [192, 114], [192, 119], [192, 120], [192, 139], [192, 152], [193, 6], [193, 8], [193, 15], [193, 37], [193, 46], [193, 77], [193, 91], [193, 114], [193, 130], [193, 139], [194, 2], [194, 33], [194, 39], [194, 73], [194, 84], [194, 105], [194, 122], [194, 124], [194, 141], [194, 149], [195, 10], [195, 41], [195, 45], [195, 50], [195, 93], [195, 107], [195, 110], [195, 131], [195, 144], [195, 189], [196, 27], [196, 35], [196, 80], [196, 89], [196, 92], [196, 94], [196, 101], [196, 117], [196, 176], [196, 182], [196, 184], [197, 25], [197, 28], [197, 43], [197, 48], [197, 99], [197, 127], [197, 133], [197, 140], [197, 146], [197, 160], [197, 174], [198, 22], [198, 29], [198, 58], [198, 68], [198, 81], [198, 102], [198, 109], [198, 111], [198, 112], [198, 123], [198, 148], [199, 1], [199, 4], [199, 44], [199, 62], [199, 95], [199, 96], [199, 108], [199, 135], [199, 151], [199, 163], [199, 165], [199, 168], [199, 183]]\nSELF.MAPPING\n{0: set(), 1: set(), 2: {73}, 4: set(), 5: set(), 6: {103}, 7: set(), 8: set(), 9: set(), 10: set(), 11: set(), 12: set(), 13: set(), 14: set(), 15: set(), 16: set(), 17: set(), 18: set(), 19: set(), 20: set(), 21: set(), 22: set(), 23: set(), 24: set(), 25: set(), 26: set(), 27: set(), 28: set(), 29: set(), 30: set(), 31: set(), 32: set(), 33: set(), 34: set(), 35: set(), 36: set(), 37: set(), 38: set(), 39: set(), 40: set(), 41: set(), 42: set(), 43: set(), 44: set(), 45: set(), 46: set(), 47: set(), 48: set(), 49: set(), 50: set(), 51: set(), 52: set(), 53: {3}, 54: set(), 55: set(), 56: set(), 57: set(), 58: set(), 59: set(), 60: set(), 61: set(), 62: set(), 63: set(), 64: set(), 65: set(), 66: set(), 67: set(), 68: set(), 69: set(), 70: set(), 71: set(), 72: set(), 74: set(), 75: set(), 76: set(), 77: set(), 78: set(), 79: set(), 80: set(), 81: set(), 82: set(), 83: set(), 84: set(), 85: set(), 86: set(), 87: set(), 88: set(), 89: set(), 90: set(), 91: set(), 92: set(), 93: set(), 94: set(), 95: set(), 96: set(), 97: set(), 98: set(), 99: set(), 100: set(), 101: set(), 102: set(), 104: set(), 105: set(), 106: set(), 107: set(), 108: set(), 109: set(), 110: set(), 111: set(), 112: set(), 113: set(), 114: set(), 115: set(), 116: set(), 117: set(), 118: set(), 119: set(), 120: set(), 121: set(), 122: set(), 123: set(), 124: {73}, 125: set(), 126: set(), 127: set(), 128: set(), 129: set(), 130: set(), 131: set(), 132: set(), 133: set(), 134: set(), 135: set(), 136: set(), 137: set(), 138: set(), 139: set(), 140: set(), 141: set(), 142: set(), 143: set(), 144: set(), 145: {103}, 146: set(), 147: set(), 148: set(), 149: set(), 150: set(), 151: set(), 152: set(), 153: set(), 154: set(), 155: set(), 156: set(), 157: set(), 158: set(), 159: set(), 160: set(), 161: set(), 162: set(), 163: set(), 164: set(), 165: set(), 166: set(), 167: set(), 168: set(), 169: set(), 170: set(), 171: {3}, 172: set(), 173: set(), 174: set(), 175: set(), 176: set(), 177: set(), 178: set(), 179: set(), 180: set(), 181: set(), 182: set(), 183: set(), 184: set(), 185: set(), 186: set(), 187: set(), 188: set(), 189: set(), 190: set(), 191: set(), 192: set(), 193: set(), 194: set(), 195: set(), 196: set(), 197: set(), 198: set(), 199: set()}\nPARENTS\n[16, 1, 1, 3, 1, 1, 16, 1, 16, 1, 16, 1, 1, 1, 16, 16, 16, 1, 16, 16, 1, 1, 16, 1, 16, 1, 1, 1, 1, 16, 16, 16, 1, 1, 16, 1, 1, 16, 16, 1, 1, 16, 16, 1, 1, 16, 16, 16, 1, 1, 16, 1, 1, 16, 1, 16, 16, 1, 16, 16, 1, 16, 1, 16, 1, 16, 16, 1, 16, 16, 1, 16, 1, 73, 1, 16, 1, 16, 16, 1, 1, 16, 1, 1, 1, 16, 16, 16, 1, 1, 16, 16, 1, 16, 1, 1, 1, 1, 16, 1, 1, 1, 16, 103, 16, 1, 16, 16, 1, 16, 16, 16, 16, 16, 16, 1, 16, 1, 1, 16, 16, 16, 1, 16, 1, 16, 16, 1, 1, 1, 16, 16, 16, 1, 1, 1, 1, 16, 1, 16, 1, 1, 16, 16, 16, 16, 1, 1, 16, 1, 1, 1, 16, 16, 1, 16, 1, 1, 16, 16, 1, 1, 16, 1, 1, 1, 16, 1, 1, 1, 16, 16, 16, 16, 1, 1, 1, 16, 16, 16, 1, 16, 1, 1, 1, 16, 16, 1, 16, 16, 16, 16, 16, 16, 1, 16, 1, 1, 16, 1]\nParents len\n200\nCLUSTERS\n{1, 3, 103, 73, 16}\nRANKS\n[24, 99, 11, 1, 6, 1, 1, 23, 1, 2, 16, 33, 1, 1, 38, 1, 98, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n3 73 103\nv_to_k\n{73: 124, 103: 145, 3: 171}\n{16: {0, 6, 8, 10, 14, 15, 16, 18, 19, 22, 24, 29, 30, 31, 34, 37, 38, 41, 42, 45, 46, 47, 50, 53, 55, 56, 58, 59, 61, 63, 65, 66, 68, 69, 71, 75, 77, 78, 81, 85, 86, 87, 90, 91, 93, 98, 102, 104, 106, 107, 109, 110, 111, 112, 113, 114, 116, 119, 120, 121, 123, 125, 126, 130, 131, 132, 137, 139, 142, 143, 144, 145, 148, 152, 153, 155, 158, 159, 162, 166, 170, 171, 172, 173, 177, 178, 179, 181, 185, 186, 188, 189, 190, 191, 192, 193, 195, 198}, 1: {1, 2, 4, 5, 7, 9, 11, 12, 13, 17, 20, 21, 23, 25, 26, 27, 28, 32, 33, 35, 36, 39, 40, 43, 44, 48, 49, 51, 52, 54, 57, 60, 62, 64, 67, 70, 72, 74, 76, 79, 80, 82, 83, 84, 88, 89, 92, 94, 95, 96, 97, 99, 100, 101, 105, 108, 115, 117, 118, 122, 124, 127, 128, 129, 133, 134, 135, 136, 138, 140, 141, 146, 147, 149, 150, 151, 154, 156, 157, 160, 161, 163, 164, 165, 167, 168, 169, 174, 175, 176, 180, 182, 183, 184, 187, 194, 196, 197, 199}, 3: set(), 73: set(), 103: set()}\n5\n{0: {0, 6, 8, 10, 14, 15, 16, 18, 19, 22, 24, 29, 30, 31, 34, 37, 38, 41, 42, 45, 46, 47, 50, 53, 55, 56, 58, 59, 61, 63, 65, 66, 68, 69, 71, 75, 77, 78, 81, 85, 86, 87, 90, 91, 93, 98, 102, 104, 106, 107, 109, 110, 111, 112, 113, 114, 116, 119, 120, 121, 123, 125, 126, 130, 131, 132, 137, 139, 142, 143, 144, 145, 148, 152, 153, 155, 158, 159, 162, 166, 170, 171, 172, 173, 177, 178, 179, 181, 185, 186, 188, 189, 190, 191, 192, 193, 195, 198}, 1: {1, 2, 4, 5, 7, 9, 11, 12, 13, 17, 20, 21, 23, 25, 26, 27, 28, 32, 33, 35, 36, 39, 40, 43, 44, 48, 49, 51, 52, 54, 57, 60, 62, 64, 67, 70, 72, 74, 76, 79, 80, 82, 83, 84, 88, 89, 92, 94, 95, 96, 97, 99, 100, 101, 105, 108, 115, 117, 118, 122, 124, 127, 128, 129, 133, 134, 135, 136, 138, 140, 141, 146, 147, 149, 150, 151, 154, 156, 157, 160, 161, 163, 164, 165, 167, 168, 169, 174, 175, 176, 180, 182, 183, 184, 187, 194, 196, 197, 199}, 2: set(), 3: set(), 4: set()}\n[0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1]\n\n\n\n\n\n\n\n\n\n\nThe code below is correct, make it efficient; remove v_to_k by chaning map_border, see if can directly do all the ops on the numpy array\nThen move to GPU, perhaps needing to make less efficient on CPU first by doing all distance calculations (200*200, only need upper triangle) on CPU, then moving over…\nQ: Do I need the mapping? EOD, just looking at parents…\n\nIn this code, np.unique(a) returns an array of the unique elements in a, sorted in ascending order. Then, np.searchsorted(unique_elements, a) returns an array of the same shape as a, where each element of a is replaced by its index in unique_elements. This effectively remaps the elements of a to the range 0 to len(a)-1."
  },
  {
    "objectID": "notes/ImplementingDbscan.html#final-cpu-implementation",
    "href": "notes/ImplementingDbscan.html#final-cpu-implementation",
    "title": "Implementing DBSCAN",
    "section": "Final CPU Implementation",
    "text": "Final CPU Implementation\n\nMake a set of core points and a mapping of border points to their core.\nUse union find to label the array of parents, forming clusters of core points or connected groups of core points.\nLabel border points with the parents of their corresponding core points.\n\n\nfrom collections import Counter\nclass DbscanCustom:\n    def __init__(self, eps, min_samples):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.edges = [] # for union find\n        self.core_points = set()\n        self.border_to_core = {}\n\n    def find(self,i):\n      res = i\n      while self.parents[res]!=res:\n        self.parents[res] = self.parents[self.parents[res]]\n        res = self.parents[res]\n      return res\n\n    def union(self, i,j):\n      pi, pj = self.find(i), self.find(j)\n      if pi == pj: return\n      if self.rank[pi] &gt;= self.rank[pj]:\n        self.parents[pj] = pi\n        self.rank[pi] += self.rank[pj]\n      else:\n        self.parents[pi] = pj\n        self.rank[pj] += self.rank[pi]\n\n\n    def fit_predict(self, X):\n        \"\"\"\n        Use union find to label the array of parents, forming clusters of core points or connected groups of core points.\n        Label border points with the parents of their corresponding core points.\n        \"\"\"\n        self.parents = list(range(len(X)))\n        self.rank = [1]*len(X)\n        self.X = X\n        self.label_points(X)\n        for i, j in self.edges:\n          if i in self.core_points and j in self.core_points:\n            self.union(i,j)\n\n        for border_idx, core_idx in self.border_to_core.items():\n          self.parents[border_idx] = self.parents[core_idx]\n\n        result_orig_label = np.array(self.parents)\n        unique_elements = np.unique(result_orig_label)\n        result_final_label = np.searchsorted(unique_elements,result_orig_label)\n        return result_final_label\n\n\n    def label_points(self, X):\n        \"\"\"1. Label the points as core, border, or noise.\"\"\"\n        self.X  = X\n        self.get_core_points()\n        self.get_border_points()\n\n    # Ignore the noise points, don't need them later\n    def get_core_points(self):\n      for i,x in enumerate(self.X):\n            if self.map_core(i):\n                self.core_points.add(i)\n\n    def get_border_points(self):\n       for i,x in enumerate(X):\n            if i not in self.core_points:\n                self.map_border(i)\n\n    def map_core(self, i):\n        \"\"\"A point is a core point if at least a specified number min_samples of neighboring points fall within eps radius\"\"\"\n        num_within = 0\n        for j,x in enumerate(self.X):\n          if i == j:\n            continue\n          if self.distance(i,j) &lt;= self.eps:\n              self.edges.append([i,j])\n              num_within += 1\n        if num_within &gt;= self.min_samples:\n          return True\n        return False\n\n    def map_border(self, border_idx):\n        \"\"\"A border point is a point that has fewer than min_samples within eps, but lies within the radius eps of a core point\"\"\"\n        min_dist = self.eps\n        min_idx = None\n        for core_idx in self.core_points:\n          if border_idx == core_idx:\n            continue\n          if self.distance(border_idx,core_idx) &lt;= self.eps:\n            if self.distance(border_idx,core_idx) &lt;= min_dist:\n              min_dist = self.distance(border_idx,core_idx)\n              min_idx = core_idx\n        if min_idx: self.border_to_core[border_idx] = min_idx\n\n    def distance(self,i,j):\n      \"\"\"Distance between points of self.X with indices i and j\"\"\"\n      return np.sqrt(np.sum((self.X[i]-self.X[j])**2))\n\n\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\ndb_custom = DbscanCustom(eps=0.2, min_samples=5)\ny_db = db_custom.fit_predict(X)\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n%timeit -n 10 db_custom.fit_predict(X)\n\n504 ms ± 220 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "notes/ImplementingDbscan.html#naive-gpu-implementation-takes-more-time-than-cpu-implementation",
    "href": "notes/ImplementingDbscan.html#naive-gpu-implementation-takes-more-time-than-cpu-implementation",
    "title": "Implementing DBSCAN",
    "section": "Naive GPU implementation (takes more time than CPU implementation)",
    "text": "Naive GPU implementation (takes more time than CPU implementation)\n\nGet distance matrix\nLabel &lt; min_dist as 1 else 0\nSum (need min_samples +1 bc of 0 diag for dist between point and itself)\nIf sum &gt; min_dist+1, then add to core\nParallelize computation of cluster labels, not sure how…\n\n\nfrom collections import Counter\nclass DbscanGPU:\n    def __init__(self, eps, min_samples):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.core_points = set()\n        self.border_to_core = {}\n\n    def find(self,i):\n      res = i\n      while self.parents[res]!=res:\n        self.parents[res] = self.parents[self.parents[res]]\n        res = self.parents[res]\n      return res\n\n    def union(self, i,j):\n      pi, pj = self.find(i), self.find(j)\n      if pi == pj: return\n      if self.rank[pi] &gt;= self.rank[pj]:\n        self.parents[pj] = pi\n        self.rank[pi] += self.rank[pj]\n      else:\n        self.parents[pi] = pj\n        self.rank[pj] += self.rank[pi]\n\n\n    def fit_predict(self, X):\n        \"\"\"\n        Use union find to label the array of parents, forming clusters of core points or connected groups of core points.\n        Label border points with the parents of their corresponding core points.\n        \"\"\"\n        self.parents = list(range(len(X)))\n        self.rank = [1]*len(X)\n        self.X = X\n        self.label_points(X)\n        #note self.dists is a square matrix\n        for i in range(len(self.dists)):\n          for j in range(len(self.dists)):\n            if self.core_points[i] and self.core_points[j] and self.dists[i,j] &lt;= self.eps:\n              self.union(i,j)\n\n        for border_idx, core_idx in self.border_to_core.items():\n          self.parents[border_idx] = self.parents[core_idx]\n\n        result_orig_label = np.array(self.parents)\n        unique_elements = np.unique(result_orig_label)\n        result_final_label = np.searchsorted(unique_elements,result_orig_label)\n        return result_final_label\n\n\n    def label_points(self, X):\n        \"\"\"1. Label the points as core, border, or noise.\"\"\"\n        self.X  = X\n        self.calc_distances()\n        self.get_core_points()\n        self.get_border_points()\n\n    # Ignore the noise points, don't need them later\n    def get_core_points(self):\n      self.core_points = torch.sum(self.dists&lt;=self.eps,dim = 1)&gt;self.min_samples\n\n\n    def get_border_points(self):\n      self.border_points = set()\n      for i,x in enumerate(self.core_points):\n        if not x: #if False\n          self.border_to_core[i]=self.dists[i][i+1:].argmin().item() + i + 1\n\n\n    def calc_distances(self):\n      \"\"\"Calculate the distance matrix between a point and every other point\"\"\"\n      self.dists=(self.X[None,:,:]-self.X[:,None,:]).norm(dim=2)\n\n\nX, y = map(torch.tensor,make_moons(n_samples=200, noise=0.05, random_state=0))\ndb_custom_gpu = DbscanGPU(eps=0.2, min_samples=5)\ny_db = db_custom_gpu.fit_predict(X)\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n%timeit -n 10 db_custom_gpu.fit_predict(X)\n\n652 ms ± 52.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "notes/ImplementingDbscan.html#simple-optimized-gpu-implementation",
    "href": "notes/ImplementingDbscan.html#simple-optimized-gpu-implementation",
    "title": "Implementing DBSCAN",
    "section": "Simple optimized GPU implementation",
    "text": "Simple optimized GPU implementation\nGitHub Copilot Your DBSCAN implementation looks quite good, but there are a few areas where it could be improved for better GPU utilization:\nBatching distance calculations: Calculating pairwise distances can be memory-intensive for large datasets. You can batch these calculations to reduce memory usage.\nAvoiding Python loops: Python loops are slow and cannot be parallelized on a GPU. You should replace the loops in fit_predict and get_border_points with vectorized operations.\nUsing in-place operations: In-place operations can reduce memory usage and improve performance. For example, you can use torch.add_ instead of + to update self.rank.\n\nclass DbscanGPU:\n    def __init__(self, eps, min_samples):\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def fit_predict(self, X):\n        # Calculate pairwise distances in batches to save memory\n        dists = torch.cdist(X, X)\n\n        # Find core points\n        core_points = (dists &lt;= self.eps).sum(dim=1) &gt;= self.min_samples\n\n        # Label each point with its own index initially\n        labels = torch.arange(len(X), device=X.device)\n\n        # Union-find for core points\n        for i in range(len(X)):\n            if core_points[i]:\n                for j in range(i + 1, len(X)):\n                    if core_points[j] and dists[i, j] &lt;= self.eps:\n                        # Find the roots of i and j\n                        root_i = self.find(labels, i)\n                        root_j = self.find(labels, j)\n\n                        # Union i and j by assigning the smaller label to the larger one\n                        if root_i &lt; root_j:\n                            labels[root_j] = root_i\n                        else:\n                            labels[root_i] = root_j\n\n        # Assign each non-core point to the same cluster as its nearest core point\n        for i in range(len(X)):\n            if not core_points[i]:\n                labels[i] = labels[dists[i, core_points].argmin()]\n\n        # Convert labels to consecutive integers starting from 0\n        _, labels = torch.unique(labels, return_inverse=True)\n\n        return labels.cpu().numpy()\n\n    @staticmethod\n    def find(labels, i):\n        if labels[i] != i:\n            labels[i] = DbscanGPU.find(labels, labels[i])\n        return labels[i]\n\n\n\nclass DbscanGPU:\n    def __init__(self, eps, min_samples):\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def fit_predict(self, X):\n        # Calculate pairwise distances in batches to save memory\n        dists = torch.cdist(X, X)\n\n        # Find core points\n        core_points = (dists &lt;= self.eps).sum(dim=1) &gt;= self.min_samples\n\n        # Label each point with its own index initially\n        labels = torch.arange(len(X), device=X.device)\n\n        # Union-find for core points\n        for i in range(len(X)):\n            if core_points[i]:\n                for j in range(i + 1, len(X)):\n                    if core_points[j] and dists[i, j] &lt;= self.eps:\n                        # Find the roots of i and j\n                        root_i = self.find(labels, i)\n                        root_j = self.find(labels, j)\n\n                        # Union i and j by assigning the smaller label to the larger one\n                        if root_i &lt; root_j:\n                            labels[root_j] = root_i\n                        else:\n                            labels[root_i] = root_j\n\n        # Assign each non-core point to the same cluster as its nearest core point\n        for i in range(len(X)):\n            if not core_points[i]:\n                labels[i] = labels[dists[i, core_points].argmin()]\n\n        # Convert labels to consecutive integers starting from 0\n        _, labels = torch.unique(labels, return_inverse=True)\n\n        return labels.cpu().numpy()\n\n    @staticmethod\n    def find(labels, i):\n        if labels[i] != i:\n            labels[i] = DbscanGPU.find(labels, labels[i])\n        return labels[i]\n\n    # @staticmethod\n    # def find(labels, i):\n    #     root = i\n    #     while labels[root] != root:\n    #         root = labels[root]\n\n    #     # Path compression\n    #     while labels[i] != i:\n    #         parent = labels[i]\n    #         labels[i] = root\n    #         i = parent\n\n    #     return root\n\n\nX, y = map(torch.tensor,make_moons(n_samples=200, noise=0.05, random_state=0))\ndb_custom_gpu = DbscanGPU(eps=0.2, min_samples=5)\ny_db = db_custom_gpu.fit_predict(X)\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n%timeit -n 10 db_custom_gpu.fit_predict(X)\n\n336 ms ± 45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n(329/504)*100\n\n65.27777777777779"
  },
  {
    "objectID": "notes/ImplementingDbscan.html#out-of-the-box-optimized-dbscan-implementation",
    "href": "notes/ImplementingDbscan.html#out-of-the-box-optimized-dbscan-implementation",
    "title": "Implementing DBSCAN",
    "section": "Out-of-the box optimized DBSCAN implementation",
    "text": "Out-of-the box optimized DBSCAN implementation\n\n!pip install \\\n    --extra-index-url=https://pypi.nvidia.com \\\n    cudf-cu12==23.12.* dask-cudf-cu12==23.12.* cuml-cu12==23.12.* \\\n    cugraph-cu12==23.12.* cuspatial-cu12==23.12.* cuproj-cu12==23.12.* \\\n    cuxfilter-cu12==23.12.* cucim-cu12==23.12.* pylibraft-cu12==23.12.* \\\n    raft-dask-cu12==23.12.*\n\n\n\nfrom cuml import DBSCAN as cuml\n\n\ndb_gpu_opt = DBSCAN(eps=0.2, min_samples=5)\ny_db = db_gpu_opt.fit_predict(X)\nplt.scatter(X[y_db == 0, 0],\n            X[y_db == 0, 1],\n            c='lightblue',\n            edgecolor='black',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.scatter(X[y_db == 1, 0],\n            X[y_db == 1, 1],\n            c='lightblue',\n            edgecolor='red',\n            marker='o',\n            s=40,\n            label='Cluster 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n%timeit -n 10 db_gpu_opt.fit_predict(X)\n\n5.7 ms ± 858 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nAlmost 100x faster on T4!\n\n\nX = torch.tensor([[1, 2],[3, 4], [5, 6]])\nX\nn,m = X.shape\nX[None,:,:]-X[:,None,:]\n\ntensor([[[ 0,  0],\n         [ 2,  2],\n         [ 4,  4]],\n\n        [[-2, -2],\n         [ 0,  0],\n         [ 2,  2]],\n\n        [[-4, -4],\n         [-2, -2],\n         [ 0,  0]]])\n\n\n\nX[None,:,:].float()\n\ntensor([[[1., 2.],\n         [3., 4.],\n         [5., 6.]]])\n\n\n\nX[:,None,:].float()\n\ntensor([[[1., 2.]],\n\n        [[3., 4.]],\n\n        [[5., 6.]]])\n\n\n\nout=(X[None,:,:].float()-X[:,None,:].float()).norm(dim=2)\nout\n\ntensor([[0.0000, 2.8284, 5.6569],\n        [2.8284, 0.0000, 2.8284],\n        [5.6569, 2.8284, 0.0000]])\n\n\n\neps=0.2\n(out &lt;= 0.2).sum(dim=1)\n\ntensor([1, 1, 1])\n\n\n\nX.shape, X[None,:,:].shape, X[:,None,:].shape\n\n(torch.Size([3, 2]), torch.Size([1, 3, 2]), torch.Size([3, 1, 2]))\n\n\n\n(X[None,:,:,None].float()-X[:,:,None].float()).norm(dim=2)\n\ntensor([[[0.],\n         [0.],\n         [0.]]])\n\n\n\nimport math\nmath.sqrt((1-6)**2)\n\n5.0\n\n\n\nX[None,:,:].shape\n\ntorch.Size([1, 3, 2])\n\n\n\nX.T[:,:,None].shape\n\ntorch.Size([2, 3, 1])\n\n\n\ntorch.sqrt(torch.sum((X[None,:,:]- X.T[:,:,None]) ** 2, dim=(1,)))\n\ntensor([[0.0000, 1.7321],\n        [1.7321, 0.0000]])\n\n\n\ntorch.sqrt(torch.sum((X[None,:,:]- X.T[:,:,None]) ** 2,dim=(0,1,2)))\n\ntensor(2.4495)\n\n\n\nimport torch\n\ndef pairwise_distance(matrix):\n    # Expand dims to allow broadcasting\n    expanded_a = matrix.unsqueeze(1)\n    expanded_b = matrix.unsqueeze(0)\n\n    # Compute pairwise distance using Euclidean distance formula\n    distances = torch.sqrt(torch.sum((expanded_a - expanded_b) ** 2, dim=-1))\n\n    return distances\n\n# Test the function\nmatrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\ndistances = pairwise_distance(matrix)\nprint(distances)\n\ntensor([[0.0000, 2.8284, 5.6569],\n        [2.8284, 0.0000, 2.8284],\n        [5.6569, 2.8284, 0.0000]])\n\n\n\nimport torch\n\ndef pairwise_distance(matrix):\n    # Expand dims to allow broadcasting\n    expanded_a = matrix[:,None,:]\n    expanded_b = matrix[None,:,:]\n\n    # Compute pairwise distance using Euclidean distance formula\n    distances = torch.sqrt(torch.sum((expanded_a - expanded_b) ** 2, dim=-1))\n\n    return distances\n\n# Test the function\nmatrix = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float)\ndistances = pairwise_distance(matrix)\nprint(distances)\n\ntensor([[0.0000, 2.8284, 5.6569],\n        [2.8284, 0.0000, 2.8284],\n        [5.6569, 2.8284, 0.0000]])\n\n\n\ndistances[1].argmin().item()\n\n1\n\n\n\ndistances[1,2]\n\ntensor(2.8284)\n\n\n\nmath.sqrt((1-5)**2+(2-6)**2)\n\n5.656854249492381\n\n\n\ndistances = torch.tensor([[1, 2, 2],[2,3,3],[1,2,1]])\ntorch.sum(distances&gt;1,dim=1)\n\ntensor([2, 3, 1])"
  },
  {
    "objectID": "notes/ImplementingDbscan.html#cuml-dbscan-naive-gpu-dbscan",
    "href": "notes/ImplementingDbscan.html#cuml-dbscan-naive-gpu-dbscan",
    "title": "Implementing DBSCAN",
    "section": "cuML DBSCAN > Naive GPU DBSCAN",
    "text": "cuML DBSCAN &gt; Naive GPU DBSCAN\nSpatial indexing is a common technique used to speed up DBSCAN implementations. The main idea is to organize the data in such a way that queries about the spatial proximity of data points can be answered more efficiently.\nIn the context of DBSCAN, one of the most computationally expensive parts of the algorithm is finding all points within a certain distance (eps) of a given point. This is because, in a naive implementation, you would need to calculate the distance between the given point and every other point in the dataset, which is an O(n) operation.\nSpatial indexes, such as k-d trees or R-trees, can significantly speed up this operation. These data structures partition the data space into regions, and points within the same region are likely to be close to each other. This allows the algorithm to quickly eliminate many points that are too far away from the given point without having to calculate the exact distance.\nIn the cuML implementation of DBSCAN, the use of spatial indexing is abstracted away by the underlying CUDA libraries. The VertexDeg::run function, which calculates the degree of each vertex in the adjacency graph representation of the dataset, is likely where the spatial indexing is used. Different algorithms can be used for this calculation based on the algo parameter, some of which may use spatial indexing.\nHowever, it’s important to note that while spatial indexing can significantly speed up DBSCAN on CPUs, the situation is a bit different on GPUs. Due to the parallel nature of GPUs, sometimes a brute-force approach (calculating the distance between all pairs of points) can be faster than building and querying a spatial index. The optimal approach depends on the specific characteristics of the GPU and the dataset."
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html",
    "href": "notes/ml-algos-numpy-torch.html",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "",
    "text": "The goal of this is to practice building basic ML algorithms from scratch in numpy/PyTorch. I use Machine Learning from Scratch series as a starting point, generally watching the first part of the video to review the setup, then building the algorithms out myself, looking at the repo for test cases/if stuck.\nimport numpy as np\nnp.random.seed(42)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.colors import ListedColormap\nimport collections\nfrom sklearn.datasets import make_blobs\nfrom time import perf_counter"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#knn",
    "href": "notes/ml-algos-numpy-torch.html#knn",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "KNN",
    "text": "KNN\n\nPredict the class based on the most common class among the k nearest neighbors\nfit: doesn’t actually “train” the model in a traditional sense, just stores the training data.\npredict: for each point, find k nearest neighbors, then find most common class among these.\n\n\nclass KNN:\n    def __init__(self,k=3):\n        self.k = k\n\n    def _dist(self,x1,x2):\n        return np.sqrt(np.sum((x1-x2)**2))\n    \n    def fit(self,X,y):\n        self.X_train = X\n        self.y_train = y\n\n    def _predict(self,x,debug=False):\n        dists=np.array([self._dist(x,x_train) for x_train in self.X_train])\n        sorted_indices = np.argsort(dists)[:self.k]\n        labels = [self.y_train[i] for i in sorted_indices]\n        most_common_labels = collections.Counter(labels).most_common(1)[0][0]\n        if debug: \n            print(\"ORIGINAL: \",dists)\n            print(\"LABELS: \",labels)\n            print(\"MOST COMMON LABEL: \", most_common_labels)\n        return most_common_labels\n    \n    def predict(self,X,debug):\n        # X can have multiple samples, so predict for each one\n        out = np.array([self._predict(x,debug) for x in X])\n        if debug: print(out)\n        return out\n\n\n\niris = datasets.load_iris()\nX,y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\ndef accuracy(y_true,y_pred):\n    return np.sum(y_true==y_pred)/len(y_true)\n\n\nk = 3\nclf = KNN(k)\nclf.fit(X_train,y_train)\npredictions  = clf.predict(X_test,debug=False)\nprint(\"Accuracy: \", accuracy(y_test,predictions))\n\nAccuracy:  1.0\n\n\n\nFirst implement a naive implementation by directly converting numpy arrays to torch.tensors and replacing numpy functions with PyTorch functions.\n\n\nclass KNNTorchNaive:\n    def __init__(self,k=3):\n        self.k = k\n\n    def _dist(self,x1,x2):\n        return torch.sqrt(torch.sum((x1-x2)**2))\n    \n    def fit(self,X,y):\n        self.X_train = X\n        self.y_train = y\n\n    def _predict(self,x,debug=False):\n        dists=torch.tensor([self._dist(x,x_train) for x_train in self.X_train])\n        sorted_indices = torch.argsort(dists)[:self.k]\n        labels = [self.y_train[i] for i in sorted_indices]\n        most_common_labels = collections.Counter(labels).most_common(1)[0][0]\n        if debug: \n            print(\"ORIGINAL: \",dists)\n            print(\"LABELS: \",labels)\n            print(\"MOST COMMON LABEL: \", most_common_labels)\n        return most_common_labels\n    \n    def predict(self,X,debug):\n        # X can have multiple samples, so predict for each one\n        out = torch.tensor([self._predict(x,debug) for x in X])\n        if debug: print(out)\n        return out\n\n\nk = 3\nX = torch.from_numpy(X).float()\ny = torch.from_numpy(y).long()\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\ndef accuracy(y_true,y_pred): return torch.sum(y_true==y_pred).item()/len(y_true)\nclf = KNNTorchNaive(k)\nclf.fit(X_train,y_train)\npredictions  = clf.predict(X_test,debug=False)\nprint(\"Accuracy: \", accuracy(y_test,predictions))\n\nAccuracy:  1.0\n\n\n\nX_train.shape\n\ntorch.Size([120, 4])\n\n\n\nclass KNNTorchBroadcast:\n    def __init__(self,k=3):\n        self.k = k\n    \n    def fit(self,X,y):\n        self.X_train = X\n        self.y_train = y\n\n    def _predict(self,x,debug=False):\n        #print(x.shape)\n        dists = torch.sqrt(torch.sum((self.X_train-x)**2,dim=1))\n        sorted_indices = torch.argsort(dists)[:self.k]\n        labels = [self.y_train[i] for i in sorted_indices] #could perhaps be optimized, but trying to keep this consistent with collections.Counter\n        most_common_labels = collections.Counter(labels).most_common(1)[0][0]\n        if debug: \n            print(\"ORIGINAL: \",dists)\n            print(\"LABELS: \",labels)\n            print(\"MOST COMMON LABEL: \", most_common_labels)\n        return most_common_labels\n    \n    def predict(self,X,debug):\n        # X can have multiple samples, so predict for each one\n        out = torch.tensor([self._predict(x,debug) for x in X])\n        if debug: print(out)\n        return out\n\n\nBroadcasting in PyTorch follows these rules:\n\n\nIf the two tensors differ in the number of dimensions, the shape of the tensor with fewer dimensions is padded with ones on its leading (left) side.\nIf the shape of the two tensors does not match in any dimension, the tensor with shape equal to 1 in that dimension is stretched to match the other shape.\nIf in any dimension the sizes disagree and neither is equal to 1, an error is raised.\n\n\nWe subtract tensor x of shape [4] from a tensor self.X_train of shape [120, 4], PyTorch automatically broadcasts x to the shape of self.X_train by repeating it along the 0th dimension.\n\nIn the dists calculation, the tensors differ in number of dimensions, so x is padded on the left with 1, becoming [1,4] according to Rule 1. Then by Rule 2, this resulting tensor is “stretched out” along 0th dimension from shape [1,4] to [120,4].\n\n\n\ndef accuracy(y_true,y_pred): return torch.sum(y_true==y_pred).item()/len(y_true)\nclf = KNNTorchBroadcast(k)\nclf.fit(X_train,y_train)\npredictions  = clf.predict(X_test,debug=False)\nprint(\"Accuracy: \", accuracy(y_test,predictions))\n\nAccuracy:  1.0"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#k-means-clustering",
    "href": "notes/ml-algos-numpy-torch.html#k-means-clustering",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nCluster an unlabeled data set into K clusters.\n\nInitialize cluster centroids randomly\nRepeat until convergence\n\nUpdate cluster labels, assigning points to nearest cluster centroid\nUpdate cluster centroids, setting them to the mean of each cluster\n\n\n\n\nclass KMeans:\n    def __init__(self, K, max_iters, plot_steps):\n        self.K = K\n        self.max_iters = max_iters\n        self.plot_steps = plot_steps\n        # list of lists of indices for each cluster\n        self.clusters = [[] for _ in range(self.K)]\n        # mean feature vector for each cluster\n        self.centroids = []\n        self.tol = 1e-20\n    def _dist(self,x1,x2):\n        return np.sqrt(np.sum((x1-x2)**2))\n    # just need predict since unsupervised with no labels\n    def predict(self,X,debug=False):\n        self.X = X #just need for plotting\n        self.n_samples, self.n_features = X.shape\n        # initialize cluster centroids randomly\n        self.centroids =[self.X[idx] for idx in np.random.choice(self.n_samples, size=self.K, replace=False)]\n        if debug: print('init centroids: ', self.centroids); print('init clusters: ',self.clusters)\n        for it in range(self.max_iters):\n            # Re-initialize the clusters since else will keep appending to old ones!!! (IMPORTANT)\n            self.clusters = [[] for _ in range(self.K)]\n            old_centroids = np.copy(self.centroids) #else will point to self.centroids even when those are re-initialized! (IMPORTANT)\n            # Update cluster labels, assigning points to nearest cluster centroid\n            for i in range(self.n_samples):\n                cluster_idx = np.argmin([self._dist(self.X[i,:],centroid) for centroid in self.centroids])\n                self.clusters[cluster_idx].append(i)\n            # Update cluster centroids, setting them to the mean of each cluster\n            for i,cluster in enumerate(self.clusters):\n                self.centroids[i] = np.mean([self.X[idx] for idx in cluster], axis = 0)\n                \n            if debug: print('centroids: ', self.centroids); print('clusters: ',self.clusters)\n            if np.all([self._dist(old_centroids[i], self.centroids[i]) &lt; self.tol for i in range(self.K)]):\n                print(f'converged in {it} iterations, breaking'); break\n        \n    def plot(self):\n        \"\"\"From https://github.com/patrickloeber/MLfromscratch/blob/master/mlfromscratch/kmeans.py\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        for i, index in enumerate(self.clusters):\n            point = self.X[index].T\n            ax.scatter(*point)\n\n        for point in self.centroids:\n            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n\n        plt.show()\n\n\nstart_time=perf_counter()\nX, y = make_blobs(\n    centers=3, n_samples=5000, n_features=2, shuffle=True, random_state=40\n)\nprint(X.shape)\n\nclusters = len(np.unique(y))\nprint(clusters)\n\nk = KMeans(K=clusters, max_iters=150, plot_steps=True)\ny_pred = k.predict(X,debug = False)\nend_time = perf_counter()\nprint(f\"Time taken: {end_time-start_time}\")\nk.plot()\n\n(5000, 2)\n3\nconverged in 8 iterations, breaking\nTime taken: 0.36868009900172183\n\n\n\n\n\n\n\n\n\n\nclass KMeansVectDist:\n    def __init__(self, K, max_iters, plot_steps):\n        self.K = K\n        self.max_iters = max_iters\n        self.plot_steps = plot_steps\n        self.clusters = [[] for _ in range(self.K)]\n        self.centroids = []\n        self.tol = 1e-20\n\n    def predict(self, X, debug=False):\n        self.X = X\n        self.n_samples, self.n_features = X.shape\n        self.centroids = self.X[np.random.choice(self.n_samples, size=self.K, replace=False)]\n\n        for it in range(self.max_iters):\n            self.clusters = [[] for _ in range(self.K)]\n            old_centroids = np.copy(self.centroids)\n\n            distances = np.sqrt(((self.X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n            closest_clusters = np.argmin(distances, axis=0)\n            for i in range(self.n_samples):\n                self.clusters[closest_clusters[i]].append(i)\n\n            for i, cluster in enumerate(self.clusters):\n                self.centroids[i] = self.X[cluster].mean(axis=0)\n\n            if np.all(np.sqrt(np.sum((old_centroids - self.centroids) ** 2, axis=1)) &lt; self.tol):\n                print(f'converged in {it} iterations, breaking')\n                break\n    def plot(self):\n        \"\"\"From https://github.com/patrickloeber/MLfromscratch/blob/master/mlfromscratch/kmeans.py\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        for i, index in enumerate(self.clusters):\n            point = self.X[index].T\n            ax.scatter(*point)\n\n        for point in self.centroids:\n            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n\n        plt.show()\n\n\nstart_time=perf_counter()\nX, y = make_blobs(\n    centers=3, n_samples=5000, n_features=2, shuffle=True, random_state=40\n)\nprint(X.shape)\n\nclusters = len(np.unique(y))\nprint(clusters)\n\nk = KMeansVectDist(K=clusters, max_iters=150, plot_steps=True)\ny_pred = k.predict(X,debug = False)\nend_time = perf_counter()\nprint(f\"Time taken: {end_time-start_time}\")\nk.plot()\n\n(5000, 2)\n3\nconverged in 7 iterations, breaking\nTime taken: 0.0077601640005013905\n\n\n\n\n\n\n\n\n\n\ntorch.tensor() vs torch.stack(): The torch.tensor() function is used to create a new tensor, while torch.stack() is used to concatenate a sequence of tensors along a new dimension.\nIn our case, [torch.mean(self.X[cluster], axis=0) for cluster in self.clusters] is a list of tensors. Each tensor is the mean of the data points in a cluster.\nIf we use torch.tensor() on this list, it will try to create a new tensor that contains these tensors, which is not allowed because all elements within a tensor must be of the same type and tensors cannot contain other tensors.\nOn the other hand, torch.stack() takes this list of tensors and concatenates them along a new dimension to create a single tensor. This is why torch.stack() is used in this case.\nIn short, torch.stack() is used to combine existing tensors into a larger tensor, while torch.tensor() is used to create a new tensor.\n\n\nclass KMeansTorchNaive:\n    def __init__(self, K, max_iters, plot_steps):\n        self.K = K\n        self.max_iters = max_iters\n        self.plot_steps = plot_steps\n        # list of lists of indices for each cluster\n        self.clusters = [[] for _ in range(self.K)]\n        # mean feature vector for each cluster\n        self.centroids = []\n        self.tol = 1e-20\n    def _dist(self,x1,x2):\n        return torch.sqrt(torch.sum((x1-x2)**2))\n    # just need predict since unsupervised with no labels\n    def predict(self,X,debug=False):\n        self.X = X #just need for plotting\n        self.n_samples, self.n_features = X.shape\n        # initialize cluster centroids randomly\n        #self.centroids =[self.X[idx] for idx in torch.random.choice(self.n_samples, size=self.K, replace=False)]\n        self.centroids = self.X[torch.randperm(self.n_samples)[:self.K]]\n\n        if debug: print('init centroids: ', self.centroids); print('init clusters: ',self.clusters)\n        for it in range(self.max_iters):\n            # Re-initialize the clusters since else will keep appending to old ones!!! (IMPORTANT)\n            self.clusters = [[] for _ in range(self.K)]\n            old_centroids = self.centroids.clone()\n            # Update cluster labels, assigning points to nearest cluster centroid\n            for i in range(self.n_samples):\n                cluster_idx = torch.argmin(torch.tensor(torch.tensor([self._dist(self.X[i,:],centroid) for centroid in self.centroids])))\n                self.clusters[cluster_idx].append(i)\n            # Update cluster centroids, setting them to the mean of each cluster\n            # for i,cluster in enumerate(self.clusters):\n            #     self.centroids[i] = torch.mean(torch.tensor([self.X[idx] for idx in cluster]))\n            self.centroids = torch.stack([torch.mean(self.X[cluster], axis=0) for cluster in self.clusters])\n            if debug: print('centroids: ', self.centroids); print('clusters: ',self.clusters)\n            if torch.all(torch.sqrt(torch.sum((self.centroids-old_centroids)**2,dim=1)) &lt; self.tol): \n            #if np.all([self._dist(old_centroids[i], self.centroids[i]) &lt; self.tol for i in range(self.K)]):\n                print(f'converged in {it} iterations, breaking'); break\n            # if torch.all([self._dist(old_centroids[i], self.centroids[i]) &lt; self.tol for i in range(self.K)]):\n            #     print(f'converged in {it} iterations, breaking'); break\n        \n    def plot(self):\n        \"\"\"From https://github.com/patrickloeber/MLfromscratch/blob/master/mlfromscratch/kmeans.py\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        for i, index in enumerate(self.clusters):\n            point = self.X[index].T\n            ax.scatter(*point)\n\n        for point in self.centroids:\n            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n\n        plt.show()\n\n\nstart_time = perf_counter()\nX, y = make_blobs(\n    centers=3, n_samples=5000, n_features=2, shuffle=True, random_state=40\n)\nX, y = torch.tensor(X), torch.tensor(y)\nprint(X.shape)\n\nclusters = len(np.unique(y))\nprint(clusters)\n\nk = KMeansTorchNaive(K=clusters, max_iters=150, plot_steps=True)\ny_pred = k.predict(X,debug = False)\nend_time = perf_counter()\nprint(f\"Time taken: {end_time-start_time}\")\nk.plot()\n\ntorch.Size([5000, 2])\n3\n\n\n/tmp/ipykernel_8198/1886197135.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  cluster_idx = torch.argmin(torch.tensor(torch.tensor([self._dist(self.X[i,:],centroid) for centroid in self.centroids])))\n\n\nconverged in 5 iterations, breaking\nTime taken: 0.8377222439994512\n\n\n\n\n\n\n\n\n\n\nOK, naive PyTorch implementation took a bit longer than basic numpy implementation.\n\n\nclass KMeansTorch:\n    def __init__(self, K, max_iters, plot_steps):\n        self.K = K\n        self.max_iters = max_iters\n        self.plot_steps = plot_steps\n        self.tol = 1e-20\n\n\n    def predict(self, X, debug=False):\n        self.X = X  # just need for plotting\n        self.n_samples, self.n_features = X.shape\n        # initialize cluster centroids randomly\n        self.centroids = self.X[torch.randperm(self.n_samples)[:self.K]]\n\n        if debug:\n            print('init centroids: ', self.centroids)\n\n        for it in range(self.max_iters):\n            old_centroids = self.centroids.clone()\n\n            # Calculate distances from each point to each centroid\n            distances = torch.cdist(self.X, self.centroids)\n            if debug: print(f\"distances of shape {distances.shape}: {distances}\")\n            # Find closest centroids\n            closest_centroids = torch.argmin(distances, dim=1) #eg, shape (5000,3)-&gt; want to find min along dim=1\n            if debug: print(f\"closest centroids: {closest_centroids}\")\n            if debug: print(f\"torch.nonzero(closest_centroids == 1, as_tuple=True): {torch.nonzero(closest_centroids == 1, as_tuple=True)}\")\n            # Update clusters using advanced indexing\n            self.clusters = [torch.nonzero(closest_centroids == i, as_tuple=True)[0] for i in range(self.K)]\n            \n            # Calculate new centroids\n            self.centroids = torch.stack(\n                [self.X[cluster].mean(dim=0) if len(cluster) &gt; 0 else old_centroids[i] for i, cluster in enumerate(self.clusters)]\n            )\n\n            if debug:\n                print('centroids: ', self.centroids)\n\n            # Check for convergence\n            if torch.all(torch.sqrt(torch.sum((self.centroids - old_centroids) ** 2, dim=1)) &lt; self.tol):\n                print(f'converged in {it} iterations, breaking')\n                break\n\n    def plot(self):\n        \"\"\"From https://github.com/patrickloeber/MLfromscratch/blob/master/mlfromscratch/kmeans.py\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 8))\n\n        for i, index in enumerate(self.clusters):\n            point = self.X[index].T\n            ax.scatter(*point)\n\n        for point in self.centroids:\n            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n\n        plt.show()\n\n\nstart_time = perf_counter()\nX, y = make_blobs(\n    centers=3, n_samples=5000, n_features=2, shuffle=True, random_state=40\n)\nX, y = torch.tensor(X), torch.tensor(y)\nprint(X.shape)\n\nclusters = len(np.unique(y))\nprint(clusters)\nk = KMeansTorch(K=clusters, max_iters=150, plot_steps=True)\ny_pred = k.predict(X,debug = False)\nend_time = perf_counter()\nprint(f\"Time taken: {end_time-start_time}\")\nk.plot()\n\ntorch.Size([5000, 2])\n3\nconverged in 7 iterations, breaking\nTime taken: 0.011911125002370682\n\n\n\n\n\n\n\n\n\n\nHowever, optimized PyTorch version was a bit quicker than vectorized numpy implementation (.004 seconds vs .007 seconds)."
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#linear-regression",
    "href": "notes/ml-algos-numpy-torch.html#linear-regression",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nPerform gradient descent using the following equations:\ndw = (1/N)*sum(2x(y_hat-y))\ndb = (1/N)*sum(2(y_hat-y))\nw = w - lr*dw\nb = b - lr*db\n\n\nclass LinearRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        self.n_examples, self.n_feats = X.shape\n        self.weights = np.zeros(self.n_feats)\n        self.bias = 0\n        for i in range(self.n_iters):\n           \n            y_pred = X @ self.weights + self.bias\n            if i == 0: print(\"X.shape, y_pred.shape: \", X.shape, y_pred.shape)\n\n            dw = (2/self.n_examples)*np.dot(X.T,y_pred-y)\n            db = (2/self.n_examples)*np.sum(y_pred-y) #np.dot(np.ones(self.n_examples),y_pred-y)\n            if i == 0: print(\"dw.shape, db.shape: \", dw.shape, db.shape)\n\n\n            self.weights -= self.lr*dw\n            self.bias -= self.lr*db\n        \n    def predict(self, X):\n        return X @ self.weights + self.bias\n\n\nX, y = datasets.make_regression(\n        n_samples=100, n_features=1, noise=20, random_state=4\n    )\n\nprint(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1234\n)\ndef mean_squared_error(y_test, predictions):\n    return np.mean((y_test-predictions)**2)\ndef r2_score(y_test, predictions):\n    ss_res = np.sum((y_test-predictions)**2) \n    ss_tot = np.sum((y_test-np.mean(y_test))**2) \n    return 1 - (ss_res/ss_tot)\nregressor = LinearRegression(learning_rate=0.01, n_iters=1000)\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)\nprint(predictions.shape)\nprint(predictions)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\naccu = r2_score(y_test, predictions)\nprint(\"Accuracy:\", accu)\n\ny_pred_line = regressor.predict(X)\ncmap = plt.get_cmap(\"viridis\")\nfig = plt.figure(figsize=(8, 6))\nm1 = plt.scatter(X_train, y_train, color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test, y_test, color=cmap(0.5), s=10)\nplt.plot(X, y_pred_line, color=\"black\", linewidth=2, label=\"Prediction\")\nplt.show()\n\n(100, 1) (100,)\nX.shape, y_pred.shape:  (80, 1) (80,)\ndw.shape, db.shape:  (1,) ()\n(20,)\n[  90.07911867   65.22240301  -45.69498186  -82.49167298   20.93910431\n  -73.92513193  -14.90998903  151.65799643   14.01724561 -102.29561058\n   15.56851863   41.60448602   26.20320259   38.12125411   37.38360674\n  -37.35823254  -83.12683724   26.30425075   38.13183211   29.45312701]\nMSE: 305.7741316085243\nAccuracy: 0.9247515208337563\n\n\n\n\n\n\n\n\n\n\nclass LinearRegressionTorch:\n    \"\"\"Can use torch.autograd automatic differentiation for gradient updates\"\"\"\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        self.n_examples, self.n_feats = X.shape\n        self.weights = torch.zeros(self.n_feats, requires_grad=True)#, torch.dtype='torch.float32')\n        self.bias = torch.zeros(1,requires_grad=True)\n        optimizer = torch.optim.SGD([self.weights, self.bias], lr=self.lr)\n        \n        for _ in range(self.n_iters):\n            y_pred = X @ self.weights + self.bias\n            loss = torch.mean((y_pred-y)**2)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n\n    def predict(self, X):\n        with torch.no_grad():\n            return X @ self.weights + self.bias\n\n\nX, y = datasets.make_regression(\n        n_samples=100, n_features=1, noise=20, random_state=4\n    )\nX, y = torch.tensor(X,  dtype=torch.float32), torch.tensor(y,  dtype=torch.float32)\nprint(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1234\n)\ndef mean_squared_error(y_test, predictions):\n    return torch.mean((y_test-predictions)**2)\ndef r2_score(y_test, predictions):\n    ss_res = torch.sum((y_test-predictions)**2) \n    ss_tot = torch.sum((y_test-torch.mean(y_test))**2) \n    return 1 - (ss_res/ss_tot)\nregressor = LinearRegressionTorch(learning_rate=0.01, n_iters=1000)\nregressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)\nprint(predictions.shape)\nprint(predictions)\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\naccu = r2_score(y_test, predictions)\nprint(\"Accuracy:\", accu)\n\ny_pred_line = regressor.predict(X).detach().numpy()\ncmap = plt.get_cmap(\"viridis\")\nfig = plt.figure(figsize=(8, 6))\nm1 = plt.scatter(X_train.detach().numpy(), y_train.detach().numpy(), color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test.detach().numpy(), y_test.detach().numpy(), color=cmap(0.5), s=10)\nplt.plot(X.detach().numpy(), y_pred_line, color=\"black\", linewidth=2, label=\"Prediction\")\nplt.show()\n\ntorch.Size([100, 1]) torch.Size([100])\ntorch.Size([20])\ntensor([  90.0789,   65.2223,  -45.6949,  -82.4915,   20.9391,  -73.9249,\n         -14.9099,  151.6576,   14.0172, -102.2954,   15.5685,   41.6044,\n          26.2031,   38.1212,   37.3835,  -37.3581,  -83.1266,   26.3042,\n          38.1317,   29.4531])\nMSE: tensor(305.7739)\nAccuracy: tensor(0.9248)\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nclass LinearRegressionIdiomaticTorch(nn.Module):\n    \"\"\"Can use torch.autograd automatic differentiation for gradient updates\"\"\"\n    def __init__(self, input_dim, output_dim=1):\n        super().__init__()\n        self.linear = nn.Linear(input_dim,output_dim)\n\n    def forward(self, X):\n        return self.linear(X)\n    \n\n\nX, y = datasets.make_regression(\n        n_samples=100, n_features=1, noise=20, random_state=4\n    )\nX, y = torch.tensor(X,  dtype=torch.float32), torch.tensor(y,  dtype=torch.float32)#.view(-1,1)\nprint(X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1234\n)\nmodel = LinearRegressionIdiomaticTorch(input_dim=X.shape[1])\n\n# Training loop\nn_iters = 1000\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\nfor i in range(n_iters):\n    optimizer.zero_grad()\n    y_pred = model(X_train)\n    if i==0: print('Shapes: y_pred.shape,y_train.shape', y_pred.shape,y_train.shape)\n    loss = criterion(y_pred.view(-1),y_train) # .view(-1) CRITICAL!\n    loss.backward()\n    optimizer.step()\n\n\nwith torch.no_grad():\n    predictions = model(X_test)\n    # print(\"predictions: \")\n    # print(predictions)\n\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\naccu = r2_score(y_test, predictions)\nprint(\"Accuracy:\", accu)\nwith torch.no_grad():\n    y_pred_line = model(X).detach().numpy()\ncmap = plt.get_cmap(\"viridis\")\nfig = plt.figure(figsize=(8, 6))\nm1 = plt.scatter(X_train.detach().numpy(), y_train.detach().numpy(), color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test.detach().numpy(), y_test.detach().numpy(), color=cmap(0.5), s=10)\nplt.plot(X.detach().numpy(), y_pred_line, color=\"black\", linewidth=2, label=\"Prediction\")\nplt.show()\n\ntorch.Size([100, 1]) torch.Size([100])\nShapes: y_pred.shape,y_train.shape torch.Size([80, 1]) torch.Size([80])\nMSE: 305.7741\nAccuracy: 0.924751527892842\n\n\n\n\n\n\n\n\n\n\nLearning moment (post conversing with GPT-40 Mini):\nWhile broadcasting allows the calculations to work, the gradients can behave differently based on how the operations are laid out, especially for loss functions that rely on precise error metrics across dimensions. When you switch to using shapes of (80,) explicitly, you simplify the relationship, making it easier for the backpropagation process to understand and compute the gradients correctly.\nThus, while the shapes may become mathematically compatible, the implicit behavior of broadcasting and element-wise operations fundamentally leads to differing gradient flows. Clarity in tensor shapes is incredibly important for ensuring that operations function as anticipated in deep learning frameworks like PyTorch.\nI had loss = criterion(y_pred.view(-1),y_train) without the .view(-1) at first, which led to just the intercept being estimated! (got a horizontal line as prediction). It seems that there’s are intricacies with broadcasting in loss calculations. It’s best to explicitly convert tensors to the right shape so that PyTorch computes the gradients correctly.\n\n\nimport torch.nn as nn\nclass SimpleNN(nn.Module):\n    \"\"\"Can use torch.autograd automatic differentiation for gradient updates\"\"\"\n    def __init__(self, input_dim, output_dim=1,hidden_dim =10):\n        super().__init__()\n        self.ln1 = nn.Linear(input_dim,hidden_dim)\n        self.relu = nn.ReLU()\n        self.ln2 = nn.Linear(hidden_dim,output_dim)\n\n    def forward(self, X):\n        x = self.ln1(X)\n        x = self.ln2(self.relu(x))\n        return x\n\n\nmodel = SimpleNN(input_dim=X.shape[1])\n\n# Training loop\nn_iters = 1000\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\nfor _ in range(n_iters):\n    y_pred = model(X_train)\n    loss = criterion(y_pred.view(-1),y_train)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\nwith torch.no_grad():\n    predictions = model(X_test)\n   # print(predictions)\n\nmse = mean_squared_error(y_test, predictions)\nprint(\"MSE:\", mse)\n\naccu = r2_score(y_test, predictions)\nprint(\"Accuracy:\", accu)\nwith torch.no_grad():\n    y_pred_line = model(X).detach().numpy()\ncmap = plt.get_cmap(\"viridis\")\nfig = plt.figure(figsize=(8, 6))\nm1 = plt.scatter(X_train.detach().numpy(), y_train.detach().numpy(), color=cmap(0.9), s=10)\nm2 = plt.scatter(X_test.detach().numpy(), y_test.detach().numpy(), color=cmap(0.5), s=10)\n\n\nplt.plot(X.detach().numpy(), y_pred_line, color=\"black\", linewidth=1, label=\"Prediction\")\nplt.show()\n\nMSE: 341.73175\nAccuracy: 0.9159026386036382"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#logistic-regression",
    "href": "notes/ml-algos-numpy-torch.html#logistic-regression",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nGradient updates are same as in linear regression, but since the targets are 0/1, convert predictions to probabilities via sigmoid.\n\n\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.001, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n       self.n_examples, self.n_feats = X.shape\n       self.weights = np.zeros(self.n_feats)\n       self.bias = 0\n       for _ in range(self.n_iters):\n           y_pred = self._sigmoid(X@self.weights+self.bias)\n\n           dw = (2/self.n_examples)*np.dot(X.T,y_pred-y)\n           db = (2/self.n_examples)*np.sum(y_pred-y)\n\n           self.weights -= self.lr * dw\n           self.bias -= self.lr * db\n\n    def predict(self, X):\n        probs = self._sigmoid(X@self.weights+self.bias)\n        return np.array([1 if prob &gt; 0.5 else 0 for prob in probs ])\n\n    def _sigmoid(self, x):\n        return 1/(1+np.exp(-x))\n\n\n# Testing\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn.model_selection import train_test_split\n    from sklearn import datasets\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy\n\n    bc = datasets.load_breast_cancer()\n    X, y = bc.data, bc.target\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1234\n    )\n\n    regressor = LogisticRegression(learning_rate=0.00005, n_iters=1000) # think they forgot 2x in grad updates, halving lr to get same accuracy\n    regressor.fit(X_train, y_train)\n    predictions = regressor.predict(X_test)\n\n    print(\"LR classification accuracy:\", accuracy(y_test, predictions))\n\nLR classification accuracy: 0.9298245614035088\n\n\n\nPyTorch version is not very numerically stable, increased precision to torch.float64 to match that of numpy, experimented with different random seeds until found one of the better ones. BCEWithLogitsLoss may help the issue in the future. Either way, there seems to be a about 3 different ‘valleys’ where the algorithm ends up ‘converging’, likely local minima on the loss surface.\n\n\nclass LogisticRegressionTorch(nn.Module):\n    def __init__(self, input_dims):\n        super().__init__()\n        self.ff = nn.Linear(input_dims,1,dtype=torch.float64)\n        self.sigm = nn.Sigmoid()\n\n    def forward(self, X):\n       return self.sigm(self.ff(X))\n\n\n# Imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\n\nbc = datasets.load_breast_cancer()\nX, y = torch.tensor(bc.data,dtype = torch.float64), torch.tensor(bc.target, dtype = torch.float64).view(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1234\n)\ntorch.manual_seed(7777777) # a bit unstable, depending on random seed, this is one of the better ones\nn_iters, lr = 10000, .00005\nn_examples, n_feats = X.shape\nlog_reg = LogisticRegressionTorch(n_feats)\nloss_fn = nn.BCELoss() # may consider nn.BCEWithLogitsLoss() for numeric stability\noptimizer = torch.optim.SGD(log_reg.parameters(),lr)\nfor _ in range(n_iters):\n    optimizer.zero_grad()\n    y_pred = log_reg(X_train).view(-1,1)\n    loss = loss_fn(y_pred,y_train)\n    loss.backward()\n    optimizer.step()\n\nwith torch.no_grad():\n    y_pred_test = log_reg(X_test)\n    y_pred_test_class = y_pred_test.round()\n    print(list(zip(y_pred_test_class,y_test)))\n\ncorrect = (y_pred_test_class==y_test).float()\naccuracy = correct.sum()/len(correct)\nprint(\"LR classification accuracy:\", accuracy.item())\n\n[(tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([1.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([1.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64)), (tensor([0.], dtype=torch.float64), tensor([0.], dtype=torch.float64))]\nLR classification accuracy: 0.9298245906829834"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#pca",
    "href": "notes/ml-algos-numpy-torch.html#pca",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "PCA",
    "text": "PCA\n\nSubtract the mean from X\nCalculate Cov(X,X)\nCalculate eigenvectors and eigenvalues of the covariance matrix\nSort the eigenvectors according to their eigenvalues in decreasing order\nChoose the first k eigenvectors as the new k dimensions\nTransform the original n-dimensional data points into k dimensions by projecting with dot product\n\nSome explanation for why this works from GPT 4omini, found points 3 and 4 to be particularly useful:\nIn PCA, we start with a dataset represented as a matrix, and we want to identify the directions (principal components) in which the data varies the most. Here’s how eigenvectors and eigenvalues come into play:\n\nCovariance Matrix: The covariance matrix of the dataset captures how the variables vary with respect to each other. When you compute the covariance matrix, you’re essentially summarizing the relationships and variances of the dimensions.\nEigenvalues and Eigenvectors: When you find the eigenvalues and eigenvectors of the covariance matrix, the eigenvectors represent the directions of maximum variance (principal components), and the corresponding eigenvalues quantify the amount of variance in those directions. Specifically:\n\nAn eigenvector with a larger eigenvalue indicates that there is more variance in the data along that direction.\nConversely, a smaller eigenvalue indicates less variance.\n\nPreservation of Variance: The reason the eigenvectors corresponding to the largest eigenvalues preserve the most variance is that they effectively align with the directions where the data is most spread out. When you project the data onto these eigenvectors (principal components), you are capturing the largest portion of the data’s variability.\nTransformation and Basis: Regarding your mention of transforming the standard basis, think of it this way: The largest eigenvalues indicate how much “stretching” occurs along the directions defined by their corresponding eigenvectors. These are the directions where the data points are furthest apart from each other, hence preserving the variance best. The eigenvalues tell you how much variance is retained when the data is projected onto the eigenvectors.\n\nIn summary, in PCA, the eigenvectors of the covariance matrix with the largest eigenvalues indicate the directions of maximum variance, and projecting data onto these eigenvectors retains the most significant patterns and variability from the original dataset. This is why they are so crucial for dimensionality reduction while retaining important information.\n\nclass PCA:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n\n    def fit(self, X, debug = False):\n        self.X_bar = np.mean(X,axis=0) # store so as to be able to subtract it later in transform\n        X-= self.X_bar  #subtract teh mean from X\n        cov = X.T @ X #calculate cov of X\n        eigenvalues, eigenvectors = np.linalg.eig(cov) #calculate eigenvectors/eigenvalues\n        if debug: print(\"eigenvalues, eigenvectors \", eigenvalues, eigenvectors)\n        eigenvectors = eigenvectors[np.argsort(eigenvalues)[::-1]] # Sort the eigenvectors according to their eigenvalues in decreasing order\n        if debug: print(eigenvectors) \n        self.components = eigenvectors[:self.n_components] # Choose the first k eigenvectors as the new k dimensions\n        if debug: print(self.components) \n           \n\n    def transform(self, X):\n        X -= self.X_bar # Must subtract the mean to center new data consistently!\n        # Transform the original n-dimensional data points into k dimensions by projecting with dot product \n        return X @ self.components.T     \n       \n# Testing\nif __name__ == \"__main__\":\n    # Imports\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n\n    # data = datasets.load_digits()\n    data = datasets.load_iris()\n    X = data.data\n    y = data.target\n\n    # Project the data onto the 2 primary principal components\n    pca = PCA(2)\n    pca.fit(X,debug = True)\n    X_projected = pca.transform(X)\n\n    print(\"Shape of X:\", X.shape)\n    print(\"Shape of transformed X:\", X_projected.shape)\n\n    x1 = X_projected[:, 0]\n    x2 = X_projected[:, 1]\n\n    plt.scatter(\n        x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n    )\n\n    plt.xlabel(\"Principal Component 1\")\n    plt.ylabel(\"Principal Component 2\")\n    plt.colorbar()\n    plt.show()\n\neigenvalues, eigenvectors  [630.0080142   36.15794144  11.65321551   3.55142885] [[ 0.36138659 -0.65658877 -0.58202985  0.31548719]\n [-0.08452251 -0.73016143  0.59791083 -0.3197231 ]\n [ 0.85667061  0.17337266  0.07623608 -0.47983899]\n [ 0.3582892   0.07548102  0.54583143  0.75365743]]\n[[ 0.36138659 -0.65658877 -0.58202985  0.31548719]\n [-0.08452251 -0.73016143  0.59791083 -0.3197231 ]\n [ 0.85667061  0.17337266  0.07623608 -0.47983899]\n [ 0.3582892   0.07548102  0.54583143  0.75365743]]\n[[ 0.36138659 -0.65658877 -0.58202985  0.31548719]\n [-0.08452251 -0.73016143  0.59791083 -0.3197231 ]]\nShape of X: (150, 4)\nShape of transformed X: (150, 2)\n\n\n/tmp/ipykernel_6079/3178312349.py:47: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n\n\n\n\n\n\n\n\n\n\nclass PCATorch:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n\n    def fit(self, X, debug = False):\n        self.X_bar = torch.mean(X,axis=0) # store so as to be able to subtract it later in transform\n        X-= self.X_bar  #subtract teh mean from X\n        cov = X.T @ X #calculate cov of X\n        eigenvalues_complex, eigenvectors_complex = torch.linalg.eig(cov) #calculate eigenvectors/eigenvalues; return complex\n        eigenvalues, eigenvalues_imag = eigenvalues_complex.real, eigenvalues_complex.imag\n        eigenvectors, eigenvectors_imag = eigenvectors_complex.real, eigenvectors_complex.imag\n        assert torch.allclose(eigenvalues_imag, torch.zeros_like(eigenvalues_imag),atol=1e-6), \"Imaginary parts of eigenvalues are not zero\"\n        assert torch.allclose(eigenvectors_imag, torch.zeros_like(eigenvectors_imag),atol=1e-6), \"Imaginary parts of eigenvectors are not zero\"\n        if debug: print(\"eigenvalues, eigenvectors \", eigenvalues, eigenvectors)\n        eigenvectors = eigenvectors[torch.argsort(eigenvalues,descending=True)] # Sort the eigenvectors according to their eigenvalues in decreasing order\n        if debug: print(eigenvectors) \n        self.components = eigenvectors[:self.n_components] # Choose the first k eigenvectors as the new k dimensions\n        if debug: print(self.components) \n           \n\n    def transform(self, X):\n        X -= self.X_bar # Must subtract the mean to center new data consistently!\n        # Transform the original n-dimensional data points into k dimensions by projecting with dot product \n        return X @ self.components.T     \n       \n# Testing\nif __name__ == \"__main__\":\n    # Imports\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n\n    # data = datasets.load_digits()\n    data = datasets.load_iris()\n    X = torch.tensor(data.data,dtype = torch.float32)\n    y = torch.tensor(data.target, dtype=torch.float32)\n\n    # Project the data onto the 2 primary principal components\n    pca = PCATorch(2)\n    pca.fit(X,debug = True)\n    X_projected = pca.transform(X)\n\n    print(\"Shape of X:\", X.shape)\n    print(\"Shape of transformed X:\", X_projected.shape)\n\n    x1 = X_projected[:, 0]\n    x2 = X_projected[:, 1]\n\n    plt.scatter(\n        x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n    )\n\n    plt.xlabel(\"Principal Component 1\")\n    plt.ylabel(\"Principal Component 2\")\n    plt.colorbar()\n    plt.show()\n\neigenvalues, eigenvectors  tensor([630.0081,  36.1579,  11.6532,   3.5514]) tensor([[-0.3614, -0.6566, -0.5820,  0.3155],\n        [ 0.0845, -0.7302,  0.5979, -0.3197],\n        [-0.8567,  0.1734,  0.0762, -0.4798],\n        [-0.3583,  0.0755,  0.5458,  0.7537]])\ntensor([[-0.3614, -0.6566, -0.5820,  0.3155],\n        [ 0.0845, -0.7302,  0.5979, -0.3197],\n        [-0.8567,  0.1734,  0.0762, -0.4798],\n        [-0.3583,  0.0755,  0.5458,  0.7537]])\ntensor([[-0.3614, -0.6566, -0.5820,  0.3155],\n        [ 0.0845, -0.7302,  0.5979, -0.3197]])\nShape of X: torch.Size([150, 4])\nShape of transformed X: torch.Size([150, 2])\n\n\n/tmp/ipykernel_6079/1788467269.py:51: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#decision-tree",
    "href": "notes/ml-algos-numpy-torch.html#decision-tree",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nTraining:\n\nCalculate information gain (IG) with each possible split. IG = Entropy(parent) - weighted avg of Entropy(children). Entropy = - Sum(p(X)*log(p(X))), where p(X)=(#x/n)\nDivide set with that feature and value that gives the greatest IG\nDivide tree and do the same for all created branches\n… repeat until reach stopping criteria (max depth, min # samples, min impurity decrease).\n\nTesting:\n\nFollow the tree until you reach a leaf node\nReturn the most common class label\n\nNot implementing this in PyTorch since it’s a mostly iterative algorithm and don’t expect a gain from GPU parallelization.\nCode overview:\n\nMake a Node class (along with feature, threshold, left, right, value properties) along including a is_leaf_node helper identifying nodes with a value as leaf nodes.\nMake a DecisionTree class containing min_samples_split, max_depth, n_feats, and root properties, as well as the rest of the functionality.\n\nThe fit method would just make sure self.n_feats is set correctly by not exceeding self.n_feats passed at initialization and call the *_fit* recursive helper.\nThe *_fit* method\n\nGets n_labels via len(np.unique(y)) from y, n_samples, n_feats from X.shape.\nGets the leaf_value via collections.Counter to use for base case of returning Node(value=leaf_value) if depth or min samples split conditions are met (or there’s just one label).\n\nFinds indices of features to split on via np.random.choice and calls *_best_criteria* method, which iteratively finds max gain along with best_feat_idx, best_thresh by looping through indices and thresholds via np.unique(X_col). It uses *_information_gain* and entropy helpers (see below) for finding the max gain.\nbest_feat_idx and best_thresh are used to *_split* X at best_feat_idx based on best_thresh. The *_split* method simply calls np.argwhere to find the left_idxs and right_idxs.\nThen left = self._fit(X[left_idxs, :], y[left_idxs], depth + 1), and right is defined similarly. Return Node(feature=best_feat_idx, threshold=best_thresh, left=left, right=right)\nInformation Gain is Entropy(parent) - weighted avg of Entropy(children). Use *_split* to get left_idxs and right_idxs along with a simple weighted average calculations, just make sure to return 0 if left_idxs or right_idxs are empty.\n\nEntropy = - Sum(p(Y)*log(p(Y))), where p(Y)=(#y/n) can be computed via np.bincount(y)\n\nThe predict method calls *_traverse_tree* for each example x in X, starting traversal at the root.\n\n*_traverse_tree* is recursive, traversing at node.left if x[node.feature] &lt;= node.threshold, similarly for right. Keep example x the same, of course, since traverse from root to leaf node for each example. Base case is node.leaf_node(), in which case return node.value.\n\n\n\n\n\nclass Node:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n        \"\"\"\n        Initialize a node in the decision tree.\n        \"\"\"\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\n    def is_leaf_node(self):\n        \"\"\"\n        Check if the node is a leaf node.\n        \"\"\"\n        return self.value is not None\n\nclass DecisionTree:\n    def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n        \"\"\"\n        Initialize the decision tree.\n        \"\"\"\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.root = None\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the decision tree to the data by calling the _fit helper.\n        \"\"\"\n        self.n_feats = X.shape[1] if not self.n_feats else min(X.shape[1], self.n_feats)\n        self.root = self._fit(X, y)\n\n    def _fit(self, X, y, depth=0):\n        \"\"\"\n        Recursively grow the decision tree using _best_criteria and _split helpers. \n        \"\"\"\n        n_labels = len(np.unique(y))\n        n_samples, n_feats = X.shape\n        \n        # Check stopping criteria\n        leaf_value = collections.Counter(y).most_common(1)[0][0]\n        if depth &gt;= self.max_depth or n_samples &lt; self.min_samples_split or n_labels == 1:\n            return Node(value=leaf_value)\n\n        # Find the best split\n        feat_idxs = np.random.choice(n_feats, size=self.n_feats, replace=False)\n        best_feat_idx, best_thresh = self._best_criteria(X, y, feat_idxs)\n\n        # Create child nodes recursively\n        left_idxs, right_idxs = self._split(X[:, best_feat_idx], best_thresh)\n        left = self._fit(X[left_idxs, :], y[left_idxs], depth + 1)\n        right = self._fit(X[right_idxs, :], y[right_idxs], depth + 1)\n        return Node(feature=best_feat_idx, threshold=best_thresh, left=left, right=right)\n\n    def _best_criteria(self, X, y, feat_idxs):\n        \"\"\"\n        Find the best split criteria.\n        \"\"\"\n        max_gain = -float('inf')\n        best_feat_idx, best_thresh = None, None\n        for feat_idx in feat_idxs:\n            X_col = X[:, feat_idx]\n            thresholds = np.unique(X_col)\n            for thresh in thresholds:\n                gain = self._information_gain(y, X_col, thresh)\n                if gain &gt; max_gain:\n                    max_gain, best_feat_idx, best_thresh = gain, feat_idx, thresh\n        return best_feat_idx, best_thresh\n    \n    def _split(self, X_column, split_thresh):\n        \"\"\"\n        Split the data based on the split threshold.\n        \"\"\"\n        left_idxs = np.argwhere(X_column &lt;= split_thresh).flatten()\n        right_idxs = np.argwhere(X_column &gt; split_thresh).flatten()\n        return left_idxs, right_idxs\n\n    @staticmethod\n    def entropy(y):\n        \"\"\"\n        Calculate the entropy of a label array.\n        \"\"\"\n        hist = np.bincount(y)\n        ps = hist / len(y)\n        return -np.sum([p * np.log2(p) for p in ps if p &gt; 0])\n\n    def _information_gain(self, y, X_column, split_thresh):\n        \"\"\"\n        Calculate the information gain of a split.\n        \"\"\"\n        parent_entropy = DecisionTree.entropy(y)\n        left_idxs, right_idxs = self._split(X_column, split_thresh)\n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 0\n\n        n = len(y)\n        n_left, n_right = len(left_idxs), len(right_idxs)\n        e_left, e_right = DecisionTree.entropy(y[left_idxs]), DecisionTree.entropy(y[right_idxs])\n        children_entropy = (n_left * e_left + n_right * e_right) / n\n        return parent_entropy - children_entropy\n\n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for the input data.\n        \"\"\"\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n\n    def _traverse_tree(self, x, node):\n        \"\"\"\n        Traverse the tree to make a prediction.\n        \"\"\"\n        if node.is_leaf_node():\n            return node.value\n        if x[node.feature] &lt;= node.threshold:\n            return self._traverse_tree(x, node.left)\n        else:\n            return self._traverse_tree(x, node.right)\n\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        \"\"\"\n        Calculate the accuracy of predictions.\n        \"\"\"\n        return np.sum(y_true == y_pred) / len(y_true)\n\n    # Load dataset\n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    # Split dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n    # Initialize and train the decision tree\n    clf = DecisionTree(max_depth=20)\n    clf.fit(X_train, y_train)\n\n    # Make predictions and calculate accuracy\n    y_pred = clf.predict(X_test)\n    acc = accuracy(y_test, y_pred)\n\n    print(\"Accuracy:\", acc)\n\nAccuracy: 0.9298245614035088"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#random-forest",
    "href": "notes/ml-algos-numpy-torch.html#random-forest",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "Random Forest",
    "text": "Random Forest\n\nDeviating from mlfromscratch/random_forest.py implementation by adding max_samples, which is customary in standard random forest implementations such as scikit’s. This allows the forest to get a greater sample variety during bootstrapping by making the rows selection random, in addition to feature selection, which is random for each decision tree.\nfit method loops over self.n_trees, taking a random sample of rows via np.random.choice, making X_sample and y_sample with it, constructing each DecisionTree, then fitting it on the samples. It appends each fitted tree to self.trees.\n\npredict method constructs an array of predictions from each individual tree predictions, swaps axes with np.swapaxes since we want all predictions for each sample to be grouped together, then returns array of most common labels for each pred (using the same logic as finding most common label for each decision tree).\n\n\nclass RandomForest:\n    def __init__(self, n_trees=10, min_samples_split=2, max_samples = 10, max_depth=100, n_feats=None):\n        self.n_trees = n_trees\n        self.min_samples_split = min_samples_split\n        self.max_samples = max_samples # add to be consistent with most implementations, including scikit's, choose total of max_samples from all samples for each tree\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            n_samples = X.shape[0]\n            self.max_samples = min(X.shape[1], self.max_samples)\n            bootstrap_examples_idxs = np.random.choice(n_samples, self.max_samples, replace=True) # bootstrap samples -&gt; replace = True\n            X_sample, y_sample = X[bootstrap_examples_idxs,:], y[bootstrap_examples_idxs] # unlike in tree, choosing ROWS/examples!  In tree, choose features.\n            tree = DecisionTree(min_samples_split=self.min_samples_split, max_depth=self.max_depth, n_feats=self.n_feats)\n            tree.fit(X_sample,y_sample)\n            self.trees.append(tree)\n\n\n    def predict(self, X):\n        # will have [[x1_tree1_pred,x2_tree1_pred,x3_tree1_pred,...],[x1_tree2_pred,x2_tree2_pred,x3_tree2_pred,...],...]\n        # want [[x1_tree1_pred,x1_tree2_pred,x1_tree3_pred,...],[x2_tree1_pred,x2_tree2_pred,x2_tree3_pred,...],...] -&gt; np.swapaxes\n        # In fit, each tree has been fit on the training data, now use tree's own fit method to make the prediction on new data.\n        preds = np.array([tree.predict(X) for tree in self.trees])\n        preds = np.swapaxes(preds,0,1)\n        return np.array([self.most_common_label(pred) for pred in preds])\n\n    def most_common_label(self,y):\n        return collections.Counter(y).most_common(1)[0][0]\n\n\n# Testing\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X = data.data\n    y = data.target\n    print(X.shape)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1234\n    )\n    np.random.seed(117)\n    clf = RandomForest(n_trees=5, max_depth=10, max_samples=25) #simple data, hard to surpass DecisionTree accuracy\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    acc = accuracy(y_test, y_pred)\n\n    print(\"Accuracy:\", acc)\n\n(569, 30)\nAccuracy: 0.9298245614035088"
  },
  {
    "objectID": "notes/ml-algos-numpy-torch.html#svm",
    "href": "notes/ml-algos-numpy-torch.html#svm",
    "title": "ML Algorithms in Numpy and PyTorch",
    "section": "SVM",
    "text": "SVM\n\n\n\nSVM Objective, source: https://www.youtube.com/watch?v=T9UcK-TxQGw\n\n\n\nTraining:\n\nInitialize the weights\nMake sure y is in set([-1,1])\nApply grad update rules for n iterations\n\nIf y (wx-b)&gt;=1, dw = 2 lambda w, db = 0\nElse, dw = 2 lambda w - y x, db = y\n\n\nPrediction:\n\nCalculate y = sign(wx-b)\n\n\n\nimport numpy as np\n\n\nclass SVM:\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_feats = X.shape[1]\n        self.w, self.b = np.zeros(n_feats), 0\n        y = np.where(y &lt;=0, -1, 1)\n        for _ in range(self.n_iters):\n            for idx, x_i in enumerate(X):\n                margin = y[idx]*(np.dot(x_i,self.w)-self.b)\n                dw = np.where(margin&gt;=1, 2*self.lambda_param*self.w, 2*self.lambda_param*self.w - np.dot(y[idx],x_i))\n                db = np.where(margin&lt;1, 0, y[idx])\n                self.w -= self.lr * dw\n                self.b -= self.lr * db\n\n\n    def predict(self, X):\n        return np.sign(X @ self.w+self.b)\n\n# Testing\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn import datasets\n    import matplotlib.pyplot as plt\n\n    X, y = datasets.make_blobs(\n        n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40\n    )\n    y = np.where(y == 0, -1, 1)\n\n    clf = SVM()\n    clf.fit(X, y)\n    predictions = clf.predict(X)\n\n    print(clf.w, clf.b)\n\n    def visualize_svm():\n        def get_hyperplane_value(x, w, b, offset):\n            return (-w[0] * x + b + offset) / w[1]\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n\n        x0_1 = np.amin(X[:, 0])\n        x0_2 = np.amax(X[:, 0])\n\n        x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n        x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n        x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n        x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n        x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n        x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n        ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n        ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n        ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n\n        x1_min = np.amin(X[:, 1])\n        x1_max = np.amax(X[:, 1])\n        ax.set_ylim([x1_min - 3, x1_max + 3])\n\n        plt.show()\n\n    visualize_svm()\n\n[0.65500741 0.14190535] 0.17900000000000013\n\n\n\n\n\n\n\n\n\n\nclass SVMVectorized:\n    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.lambda_param = lambda_param\n        self.n_iters = n_iters\n        self.w = None\n        self.b = None\n\n    def fit(self, X, y):\n        n_feats = X.shape[1]\n        self.w, self.b = np.zeros(n_feats), 0\n        y = np.where(y &lt;= 0, -1, 1)\n        \n        for _ in range(self.n_iters):\n            margins = y * (X @ self.w-self.b)\n            misclassified = margins &lt; 1 \n\n            self.w -= self.lr*(2*self.lambda_param*self.w - np.dot(misclassified*y,X)) #correctly classified examples have hinge loss 0\n            self.b -= self.lr*np.sum(misclassified*y)\n\n    def predict(self, X):\n        return np.sign(X @ self.w -self.b)\n\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn import datasets\n    import matplotlib.pyplot as plt\n\n    X, y = datasets.make_blobs(\n        n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40\n    )\n    y = np.where(y == 0, -1, 1)\n\n    clf = SVMVectorized()\n    clf.fit(X, y)\n    predictions = clf.predict(X)\n\n    print(clf.w, clf.b)\n\n    def visualize_svm():\n        def get_hyperplane_value(x, w, b, offset):\n            return (-w[0] * x + b + offset) / w[1]\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n\n        x0_1 = np.amin(X[:, 0])\n        x0_2 = np.amax(X[:, 0])\n\n        x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n        x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n\n        x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n        x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n\n        x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n        x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n\n        ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n        ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n        ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n\n        x1_min = np.amin(X[:, 1])\n        x1_max = np.amax(X[:, 1])\n        ax.set_ylim([x1_min - 3, x1_max + 3])\n\n        plt.show()\n\n    visualize_svm()\n\n[0.60926001 0.16851888] -0.054000000000000034\n\n\n\n\n\n\n\n\n\n\nclass SVMTorch(nn.Module):\n    def __init__(self, input_dim, lambda_param=0.01):\n        super().__init__()\n        self.lin = nn.Linear(input_dim,1)\n        self.lambda_param = lambda_param\n    def forward(self, X):\n        return self.lin(X)\n    def hinge_loss(self, y_pred, y):\n        h_loss = torch.mean(torch.clamp(1-y*y_pred,min=0))\n        h_reg = self.lambda_param*0.5*torch.linalg.vector_norm(self.lin.weight, ord=2)**2\n        return h_reg + h_loss\n\n        \n\n\nif __name__ == \"__main__\":\n    # Imports\n    from sklearn import datasets\n    import matplotlib.pyplot as plt\n\n    X, y = datasets.make_blobs(\n        n_samples=50, n_features=2, centers=2, cluster_std=1.05, random_state=40\n    )\n    y = np.where(y == 0, -1, 1)\n    X, y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n\n    # training loop\n    n_iters, lr, lambda_param = 1000, 0.001, 0.01\n    clf = SVMTorch(input_dim=X.shape[1], lambda_param=lambda_param)\n    optimizer = torch.optim.SGD(params = clf.parameters(), lr=lr)\n    for _ in range(n_iters):\n        optimizer.zero_grad()\n        y_pred = clf.forward(X).squeeze()\n        loss = clf.hinge_loss(y_pred,y)\n        loss.backward()\n        optimizer.step()\n    # preds\n    with torch.no_grad():\n        predictions = torch.sign(X @ clf.lin.weight.T - clf.lin.bias)\n\n    print(clf.lin.weight, clf.lin.bias)\n\n    def visualize_svm(X,y,clf):\n        def get_hyperplane_value(x, w, b, offset):\n            return (-w[0] * x + b + offset) / w[1]\n\n        fig = plt.figure()\n        ax = fig.add_subplot(1, 1, 1)\n        X, y = X.cpu().numpy(), y.cpu().numpy()\n        w, b = clf.lin.weight.cpu().detach().numpy().flatten(), clf.lin.bias.cpu().detach().numpy().flatten()\n        plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y)\n       \n        x0_1 = np.amin(X[:, 0])\n        x0_2 = np.amax(X[:, 0])\n\n        x1_1 = get_hyperplane_value(x0_1, w, b, 0)\n        x1_2 = get_hyperplane_value(x0_2, w, b, 0)\n\n        x1_1_m = get_hyperplane_value(x0_1, w, b, -1)\n        x1_2_m = get_hyperplane_value(x0_2, w, b, -1)\n\n        x1_1_p = get_hyperplane_value(x0_1, w, b, 1)\n        x1_2_p = get_hyperplane_value(x0_2, w, b, 1)\n\n        ax.plot([x0_1, x0_2], [x1_1, x1_2], \"y--\")\n        ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"k\")\n        ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"k\")\n\n        x1_min = np.amin(X[:, 1])\n        x1_max = np.amax(X[:, 1])\n        ax.set_ylim([x1_min - 3, x1_max + 3])\n\n        plt.show()\n\n    visualize_svm(X,y,clf)\n\nParameter containing:\ntensor([[0.7446, 0.2005]], requires_grad=True) Parameter containing:\ntensor([-0.3815], requires_grad=True)"
  },
  {
    "objectID": "notes/Linux.html",
    "href": "notes/Linux.html",
    "title": "Linux notes",
    "section": "",
    "text": "Ways of installing packages on Ubuntu:\n\n\n\n\nimage.png\n\n\n\nso which nvcc yields /usr/bin/nvcc while importing trl got\n“name”: “RuntimeError”, “message”: “Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\n\nFileNotFoundError: [Errno 2] No such file or directory: ‘/usr/local/cuda/bin/nvcc’ - Fixed by making a symbolic link: sudo mkdir -p /usr/local/cuda/bin /usr/local/bin$ sudo ln -s /usr/bin/nvcc /usr/local/cuda/bin/nvcc\n\nAll of the sudden lost GPU, could not figure it out, when tried to purge, saw: sudo apt-get purge nvidia sudo apt-get autoremove sudo apt-get autoclean [sudo] password for mainuser: E: dpkg was interrupted, you must manually run ‘sudo dpkg –configure -a’ to correct the problem. E: dpkg was interrupted, you must manually run ‘sudo dpkg –configure -a’ to correct the problem. Reading package lists… Done Building dependency tree… Done Reading state information… Done When I reran the recommended command, the drivers were alive and well!*\n\nllll"
  },
  {
    "objectID": "notes/FastAPI.html",
    "href": "notes/FastAPI.html",
    "title": "FastAPI notes",
    "section": "",
    "text": "Build APIs based on standard Python type hints\nAutomatically generate interactive documentation\nFast to code, fewer bugs\n\n\n#! pip install fastapi\n#! pip install \"uvicorn[standard]\"\n\n\nIf the following contents are in main.py, can run via uvicorn main:app --reload.\n\nmain refers to main.py and app refers to the object inside main.py.\n--reload reloads the page upon changes, to be used during dev, not prod.\n\nCan see documentation conforming to OpenAPI standard in http://127.0.0.1:8000/docs, from which you can use the endpoints!\nhttp://127.0.0.1:8000/redoc returns documentation in alternative format.\nUse async def to make the functions non-blocking, enabling other tasks to run concurrently. Useful when function performs I/O-bound operations, such as database queries, file I/O, or network requests, and when need to handle a large number of concurrent requests efficiently.\nType hints will be validated with Pydantic, so if use a non-int in /items/{item_id}, will get an error.\nOrder matters: If read_user_current is placed after read_user, will get an error since FastAPI will read functions top-down and try to validate input to be an integer.\nUse Enums if path parameter must come from a certain list of values. If improper parameter is passed, FastAPI will list available values!\nTo have paths be read correctly, use :path path converter, allowing the parameter to capture the entire path, including slashes.\nread_animal without additional parameters will read off animals 0-10. With additional parameters, can specify which ones we want via query parameters, as in http://127.0.0.1:8000/animals/?skip=0&limit=2. Here, ? denotes start of query parameters and & separates them. Can also pass optional parameter as http://127.0.0.1:8000/animals/?skip=0&limit=2&optional_param=3, just make sure to specify it as typing.Optional.\nCan pass and use optional parameters as in read_user_item.\nRequest body is data sent by client to the API and response body is data sent from API to client. Use Pydantic to specify request body with POST request type.\n\nTo send a post request, could test it out in /docs or with curl -X POST “http://127.0.0.1:8000/books/” -H “Content-Type: application/json” -d ‘{ “name”: “The Great Gatsby”, “author”: “F. Scott Fitzgerald”, “description”: “A novel set in the 1920s”, “price”: 10.99 }’\nThen can go to /books endpoint to see the books printed.\n\n\n\nfrom fastapi import FastAPI\nfrom enum import Enum\nimport typing as t\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n@app.get(\"/\") #route/endpoint\ndef home_page():\n    return {\"message\":\"Hello World!\"}\n\n@app.get(\"/items/{item_id}\") #item_id is the path parameter\nasync def read_item(item_id: int):\n    return {\"item_id\":item_id}\n\n@app.get(\"/users/me\") # will not work if placed after, must be before to be valid\nasync def read_user_current():\n    return {\"user_id\":\"Current user\"}\n\n@app.get(\"/users/{user_id}\") \nasync def read_user(user_id: int):\n    return {\"user_id\":user_id}\n\nclass ModelName(str,Enum):\n    ALEXNET = 'ALEXNET'\n    RESNET = 'RESNET'\n    LENET = 'LENET'\n\n@app.get(\"/models/{model_name}\")\nasync def get_model(model_name: ModelName):\n    if model_name == ModelName.ALEXNET:\n        return {'model_name':model_name}\n    elif model_name.value == \"LENET\":\n        return {'model_name': model_name}\n    else:\n        return {'model_name':f\"You have selected {model_name.value}\"}\n    \n@app.get(\"files/{file_path:path}\")\nasync def read_file(file_path:str):\n    return {\"file_path\":file_path}\n\nanimal_db = [{\"animal_name\":'cat'},{\"animal_name\":'llama'},{\"animal_name\":'alpaca'}]\n\n@app.get(\"/animals/\")\nasync def read_animal(skip: int=0, limit: int=10, optional_param: t.Optional[int]=None):\n    return {\"animals\": animal_db[skip:skip+limit], \"optional_parameter\":optional_param}\n\n@app.get(\"/users/{user_id}/items/{item_id}\")\nasync def read_user_item(\n    user_id: int, item_id: int, q: t.Optional[str]=None, short:bool=False\n):\n    item = {\"item_id\":item_id, \"owner_id\":user_id}\n    if q:\n        item.update({\"q\":q})\n    if not short:\n        item.update({'description':'great item with long description'})\n    return item\n\nbooks_db = []\nclass Book(BaseModel):\n    name:str\n    author:str\n    description:t.Optional[str]\n    price:float\n\n@app.post(\"/books/\")\nasync def create_item(book:Book):\n    books_db.append(book)\n    return book \n@app.get(\"/books/\")\nasync def get_books():\n    return books_db\n\n\n\nQ: What’s the difference between WSGI and ASGI gateways as it pertains to Django and FastAPI? WSGI (Web Server Gateway Interface) and ASGI (Asynchronous Server Gateway Interface) are two different specifications for Python web servers and applications. They serve as interfaces between web servers and web applications or frameworks. Here’s a detailed comparison of WSGI and ASGI, particularly in the context of Django and FastAPI:\n\nWSGI (Web Server Gateway Interface) Synchronous:\n\nWSGI is designed for synchronous web applications. It handles one request at a time per worker, which can lead to inefficiencies when dealing with I/O-bound operations like database queries or external API calls. Django:\nDjango is traditionally a WSGI-based framework. It works well for most web applications but can struggle with real-time features like WebSockets or long-polling due to its synchronous nature. Common WSGI servers for Django include Gunicorn and uWSGI. Concurrency:\nWSGI applications handle concurrency by using multiple worker processes or threads. Each worker handles one request at a time. Deployment:\nWSGI applications are typically deployed using WSGI servers like Gunicorn, uWSGI, or mod_wsgi (for Apache). - ASGI (Asynchronous Server Gateway Interface) Asynchronous:\nASGI is designed for asynchronous web applications. It supports both synchronous and asynchronous code, allowing for more efficient handling of I/O-bound operations and real-time features. FastAPI:\nFastAPI is an ASGI-based framework. It is built from the ground up to support asynchronous programming, making it ideal for applications that require high concurrency, real-time communication, or WebSockets. Common ASGI servers for FastAPI include Uvicorn and Daphne. Concurrency:\nASGI applications can handle many requests concurrently using asynchronous I/O. This allows for more efficient use of resources, especially for I/O-bound tasks. Deployment:\nASGI applications are typically deployed using ASGI servers like Uvicorn, Daphne, or Hypercorn.\n\n#!pip install nest_asyncio # run asyncio within Jupyter's already running even loop\n\nRequirement already satisfied: nest_asyncio in /home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages (1.6.0)\n\n\n\n# import asyncio\n# async def printer(name: str, times: int)-&gt;None:\n#     for i in range(times):\n#         print(name)\n#         await asyncio.sleep(1)\n# async def main():\n#     await asyncio.gather(\n#         printer(\"A\",3),\n#         printer(\"B\",3)\n#     )\n# asyncio.run(main())\n\n# adopting code since Jupyter has it's own event loop\nimport asyncio\nimport nest_asyncio\n\n# Apply nest_asyncio to allow nested event loops\nnest_asyncio.apply()\n\nasync def printer(name: str, times: int) -&gt; None:\n    for i in range(times):\n        print(name)\n        await asyncio.sleep(1)\n\nasync def main():\n    await asyncio.gather(\n        printer(\"A\", 3),\n        printer(\"B\", 3)\n    )\n\n# Await the main coroutine directly\nawait main()\n\nA\nB\nA\nB\nA\nB\n\n\n\nasyncio.sleep(1) was added since writing code in a coroutine doesn’t necessarily mean it will not block. Computations are blocking! I/O opps will not block or we could use multiprocessing.\nPath parameters and their validation\n\n\nfrom fastapi import FastAPI, Path\napp = FastAPI()\n\n@app.get('/license-plates/{license}')\nasync def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n    return {\"license\":license}\n\n/tmp/ipykernel_6460/3090869963.py:5: DeprecationWarning: `regex` has been deprecated, please use `pattern` instead\n  async def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n\n\n\nIn FastAPI, … above indicate that we don’t want a default value. RegEx validates French license plates like AB-123-CD."
  },
  {
    "objectID": "notes/FastAPI.html#basics",
    "href": "notes/FastAPI.html#basics",
    "title": "FastAPI notes",
    "section": "",
    "text": "Build APIs based on standard Python type hints\nAutomatically generate interactive documentation\nFast to code, fewer bugs\n\n\n#! pip install fastapi\n#! pip install \"uvicorn[standard]\"\n\n\nIf the following contents are in main.py, can run via uvicorn main:app --reload.\n\nmain refers to main.py and app refers to the object inside main.py.\n--reload reloads the page upon changes, to be used during dev, not prod.\n\nCan see documentation conforming to OpenAPI standard in http://127.0.0.1:8000/docs, from which you can use the endpoints!\nhttp://127.0.0.1:8000/redoc returns documentation in alternative format.\nUse async def to make the functions non-blocking, enabling other tasks to run concurrently. Useful when function performs I/O-bound operations, such as database queries, file I/O, or network requests, and when need to handle a large number of concurrent requests efficiently.\nType hints will be validated with Pydantic, so if use a non-int in /items/{item_id}, will get an error.\nOrder matters: If read_user_current is placed after read_user, will get an error since FastAPI will read functions top-down and try to validate input to be an integer.\nUse Enums if path parameter must come from a certain list of values. If improper parameter is passed, FastAPI will list available values!\nTo have paths be read correctly, use :path path converter, allowing the parameter to capture the entire path, including slashes.\nread_animal without additional parameters will read off animals 0-10. With additional parameters, can specify which ones we want via query parameters, as in http://127.0.0.1:8000/animals/?skip=0&limit=2. Here, ? denotes start of query parameters and & separates them. Can also pass optional parameter as http://127.0.0.1:8000/animals/?skip=0&limit=2&optional_param=3, just make sure to specify it as typing.Optional.\nCan pass and use optional parameters as in read_user_item.\nRequest body is data sent by client to the API and response body is data sent from API to client. Use Pydantic to specify request body with POST request type.\n\nTo send a post request, could test it out in /docs or with curl -X POST “http://127.0.0.1:8000/books/” -H “Content-Type: application/json” -d ‘{ “name”: “The Great Gatsby”, “author”: “F. Scott Fitzgerald”, “description”: “A novel set in the 1920s”, “price”: 10.99 }’\nThen can go to /books endpoint to see the books printed.\n\n\n\nfrom fastapi import FastAPI\nfrom enum import Enum\nimport typing as t\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n@app.get(\"/\") #route/endpoint\ndef home_page():\n    return {\"message\":\"Hello World!\"}\n\n@app.get(\"/items/{item_id}\") #item_id is the path parameter\nasync def read_item(item_id: int):\n    return {\"item_id\":item_id}\n\n@app.get(\"/users/me\") # will not work if placed after, must be before to be valid\nasync def read_user_current():\n    return {\"user_id\":\"Current user\"}\n\n@app.get(\"/users/{user_id}\") \nasync def read_user(user_id: int):\n    return {\"user_id\":user_id}\n\nclass ModelName(str,Enum):\n    ALEXNET = 'ALEXNET'\n    RESNET = 'RESNET'\n    LENET = 'LENET'\n\n@app.get(\"/models/{model_name}\")\nasync def get_model(model_name: ModelName):\n    if model_name == ModelName.ALEXNET:\n        return {'model_name':model_name}\n    elif model_name.value == \"LENET\":\n        return {'model_name': model_name}\n    else:\n        return {'model_name':f\"You have selected {model_name.value}\"}\n    \n@app.get(\"files/{file_path:path}\")\nasync def read_file(file_path:str):\n    return {\"file_path\":file_path}\n\nanimal_db = [{\"animal_name\":'cat'},{\"animal_name\":'llama'},{\"animal_name\":'alpaca'}]\n\n@app.get(\"/animals/\")\nasync def read_animal(skip: int=0, limit: int=10, optional_param: t.Optional[int]=None):\n    return {\"animals\": animal_db[skip:skip+limit], \"optional_parameter\":optional_param}\n\n@app.get(\"/users/{user_id}/items/{item_id}\")\nasync def read_user_item(\n    user_id: int, item_id: int, q: t.Optional[str]=None, short:bool=False\n):\n    item = {\"item_id\":item_id, \"owner_id\":user_id}\n    if q:\n        item.update({\"q\":q})\n    if not short:\n        item.update({'description':'great item with long description'})\n    return item\n\nbooks_db = []\nclass Book(BaseModel):\n    name:str\n    author:str\n    description:t.Optional[str]\n    price:float\n\n@app.post(\"/books/\")\nasync def create_item(book:Book):\n    books_db.append(book)\n    return book \n@app.get(\"/books/\")\nasync def get_books():\n    return books_db\n\n\n\nQ: What’s the difference between WSGI and ASGI gateways as it pertains to Django and FastAPI? WSGI (Web Server Gateway Interface) and ASGI (Asynchronous Server Gateway Interface) are two different specifications for Python web servers and applications. They serve as interfaces between web servers and web applications or frameworks. Here’s a detailed comparison of WSGI and ASGI, particularly in the context of Django and FastAPI:\n\nWSGI (Web Server Gateway Interface) Synchronous:\n\nWSGI is designed for synchronous web applications. It handles one request at a time per worker, which can lead to inefficiencies when dealing with I/O-bound operations like database queries or external API calls. Django:\nDjango is traditionally a WSGI-based framework. It works well for most web applications but can struggle with real-time features like WebSockets or long-polling due to its synchronous nature. Common WSGI servers for Django include Gunicorn and uWSGI. Concurrency:\nWSGI applications handle concurrency by using multiple worker processes or threads. Each worker handles one request at a time. Deployment:\nWSGI applications are typically deployed using WSGI servers like Gunicorn, uWSGI, or mod_wsgi (for Apache). - ASGI (Asynchronous Server Gateway Interface) Asynchronous:\nASGI is designed for asynchronous web applications. It supports both synchronous and asynchronous code, allowing for more efficient handling of I/O-bound operations and real-time features. FastAPI:\nFastAPI is an ASGI-based framework. It is built from the ground up to support asynchronous programming, making it ideal for applications that require high concurrency, real-time communication, or WebSockets. Common ASGI servers for FastAPI include Uvicorn and Daphne. Concurrency:\nASGI applications can handle many requests concurrently using asynchronous I/O. This allows for more efficient use of resources, especially for I/O-bound tasks. Deployment:\nASGI applications are typically deployed using ASGI servers like Uvicorn, Daphne, or Hypercorn.\n\n#!pip install nest_asyncio # run asyncio within Jupyter's already running even loop\n\nRequirement already satisfied: nest_asyncio in /home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages (1.6.0)\n\n\n\n# import asyncio\n# async def printer(name: str, times: int)-&gt;None:\n#     for i in range(times):\n#         print(name)\n#         await asyncio.sleep(1)\n# async def main():\n#     await asyncio.gather(\n#         printer(\"A\",3),\n#         printer(\"B\",3)\n#     )\n# asyncio.run(main())\n\n# adopting code since Jupyter has it's own event loop\nimport asyncio\nimport nest_asyncio\n\n# Apply nest_asyncio to allow nested event loops\nnest_asyncio.apply()\n\nasync def printer(name: str, times: int) -&gt; None:\n    for i in range(times):\n        print(name)\n        await asyncio.sleep(1)\n\nasync def main():\n    await asyncio.gather(\n        printer(\"A\", 3),\n        printer(\"B\", 3)\n    )\n\n# Await the main coroutine directly\nawait main()\n\nA\nB\nA\nB\nA\nB\n\n\n\nasyncio.sleep(1) was added since writing code in a coroutine doesn’t necessarily mean it will not block. Computations are blocking! I/O opps will not block or we could use multiprocessing.\nPath parameters and their validation\n\n\nfrom fastapi import FastAPI, Path\napp = FastAPI()\n\n@app.get('/license-plates/{license}')\nasync def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n    return {\"license\":license}\n\n/tmp/ipykernel_6460/3090869963.py:5: DeprecationWarning: `regex` has been deprecated, please use `pattern` instead\n  async def get_license_plate(id: int = Path(...,regex=r\"^\\w{2}-\\d{3}-\\w{2}\")):\n\n\n\nIn FastAPI, … above indicate that we don’t want a default value. RegEx validates French license plates like AB-123-CD."
  },
  {
    "objectID": "notes/FastAPI.html#notes-by-key-topic",
    "href": "notes/FastAPI.html#notes-by-key-topic",
    "title": "FastAPI notes",
    "section": "Notes by Key Topic",
    "text": "Notes by Key Topic\n\nInstallation, virtual environment (conda), running, first app\n\n%% bash\nconda create --name fastapi-env python=3.11\nconda activate fastapi-env \npip install fastapi[all]\n\n\nIf FastAPI app is called app in main file, run as follows: uvicorn main:app --reload\nAccess interactive documentation using http://127.0.0.1:8000/docs (using Swagger UI) or http://127.0.0.1:8000/redoc (using ReDoc)\n\n\n\nDefining routes with path and query parameters (for user input) and validating requests.\n\nDefining path parameters: user_id is a path parameter FastAPI will convert to an integer.\n\n\nfrom fastapi import FastAPI\napp = FastAPI() \n\n@app.get(\"/users/{user_id}\")\ndef read_user(user_id:int):\n    return {\"user_id\":user_id}\n\n\nDefining query parameters via function parameters with default values:\n\n\n@app.get(\"/users/\")\ndef read_user(skip:int=0,limit:int=10):\n    return {\"skip\":skip, \"limit\":limit}\n\nIf a user accesses /users/?skip=5&limit=15, FastAPI will return {\"skip\":5, \"limit\":15}\n\nRequest validation with Pydantic below. Make the request as follows:\ncurl -X POST \"http://127.0.0.1:8000/users/\" -H \"Content-Type: application/json\" -d '{\"id\":1, \"name\":\"John Smith\", \"email\":\"john@example.com\"}'     --X POST: use HTTP method to post data     --H “Content-Type: application/json”`: add HTTP header to the request and specify that the data being sent is in JSON format\n\n-d ‘{“id”:1, “name”:“John Smith”, “email”:“john@example.com”}’: send the specified data in the request body\n\n\n\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\n@app.post(\"/users/\")\ndef create_user(user:User):\n    return {\"id\": user.id, \"name\": user.name, \"email\":user.email}\n\n\nCombining path ahd query parameters with Pydantic: user_id is a path parameter and details is a query parameter that modifies the response.\nRead simply as curl \"http://127.0.0.1:8000/users/1\"\n\n\n@app.get(\"/users/{user_id}\")\ndef read_user(user_id: int, details: bool=False):\n    if details:\n        return {\"user_id\":user_id, \"details\":\"Detailed info\"}\n    return {\"user_id\":user_id}\n\n\n\nRequest and response models\n\nRequest models define the structure of the data that your API expects to receive in the request body. They are used to validate and parse the incoming data.\nResponse models define the structure of the data that your API returns in response. They ensure that the response data is correctly formatted and validated.\ncurl -X POST \"http://127.0.0.1:8000/users/\" -H \"Content-Type: application/json\" -d '{\"id\":1, \"name\":\"John Smith\", \"email\":\"john@example.com\", \"age\":30}'\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import Optional\n\napp = FastAPI()\n\nclass UserCreate(BaseModel):\n    id: int\n    name: str\n    email: str\n    age: Optional[int] = None\n\nclass UserResponse(BaseModel):\n    id: int\n    name: str\n    email: str\n    age: Optional[int] = None\n    is_active:  bool\n\n@app.post(\"/users/\", response_model=UserResponse)\nasync def create_user(user:UserCreate): #validate incoming data\n    user_response = UserResponse(       #validate outgoing data\n        id= user.id,\n        name=user.name,\n        email=user.email,\n        age=user.age,\n        is_active=True\n    )\n    return user_response\n\n\n\nDependency Injection\n\nInject dependencies (database connections, configuration settings, other shared resources) into your functions or classes.\nSeparate concerns between the logic of the endpoint and the more generic logic for the pagination parameters.\nIdeal for utility logic to retrieve or validate data, make security checks, or call external logic that will be needed several times across the application.\nNotes following “Building Data Science Applications with FastAPI” by François Voron Chapter 2: Python specificities -&gt; asyncio\n\n\nfrom fastapi import Depends, FastAPI\n\napp = FastAPI()\n\nasync def pagination(skip:int=0,limit:int=10)-&gt;tuple[int,int]:\n    return (skip,limit)\n\n@app.get(\"/items\")\nasync def list_items(p:tuple[int,int]=Depends(pagination)):\n    skip,limit = p\n    return {\"skip\":skip, \"limit\":limit}\n\n@app.get(\"/things\")\nasync def list_things(p:tuple[int,int]=Depends(pagination)):\n    skip,limit = p\n    return {\"skip\":skip, \"limit\":limit}\n\n\nFastAPI limitation: Depends function is not able to forward the type of the dependency function, so we have to do this manually above.\nRaising a 404 error:\n\n\nfrom fastapi import Depends, FastAPI, HTTPException, status\nfrom pydantic import BaseModel\n\n\nclass Post(BaseModel):\n    id: int\n    title: str\n    content: str\n\n\nclass PostUpdate(BaseModel):\n    title: str | None\n    content: str | None\n\n\nclass DummyDatabase:\n    posts: dict[int, Post] = {}\n\n\ndb = DummyDatabase()\ndb.posts = {\n    1: Post(id=1, title=\"Post 1\", content=\"Content 1\"),\n    2: Post(id=2, title=\"Post 2\", content=\"Content 2\"),\n    3: Post(id=3, title=\"Post 3\", content=\"Content 3\"),\n}\n\n\napp = FastAPI()\n\n\nasync def get_post_or_404(id: int) -&gt; Post:\n    try:\n        return db.posts[id]\n    except KeyError:\n        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND)\n\n\n@app.get(\"/posts/{id}\")\nasync def get(post: Post = Depends(get_post_or_404)):\n    return post\n\n\n@app.patch(\"/posts/{id}\")\nasync def update(post_update: PostUpdate, post: Post = Depends(get_post_or_404)):\n    updated_post = post.copy(update=post_update.dict())\n    db.posts[post.id] = updated_post\n    return updated_post\n\n\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete(post: Post = Depends(get_post_or_404)):\n    db.posts.pop(post.id)\n\n\nCreating and using a parametrized dependency with a class\n\nSuppose we wanted to dynamically cap the limit value in the pagination example…-&gt; would need to do this with a class!\n\n\nfrom fastapi import Depends, FastAPI, Query\n\napp = FastAPI()\n\nclass Pagination:\n    def __init__(self, maximum_limit:int = 100):\n        self.maximum_limit = maximum_limit\n    async def __call__(\n            self,\n            skip: int = Query(0, ge=0),\n            limit: int = Query(10,ge=0)\n    ) -&gt; tuple[int,int]:\n        capped_limit = min(self.maximum_limit, limit)\n        return (skip, capped_limit)\n# hardcoded below, but could come from config file or env variable\npagination = Pagination(maximum_limit=50)\n\n@app.get(\"/items\")\nasync def list_items(p: tuple[int, int] = Depends(pagination)):\n    skip, limit = p\n    return {\"skip\": skip, \"limit\": limit}\n\n\n@app.get(\"/things\")\nasync def list_things(p: tuple[int, int] = Depends(pagination)):\n    skip, limit = p\n    return {\"skip\": skip, \"limit\": limit}\n\n\nNote: n FastAPI, Query is used to define and validate query parameters for your API endpoints. Query parameters are the key-value pairs that appear after the ? in a URL. They are typically used to filter, sort, or paginate data.\nDepends simply expects a callable: in can be __call__ or another function as below. Note that the pattern below could be used to apply different preprocessing steps, depending on the data, in the ML context:\n\n\nfrom fastapi import Depends, FastAPI, Query\n\napp = FastAPI()\n\n\nclass Pagination:\n    def __init__(self, maximum_limit: int = 100):\n        self.maximum_limit = maximum_limit\n\n    async def skip_limit(\n        self,\n        skip: int = Query(0, ge=0),\n        limit: int = Query(10, ge=0),\n    ) -&gt; tuple[int, int]:\n        capped_limit = min(self.maximum_limit, limit)\n        return (skip, capped_limit)\n\n    async def page_size(\n        self,\n        page: int = Query(1, ge=1),\n        size: int = Query(10, ge=0),\n    ) -&gt; tuple[int, int]:\n        capped_size = min(self.maximum_limit, size)\n        return (page, capped_size)\n\n\npagination = Pagination(maximum_limit=50)\n\n\n@app.get(\"/items\")\nasync def list_items(p: tuple[int, int] = Depends(pagination.skip_limit)):\n    skip, limit = p\n    return {\"skip\": skip, \"limit\": limit}\n\n\n@app.get(\"/things\")\nasync def list_things(p: tuple[int, int] = Depends(pagination.page_size)):\n    page, size = p\n    return {\"page\": page, \"size\": size}\n\n\nUsing dependency injection to manage db connection, ensuring that each request gets a fresh connection and that connections are properly closed after use:\n\n\nfrom fastapi import FastAPI, Depends\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine, Column, Integer, String\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nDATABASE_URL = \"sqlite:///./test.db\"\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Define a User model\nclass User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String, index=True)\n    email = Column(String, unique=True, index=True)\n\napp = FastAPI()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n# Endpoint to create a new user\n@app.post(\"/users/\")\nasync def create_user(name: str, email: str, db: Session = Depends(get_db)):\n    user = User(name=name, email=email)\n    db.add(user)\n    db.commit()\n    db.refresh(user)\n    return user\n\n@app.get(\"/users/\")\nasync def read_users(db: Session=Depends(get_db)):\n    users = db.query(User).all()\n    return users\n\n# Run the application\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nAn example in LLM context:\n\n\nfrom fastapi import FastAPI, Depends\nfrom pydantic import BaseModel\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\napp = FastAPI()\n\nclass LLM:\n    def __init__(self, model_name: str = \"distilbert-base-uncased-finetuned-sst-2-english\"):\n        # Load the pre-trained model and tokenizer from Hugging Face\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        self.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def predict(self, text: str, preprocess_steps: list):\n        # Preprocess the text\n        text = self.text_processor.preprocess(text, preprocess_steps)\n        # Tokenize the input text\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        inputs = {k: v.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for k, v in inputs.items()}\n        # Perform prediction using the model\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Get the predicted class\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        predicted_class = torch.argmax(predictions, dim=1).item()\n        return predicted_class\n\n# Create a global LLM instance\nllm_instance = LLM()\n\n# Dependency to get the global LLM instance\ndef get_llm():\n    return llm_instance\n\n# Request model for prediction\nclass PredictionRequest(BaseModel):\n    text: str\n\n# Response model for prediction\nclass PredictionResponse(BaseModel):\n    prediction: int\n\n# Use the LLM dependency in an endpoint\n@app.post(\"/predict/\", response_model=PredictionResponse)\nasync def predict(request: PredictionRequest, llm: LLM = Depends(get_llm)):\n    prediction = llm.predict(request.text)\n    return {\"prediction\": prediction}\n\n# Run the application\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\nRun as follows: curl -X POST \"http://127.0.0.1:8000/predict/\" -H \"Content-Type: application/json\" -d '{\"text\": \"I love FastAPI!\"}'\nWill return something like the following: {     \"prediction\": 1 }\n\n\n\n\nasync/await syntax for LLM calls that are I/O-bound\n\nfrom fastapi import FastAPI, HTTPException, Depends\nfrom pydantic import BaseModel\nimport httpx\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nHUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n\nclass OpenAIRequest(BaseModel):\n    prompt: str\n\nclass HuggingFaceRequest(BaseModel):\n    prompt: str\n\nclass OpenAIResponse(BaseModel):\n    id: str\n    object: str\n    created: int\n    model: str\n    choices: list\n\nclass HuggingFaceResponse(BaseModel):\n    generated_text: str\n\n@app.post(\"/openai-generate/\", response_model=OpenAIResponse) #respone should conform to the given Pydantic model\nasync def openai_generate(request: OpenAIRequest):\n    url = \"https://api.openai.com/v1/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"text-davinci-003\",\n        \"prompt\": request.prompt,\n        \"max_tokens\": 100,\n    }\n    async with httpx.AsyncClient() as client: #always async with context managers\n        try:\n            response = await client.post(url, headers=headers, json=data)\n            response.raise_for_status()  # Raises an error for 4xx/5xx responses\n            return response.json()\n        except httpx.HTTPStatusError as exc:\n            raise HTTPException(status_code=exc.response.status_code, detail=exc.response.text)\n        \n@app.post(\"/huggingface-generate\", response_model=HuggingFaceResponse)\nasync def huggingface_generate(request: HuggingFaceRequest):\n    url = \"https://api-inference.huggingface.co/models/gpt2\"\n    headers = {\n        \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n    }\n    data = {\n        \"inputs\": request.prompt,\n        \"options\": {\"use_cache\": False},\n    }    \n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.post(url, headers = headers, json=data)\n            response.raise_for_status()\n            return {\"generated_text\": response.json()[0]['generated_text']}\n        except httpx.HTTPStatusError as exc:\n            raise HTTPException(status_code=exc.response.status_code, detail=exc.response.text)\n# To run the app: uvicorn your_file_name:app --reload\n\n\n\nCustom error handling\n\nfrom fastapi import FastAPI, HTTPException, Depends, Request\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nimport httpx\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\nLLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n\nclass LLMError(Exception):\n    def __init__(self, message: str):\n        self.message = message\n\n@app.exception_handler(LLMError)\nasync def llm_exception_handler(request: Request, exc: LLMError):\n    return JSONResponse(\n        status_code=500,\n        content={\"message\":f\"LLM error occurred: {exc.message}\"}\n    )\nclass GenerateRequest(BaseModel):\n    prompt: str\n\nclass GenerateResponse(BaseModel):\n    generated_text: str\n\n@app.post(\"/generate\", response_model=GenerateResponse)\nasync def generate(request: GenerateRequest):\n    if not request.prompt:\n        raise HTTPException(status_code=400, detail=\"Prompt cannot be empty!\")\n    url = \"https://api.llm.example/generate\"\n    headers = {\n        \"Authorization\": f\"Bearer {LLM_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\"prompt\": request.prompt}\n\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.post(url, headers=headers, json=data)\n            response.raise_for_status()  # Raise an error for 4xx/5xx responses\n            response_data = response.json()\n            return {\"generated_text\": response_data.get(\"generated_text\", \"\")}\n        except httpx.HTTPStatusError as exc:\n            raise LLMError(message = exc.response.text)\n        except Exception as exc:\n            raise LLMError(message=str(exc))\n\n# To run the app: uvicorn your_file_name:app --reload\n\n\n\nServing static files (like HTML, CSS, JS) with FastAPI: Jinja2+JS\n\n# Given the following file structure\n# /my_fastapi_app\n# ├── app.py               # Your FastAPI application\n# ├── static               # Directory for static files\n# │   ├── css\n# │   │   └── styles.css\n# │   ├── js\n# │   │   └── script.js\n# │   └── index.html\n# └── requirements.txt\n\n\n# index.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"/static/css/styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{{ title }}&lt;/h1&gt;\n    &lt;p&gt;{{ description }}&lt;/p&gt;\n    &lt;form id=\"generate-form\"&gt;\n        &lt;label for=\"prompt\"&gt;Enter your prompt:&lt;/label&gt;\n        &lt;input type=\"text\" id=\"prompt\" name=\"prompt\" required&gt;\n        &lt;button type=\"submit\"&gt;Generate&lt;/button&gt;\n    &lt;/form&gt;\n    &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;script src=\"/static/js/scripts.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n# script.js\ndocument.getElementById('generate-form').addEventListener('submit', async (event) =&gt; {\n    event.preventDefault();\n    const prompt = document.getElementById('prompt').value;\n    const response = await fetch('/generate', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ prompt })\n    });\n    const data = await response.json();\n    document.getElementById('result').innerText = data.generated_text;\n});\n\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\n\napp = FastAPI()\n\n#mount the static directory\napp.mount(\"/static\",StaticFiles(directory=\"static\"),name='static')\n\n# @app.get(\"/\",response_class=HTMLResponse)\n# async def read_index():\n#     with open(\"static/index.html\") as f:\n#         return f.read()\n    \n# Or better, serve HTML templates more dynamically:\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def read_root(request: Request):\n    context = {\n        \"request\": request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a large language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n# To run the app: uvicorn main:app --reload\n\n\nWhen you navigate to http://127.0.0.1:8000/, it will serve your index.html file.\napp.mount: specify URL path+directory for static files, allowing FastAPI to handle requests to those files automatically.\n\n\n\nServing static files (like HTML, CSS, JS) with FastAPI: Jinja2+HTMX\n\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nimport httpx\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\n# Mount the static directory\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Serve HTML templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n@app.get('/', response_class=HTMLResponse)\nasync def read_root(request:Request):\n    context = {\n        \"request\":request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n@app.post(\"/generate\",response_class=HTMLResponse)\nasync def generate_text(request: Request, prompt: str = Form(...))\n    if not prompt:\n        return HTMLResponse(\"&lt;div id='result'&gt;&lt;p&gt;Error: Error: Prompt cannot be empty!&lt;/p&gt;&lt;/div&gt;\")\n        url = \"https://api.llm.example/generate\"\n    headers = {\n        \"Authorization\": f\"Bearer {os.getenv('LLM_API_KEY')}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\"prompt\": prompt}\n\n    async with httpx.AsyncClient() as client:\n        try:\n            response = await client.post(url, headers=headers, json=data)\n            response.raise_for_status()  # Raise an error for 4xx/5xx responses\n            response_data = response.json()\n            generated_text = response_data.get(\"generated_text\", \"\")\n            return HTMLResponse(f\"&lt;div id='result'&gt;&lt;h2&gt;Generated Text:&lt;/h2&gt;&lt;p&gt;{generated_text}&lt;/p&gt;&lt;/div&gt;\") # update the #result endpoint\n        except httpx.HTTPStatusError as exc:\n            return HTMLResponse(f\"&lt;div id='result'&gt;&lt;p&gt;Error: {exc.response.text}&lt;/p&gt;&lt;/div&gt;\")\n        except Exception as exc:\n            return HTMLResponse(f\"&lt;div id='result'&gt;&lt;p&gt;Error: {str(exc)}&lt;/p&gt;&lt;/div&gt;\")\n\n\n\nNo longer any JavaScript found in the directory, all taken care of with HTMX as follows:\n\nHTML Template with HTMX:\n\nThe index.html template includes the HTMX library by adding a &lt;script&gt; tag that loads HTMX from a CDN.\nThe form uses HTMX attributes (hx-post, hx-target, hx-swap) to handle form submission and update the result dynamically:\nhx-post=“/generate”: Sends a POST request to the /generate endpoint when the form is submitted.\nhx-target=“#result”: Specifies the element (#result) to update with the server’s response.\nhx-swap=“innerHTML”: Replaces the inner HTML of the target element with the server’s response.\n\nFastAPI Endpoint:\n\nThe /generate endpoint processes the form submission, interacts with the LLM API, and returns an HTML response with the generated text or an error message.\nThe response is an HTMLResponse that updates the #result element in the HTML template.*\n\n\n\n\n# .\n# ├── main.py\n# ├── static\n# │   ├── css\n# │   │   └── styles.css\n# └── templates\n#     └── index.html\n# templates/index.htmx\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{{ title }}&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"/static/css/styles.css\"&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org@1.6.1\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;{{ title }}&lt;/h1&gt;\n    &lt;p&gt;{{ description }}&lt;/p&gt;\n    &lt;form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\"&gt;\n        &lt;label for=\"prompt\"&gt;Enter your prompt:&lt;/label&gt;\n        &lt;input type=\"text\" id=\"prompt\" name=\"prompt\" required&gt;\n        &lt;button type=\"submit\"&gt;Generate&lt;/button&gt;\n    &lt;/form&gt;\n    &lt;div id=\"result\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\nPackaging with Docker\n\n.\n├── Dockerfile\n├── main.py\n├── requirements.txt\n├── static\n│   ├── css\n│   │   └── styles.css\n└── templates\n    └── index.html\n\n\ntemplates/index.html remains the same file compatible with HTMX\n\n\n# requirements.txt\nfastapi\nuvicorn\ntorch==1.9.0+cu111  # Ensure this matches the CUDA version in the Docker image\ntransformers\npython-dotenv\ngoogle-cloud-storage\n\n\nFor GPU support…\n\nInstall NVIDIA Docker: Install the NVIDIA Docker runtime on your host machine.\nModify the Dockerfile: Use a base image that includes CUDA and cuDNN libraries.\nUpdate the Docker Run Command: Use the –gpus flag to allocate GPU resources to the container.\nEnsure PyTorch is Installed with CUDA Support: Make sure the PyTorch version installed in the container supports CUDA.\n\n# Use the official NVIDIA CUDA runtime as a parent image\nFROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN apt-get update && apt-get install -y \\\n    python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN pip3 install --no-cache-dir -r requirements.txt\n\n# Expose port 80 to the outside world\nEXPOSE 80\n\n# Run the FastAPI application\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n\n\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\n# Mount the static directory\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Serve HTML templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Load the model and tokenizer from a shared storage location or model server\nmodel_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def read_root(request: Request):\n    context = {\n        \"request\": request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a large language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n@app.post(\"/generate\", response_class=HTMLResponse)\nasync def generate_text(request: Request, prompt: str = Form(...)):\n    if not prompt:\n        return HTMLResponse(\"&lt;div id='result'&gt;&lt;p&gt;Error: Prompt cannot be empty!&lt;/p&gt;&lt;/div&gt;\")\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return HTMLResponse(f\"&lt;div id='result'&gt;&lt;h2&gt;Generated Text:&lt;/h2&gt;&lt;p&gt;{generated_text}&lt;/p&gt;&lt;/div&gt;\")\n\n\n%% bash\ndocker build -t my-fastapi-app .\ndocker run --gpus all --env-file .env -d -p 80:80 --name my-fastapi-container my-fastapi-app\n\n\n\nDeploying on GCP\nStep 1: Set Up GCP Project: Create a GCP project and enable necessary APIs.\nStep 2: Install and Configure Google Cloud SDK: Install the Google Cloud SDK and authenticate.\nStep 3:Build and Push Docker Image to GCR: Build the Docker image and push it to Google Container Registry.\nStep 4:Create a GKE Cluster: Create a Kubernetes cluster on GKE.\nStep 5:Deploy the Application on GKE: Deploy the FastAPI application on the GKE cluster.\nStep 6:Set Up Google Cloud Storage: Store the model in GCS and access it from the application.\n\nStep 1: Set Up GCP Project: Create a GCP project and enable necessary APIs.\nCreate a GCP Project: Go to the Google Cloud Console.\nCreate a new project.\nEnable the following APIs: Kubernetes Engine API, Container Registry API, Cloud Storage API\n\n\nStep 2: Install and Configure Google Cloud SDK\nInstall Google Cloud SDK: https://cloud.google.com/sdk/docs/install\nAuthenticate with GCP: gcloud init; gcloud auth login\nSet the Project: gcloud config set project YOUR_PROJECT_ID\n\n\nStep 3: Build and Push Docker Image to GCR\ndocker build -t gcr.io/YOUR_PROJECT_ID/my-fastapi-app .\ndocker push gcr.io/YOUR_PROJECT_ID/my-fastapi-app\n\n\nStep 4:Create a GKE Cluster.\ngcloud container clusters create my-cluster --num-nodes=3\ngcloud container clusters get-credentials my-cluster\n\n\nStep 5:Deploy the Application on GKE: Deploy the FastAPI application on the GKE cluster.\n\nCreate a Kubernetes Deployment deployment.yaml\n\n\n## deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-fastapi-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-fastapi-app\n  template:\n    metadata:\n      labels:\n        app: my-fastapi-app\n    spec:\n      containers:\n      - name: my-fastapi-app\n        image: gcr.io/YOUR_PROJECT_ID/my-fastapi-app\n        ports:\n        - containerPort: 80\n        env:\n        - name: MODEL_NAME\n          value: \"gs://YOUR_BUCKET_NAME/model\"\n\n\nCreate a Kubernetes Service service.yaml\n\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-fastapi-app\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-fastapi-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n\n\nDeploy to GKE\n\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n\n\nStep 6: Set up Google Cloud Storage, upload model to bucket, update main.py to load model from GCS\ngsutil mb gs://YOUR_BUCKET_NAME\ngsutil cp model/* gs://YOUR_BUCKET_NAME/model/\n\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nfrom google.cloud import storage\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\n# Mount the static directory\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Serve HTML templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Load the model and tokenizer from GCS\ndef download_blob(bucket_name, source_blob_name, destination_file_name):\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n\nmodel_path = \"/app/model\"\nos.makedirs(model_path, exist_ok=True)\ndownload_blob(\"YOUR_BUCKET_NAME\", \"model/pytorch_model.bin\", f\"{model_path}/pytorch_model.bin\")\ndownload_blob(\"YOUR_BUCKET_NAME\", \"model/config.json\", f\"{model_path}/config.json\")\ndownload_blob(\"YOUR_BUCKET_NAME\", \"model/tokenizer_config.json\", f\"{model_path}/tokenizer_config.json\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def read_root(request: Request):\n    context = {\n        \"request\": request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a large language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n@app.post(\"/generate\", response_class=HTMLResponse)\nasync def generate_text(request: Request, prompt: str = Form(...)):\n    if not prompt:\n        return HTMLResponse(\"&lt;div id='result'&gt;&lt;p&gt;Error: Prompt cannot be empty!&lt;/p&gt;&lt;/div&gt;\")\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return HTMLResponse(f\"&lt;div id='result'&gt;&lt;h2&gt;Generated Text:&lt;/h2&gt;&lt;p&gt;{generated_text}&lt;/p&gt;&lt;/div&gt;\")\n\n# To run the app: uvicorn main:app --reload\n\n\n\n\nLoading the model and making predictions with FastAPI app\n\nfrom fastapi import FastAPI, Request, Form\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\n# Mount the static directory\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Serve HTML templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Load the model and tokenizer\nmodel_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def read_root(request: Request):\n    context = {\n        \"request\": request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a large language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n@app.post(\"/generate\", response_class=HTMLResponse)\nasync def generate_text(request: Request, prompt: str = Form(...)):\n    if not prompt:\n        return HTMLResponse(\"&lt;div id='result'&gt;&lt;p&gt;Error: Prompt cannot be empty!&lt;/p&gt;&lt;/div&gt;\")\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return HTMLResponse(f\"&lt;div id='result'&gt;&lt;h2&gt;Generated Text:&lt;/h2&gt;&lt;p&gt;{generated_text}&lt;/p&gt;&lt;/div&gt;\")\n\n@app.post(\"/api/generate\", response_class=JSONResponse)\nasync def api_generate_text(prompt: str):\n    if not prompt:\n        return JSONResponse(status_code=400, content={\"error\": \"Prompt cannot be empty!\"})\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return JSONResponse(content={\"generated_text\": generated_text})\n\n# To run the app: uvicorn main:app --reload\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;LLM Text Generator&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"/static/css/styles.css\"&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org@1.6.1\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;LLM Text Generator&lt;/h1&gt;\n    &lt;p&gt;Generate text using a large language model.&lt;/p&gt;\n    &lt;form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\"&gt;\n        &lt;label for=\"prompt\"&gt;Enter your prompt:&lt;/label&gt;\n        &lt;input type=\"text\" id=\"prompt\" name=\"prompt\" required&gt;\n        &lt;button type=\"submit\"&gt;Generate&lt;/button&gt;\n    &lt;/form&gt;\n    &lt;div id=\"result\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nFrom the user’s perspective, the interaction with the FastAPI application will be straightforward and intuitive. Here’s how each endpoint will be experienced by the user:\n\nUser Experience\n\nAccessing the Application (/ Endpoint):\n\nWhen the user navigates to the root URL (http://127.0.0.1:8000/), they will see an HTML page with a form where they can input a prompt.\nThe form will have a text input field for the prompt and a submit button.\n\nSubmitting the Form (/generate Endpoint):\n\nWhen the user types a prompt into the form and clicks the submit button, the form data is sent to the /generate endpoint.\nThe /generate endpoint processes the form submission, queries the LLM, and returns the generated text as part of the HTML response.\nThe user will see the generated text displayed on the same page below the form.\n\nAPI Interaction (/api/generate Endpoint):\n\nThe /api/generate endpoint is designed for programmatic access, such as from a frontend application or another service.\nUsers or developers can send a JSON request to this endpoint with the prompt, and it will return the generated text as a JSON response.\nThis endpoint is useful for integrating the LLM functionality into other applications or services.\n\n\n\n\n\nWriting tests\n\nUnit tests\n\nimport os\nimport pytest\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n@pytest.fixture\ndef model_and_tokenizer():\n    model_name = os.getenv(\"MODEL_NAME\", \"gpt2\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n    return model, tokenizer\n\ndef test_model_loading(model_and_tokenizer):\n    model, tokenizer = model_and_tokenizer\n    assert model is not None, \"Model should be loaded\"\n    assert tokenizer is not None, \"Tokenizer should be loaded\"\n    assert model.device.type in [\"cuda\", \"cpu\"], \"Model should be on CUDA or CPU\"\n\n\n\nIntegration tests\n\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\ndef test_read_root():\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert \"LLM Text Generator\" in response.text\n\ndef test_generate_text():\n    response = client.post(\"/generate\", data={\"prompt\": \"Hello, world!\"})\n    assert response.status_code == 200\n    assert \"Generated Text:\" in response.text\n\ndef test_api_generate_text():\n    response = client.post(\"/api/generate\", json={\"prompt\": \"Hello, world!\"})\n    assert response.status_code == 200\n    json_response = response.json()\n    assert \"generated_text\" in json_response\n    assert isinstance(json_response[\"generated_text\"], str)\n\n\n\n\nWorking with a database (Postresql)\n\n# requirements.txt\nfastapi\nuvicorn\ntorch\ntransformers\npython-dotenv\npasslib[bcrypt]\npyjwt\nsqlalchemy\ndatabases\nasyncpg\npsycopg2-binary\n\n\n# models.py\nfrom sqlalchemy import Column, Integer, String, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True, index=True)\n    username = Column(String, unique=True, index=True)\n    full_name = Column(String)\n    email = Column(String, unique=True, index=True)\n    hashed_password = Column(String)\n    disabled = Column(Boolean, default=False)\n\n\nclass UserInDB(User):\n    hashed_password: str\n\n\n# database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom .models import Base\nfrom passlib.context import CryptContext\n\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\nDATABASE_URL = \"postgresql+asyncpg://user:password@localhost/dbname\"\n\nengine = create_async_engine(DATABASE_URL, echo=True)  # echo to see SQL in terminal\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine, class_=AsyncSession)\n\nasync def init_db():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n# Dependency to get the database session\nasync def get_db():\n    async with SessionLocal() as session:\n        yield session\n\n\n# crud.py\nfrom sqlalchemy.orm import Session\nfrom .models import User\nfrom .schemas import UserCreate, UserUpdate\nfrom .auth import get_password_hash\n\ndef get_user(db: Session, user_id: int):\n    return db.query(User).filter(User.id == user_id).first()\n\ndef get_user_by_username(db: Session, username: str):\n    return db.query(User).filter(User.username == username).first()\n\ndef create_user(db: Session, user: UserCreate):\n    hashed_password = get_password_hash(user.password)\n    db_user = User(\n        username=user.username,\n        full_name=user.full_name,\n        email=user.email,\n        hashed_password=hashed_password,\n        disabled=user.disabled,\n    )\n    db.add(db_user)\n    db.commit()\n    db.refresh(db_user)\n    return db_user\n\ndef update_user(db: Session, user_id: int, user: UserUpdate):\n    db_user = get_user(db, user_id)\n    if db_user:\n        db_user.username = user.username\n        db_user.full_name = user.full_name\n        db_user.email = user.email\n        if user.password:\n            db_user.hashed_password = get_password_hash(user.password)\n        db_user.disabled = user.disabled\n        db.commit()\n        db.refresh(db_user)\n    return db_user\n\ndef delete_user(db: Session, user_id: int):\n    db_user = get_user(db, user_id)\n    if db_user:\n        db.delete(db_user)\n        db.commit()\n    return db_user\n\n\n# auth.py\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom jose import JWTError, jwt\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.future import select\nfrom .models import User\nfrom .database import get_db, pwd_context\n\n# Secret key to encode and decode JWT tokens\nSECRET_KEY = \"your_secret_key\"\nALGORITHM = \"HS256\"\nACCESS_TOKEN_EXPIRE_MINUTES = 30\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\ndef verify_password(plain_password: str, hashed_password: str) -&gt; bool:\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password: str) -&gt; str:\n    return pwd_context.hash(password)\n\nasync def authenticate_user(db: AsyncSession, username: str, password: str):\n    result = await db.execute(select(User).filter(User.username == username))\n    user = result.scalars().first()\n    if not user:\n        return False\n    if not verify_password(password, user.hashed_password):\n        return False\n    return user\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.now() + expires_delta\n    else:\n        expire = datetime.now() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\nasync def get_current_user(token: str = Depends(oauth2_scheme), db: AsyncSession = Depends(get_db)):\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n        username: str = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    result = await db.execute(select(User).filter(User.username == username))\n    user = result.scalars().first()\n    if user is None:\n        raise credentials_exception\n    return user\n\nasync def get_current_active_user(current_user: User = Depends(get_current_user)):\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n\n\n#schemas.py\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\nclass TokenData(BaseModel):\n    username: Optional[str] = None\n\n\n# main.py\nfrom fastapi import FastAPI, Depends, HTTPException, Request, Form\nfrom fastapi.responses import HTMLResponse, JSONResponse\nfrom fastapi.security import OAuth2PasswordRequestForm\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom sqlalchemy.ext.asyncio import AsyncSession\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nfrom dotenv import load_dotenv\nfrom .auth import authenticate_user, create_access_token, get_current_active_user\nfrom .models import User\nfrom .database import get_db, init_db\nfrom .crud import get_user_by_username, create_user\nfrom .schemas import UserCreate, Token\nfrom contextlib import asynccontextmanager\n\n# Load environment variables from .env file\nload_dotenv()\n\napp = FastAPI()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup event\n    await init_db()\n    yield\n    # Shutdown event\n    # Perform any necessary cleanup here\n# Mount the static directory\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n# Serve HTML templates\ntemplates = Jinja2Templates(directory=\"templates\")\n\n# Load the model and tokenizer\nmodel_name = os.getenv(\"MODEL_NAME\", \"gpt2\")  # Replace with your model path or name\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n@app.post(\"/token\", response_model=Token)\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: AsyncSession = Depends(get_db)):\n    user = await authenticate_user(db, form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(\n            status_code=401,\n            detail=\"Incorrect username or password\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(\n        data={\"sub\": user.username}, expires_delta=access_token_expires\n    )\n    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n\n@app.post(\"/users/\", response_model=User)\nasync def create_new_user(user: UserCreate, db: AsyncSession = Depends(get_db)):\n    db_user = await get_user_by_username(db, user.username)\n    if db_user:\n        raise HTTPException(status_code=400, detail=\"Username already registered\")\n    return await create_user(db, user)\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def read_root(request: Request):\n    context = {\n        \"request\": request,\n        \"title\": \"LLM Text Generator\",\n        \"description\": \"Generate text using a large language model.\"\n    }\n    return templates.TemplateResponse(\"index.html\", context)\n\n@app.post(\"/generate\", response_class=HTMLResponse)\nasync def generate_text(request: Request, prompt: str = Form(...), current_user: User = Depends(get_current_active_user)):\n    if not prompt:\n        return HTMLResponse(\"&lt;div id='result'&gt;&lt;p&gt;Error: Prompt cannot be empty!&lt;/p&gt;&lt;/div&gt;\")\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return HTMLResponse(f\"&lt;div id='result'&gt;&lt;h2&gt;Generated Text:&lt;/h2&gt;&lt;p&gt;{generated_text}&lt;/p&gt;&lt;/div&gt;\")\n\n@app.post(\"/api/generate\", response_class=JSONResponse)\nasync def api_generate_text(prompt: str, current_user: User = Depends(get_current_active_user)):\n    if not prompt:\n        return JSONResponse(status_code=400, content={\"error\": \"Prompt cannot be empty!\"})\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs[\"input_ids\"], max_length=100)\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return JSONResponse(content={\"generated_text\": generated_text})\n\n# To run the app: uvicorn main:app --reload\n\n\n# index.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;LLM Text Generator&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"/static/css/styles.css\"&gt;\n    &lt;script src=\"https://unpkg.com/htmx.org@1.6.1\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;LLM Text Generator&lt;/h1&gt;\n    &lt;p&gt;Generate text using a large language model.&lt;/p&gt;\n\n    &lt;!-- User Registration Form --&gt;\n    &lt;div id=\"registration\"&gt;\n        &lt;h2&gt;Register&lt;/h2&gt;\n        &lt;form hx-post=\"/users/\" hx-target=\"#registration-result\" hx-swap=\"innerHTML\"&gt;\n            &lt;label for=\"username\"&gt;Username:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"username\" name=\"username\" required&gt;\n            &lt;label for=\"full_name\"&gt;Full Name:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"full_name\" name=\"full_name\"&gt;\n            &lt;label for=\"email\"&gt;Email:&lt;/label&gt;\n            &lt;input type=\"email\" id=\"email\" name=\"email\" required&gt;\n            &lt;label for=\"password\"&gt;Password:&lt;/label&gt;\n            &lt;input type=\"password\" id=\"password\" name=\"password\" required&gt;\n            &lt;button type=\"submit\"&gt;Register&lt;/button&gt;\n        &lt;/form&gt;\n        &lt;div id=\"registration-result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;!-- User Login Form --&gt;\n    &lt;div id=\"login\"&gt;\n        &lt;h2&gt;Login&lt;/h2&gt;\n        &lt;form hx-post=\"/token\" hx-target=\"#login-result\" hx-swap=\"innerHTML\"&gt;\n            &lt;label for=\"login-username\"&gt;Username:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"login-username\" name=\"username\" required&gt;\n            &lt;label for=\"login-password\"&gt;Password:&lt;/label&gt;\n            &lt;input type=\"password\" id=\"login-password\" name=\"password\" required&gt;\n            &lt;button type=\"submit\"&gt;Login&lt;/button&gt;\n        &lt;/form&gt;\n        &lt;div id=\"login-result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Text Generation Form --&gt;\n    &lt;div id=\"text-generation\"&gt;\n        &lt;h2&gt;Generate Text&lt;/h2&gt;\n        &lt;form hx-post=\"/generate\" hx-target=\"#result\" hx-swap=\"innerHTML\"&gt;\n            &lt;label for=\"prompt\"&gt;Enter your prompt:&lt;/label&gt;\n            &lt;input type=\"text\" id=\"prompt\" name=\"prompt\" required&gt;\n            &lt;button type=\"submit\"&gt;Generate&lt;/button&gt;\n        &lt;/form&gt;\n        &lt;div id=\"result\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "notes/FastAPI.html#above-in-progress",
    "href": "notes/FastAPI.html#above-in-progress",
    "title": "FastAPI notes",
    "section": "Above In progress",
    "text": "Above In progress"
  },
  {
    "objectID": "projects/indiv_projects/sql-finetune/sql-finetune.html",
    "href": "projects/indiv_projects/sql-finetune/sql-finetune.html",
    "title": "Sample-efficient SQL fine-tuning",
    "section": "",
    "text": "Source: European Southern Observatory, CC BY 2.0 image\n\n\n\nThe goal is to focus on sample efficiency and to use a handful of high quality examples to fine tune smaller LLMs on a database. The hope is that the models get used to domain-specific language, adhere better to SQLite standards, and generate SQL that would answer questions about the database with high accuracy. A database about exoplanets was chosen, but the process generalizes to text-to-SQL fine tuning on an arbitrary database.\nData: A small custom dataset of 50 training examples and 10 validation examples was created based on https://www.kaggle.com/datasets/adityamishraml/nasaexoplanets/data. SQLite table exoplanets was made from the data, along with a reference_planets table made by inserting (name, mass) VALUES (‘Jupiter’, 1.898e27) and (name, mass) VALUES (‘Earth’, 5.972e24). The mass_wrt column in exoplanets maps to the mass column in reference_planets, allowing for more complex queries involving joins. The data is now available on Hugging Face Datasets at dpv/exoplanets-sql. See exo_to_sql.ipynb for some of the test functions developed: the goal is to test the queries by running them against the database and i.) recording the execution rate and ii.) obtaining the result accuracy for queries in the validation data set.\nFine tuning premai-io/prem-1B-SQL: execution rate is 70-80% (up from 50%) and accuracy is around 30% (up from 10%) with premsql within 10 epochs/2 minutes of fine tuning all of the model weights in full precision. The big unknown is the exact the prompt format used by premsql, which I could not exactly determine. Perhaps fixing this could lead to much better results. See notebook Fine_tune_premsql.ipynb for details.\nFine tuning meta-llama/Llama-3.2-3B-Instruct with LoRA: execution rate starts at 80-90% percent for the original model, but the results are not accurate (no full matches, but did not investigate close matches). Fine tuning on the exoplanets data set took less than 2 minutes and still results in a 90% execution rate but now also a result accuracy of 70% (with affordances granted to differences in column names/multiple ways to interpret one of the questions). In addition, the original model with weights in full precision took over 6.5 minutes to generate 10 queries whereas the fine tuned LoRA version took 12 seconds per 10 queries. See notebook Fine_tune_llama3.ipynb for details.\nmeta-llama/Llama-3.2-3B-Instruct full fine tune: Full fine tune trained stably but did not markedly improve results. Interestingly, ‘INTERVAL’ was still used in generated query after 15 epochs, but not after 25 epochs of fine tuning. This suggests that doing stage 1 SQLite fine tune and longer training are possible substitutes. See Fine_tune_llama3_fullft.ipynb.\nmeta-llama/Llama-3.1-8B-Instruct LoRA fine tune: As expected, the larger model was more accurate than smaller ones out of the box (getting 40% accuracy prior to fine tuning). LoRA fine tune of meta-llama/Llama-3.1-8B-Instruct for 20 epochs led to 50% execution accuracy and about 70% ‘fuzzy match’ accuracy (which gives affordances to differing column names and other minor differences), see Fine_tune_llama3_8b_longer_train.ipynb.\n\nAdditional training with extra data and other observations:\n\nAfter identifying the validation examples that have proven to be the most difficult, I generated 10 more examples of a type resembling (but not too similar to) those examples. See https://huggingface.co/datasets/dpv/exoplanets-sql2 for data where these examples were added to the training set. As the rest of the approach, this runs the risk of overfitting to the validation set, of course. While in production it would be necessary to have a dedicated test set to rule this out, the approach nonetheless offers a path forward. With these examples, meta-llama/Llama-3.2-3B-Instruct fine tuned with LoRA gave similar performance, but DoRA yielded 100% execution rate and 80% accuracy (with affordances given to column name differences, for example). See Fine_tune_llama3_dora_extra10.ipynb. Additional possibility is to first fine tune the model on SQLite data (such as umesh16071973/SQLite_Training_Dataset from Hugging Face), yet it appears that even with a handful of examples, the model almost never makes mistakes in regards to using the correct SQL dialect.\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/inventory-classes/inventory-classes.html",
    "href": "projects/indiv_projects/inventory-classes/inventory-classes.html",
    "title": "Inventory classes with pytest and pdoc",
    "section": "",
    "text": "Source: Image by the author\n\n\n\nInventory-classes-with-pytest-and-pdoc: Inventory application for a tech video channel featuring computer builds with inventory classes in object-oriented Python, 90%+ coverage tests with pytest, comprehensive API documentation with pdoc, and packaging inside Docker.\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/arxiv/arxiv-llms-assistant.html",
    "href": "projects/indiv_projects/arxiv/arxiv-llms-assistant.html",
    "title": "arXiv LLMs Assistant",
    "section": "",
    "text": "Applied exllamav2 6.0bpw quantization to run Mixtral-8x7B on personal GPUs with virtually no loss in quality and evaluate RAG pipeline configurations to find the best performing one (evaluation score ~90%).\nBuilt an assistant to study the LLM domain, compare out-of-repository papers with papers in a personal repo (Zotero) and recommend recent papers to read along with lists of question/answer pairs.\nOrchestrated a multi-faceted evaluation of RAG setups, implementing feedback mechanisms that led to the selection of the most effective configuration; this process improved question relevance, elevating overall project outcomes.\nBuilt a command line utility to ask questions about the contents of uploaded papers and generate question/answer sets based on these papers.\nGitHub link"
  },
  {
    "objectID": "projects/indiv_projects/project-bird/project-bird.html",
    "href": "projects/indiv_projects/project-bird/project-bird.html",
    "title": "Project Bird",
    "section": "",
    "text": "Source: Image by the author\n\n\n\n\nGCP ETL project to pull data from eBird API 2.0 and display a map of recent bird observations in a given area: a small side project to practice GCP skills and see more birds.\n\nSchedule an Airflow job to obtain data from eBird API 2.0 and store in Cloud Storage.\nClean the data via an Apache Beam Dataflow job and push the results to BigQuery.\nUse a Cloud function to trigger the Dataflow job in Step 2 on file upload to Cloud Storage bucket.\nBuild geographic visualization with BigQuery Geo Viz.\n\nGitHub link"
  },
  {
    "objectID": "blog/posts/kto-optimization/kto-optimization.html",
    "href": "blog/posts/kto-optimization/kto-optimization.html",
    "title": "KTO Optimization - Write-up forthcoming",
    "section": "",
    "text": "KTO Optimization - Write-up forthcoming"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "The following project/write-up arises from my interest in understanding the under-the-hood details of training transformers. I’ve studied Andrej Karphaty’s video “Let’s build GPT: from scratch, in code, spelled out.” when it came out in 2023, and started working on describing parts he didn’t yet explain. Then “Let’s reproduce GPT-2 (124M)” came out, which answered a lot of my questions and compelled me to finish up the write up and make it available to others. My hope is that it will serve as a good review to those studying Andrej’s work, or those who do not have time to carefully work through a 4H+ video (which is highly, highly recommended).\n\nPart 1 focuses on building the transformer, Part 2 on optimizing it, and in the parts that follow, I hope to describe my own tokenization and training experiments.\n\n\nfrom dataclasses import dataclass\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\n\n\nWe’ll be implementing the right hand side (the decoder) that composes GPT-2: each unit on the right will be a block in our transformer.\n\n\n\n\nTransformer Architecture from “Attention Is All You Need” by Vaswani et al.\n\n\n\nThe configuration below is the configuration for the entire tranformer, with each layer h pertaining to one of the blocks. We want to replicate the following structure from a GPT-2 model in Huggingface Transformers:\n\n\n\n\nSource: HF Transformer, image by the author\n\n\n\nThe code below is the skeleton on GPT2 config and main module that will allow us to replicate that structure:\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 256\n    vocab_size: int = 65\n    n_layer: int = 6\n    n_head: int = 8\n    n_embd: int = 384\n\nclass GPT(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        # With nn.ModuleDict() index into submodules just like a dictionary\n        self.tranformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n\nnn.ModuleDict allows you to index into submodules using keys, just like a dictionary.\nnn.ModuleList allows us to index into each individual layer using an index, just like with a list\n\n\n\n\n\nNow let’s implement the Block,…\nUnlike the original GPT2 paper, establish a clean residual pathway by taking the layer norm of x and applying attention/multilayer perceptron layer to it then adding it to the input x. Since addition allows for an adulterated gradient flow during backpropagation, this pre-layer norm configuration is the better than the post-layer norm configuration where the norm is applied after the addition. More formally, Xiong et al. (2020) have shown that if post-layer norm is used, a warm-up stage is needed to avoid training instability whereas if pre-layer norm is used, the gradients are well-behaved at initialization. See the difference between original (Post-LN) GPT-2 implementation and the ‘corrected’ pre-LN implementation used here:\n\n\n\n\nSource: “On Layer Normalization in the Transformer Architecture” by Xiong et al. 2020\n\n\n\nFinally, onto the Block:\n\n\nclass Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config    \n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNote again how the layer norm is applied before the addition to the residual stream.\nAndrej notes that attention is a communication operation, where tokens communicate with each other and aggregate information. Thus attention can be thought of as a pooling function/weighted sum function/reduce operation. On the other hand, the multilayer perceptron (MLP) is applied to each token individually, with no information exchanged between the tokens. Thus attention is a reduce and MLP is the map operation and a transformer is a repeated application of MapReduce.\n\n\n\n\n\nBriefly summarizing from Andrej’s previous video (Let’s build GPT: from scratch, in code, spelled out.), multilayer perceptron is implemented using a standard “bottleneck architecture” where the dimensions are first expanded to learn more complex representations, nonlinearity is applied to help the model learn more complex patterns, and finally the data is projected down again to keep the computational complexity in check.\n\n\nclass MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x    \n\n\nGPT-2 used an approximate version of GeLU because at the time of GPT-2’s creation, the erf function was very slow in TensorFlow and GPT-2 and the approximate version was used. Today there’s no reason to use the approximate version but Andrej is using the tanh approximation for veracity.\n\nAlso, GeLU is better than ReLU due to dead neuron problem since a local gradient is always present as seen below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/generated/torch.nn.GELU.html\n\n\n\n\n\n\nAndrej’s attention implementation is a more efficient implementation of the following simple one from “Lets build GPT: from scratch, in code, spelled out”:\n\n\n\n\nSource: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n\n\n\nNotice how in the implemenation above it is clear that the heads are parallel streams whose outputs are concatenated.\nThe idea of the more efficient implementation is to make another batch dimension with nh so that PyTorch effectively makes batches of dimension (B,nh) and applies all the operations on both B and nh in parallel.\n\nEach token emits query, key, and value. Queries and keys first multiply each other to deterimine “how interesting they find each other”.\n\nNext, we apply an autoregressive mask to make sure the tokens only attend to tokens before them.\n\nThe softmax normalizes the attention so it sums to 1.\nThe matrix multiply of attention with the values is a way, at every single token, to do a weigthed sum of the tokens each token finds intersting.\n\nTranspose, contiguous, and view reassembles everything in memory and performs what is equivalent of a concatenation operation.\n\ny.transpose(1,2): This line swaps the second and third dimensions of y. So the shape of y changes from (B, nh, T, hs) to (B, T, nh, hs).\n.contiguous(): This is used to ensure that the tensor is stored in a contiguous block of memory, which is required for some operations in PyTorch, including view.\n.view(B, T, C): This line reshapes the tensor y to have dimensions (B, T, C). Here, C is equal to nh*hs, which means that the last two dimensions of y (nh and hs) are flattened into a single dimension. This effectively concatenates the outputs of all the attention heads side by side.\n\nFinally, the output projection doesn’t change the dimension of y, but does introduce another learnable transformation so that the output can be projected in a way that is most useful for downstream tasks.\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        #key, qury, value projections for all heads in a batch\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        # mask to prevent attention to future tokens\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size)) # becomes available as self.bias\n    def forward(self, x):\n        B, T, C = x.size()\n        # calculate query, key, value for all heads in batch\n        qkv = self.c_attn(x) # (B,T, self.n_embd) x (self.n_embd,self.n_embd*3) = (B,T,self.n_embd*3)\n        q, k, v  = qkv.split(self.n_embd, dim=2) # (B,T,self.n_embd) x 3; make each split size self.n_embd by splitting dim 2\n        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # (B, nh, T, hs)\n        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # attention materializes a large (T,T) matrix fo each query and key\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # Change (B, nh, T, hs) to (B, T, nh, hs) with transpose, reassemle in memory, (B,T,C) makes nh*hs = n_embd (C)\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        # output projection: additional learnable transformation\n        y = self.c_proj(y) # (B, T, C)@(C, C) = (B, T, C)\n        return y\n\n\n\n\n\nIgnore the attention mask buffers (these are not parameters)\nThe weights in Hugging Face version are transposed (as they are in the original TensorFlow implementation) from what PyTorch needs because they use Conv1D module. Since we want to use plan nn.Linear, we hardcode these and transpose them.\n\n\nclass GPT(nn.Module):\n    ...\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type) # HF GPT2LMHeadModel has .from_pretrained method, just like ours\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n\n\n\n\nThe nn.Embedding layer in PyTorch takes an input tensor of arbitrary shape with values in the range [0, vocab_size) and maps each integer in that range to a dense vector of size C (the embedding dimension). In our case, we have an input tensor idx of shape (B, T), where B is the batch size and T is the sequence length, then applying the embedding layer to idx will yield a tensor of shape (B, T, C). This is because each integer in idx is replaced with its corresponding embedding vector. Since the embedding vectors have C elements, this adds an extra dimension of size C to the output. So for every batch and every sequence position, an embedding of size C is constructed, resulting in an output of shape (B, T, C).\nAlso note that the position embedding is broadcast to the token embedding and the same position embedding vector is learned at (T,C) for every element in B. This works because the position embeddings are independent of the specific sequence it’s in, so it can be shared across all sequences in the batch.\n\n\nclass GPT2(nn.Module):\n    ...    \n    def forward(self, idx):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        return logits\n    \n\n\n\n\n\nThe goal is to get close to generations from the Hugging Face pipeline:\n\n\nfrom transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n\nTo generate next token predictions, get the next top 50 tokens so that the model does not deviate too much from likely tokens, and sample one token from this distribution.\nConcatenate the token obtained from sampling with input (or input plus previously sampled tokens) at each step.\nThe sampling will not match the Hugging Face generations exactly since there’s likely a parameter hiding in the pipeline that’s different, but will be sensible English.\n\n\nnum_return_sequences = 5\nmax_length = 30\nmodel = GPT.from_pretrained('gpt2')\nmodel.eval()\nmodel.to('cuda')\n\n# prefix tokens\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\ntokens = enc.encode(\"Hello I'm a language model, \")\nx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences,1).to('cuda') # (5,8) since sent. tokenized to 8 tokens\n\n# generate: with each loop iteration, generate one more token\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nwhile x.size(1) &lt; max_length:\n    with torch.no_grad():\n        logits = model(x) # (B,T,vocab_size)\n        logits = logits[:, -1, :]  # take the logits at the last position\n        probs = F.softmax(logits, dim=-1) # get the probabilities\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # get the top-50 tokens\n        ix = torch.multinomial(topk_probs, num_samples=1) # sample from the top 50\n        xcol = torch.gather(topk_indices, -1, ix) # select the indices of the sampled tokens\n        x = torch.cat((x, xcol), dim=1) # append the sampled token to the sequence\n\nfor i in range(num_return_sequences):\n    tokens = x[i,:max_length].tolist()\n    decoded = enc.decode(tokens)\n    print('&gt;',decoded)\n\n\n\n\n\nTo do this, simply replace the GPT model initialization as below:\n\n\n#model = GPT.from_pretrained('gpt2')\nmodel = GPT(GPTConfig())\n\n\n\n\n\nDetect the most powerful device available and use it\n\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n    print(\"using GPU\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = 'mps'\n    print(\"using MPS\")\n\n\nNote that we guarded against device mismatch by initializing pos on the correct device.\n\n\npos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n\n\n\n\n\nAndrej’s favorite debugging dataset is Tiny Shakespeare which can be loaded and previewed as below:\n\n\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open(\"input.txt\", \"r\") as f:\n    text = f.read()\ndata = text[:1000]\nprint(data[:100])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\n\n\nTo get the corresponding GPT-2 tokens, load the tokenizer via tiktoken’s get_encoding method and encode the text data.\n\n\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(data)\nprint(tokens[:24])\n\n[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n\n\n\nAndrej’s favorite way to create a batch is to use view, which returns a new tensor with same data but different shape. It’s a view because the returned tensor shares the same underlying data with the original tensor and a change in one will affect the other.\nSince the desired outputs for every token in the (B,T) batch are just to the right of that token, extend the buffer by one element, take all but the last token as inputs and the first token onwards as outputs:\n\n\nimport torch\nbuf = torch.tensor(tokens[:24+1])\nx = buf[:-1].view(4,6)\ny = buf[1:].view(4,6)\nprint(x)\nprint(y)\n\ntensor([[ 5962, 22307,    25,   198,  8421,   356],\n        [ 5120,   597,  2252,    11,  3285,   502],\n        [ 2740,    13,   198,   198,  3237,    25],\n        [  198,  5248,   461,    11,  2740,    13]])\ntensor([[22307,    25,   198,  8421,   356,  5120],\n        [  597,  2252,    11,  3285,   502,  2740],\n        [   13,   198,   198,  3237,    25,   198],\n        [ 5248,   461,    11,  2740,    13,   198]])\n\n\n\n\n\n\nPass in the optional targets and calculate cross entropy loss. Cross entropy loss in PyTorch expects (BT,vocab_size) logits and (BT,) targets, so reshape it with view.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def forward(self, idx, targets = None):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        # --- Added code ---\n        loss = None\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        if targets: \n            # F.cross_entropy expects (B, T, vocab_size)-&gt; (B*T, vocab_size) shapes for logits\n            # and (B*T,) shape for targets. \n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n\n\n\n\nSee the Andrew Ng’s videos for a review of momentum and RMSProp that compose the Adam optimizer.\nMake sure to zero the gradients since loss.backwards() always accumulates gradients.\n\noptimizer.step() will update the parameters to (ideally) decrease the loss.\n\nloss.item() will convert the loss to a float that’s placed on CPU.\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\nfor i in range(50):\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n\n\nRunning the training loop will yield the following output:\n\n\n\n\nSource: initial training, image by the author\n\n\n\nNote that when the weights are randomly initialized, we expect each token in 0-50256 range to be equally likely. Thus we expect loss to be around -ln(1/50257) = 10.82. Currently, the loss starts around this value, which is a good sanity check.\nAlso, we can make sure that the training is set up correctly by overfitting on a single batch. Running the training loop for 500 iterations on the same batch yields: iteration 499, loss = 0.0008159472490660846\n\n\n\n\n\nTo build a simple data loader, advance by batches of size BT, set the input x* and output y as before, and loop around to the start of our tokens if run out of them:\n\n\nclass DataLoaderLite:\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = 0\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T\n        # if run out of tokens, loop around to zero\n        if self.current_position + B*T &gt;= len(self.tokens):\n            self.current_position = 0\n        return x, y\n\n\nUsing this data loader, loading and training can now be done concisely as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nNote that even with different batches of text loaded at every step, the loss still goes down quickly because many of the tokens will not occur in out data set and GPT2 can, for example, drive down the biases (mask) of all the logits that don’t occur in our data to negative infinity.\n\n\n\n\n\nObserve that transformer.wte.weight and lm_head.weight are of the same shape (50257*768).\nIn addition, we can below that these point to the same tensor.\n\n\n(sd_hf['lm_head.weight']==sd_hf['transformer.wte.weight']).all() # tensor(True)\nsd_hf['lm_head.weight'].data_ptr()==sd_hf['transformer.wte.weight'].data_ptr()\n\n\nThis weight tying scheme comes from p.5 of Attention is All You Need paper, which in turn used the work of Press and Wolf (2017). The idea is that if two tokens are very similar semantically, they should be close in the token embedding space. For the same reason, we expect these tokens to have similar probabilities in the output of a transformer. Press and Wolf (2017) argue that tying these weights leads to better performance. The weight scheme can be implemented in code as\n\n\nself.transformer.wte.weight = self.lm_head.weight\n\n\nNote that this weight sharing scheme also entails a significant memory savings of 38,597,376 parameters (768*50257) for the 124M model, which amounts to about 30% of the weights.\n\n\n\n\n\nOpenAI initialized the weights with mean 0 and standard deviation with 0.02 and the bias with 0. They initialized the token embeddings with mean 0 and standard deviation with 0.02 and position embeddings with 0.01.\nFollowing Xavier initialization, the standard deviation should be 1/sqrt(in_features_for_layer), and with 768 to 1600 features used by GPT-2 models of different sizes, 0.02 is approximately correct. Ideally, though, one would want the standard deviation to scale down more precisely with the model size.\nIn code, this leads to\n\n\nclass GPT(nn.Module):\n    ...\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n    \n\n\nIn addition, observe that our Block’s forward method contains a residual stream:\n\n\nclass Block(nn.Module):\n    ...\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNot accounting for this in initialization would lead to exploding variance as seen from the following simple example:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += torch.randn(768)\nprint(x.std())\n\ntensor(10.0776)\n\n\n\nHowever, scaling it by 1/sqrt(N) approximately yields the original standard deviation of 1 as seen below:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += n**-.5*torch.randn(768)\nprint(x.std())\n\ntensor(0.9894)\n\n\n\nTo implement it in our transformer, add flags in the attention and mlp modules since these have residual connections:\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        ...\nclass MLP(nn.Module):\n    def __init__(self,config):\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n     \n\n\nThen in the GPT module, check if a module has this attribute and adjust the standard deviation accordingly. Note that 2 multiplies self.config.n_layers because the residual connection is used twice: once for attention and once for the mlp module.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                # number of residual layers is double self.config.n_layers\n                # one for attention, one for mlp\n                std *= (2*self.config.n_layers)**-0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std) \n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n\n\n\n\nPress, O., & Wolf, L. (2017). Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859v3.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762 .\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., & Liu, T. (2020). On Layer Normalization in the Transformer Architecture. arXiv preprint arXiv:2002.04745."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#overall-tranformer-structure",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#overall-tranformer-structure",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "We’ll be implementing the right hand side (the decoder) that composes GPT-2: each unit on the right will be a block in our transformer.\n\n\n\n\nTransformer Architecture from “Attention Is All You Need” by Vaswani et al.\n\n\n\nThe configuration below is the configuration for the entire tranformer, with each layer h pertaining to one of the blocks. We want to replicate the following structure from a GPT-2 model in Huggingface Transformers:\n\n\n\n\nSource: HF Transformer, image by the author\n\n\n\nThe code below is the skeleton on GPT2 config and main module that will allow us to replicate that structure:\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 256\n    vocab_size: int = 65\n    n_layer: int = 6\n    n_head: int = 8\n    n_embd: int = 384\n\nclass GPT(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        # With nn.ModuleDict() index into submodules just like a dictionary\n        self.tranformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd)\n            )\n        )\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n\nnn.ModuleDict allows you to index into submodules using keys, just like a dictionary.\nnn.ModuleList allows us to index into each individual layer using an index, just like with a list"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#transformer-block",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#transformer-block",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Now let’s implement the Block,…\nUnlike the original GPT2 paper, establish a clean residual pathway by taking the layer norm of x and applying attention/multilayer perceptron layer to it then adding it to the input x. Since addition allows for an adulterated gradient flow during backpropagation, this pre-layer norm configuration is the better than the post-layer norm configuration where the norm is applied after the addition. More formally, Xiong et al. (2020) have shown that if post-layer norm is used, a warm-up stage is needed to avoid training instability whereas if pre-layer norm is used, the gradients are well-behaved at initialization. See the difference between original (Post-LN) GPT-2 implementation and the ‘corrected’ pre-LN implementation used here:\n\n\n\n\nSource: “On Layer Normalization in the Transformer Architecture” by Xiong et al. 2020\n\n\n\nFinally, onto the Block:\n\n\nclass Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config    \n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNote again how the layer norm is applied before the addition to the residual stream.\nAndrej notes that attention is a communication operation, where tokens communicate with each other and aggregate information. Thus attention can be thought of as a pooling function/weighted sum function/reduce operation. On the other hand, the multilayer perceptron (MLP) is applied to each token individually, with no information exchanged between the tokens. Thus attention is a reduce and MLP is the map operation and a transformer is a repeated application of MapReduce."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#multilayer-perceptron",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#multilayer-perceptron",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Briefly summarizing from Andrej’s previous video (Let’s build GPT: from scratch, in code, spelled out.), multilayer perceptron is implemented using a standard “bottleneck architecture” where the dimensions are first expanded to learn more complex representations, nonlinearity is applied to help the model learn more complex patterns, and finally the data is projected down again to keep the computational complexity in check.\n\n\nclass MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x    \n\n\nGPT-2 used an approximate version of GeLU because at the time of GPT-2’s creation, the erf function was very slow in TensorFlow and GPT-2 and the approximate version was used. Today there’s no reason to use the approximate version but Andrej is using the tanh approximation for veracity.\n\nAlso, GeLU is better than ReLU due to dead neuron problem since a local gradient is always present as seen below:\n\n\n\n\nSource: https://pytorch.org/docs/stable/generated/torch.nn.GELU.html"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#causal-self-attention",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#causal-self-attention",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Andrej’s attention implementation is a more efficient implementation of the following simple one from “Lets build GPT: from scratch, in code, spelled out”:\n\n\n\n\nSource: https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py\n\n\n\nNotice how in the implemenation above it is clear that the heads are parallel streams whose outputs are concatenated.\nThe idea of the more efficient implementation is to make another batch dimension with nh so that PyTorch effectively makes batches of dimension (B,nh) and applies all the operations on both B and nh in parallel.\n\nEach token emits query, key, and value. Queries and keys first multiply each other to deterimine “how interesting they find each other”.\n\nNext, we apply an autoregressive mask to make sure the tokens only attend to tokens before them.\n\nThe softmax normalizes the attention so it sums to 1.\nThe matrix multiply of attention with the values is a way, at every single token, to do a weigthed sum of the tokens each token finds intersting.\n\nTranspose, contiguous, and view reassembles everything in memory and performs what is equivalent of a concatenation operation.\n\ny.transpose(1,2): This line swaps the second and third dimensions of y. So the shape of y changes from (B, nh, T, hs) to (B, T, nh, hs).\n.contiguous(): This is used to ensure that the tensor is stored in a contiguous block of memory, which is required for some operations in PyTorch, including view.\n.view(B, T, C): This line reshapes the tensor y to have dimensions (B, T, C). Here, C is equal to nh*hs, which means that the last two dimensions of y (nh and hs) are flattened into a single dimension. This effectively concatenates the outputs of all the attention heads side by side.\n\nFinally, the output projection doesn’t change the dimension of y, but does introduce another learnable transformation so that the output can be projected in a way that is most useful for downstream tasks.\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        #key, qury, value projections for all heads in a batch\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd*3)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        # mask to prevent attention to future tokens\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                             .view(1, 1, config.block_size, config.block_size)) # becomes available as self.bias\n    def forward(self, x):\n        B, T, C = x.size()\n        # calculate query, key, value for all heads in batch\n        qkv = self.c_attn(x) # (B,T, self.n_embd) x (self.n_embd,self.n_embd*3) = (B,T,self.n_embd*3)\n        q, k, v  = qkv.split(self.n_embd, dim=2) # (B,T,self.n_embd) x 3; make each split size self.n_embd by splitting dim 2\n        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2) # (B, nh, T, hs)\n        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        # attention materializes a large (T,T) matrix fo each query and key\n        att = (q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) = (B, nh, T, T)\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n        # Change (B, nh, T, hs) to (B, T, nh, hs) with transpose, reassemle in memory, (B,T,C) makes nh*hs = n_embd (C)\n        y = y.transpose(1,2).contiguous().view(B, T, C) \n        # output projection: additional learnable transformation\n        y = self.c_proj(y) # (B, T, C)@(C, C) = (B, T, C)\n        return y"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#copy-over-the-hugging-face-gpt-2-model-parameters-into-our-model",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Ignore the attention mask buffers (these are not parameters)\nThe weights in Hugging Face version are transposed (as they are in the original TensorFlow implementation) from what PyTorch needs because they use Conv1D module. Since we want to use plan nn.Linear, we hardcode these and transpose them.\n\n\nclass GPT(nn.Module):\n    ...\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type) # HF GPT2LMHeadModel has .from_pretrained method, just like ours\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#forward",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#forward",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "The nn.Embedding layer in PyTorch takes an input tensor of arbitrary shape with values in the range [0, vocab_size) and maps each integer in that range to a dense vector of size C (the embedding dimension). In our case, we have an input tensor idx of shape (B, T), where B is the batch size and T is the sequence length, then applying the embedding layer to idx will yield a tensor of shape (B, T, C). This is because each integer in idx is replaced with its corresponding embedding vector. Since the embedding vectors have C elements, this adds an extra dimension of size C to the output. So for every batch and every sequence position, an embedding of size C is constructed, resulting in an output of shape (B, T, C).\nAlso note that the position embedding is broadcast to the token embedding and the same position embedding vector is learned at (T,C) for every element in B. This works because the position embeddings are independent of the specific sequence it’s in, so it can be shared across all sequences in the batch.\n\n\nclass GPT2(nn.Module):\n    ...    \n    def forward(self, idx):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        return logits"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#generate-next-sequence-predictions-with-weights-from-pretrained-gpt2-model",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "The goal is to get close to generations from the Hugging Face pipeline:\n\n\nfrom transformers import pipeline, set_seed\ngenerator = pipeline('text-generation', model='gpt2')\nset_seed(42)\ngenerator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n\nTo generate next token predictions, get the next top 50 tokens so that the model does not deviate too much from likely tokens, and sample one token from this distribution.\nConcatenate the token obtained from sampling with input (or input plus previously sampled tokens) at each step.\nThe sampling will not match the Hugging Face generations exactly since there’s likely a parameter hiding in the pipeline that’s different, but will be sensible English.\n\n\nnum_return_sequences = 5\nmax_length = 30\nmodel = GPT.from_pretrained('gpt2')\nmodel.eval()\nmodel.to('cuda')\n\n# prefix tokens\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\ntokens = enc.encode(\"Hello I'm a language model, \")\nx = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences,1).to('cuda') # (5,8) since sent. tokenized to 8 tokens\n\n# generate: with each loop iteration, generate one more token\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nwhile x.size(1) &lt; max_length:\n    with torch.no_grad():\n        logits = model(x) # (B,T,vocab_size)\n        logits = logits[:, -1, :]  # take the logits at the last position\n        probs = F.softmax(logits, dim=-1) # get the probabilities\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # get the top-50 tokens\n        ix = torch.multinomial(topk_probs, num_samples=1) # sample from the top 50\n        xcol = torch.gather(topk_indices, -1, ix) # select the indices of the sampled tokens\n        x = torch.cat((x, xcol), dim=1) # append the sampled token to the sequence\n\nfor i in range(num_return_sequences):\n    tokens = x[i,:max_length].tolist()\n    decoded = enc.decode(tokens)\n    print('&gt;',decoded)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#initialize-a-random-model",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#initialize-a-random-model",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "To do this, simply replace the GPT model initialization as below:\n\n\n#model = GPT.from_pretrained('gpt2')\nmodel = GPT(GPTConfig())"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#autodetect-device",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#autodetect-device",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Detect the most powerful device available and use it\n\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n    print(\"using GPU\")\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = 'mps'\n    print(\"using MPS\")\n\n\nNote that we guarded against device mismatch by initializing pos on the correct device.\n\n\npos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#load-the-tiny-shakespeare-data-set-for-quick-debugging-and-load-a-batch-of-data",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Andrej’s favorite debugging dataset is Tiny Shakespeare which can be loaded and previewed as below:\n\n\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open(\"input.txt\", \"r\") as f:\n    text = f.read()\ndata = text[:1000]\nprint(data[:100])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\n\n\nTo get the corresponding GPT-2 tokens, load the tokenizer via tiktoken’s get_encoding method and encode the text data.\n\n\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(data)\nprint(tokens[:24])\n\n[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n\n\n\nAndrej’s favorite way to create a batch is to use view, which returns a new tensor with same data but different shape. It’s a view because the returned tensor shares the same underlying data with the original tensor and a change in one will affect the other.\nSince the desired outputs for every token in the (B,T) batch are just to the right of that token, extend the buffer by one element, take all but the last token as inputs and the first token onwards as outputs:\n\n\nimport torch\nbuf = torch.tensor(tokens[:24+1])\nx = buf[:-1].view(4,6)\ny = buf[1:].view(4,6)\nprint(x)\nprint(y)\n\ntensor([[ 5962, 22307,    25,   198,  8421,   356],\n        [ 5120,   597,  2252,    11,  3285,   502],\n        [ 2740,    13,   198,   198,  3237,    25],\n        [  198,  5248,   461,    11,  2740,    13]])\ntensor([[22307,    25,   198,  8421,   356,  5120],\n        [  597,  2252,    11,  3285,   502,  2740],\n        [   13,   198,   198,  3237,    25,   198],\n        [ 5248,   461,    11,  2740,    13,   198]])"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#update-the-forward-pass-to-calculate-the-loss",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#update-the-forward-pass-to-calculate-the-loss",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Pass in the optional targets and calculate cross entropy loss. Cross entropy loss in PyTorch expects (BT,vocab_size) logits and (BT,) targets, so reshape it with view.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def forward(self, idx, targets = None):\n        # input indices are always of shape (B, T) where B is batch size and T is block size\n        B, T = idx.size()\n        assert T &lt;= self.config.block_size, \"Cannot forward, model block size is exhausted.\"\n        # forward the token and position embeddings\n        pos = torch.arange(0, T , dtype=torch.long, device=idx.device) # shape (T,)\n        tok_emb = self.transformer.wte(idx) # (B,T)-&gt; (B, T, C)\n        pos_emb = self.transformer.wpe(pos) # (T,)-&gt;     (T, C) \n        x = tok_emb + pos_emb               # (B, T, C) + (T, C) -&gt; (B, T, C) via broadcasting\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layer norm and the classifier head\n        x = self.transformer.ln_f(x)\n        # --- Added code ---\n        loss = None\n        logits = self.lm_head(x) # shape (B, T, vocab_size)\n        if targets: \n            # F.cross_entropy expects (B, T, vocab_size)-&gt; (B*T, vocab_size) shapes for logits\n            # and (B*T,) shape for targets. \n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#optimizer-and-training-loop",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#optimizer-and-training-loop",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "See the Andrew Ng’s videos for a review of momentum and RMSProp that compose the Adam optimizer.\nMake sure to zero the gradients since loss.backwards() always accumulates gradients.\n\noptimizer.step() will update the parameters to (ideally) decrease the loss.\n\nloss.item() will convert the loss to a float that’s placed on CPU.\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\nfor i in range(50):\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n\n\nRunning the training loop will yield the following output:\n\n\n\n\nSource: initial training, image by the author\n\n\n\nNote that when the weights are randomly initialized, we expect each token in 0-50256 range to be equally likely. Thus we expect loss to be around -ln(1/50257) = 10.82. Currently, the loss starts around this value, which is a good sanity check.\nAlso, we can make sure that the training is set up correctly by overfitting on a single batch. Running the training loop for 500 iterations on the same batch yields: iteration 499, loss = 0.0008159472490660846"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#data-loader-lite",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#data-loader-lite",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "To build a simple data loader, advance by batches of size BT, set the input x* and output y as before, and loop around to the start of our tokens if run out of them:\n\n\nclass DataLoaderLite:\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n        # load the data from disk into memory\n        with open(\"input.txt\", \"r\") as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        self.tokens = torch.tensor(enc.encode(text))\n        print(f\"total tokens: {len(self.tokens)}\")\n        print(f\"1 epoch = {len(self.tokens)//(B*T)} batches\")\n    \n        self.current_position = 0\n    \n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position:self.current_position+B*T+1]\n        x = (buf[:-1]).view(B,T)\n        y = (buf[1:]).view(B,T)\n        self.current_position += B*T\n        # if run out of tokens, loop around to zero\n        if self.current_position + B*T &gt;= len(self.tokens):\n            self.current_position = 0\n        return x, y\n\n\nUsing this data loader, loading and training can now be done concisely as below:\n\n\ntrain_loader = DataLoaderLite(B=4,T=32)\nmodel = GPT(GPTConfig())\nmodel.to(device)\n# create an optimizer object\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\nfor i in range(50):\n    x,y = train_loader.next_batch()\n    x,y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x,y)\n    loss.backward()\n    optimizer.step()\n    print(f'iteration {i}, loss = {loss.item()}')\n    \n\n\nNote that even with different batches of text loaded at every step, the loss still goes down quickly because many of the tokens will not occur in out data set and GPT2 can, for example, drive down the biases (mask) of all the logits that don’t occur in our data to negative infinity."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#parameter-sharing-via-weight-tying",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#parameter-sharing-via-weight-tying",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Observe that transformer.wte.weight and lm_head.weight are of the same shape (50257*768).\nIn addition, we can below that these point to the same tensor.\n\n\n(sd_hf['lm_head.weight']==sd_hf['transformer.wte.weight']).all() # tensor(True)\nsd_hf['lm_head.weight'].data_ptr()==sd_hf['transformer.wte.weight'].data_ptr()\n\n\nThis weight tying scheme comes from p.5 of Attention is All You Need paper, which in turn used the work of Press and Wolf (2017). The idea is that if two tokens are very similar semantically, they should be close in the token embedding space. For the same reason, we expect these tokens to have similar probabilities in the output of a transformer. Press and Wolf (2017) argue that tying these weights leads to better performance. The weight scheme can be implemented in code as\n\n\nself.transformer.wte.weight = self.lm_head.weight\n\n\nNote that this weight sharing scheme also entails a significant memory savings of 38,597,376 parameters (768*50257) for the 124M model, which amounts to about 30% of the weights."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#model-initialization",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#model-initialization",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "OpenAI initialized the weights with mean 0 and standard deviation with 0.02 and the bias with 0. They initialized the token embeddings with mean 0 and standard deviation with 0.02 and position embeddings with 0.01.\nFollowing Xavier initialization, the standard deviation should be 1/sqrt(in_features_for_layer), and with 768 to 1600 features used by GPT-2 models of different sizes, 0.02 is approximately correct. Ideally, though, one would want the standard deviation to scale down more precisely with the model size.\nIn code, this leads to\n\n\nclass GPT(nn.Module):\n    ...\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n    \n\n\nIn addition, observe that our Block’s forward method contains a residual stream:\n\n\nclass Block(nn.Module):\n    ...\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nNot accounting for this in initialization would lead to exploding variance as seen from the following simple example:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += torch.randn(768)\nprint(x.std())\n\ntensor(10.0776)\n\n\n\nHowever, scaling it by 1/sqrt(N) approximately yields the original standard deviation of 1 as seen below:\n\n\nx = torch.zeros(768)\nn = 100\nfor _ in range(n):\n    x += n**-.5*torch.randn(768)\nprint(x.std())\n\ntensor(0.9894)\n\n\n\nTo implement it in our transformer, add flags in the attention and mlp modules since these have residual connections:\n\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config) -&gt; None:\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n        ...\nclass MLP(nn.Module):\n    def __init__(self,config):\n        ...\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n     \n\n\nThen in the GPT module, check if a module has this attribute and adjust the standard deviation accordingly. Note that 2 multiplies self.config.n_layers because the residual connection is used twice: once for attention and once for the mlp module.\n\n\nclass GPT(nn.Module):\n    ...\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n                # number of residual layers is double self.config.n_layers\n                # one for attention, one for mlp\n                std *= (2*self.config.n_layers)**-0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std) \n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 1: Build GPT-2.html#references",
    "title": "Part 1 - Building GPT-2 Architecture",
    "section": "",
    "text": "Press, O., & Wolf, L. (2017). Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859v3.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention Is All You Need. arXiv preprint arXiv:1706.03762 .\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., & Liu, T. (2020). On Layer Normalization in the Transformer Architecture. arXiv preprint arXiv:2002.04745."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "In Part3: Train GPT-2, all the pieces of training GPT-2 from scratch were present except training the tokenizer. For that, I took the GPT-2 tokenizer settings available from Hugging Face and trained a new tokenizer with a smaller vocabulary. In this part, I’ll attempt to train a GPT-2-style tokenizer from scratch following Let’s build the GPT Tokenizer. The custom tokenizer will not match the GPT-2 tokenizer exactly (likely since there are implementation details OpenAI didn’t discuss) and will not be fast enough to use in it’s base Python version. However, the main goal here is to study the mechanics of the tokenization process.\nGPT-2 tokenizer uses the UTF-8 unicode encoding standard. In the context of UTF-8, a code point is a unique numerical value assigned to a character, symbol, or glyph in the Unicode character set. UTF-8 encodes ASCII characters in 1 byte, and may use up to 4 bytes for non-ASCII characters as described in the following Wikipedia article:\n\n\n\n\nSource: https://en.wikipedia.org/wiki/UTF-8\n\n\n\nBefore the text is tokenized, it is converted to UTF-8. This provides many advantages, including\n\nUnified representation across languages and scripts\nHandling of rare tokens\nBetter handling of ‘subwords’ for languages where this concept is different than, say, that of standard English\n\nA sample of Unicode text is below:\n\n\ntext = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view. I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\"\ntokens = text.encode(\"utf-8\")\nprint(tokens)\ntokens = list(map(int,tokens)) # convert to ints for convenience\nprint(tokens)\nprint(f\"length of text: {len(text)}\")\nprint(f\"length of tokens: {len(tokens)}\") #larger because complex characters become multiply bytes (up to 4)\n\nb'\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef\\xbd\\x83\\xef\\xbd\\x8f\\xef\\xbd\\x84\\xef\\xbd\\x85! \\xf0\\x9f\\x85\\xa4\\xf0\\x9f\\x85\\x9d\\xf0\\x9f\\x85\\x98\\xf0\\x9f\\x85\\x92\\xf0\\x9f\\x85\\x9e\\xf0\\x9f\\x85\\x93\\xf0\\x9f\\x85\\x94\\xe2\\x80\\xbd \\xf0\\x9f\\x87\\xba\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb3\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xae\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa8\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb4\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa9\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xaa! \\xf0\\x9f\\x98\\x84 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \\xe2\\x80\\x9csupport Unicode\\xe2\\x80\\x9d in our software (whatever that means\\xe2\\x80\\x94like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don\\xe2\\x80\\x99t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode\\xe2\\x80\\x99s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I\\xe2\\x80\\x99ll give an introduction to it from a programmer\\xe2\\x80\\x99s point of view. I\\xe2\\x80\\x99m going to focus on the character set and what\\xe2\\x80\\x99s involved in working with strings and files of Unicode text. However, in this article I\\xe2\\x80\\x99m not going to talk about fonts, text layout/shaping/rendering, or localization in detail\\xe2\\x80\\x94those are separate issues, beyond my scope (and knowledge) here.'\n[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46, 32, 65, 32, 102, 101, 119, 32, 109, 111, 110, 116, 104, 115, 32, 97, 103, 111, 44, 32, 73, 32, 103, 111, 116, 32, 105, 110, 116, 101, 114, 101, 115, 116, 101, 100, 32, 105, 110, 32, 85, 110, 105, 99, 111, 100, 101, 32, 97, 110, 100, 32, 100, 101, 99, 105, 100, 101, 100, 32, 116, 111, 32, 115, 112, 101, 110, 100, 32, 115, 111, 109, 101, 32, 116, 105, 109, 101, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 109, 111, 114, 101, 32, 97, 98, 111, 117, 116, 32, 105, 116, 32, 105, 110, 32, 100, 101, 116, 97, 105, 108, 46, 32, 73, 110, 32, 116, 104, 105, 115, 32, 97, 114, 116, 105, 99, 108, 101, 44, 32, 73, 226, 128, 153, 108, 108, 32, 103, 105, 118, 101, 32, 97, 110, 32, 105, 110, 116, 114, 111, 100, 117, 99, 116, 105, 111, 110, 32, 116, 111, 32, 105, 116, 32, 102, 114, 111, 109, 32, 97, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 226, 128, 153, 115, 32, 112, 111, 105, 110, 116, 32, 111, 102, 32, 118, 105, 101, 119, 46, 32, 73, 226, 128, 153, 109, 32, 103, 111, 105, 110, 103, 32, 116, 111, 32, 102, 111, 99, 117, 115, 32, 111, 110, 32, 116, 104, 101, 32, 99, 104, 97, 114, 97, 99, 116, 101, 114, 32, 115, 101, 116, 32, 97, 110, 100, 32, 119, 104, 97, 116, 226, 128, 153, 115, 32, 105, 110, 118, 111, 108, 118, 101, 100, 32, 105, 110, 32, 119, 111, 114, 107, 105, 110, 103, 32, 119, 105, 116, 104, 32, 115, 116, 114, 105, 110, 103, 115, 32, 97, 110, 100, 32, 102, 105, 108, 101, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 101, 32, 116, 101, 120, 116, 46, 32, 72, 111, 119, 101, 118, 101, 114, 44, 32, 105, 110, 32, 116, 104, 105, 115, 32, 97, 114, 116, 105, 99, 108, 101, 32, 73, 226, 128, 153, 109, 32, 110, 111, 116, 32, 103, 111, 105, 110, 103, 32, 116, 111, 32, 116, 97, 108, 107, 32, 97, 98, 111, 117, 116, 32, 102, 111, 110, 116, 115, 44, 32, 116, 101, 120, 116, 32, 108, 97, 121, 111, 117, 116, 47, 115, 104, 97, 112, 105, 110, 103, 47, 114, 101, 110, 100, 101, 114, 105, 110, 103, 44, 32, 111, 114, 32, 108, 111, 99, 97, 108, 105, 122, 97, 116, 105, 111, 110, 32, 105, 110, 32, 100, 101, 116, 97, 105, 108, 226, 128, 148, 116, 104, 111, 115, 101, 32, 97, 114, 101, 32, 115, 101, 112, 97, 114, 97, 116, 101, 32, 105, 115, 115, 117, 101, 115, 44, 32, 98, 101, 121, 111, 110, 100, 32, 109, 121, 32, 115, 99, 111, 112, 101, 32, 40, 97, 110, 100, 32, 107, 110, 111, 119, 108, 101, 100, 103, 101, 41, 32, 104, 101, 114, 101, 46]\nlength of text: 1021\nlength of tokens: 1116\n\n\n\nThe main idea of the Byte Pair Encoding (BPE) tokenizer is to identify and merge most common pairs, add them to vocabulary, and replace the common pairs with their merged representations. This allows for more compact representations of text, allowing us to fit more information in our limited context window.\n\nAndrej notes that he would like nothing less than to have models that work directly with Unicode code points, bypassing tokenization completely. This would avoid many of the issues related to tokenization. The main challenge with this approach currently is that the sequences would get too stretched out, allowing smaller effective context.\nMy version of implementing the merging logic is below:\n\n\ndef most_common_pair(text,debug=False):\n    \"\"\"DPV's implementation of getting most common pair\"\"\"\n    d = dict()\n    for char1, char2 in zip(text[:-1],text[1:]):\n        d[(char1,char2)] = d.get((char1,char2),0)+1\n    max_pair= max(d,key = d.get)\n    if debug:\n        print(f\"Number of occurrences at {max_pair} is {d[max_pair]}\")\n        print(d)\n    return max_pair\nmost_common_pair(text,debug=True)\n\nNumber of occurrences at ('e', ' ') is 32\n{('Ｕ', 'ｎ'): 1, ('ｎ', 'ｉ'): 1, ('ｉ', 'ｃ'): 1, ('ｃ', 'ｏ'): 1, ('ｏ', 'ｄ'): 1, ('ｄ', 'ｅ'): 1, ('ｅ', '!'): 1, ('!', ' '): 2, (' ', '🅤'): 1, ('🅤', '🅝'): 1, ('🅝', '🅘'): 1, ('🅘', '🅒'): 1, ('🅒', '🅞'): 1, ('🅞', '🅓'): 1, ('🅓', '🅔'): 1, ('🅔', '‽'): 1, ('‽', ' '): 1, (' ', '🇺'): 1, ('🇺', '\\u200c'): 1, ('\\u200c', '🇳'): 1, ('🇳', '\\u200c'): 1, ('\\u200c', '🇮'): 1, ('🇮', '\\u200c'): 1, ('\\u200c', '🇨'): 1, ('🇨', '\\u200c'): 1, ('\\u200c', '🇴'): 1, ('🇴', '\\u200c'): 1, ('\\u200c', '🇩'): 1, ('🇩', '\\u200c'): 1, ('\\u200c', '🇪'): 1, ('🇪', '!'): 1, (' ', '😄'): 1, ('😄', ' '): 1, (' ', 'T'): 1, ('T', 'h'): 1, ('h', 'e'): 8, ('e', ' '): 32, (' ', 'v'): 2, ('v', 'e'): 6, ('e', 'r'): 12, ('r', 'y'): 2, ('y', ' '): 3, (' ', 'n'): 3, ('n', 'a'): 1, ('a', 'm'): 5, ('m', 'e'): 9, (' ', 's'): 11, ('s', 't'): 7, ('t', 'r'): 5, ('r', 'i'): 6, ('i', 'k'): 2, ('k', 'e'): 2, ('e', 's'): 6, ('s', ' '): 18, (' ', 'f'): 9, ('f', 'e'): 2, ('e', 'a'): 5, ('a', 'r'): 13, ('r', ' '): 8, (' ', 'a'): 21, ('a', 'n'): 15, ('n', 'd'): 13, ('d', ' '): 13, ('a', 'w'): 1, ('w', 'e'): 3, (' ', 'i'): 17, ('i', 'n'): 28, ('n', 't'): 9, ('t', 'o'): 7, ('o', ' '): 7, (' ', 't'): 20, ('t', 'h'): 14, (' ', 'h'): 2, ('r', 't'): 5, ('t', 's'): 4, (' ', 'o'): 8, ('o', 'f'): 5, ('f', ' '): 4, (' ', 'p'): 5, ('p', 'r'): 3, ('r', 'o'): 5, ('o', 'g'): 3, ('g', 'r'): 3, ('r', 'a'): 5, ('m', 'm'): 3, ('r', 's'): 3, (' ', 'w'): 7, ('w', 'o'): 2, ('o', 'r'): 9, ('r', 'l'): 1, ('l', 'd'): 1, ('d', 'w'): 1, ('w', 'i'): 2, ('i', 'd'): 3, ('d', 'e'): 12, ('e', '.'): 2, ('.', ' '): 7, (' ', 'W'): 1, ('W', 'e'): 1, ('a', 'l'): 4, ('l', 'l'): 4, ('l', ' '): 4, (' ', 'k'): 2, ('k', 'n'): 2, ('n', 'o'): 4, ('o', 'w'): 3, ('w', ' '): 2, ('o', 'u'): 7, ('u', 'g'): 1, ('g', 'h'): 2, ('h', 't'): 2, ('t', ' '): 15, (' ', '“'): 1, ('“', 's'): 1, ('s', 'u'): 3, ('u', 'p'): 2, ('p', 'p'): 2, ('p', 'o'): 3, (' ', 'U'): 6, ('U', 'n'): 6, ('n', 'i'): 7, ('i', 'c'): 8, ('c', 'o'): 7, ('o', 'd'): 7, ('e', '”'): 1, ('”', ' '): 1, ('n', ' '): 15, ('u', 'r'): 1, ('s', 'o'): 2, ('f', 't'): 2, ('t', 'w'): 1, ('w', 'a'): 1, ('r', 'e'): 8, (' ', '('): 2, ('(', 'w'): 1, ('w', 'h'): 3, ('h', 'a'): 7, ('a', 't'): 6, ('t', 'e'): 10, ('e', 'v'): 3, (' ', 'm'): 6, ('n', 's'): 2, ('s', '—'): 1, ('—', 'l'): 1, ('l', 'i'): 3, (' ', 'u'): 1, ('u', 's'): 6, ('s', 'i'): 1, ('n', 'g'): 13, ('g', ' '): 8, ('w', 'c'): 1, ('c', 'h'): 2, ('r', '_'): 1, ('_', 't'): 1, ('f', 'o'): 4, ('g', 's'): 2, ('s', ','): 6, (',', ' '): 11, (' ', 'r'): 2, ('i', 'g'): 1, ('t', '?'): 1, ('?', ')'): 1, (')', '.'): 1, (' ', 'B'): 1, ('B', 'u'): 1, ('u', 't'): 4, (' ', 'c'): 3, ('c', 'a'): 3, (' ', 'b'): 4, ('b', 'e'): 3, ('a', 'b'): 3, ('b', 's'): 1, ('r', 'u'): 1, ('s', 'e'): 4, ('e', ','): 2, (' ', 'd'): 6, ('d', 'i'): 2, ('i', 'v'): 2, ('v', 'i'): 2, ('h', 'o'): 3, ('s', 'a'): 1, ('d', '-'): 1, ('-', 'p'): 1, ('p', 'a'): 2, ('a', 'g'): 2, ('g', 'e'): 2, (' ', 'S'): 1, ('S', 't'): 1, ('t', 'a'): 5, ('d', 'a'): 2, ('r', 'd'): 1, ('p', 'l'): 2, ('l', 'u'): 1, ('i', 't'): 5, ('d', 'o'): 2, ('o', 'z'): 1, ('z', 'e'): 1, ('e', 'n'): 5, ('l', 'e'): 8, ('e', 'm'): 1, ('n', 'n'): 1, ('n', 'e'): 1, ('e', 'x'): 3, ('x', 'e'): 1, ('e', 'p'): 3, ('o', 't'): 3, ('m', 'o'): 3, ('a', ' '): 2, (' ', 'l'): 4, ('t', 't'): 1, ('t', 'l'): 1, ('t', 'i'): 9, ('i', 'm'): 2, ('m', 'i'): 1, ('g', '.'): 1, (' ', 'I'): 6, ('I', ' '): 2, ('o', 'n'): 8, ('n', '’'): 1, ('’', 't'): 1, ('b', 'l'): 1, ('l', 'a'): 2, ('i', 'l'): 4, ('f', 'i'): 2, ('o', 'l'): 2, ('h', 'i'): 3, ('m', 'y'): 2, ('y', 's'): 1, ('i', 'o'): 4, (' ', 'e'): 1, (' ', '3'): 1, ('3', '0'): 1, ('0', ' '): 1, (' ', 'y'): 1, ('y', 'e'): 1, ('a', 'f'): 1, ('e', '’'): 1, ('’', 's'): 3, ('n', 'c'): 1, ('c', 'e'): 1, ('p', 't'): 1, ('n', '.'): 1, (' ', 'A'): 1, ('A', ' '): 1, ('e', 'w'): 2, ('h', 's'): 1, ('g', 'o'): 4, ('o', ','): 1, (' ', 'g'): 4, ('e', 'd'): 4, ('e', 'c'): 1, ('c', 'i'): 1, ('s', 'p'): 1, ('p', 'e'): 2, ('o', 'm'): 2, ('r', 'n'): 1, ('b', 'o'): 2, ('e', 't'): 3, ('a', 'i'): 2, ('l', '.'): 1, ('I', 'n'): 1, ('i', 's'): 3, ('c', 'l'): 2, ('I', '’'): 3, ('’', 'l'): 1, ('g', 'i'): 1, ('d', 'u'): 1, ('u', 'c'): 1, ('c', 't'): 2, ('f', 'r'): 1, ('m', ' '): 3, ('r', '’'): 1, ('o', 'i'): 3, ('i', 'e'): 1, ('w', '.'): 1, ('’', 'm'): 2, ('o', 'c'): 2, ('c', 'u'): 1, ('a', 'c'): 1, ('t', '’'): 1, ('n', 'v'): 1, ('v', 'o'): 1, ('l', 'v'): 1, ('r', 'k'): 1, ('k', 'i'): 1, ('h', ' '): 1, ('x', 't'): 2, ('t', '.'): 1, (' ', 'H'): 1, ('H', 'o'): 1, ('r', ','): 1, ('l', 'k'): 1, ('k', ' '): 1, ('a', 'y'): 1, ('y', 'o'): 2, ('t', '/'): 1, ('/', 's'): 1, ('s', 'h'): 1, ('a', 'p'): 1, ('p', 'i'): 1, ('g', '/'): 1, ('/', 'r'): 1, ('g', ','): 1, ('l', 'o'): 1, ('i', 'z'): 1, ('z', 'a'): 1, ('l', '—'): 1, ('—', 't'): 1, ('o', 's'): 1, ('s', 's'): 1, ('u', 'e'): 1, ('e', 'y'): 1, ('s', 'c'): 1, ('o', 'p'): 1, ('(', 'a'): 1, ('w', 'l'): 1, ('d', 'g'): 1, ('e', ')'): 1, (')', ' '): 1}\n\n\n('e', ' ')\n\n\n\nmost_common_pair(tokens,debug=True)\n\nNumber of occurrences at (101, 32) is 32\n{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 18, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 8, (101, 32): 32, (32, 118): 2, (118, 101): 6, (101, 114): 12, (114, 121): 2, (121, 32): 3, (32, 110): 3, (110, 97): 1, (97, 109): 5, (109, 101): 9, (32, 115): 11, (115, 116): 7, (116, 114): 5, (114, 105): 6, (105, 107): 2, (107, 101): 2, (101, 115): 6, (115, 32): 18, (32, 102): 9, (102, 101): 2, (101, 97): 5, (97, 114): 13, (114, 32): 8, (32, 97): 21, (97, 110): 15, (110, 100): 13, (100, 32): 13, (97, 119): 1, (119, 101): 3, (32, 105): 17, (105, 110): 28, (110, 116): 9, (116, 111): 7, (111, 32): 7, (32, 116): 20, (116, 104): 14, (32, 104): 2, (114, 116): 5, (116, 115): 4, (32, 111): 8, (111, 102): 5, (102, 32): 4, (32, 112): 5, (112, 114): 3, (114, 111): 5, (111, 103): 3, (103, 114): 3, (114, 97): 5, (109, 109): 3, (114, 115): 3, (32, 119): 7, (119, 111): 2, (111, 114): 9, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 2, (105, 100): 3, (100, 101): 12, (101, 46): 2, (46, 32): 7, (32, 87): 1, (87, 101): 1, (97, 108): 4, (108, 108): 4, (108, 32): 4, (32, 107): 2, (107, 110): 2, (110, 111): 4, (111, 119): 3, (119, 32): 2, (111, 117): 7, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 15, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 3, (117, 112): 2, (112, 112): 2, (112, 111): 3, (32, 85): 6, (85, 110): 6, (110, 105): 7, (105, 99): 8, (99, 111): 7, (111, 100): 7, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 15, (117, 114): 1, (115, 111): 2, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 8, (32, 40): 2, (40, 119): 1, (119, 104): 3, (104, 97): 7, (97, 116): 6, (116, 101): 10, (101, 118): 3, (32, 109): 6, (110, 115): 2, (115, 226): 1, (128, 148): 2, (148, 108): 1, (108, 105): 3, (32, 117): 1, (117, 115): 6, (115, 105): 1, (110, 103): 13, (103, 32): 8, (119, 99): 1, (99, 104): 2, (114, 95): 1, (95, 116): 1, (102, 111): 4, (103, 115): 2, (115, 44): 6, (44, 32): 11, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 4, (32, 99): 3, (99, 97): 3, (32, 98): 4, (98, 101): 3, (97, 98): 3, (98, 115): 1, (114, 117): 1, (115, 101): 4, (101, 44): 2, (32, 100): 6, (100, 105): 2, (105, 118): 2, (118, 105): 2, (104, 111): 3, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 2, (97, 103): 2, (103, 101): 2, (32, 83): 1, (83, 116): 1, (116, 97): 5, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 5, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 5, (108, 101): 8, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 3, (120, 101): 1, (101, 112): 3, (111, 116): 3, (109, 111): 3, (97, 32): 2, (32, 108): 4, (116, 116): 1, (116, 108): 1, (116, 105): 9, (105, 109): 2, (109, 105): 1, (103, 46): 1, (32, 73): 6, (73, 32): 2, (111, 110): 8, (110, 226): 1, (128, 153): 7, (153, 116): 1, (98, 108): 1, (108, 97): 2, (105, 108): 4, (102, 105): 2, (111, 108): 2, (104, 105): 3, (109, 121): 2, (121, 115): 1, (105, 111): 4, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 3, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1, (32, 65): 1, (65, 32): 1, (101, 119): 2, (104, 115): 1, (103, 111): 4, (111, 44): 1, (32, 103): 4, (101, 100): 4, (101, 99): 1, (99, 105): 1, (115, 112): 1, (112, 101): 2, (111, 109): 2, (114, 110): 1, (98, 111): 2, (101, 116): 3, (97, 105): 2, (108, 46): 1, (73, 110): 1, (105, 115): 3, (99, 108): 2, (73, 226): 3, (153, 108): 1, (103, 105): 1, (100, 117): 1, (117, 99): 1, (99, 116): 2, (102, 114): 1, (109, 32): 3, (114, 226): 1, (111, 105): 3, (105, 101): 1, (119, 46): 1, (153, 109): 2, (111, 99): 2, (99, 117): 1, (97, 99): 1, (116, 226): 1, (110, 118): 1, (118, 111): 1, (108, 118): 1, (114, 107): 1, (107, 105): 1, (104, 32): 1, (120, 116): 2, (116, 46): 1, (32, 72): 1, (72, 111): 1, (114, 44): 1, (108, 107): 1, (107, 32): 1, (97, 121): 1, (121, 111): 2, (116, 47): 1, (47, 115): 1, (115, 104): 1, (97, 112): 1, (112, 105): 1, (103, 47): 1, (47, 114): 1, (103, 44): 1, (108, 111): 1, (105, 122): 1, (122, 97): 1, (108, 226): 1, (148, 116): 1, (111, 115): 1, (115, 115): 1, (117, 101): 1, (101, 121): 1, (115, 99): 1, (111, 112): 1, (40, 97): 1, (119, 108): 1, (100, 103): 1, (101, 41): 1, (41, 32): 1}\n\n\n(101, 32)\n\n\n\nAndrej splits this logic into the following two utility functions:\n\n\n# Andrej's implementation\ndef get_stats(ids, counts=None):\n    \"\"\"\n    Given a list of integers, return a dictionary of counts of consecutive pairs\n    Example: [1, 2, 3, 1, 2] -&gt; {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n    Optionally allows to update an existing dictionary of counts\n    \"\"\"\n    counts = {} if counts is None else counts\n    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\nstats = get_stats(tokens)\ntop_pair = max(stats, key=stats.get)\ntop_pair\n\n(101, 32)\n\n\n\ndef merge(ids, pair, idx):\n    \"\"\"Andrej's/DPV's implementations match\"\"\"\n    i = 0\n    out = []\n    while i &lt; len(ids):\n        if (i&lt;len(ids)-1) and(ids[i],ids[i+1])==pair:\n            out.append(idx)\n            i+=2 # merging!\n        else:\n            out.append(ids[i])\n            i+=1 #moving by one\n    return out\nprint(merge(tokens, top_pair, 256))\n\n[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46, 32, 65, 32, 102, 101, 119, 32, 109, 111, 110, 116, 104, 115, 32, 97, 103, 111, 44, 32, 73, 32, 103, 111, 116, 32, 105, 110, 116, 101, 114, 101, 115, 116, 101, 100, 32, 105, 110, 32, 85, 110, 105, 99, 111, 100, 256, 97, 110, 100, 32, 100, 101, 99, 105, 100, 101, 100, 32, 116, 111, 32, 115, 112, 101, 110, 100, 32, 115, 111, 109, 256, 116, 105, 109, 256, 108, 101, 97, 114, 110, 105, 110, 103, 32, 109, 111, 114, 256, 97, 98, 111, 117, 116, 32, 105, 116, 32, 105, 110, 32, 100, 101, 116, 97, 105, 108, 46, 32, 73, 110, 32, 116, 104, 105, 115, 32, 97, 114, 116, 105, 99, 108, 101, 44, 32, 73, 226, 128, 153, 108, 108, 32, 103, 105, 118, 256, 97, 110, 32, 105, 110, 116, 114, 111, 100, 117, 99, 116, 105, 111, 110, 32, 116, 111, 32, 105, 116, 32, 102, 114, 111, 109, 32, 97, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 226, 128, 153, 115, 32, 112, 111, 105, 110, 116, 32, 111, 102, 32, 118, 105, 101, 119, 46, 32, 73, 226, 128, 153, 109, 32, 103, 111, 105, 110, 103, 32, 116, 111, 32, 102, 111, 99, 117, 115, 32, 111, 110, 32, 116, 104, 256, 99, 104, 97, 114, 97, 99, 116, 101, 114, 32, 115, 101, 116, 32, 97, 110, 100, 32, 119, 104, 97, 116, 226, 128, 153, 115, 32, 105, 110, 118, 111, 108, 118, 101, 100, 32, 105, 110, 32, 119, 111, 114, 107, 105, 110, 103, 32, 119, 105, 116, 104, 32, 115, 116, 114, 105, 110, 103, 115, 32, 97, 110, 100, 32, 102, 105, 108, 101, 115, 32, 111, 102, 32, 85, 110, 105, 99, 111, 100, 256, 116, 101, 120, 116, 46, 32, 72, 111, 119, 101, 118, 101, 114, 44, 32, 105, 110, 32, 116, 104, 105, 115, 32, 97, 114, 116, 105, 99, 108, 256, 73, 226, 128, 153, 109, 32, 110, 111, 116, 32, 103, 111, 105, 110, 103, 32, 116, 111, 32, 116, 97, 108, 107, 32, 97, 98, 111, 117, 116, 32, 102, 111, 110, 116, 115, 44, 32, 116, 101, 120, 116, 32, 108, 97, 121, 111, 117, 116, 47, 115, 104, 97, 112, 105, 110, 103, 47, 114, 101, 110, 100, 101, 114, 105, 110, 103, 44, 32, 111, 114, 32, 108, 111, 99, 97, 108, 105, 122, 97, 116, 105, 111, 110, 32, 105, 110, 32, 100, 101, 116, 97, 105, 108, 226, 128, 148, 116, 104, 111, 115, 256, 97, 114, 256, 115, 101, 112, 97, 114, 97, 116, 256, 105, 115, 115, 117, 101, 115, 44, 32, 98, 101, 121, 111, 110, 100, 32, 109, 121, 32, 115, 99, 111, 112, 256, 40, 97, 110, 100, 32, 107, 110, 111, 119, 108, 101, 100, 103, 101, 41, 32, 104, 101, 114, 101, 46]\n\n\n\nmerge([5,6,6,7,9,1],(6,7),99)\n\n[5, 6, 99, 9, 1]\n\n\n\nThe following can be used to train the tokenizer, which in the BPE case amounts to finding and replacing the most common merge pairs:\n\n\n# 'train' the tokenizer\nvocab_size = 276\nnum_merges = vocab_size - 256\nids = list(tokens)\ninit_num_tokens = len(tokens)\nprint(init_num_tokens)\nmerges = dict()\nfor i in range(num_merges):\n    stats = get_stats(tokens)\n    top_pair = max(stats, key=stats.get)\n    tokens = merge(tokens, top_pair, 256+i)\n    merges[top_pair] = 256+i #don't forget to record that the pair of integers merged into newly minted int\nfinal_num_tokens = len(tokens)\nprint(final_num_tokens)\nprint(f\"compression ratio: {init_num_tokens/final_num_tokens:.2f}\")\n\n1116\n847\ncompression ratio: 1.32\n\n\n\nAndrej notes that every time we merge tokens they become eligible for a merge in the next round.\nHe also points out that the tokenizer is a completely separate step from the LLM training, it’s main purpose being to convert raw text to sequences of integers. The training step for the tokenizer may be different from the training data, perhaps being a multilingual mixture if we expect the model to be used with different languages in the future and we want compact representations for those languages.\nThe following is a way to decode a Unicode string of bytes:\n\n\n# Following Andrej, a bit thorny especially with error handling in .decode: \n# creating a mapping from byte values to byte objects, merging pairs of byte values into new byte objects, and providing a function to decode lists of byte values into strings.\nvocab = {idx: bytes([idx]) for idx in range(256)}\nfor (p0,p1), idx in merges.items(): # runs in insertion order post Python 3.7\n    vocab[idx] = vocab[p0] + vocab[p1]\n\ndef decode(ids):\n    # given ids (lists of ints), return a string\n    tokens = b\"\".join(vocab[idx] for idx in ids) # ids -&gt; tokens in raw byte form\n    text = tokens.decode(\"utf-8\",errors='replace')  # raw bytes -&gt; string; unicode chars can come in sequences of bytes and must come together to be decoded meaningfully \n    return text    \n\n\nThe idea with encoding is to find the pair with the lowest index from the merges dictionary obtained during training. This is because that pair got merged earliest. If there’s nothing to merge (float(’inf) returned),the pair would be the first element of stats and will not be in the merges dictionary. We should then break out of the while loop.\n\n\ndef encode(text):\n    # Given a string, return list of integers (tokens)\n    tokens = list(text.encode(\"utf-8\"))\n    while len(tokens)&gt;=2: # need at least 2 chars else stats will be empty, causing an issue inside min\n        stats = get_stats(tokens)\n        top_pair = min(stats,key=lambda pair: merges.get(pair,float('inf')))\n        if top_pair not in merges: break #nothing else can be merged\n        idx = merges[top_pair]\n        tokens = merge(tokens,top_pair,idx)\n        #del merges[top_pair] # my original idea with 'while merges:', Andrej's is better\n    return tokens            \n\n\n\n\nFollowing Andrej’s suggestion, I implemented the basic BPE tokenizer as an exercise:\n\n\nclass BasicTokenizer:\n    def __init__(self) -&gt; None:\n        self.merges = dict()\n        #self.vocab_size = vocab_size\n        \n    @staticmethod\n    def get_stats(ids, counts=None):\n        \"\"\"\n        Given a list of integers, return a dictionary of counts of consecutive pairs\n        Example: [1, 2, 3, 1, 2] -&gt; {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n        Optionally allows to update an existing dictionary of counts\n        \"\"\"\n        counts = {} if counts is None else counts\n        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n            counts[pair] = counts.get(pair, 0) + 1\n        return counts\n    \n    @staticmethod\n    def merge(ids, pair, idx):\n        \"\"\"Andrej's/DPV's implementations match\"\"\"\n        i = 0\n        out = []\n        while i &lt; len(ids):\n            if (i&lt;len(ids)-1) and(ids[i],ids[i+1])==pair:\n                out.append(idx)\n                i+=2 # merging!\n            else:\n                out.append(ids[i])\n                i+=1 #moving by one\n        return out\n    \n    def train(self, text, vocab_size, verbose=False):\n        #vocab_size = 276\n        num_merges = vocab_size - 256\n        tokens = text.encode(\"utf-8\")\n        print(f\"tokens: {tokens}\")\n        init_num_tokens = len(tokens)\n        for i in range(num_merges):\n            stats = BasicTokenizer.get_stats(tokens)\n            top_pair = max(stats, key=stats.get)\n            tokens = BasicTokenizer.merge(tokens, top_pair, 256+i)\n            self.merges[top_pair] = 256+i\n        final_num_tokens = len(tokens)\n        if verbose:\n            print(init_num_tokens)\n            print(final_num_tokens)\n            print(f\"compression ratio: {init_num_tokens/final_num_tokens:.2f}\")\n            print(f\"merges: {self.merges}\")\n\n    def encode(self, text):\n        # Given a string, return list of integers (tokens)\n        tokens = list(text.encode(\"utf-8\"))\n        while len(tokens)&gt;=2:\n            stats = BasicTokenizer.get_stats(tokens)\n            top_pair = min(stats,key=lambda pair: self.merges.get(pair,float('inf')))\n            if top_pair not in self.merges: break\n            idx = self.merges[top_pair]\n            tokens = BasicTokenizer.merge(tokens,top_pair,idx)\n            #del self.merges[top_pair]\n        return tokens       \n    def decode(self, ids):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0,p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n\n        # given ids (lists of ints), return a Python string\n        tokens = b\"\".join(vocab[idx] for idx in ids)\n        text = tokens.decode(\"utf-8\",errors='replace')        \n        return text \n\n\nI tested it to make sure it works on both the original Unicode text sample and on Taylor Swift’s Wikipedia page (which is on the longer spectrum of Wikipedia pages):\n\n\ntext = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view. I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\"\ntokenizer = BasicTokenizer()\ntokenizer.train(text,vocab_size=276,verbose=True)\n\ntokens: b'\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef\\xbd\\x83\\xef\\xbd\\x8f\\xef\\xbd\\x84\\xef\\xbd\\x85! \\xf0\\x9f\\x85\\xa4\\xf0\\x9f\\x85\\x9d\\xf0\\x9f\\x85\\x98\\xf0\\x9f\\x85\\x92\\xf0\\x9f\\x85\\x9e\\xf0\\x9f\\x85\\x93\\xf0\\x9f\\x85\\x94\\xe2\\x80\\xbd \\xf0\\x9f\\x87\\xba\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb3\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xae\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa8\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb4\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa9\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xaa! \\xf0\\x9f\\x98\\x84 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \\xe2\\x80\\x9csupport Unicode\\xe2\\x80\\x9d in our software (whatever that means\\xe2\\x80\\x94like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don\\xe2\\x80\\x99t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode\\xe2\\x80\\x99s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I\\xe2\\x80\\x99ll give an introduction to it from a programmer\\xe2\\x80\\x99s point of view. I\\xe2\\x80\\x99m going to focus on the character set and what\\xe2\\x80\\x99s involved in working with strings and files of Unicode text. However, in this article I\\xe2\\x80\\x99m not going to talk about fonts, text layout/shaping/rendering, or localization in detail\\xe2\\x80\\x94those are separate issues, beyond my scope (and knowledge) here.'\n1116\n847\ncompression ratio: 1.32\nmerges: {(101, 32): 256, (105, 110): 257, (226, 128): 258, (115, 32): 259, (240, 159): 260, (97, 110): 261, (32, 116): 262, (116, 32): 263, (97, 114): 264, (257, 103): 265, (101, 114): 266, (100, 32): 267, (44, 32): 268, (111, 114): 269, (262, 104): 270, (105, 99): 271, (111, 110): 272, (260, 133): 273, (260, 135): 274, (115, 116): 275}\n\n\n\nfrom pathlib import Path\ntokenizer = BasicTokenizer()\ntaylor_text = Path('taylorswift.txt').read_text().strip()\ntokenizer.train(taylor_text,vocab_size=1276,verbose=True)\n\ntokens: b'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaThe Free Encyclopedia\\n\\nSearch\\nCreate account\\nLog in\\n\\nPersonal tools\\nContents  hide\\n(Top)\\nLife and career\\nToggle Life and career subsection\\nArtistry\\nToggle Artistry subsection\\nAccolades and achievements\\nCultural status\\nToggle Cultural status subsection\\nWealth\\nToggle Wealth subsection\\nDiscography\\nFilmography\\nTours\\nSee also\\nFootnotes\\nReferences\\nToggle References subsection\\nExternal links\\nTaylor Swift\\n\\n136 languages\\nArticle\\nTalk\\nRead\\nView source\\nView history\\n\\nTools\\n Featured article\\nPage semi-protected\\nFrom Wikipedia, the free encyclopedia\\nFor the album, see Taylor Swift (album).\\nTaylor Swift\\nPortrait of Taylor Swift in a cocktail dress\\nSwift at the 2023 MTV Video Music Awards\\nBorn\\tTaylor Alison Swift\\nDecember 13, 1989 (age 34)\\nWest Reading, Pennsylvania, US\\nOccupations\\nSinger-songwriter producer director businesswoman actress\\nYears active\\t2004\\xe2\\x80\\x93present\\nWorks\\nAlbumssinglessongsvideosperformances\\nRelatives\\nAustin Swift (brother)\\nMarjorie Finlay (grandmother)\\nAwards\\tFull list\\nMusical career\\nOrigin\\tNashville, Tennessee, US\\nGenres\\nPop country folk rock alternative\\nInstruments\\nVocals guitar banjo piano ukulele\\nLabels\\nRCA Republic Big Machine\\nWebsite\\twww.taylorswift.com Edit this at Wikidata\\nSignature\\n\\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry, songwriting, and entrepreneurship have influenced the music industry, popular culture, and politics, and her life is a subject of widespread media coverage.\\nSwift began professional songwriting at 14 and signed with Big Machine Records in 2005 to become a country singer. She released six studio albums under the label, four of them to country radio, starting with Taylor Swift (2006). Her next, Fearless (2008), explored country pop, and its singles \"Love Story\" and \"You Belong with Me\" catapulted her to mainstream fame. Speak Now (2010) infused rock influences, while Red (2012) experimented with electronic elements and featured Swift\\'s first Billboard Hot 100 number-one song, \"We Are Never Ever Getting Back Together\". She departed from her country image with 1989 (2014), a synth-pop album supported by the chart-topping songs \"Shake It Off\", \"Blank Space\", and \"Bad Blood\". Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single \"Look What You Made Me Do\".\\nAfter signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylor\\'s Version after a dispute with Big Machine. These albums spawned the number-one songs \"Cruel Summer\", \"Cardigan\", \"Willow\", \"Anti-Hero\", \"All Too Well\", and \"Is It Over Now?\". Her Eras Tour (2023\\xe2\\x80\\x932024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed several music videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021).\\nOne of the world\\'s best-selling musicians, with over 200 million records sold as of 2019, Swift has been named Global Recording Artist of the Year three times by the International Federation of the Phonographic Industry, whereas six of her albums have opened with over a million sales in a week. She is the highest-grossing female touring act, the most-streamed woman on Spotify and Apple Music, and the first billionaire with music as the main source of income. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stone\\'s 100 Greatest Songwriters of All Time, Billboard\\'s Greatest of All Time Artists, and Forbes\\' World\\'s 100 Most Powerful Women. Her accolades include 14 Grammy Awards (featuring a record four Album of the Year wins), a Primetime Emmy Award, 40 American Music Awards, 40 Billboard Music Awards, and 23 MTV Video Music Awards.\\nLife and career\\nEarly life\\n\\nSwift\\'s childhood home in Wyomissing, Pennsylvania\\nTaylor Alison Swift was born on December 13, 1989,[1] in West Reading, Pennsylvania.[2] She is named after singer-songwriter James Taylor.[3] Her father, Scott Kingsley Swift, is a former stockbroker for Merrill Lynch[4] and her mother, Andrea Gardner Swift (n\\xc3\\xa9e Finlay), worked for a time as a mutual fund marketing executive.[5] Taylor has a younger brother, actor Austin Swift.[6]\\nSwift\\'s mother is of Scottish and German descent, and her father is of Scottish and English descent with distant Italian ancestry.[7][8] Swift\\'s paternal great-great-grandfather, Charles Carmine Antonio Baldi, was an Italian immigrant entrepreneur and community leader who opened several businesses in Philadelphia in the 1800s.[9][10][8] Her maternal grandmother, Marjorie (n\\xc3\\xa9e Moehlenkamp) Finlay, was an opera singer.[11]\\nSwift spent her early years on a Christmas tree farm that her father had purchased from one of his clients.[12][13] She is a Christian.[14] She attended preschool and kindergarten at Alvernia Montessori School, run by Bernardine Franciscan sisters,[15] before transferring to the Wyndcroft School.[16] The family moved to a rented house in Wyomissing, Pennsylvania,[17] where Swift attended Wyomissing Area Junior/Senior High School.[18]\\nAt age nine, Swift became interested in musical theater and performed in four Berks Youth Theatre Academy productions.[19] She also traveled regularly to New York City for vocal and acting lessons.[20] Swift later shifted her focus toward country music, inspired by Shania Twain\\'s songs, which made her \"want to just run around the block four times and daydream about everything\".[21] She spent weekends performing at local festivals and events.[22][23] After watching a documentary about Faith Hill, Swift felt she needed to move to Nashville, Tennessee, to pursue a career in music.[24] She traveled there with her mother at age eleven to visit record labels and submitted demo tapes of Dolly Parton and Dixie Chicks karaoke covers.[25] She was rejected, however, because \"everyone in that town wanted to do what I wanted to do. So, I kept thinking to myself, I need to figure out a way to be different.\"[26] She spent summers in Stone Harbor, New Jersey until she was 14 years old, performing in a local coffee shop.[27][28]\\nWhen Swift was around 12 years old, musician Ronnie Cremer taught her to play guitar. Cremer helped with her first efforts as a songwriter, leading her to write \"Lucky You\".[29] In 2003, Swift and her parents started working with New York\\xe2\\x80\\x93based talent manager Dan Dymtrow. With his help, Swift modeled for Abercrombie & Fitch as part of their \"Rising Stars\" campaign, had an original song included on a Maybelline compilation CD, and met with major record labels.[30] After performing original songs at an RCA Records showcase, Swift, then 13 years old, was given an artist development deal and began making frequent trips to Nashville with her mother.[31][32][33] To help Swift break into the country music scene, her father transferred to Merrill Lynch\\'s Nashville office when she was 14 years old, and the family relocated to Hendersonville, Tennessee.[12][34] Swift attended Hendersonville High School[35] before transferring to Aaron Academy after two years, which better accommodated her touring schedule through homeschooling. She graduated one year early.[36][37]\\n2004\\xe2\\x80\\x932008: Career beginnings and first album\\nIn Nashville, Swift worked with experienced Music Row songwriters such as Troy Verges, Brett Beavers, Brett James, Mac McAnally, and the Warren Brothers[38][39] and formed a lasting working relationship with Liz Rose.[40] They began meeting for two-hour writing sessions every Tuesday afternoon after school.[41] Rose called the sessions \"some of the easiest I\\'ve ever done. Basically, I was just her editor. She\\'d write about what happened in school that day. She had such a clear vision of what she was trying to say. And she\\'d come in with the most incredible hooks.\" Swift became the youngest artist signed by the Sony/ATV Tree publishing house,[42] but left then BMG-owned RCA Records (later bought by Sony Music) at the age of 14 due to the label\\'s lack of care and them \"cut[ting] other people\\'s stuff\". She was also concerned that development deals can shelve artists[33][23] and recalled: \"I genuinely felt that I was running out of time. I wanted to capture these years of my life on an album while they still represented what I was going through.\"[43]\\nTaylor Swift singing on a microphone and playing a guitar\\nSwift opening for Brad Paisley in 2007. To promote her first album, she opened tours for other country musicians in 2007 and 2008.[44]\\nAt an industry showcase at Nashville\\'s Bluebird Cafe in 2005, Swift caught the attention of Scott Borchetta, a DreamWorks Records executive who was preparing to form an independent record label, Big Machine Records. She had first met Borchetta in 2004.[45] She was one of Big Machine\\'s first signings,[33] and her father purchased a three-percent stake in the company for an estimated $120,000.[46][47] She began working on her eponymous debut album with Nathan Chapman.[23] Swift wrote or co-wrote all album tracks, and co-writers included Rose, Robert Ellis Orrall, Brian Maher, and Angelo Petraglia.[48] Taylor Swift was released on October 24, 2006.[49] Country Weekly critic Chris Neal deemed Swift better than previous aspiring teenage country singers because of her \"honesty, intelligence and idealism\".[50] The album peaked at number five on the US Billboard 200, on which it spent 157 weeks\\xe2\\x80\\x94the longest stay on the chart by any release in the US in the 2000s decade.[51] Swift became the first female country music artist to write or co-write every track on a US platinum-certified debut album.[52]\\nBig Machine Records was still in its infancy during the June 2006 release of the lead single, \"Tim McGraw\", which Swift and her mother helped promote by packaging and sending copies of the CD single to country radio stations. As there was not enough furniture at the label yet, they would sit on the floor to do so.[53] She spent much of 2006 promoting Taylor Swift with a radio tour and television appearances; she opened for Rascal Flatts on select dates during their 2006 tour,[54] as a replacement for Eric Church.[55] Borchetta said that although record industry peers initially disapproved of his signing a 15-year-old singer-songwriter, Swift tapped into a previously unknown market\\xe2\\x80\\x94teenage girls who listen to country music.[53][12]\\nFollowing \"Tim McGraw\", four more singles were released throughout 2007 and 2008: \"Teardrops on My Guitar\", \"Our Song\", \"Picture to Burn\" and \"Should\\'ve Said No\". All appeared on Billboard\\'s Hot Country Songs, with \"Our Song\" and \"Should\\'ve Said No\" reaching number one. With \"Our Song\", Swift became the youngest person to single-handedly write and sing a number-one song on the chart.[56] \"Teardrops on My Guitar\" reached number thirteen on the US Billboard Hot 100.[57] Swift also released two EPs, The Taylor Swift Holiday Collection in October 2007 and Beautiful Eyes in July 2008.[58][59] She promoted her debut album extensively as the opening act for other country musicians\\' tours in 2006 and 2007, including those by George Strait,[60] Brad Paisley,[61] and Tim McGraw and Faith Hill.[62]\\nSwift won multiple accolades for Taylor Swift. She was one of the recipients of the Nashville Songwriters Association\\'s Songwriter/Artist of the Year in 2007, becoming the youngest person given the title.[63] She also won the Country Music Association\\'s Horizon Award for Best New Artist,[64] the Academy of Country Music Awards\\' Top New Female Vocalist,[65] and the American Music Awards\\' Favorite Country Female Artist honor.[66] She was also nominated for Best New Artist at the 50th Annual Grammy Awards.[67] In 2008, she opened for Rascal Flatts again[68] and briefly dated the singer Joe Jonas.[69][70]\\n2008\\xe2\\x80\\x932010: Fearless\\nTaylor Swift in 2009\\nSwift at the 2009 premiere of Hannah Montana: The Movie. She had a cameo appearance in the film and wrote two songs for its soundtrack.[71][72]\\nSwift\\'s second studio album, Fearless, was released on November 11, 2008, in North America,[73] and in March 2009 in other markets.[74] Critics lauded Swift\\'s honest and vulnerable songwriting in contrast to other teenage singers.[75] Five singles were released in 2008\\xe2\\x80\\x932009: \"Love Story\", \"White Horse\", \"You Belong with Me\", \"Fifteen\", and \"Fearless\". The first single peaked at number four on the Billboard Hot 100 and number one in Australia.[57][76] It was the first country song to top Billboard\\'s Pop Songs chart.[77] \"You Belong with Me\" was the album\\'s highest-charting single on the Billboard Hot 100, peaking at number two,[78] and was the first country song to top Billboard\\'s all-genre Radio Songs chart.[79] All five singles were Hot Country Songs top-10 entries, with \"Love Story\" and \"You Belong with Me\" topping the chart.[80] Fearless became her first number-one album on the Billboard 200 and 2009\\'s top-selling album in the US.[81] The Fearless Tour, Swift\\'s first headlining concert tour, grossed over $63 million.[82] Journey to Fearless, a documentary miniseries, aired on television and was later released on DVD and Blu-ray.[83] Swift performed as a supporting act for Keith Urban\\'s Escape Together World Tour in 2009.[84]\\nIn 2009, the music video for \"You Belong with Me\" was named Best Female Video at the 2009 MTV Video Music Awards.[85] Her acceptance speech was interrupted by rapper Kanye West,[86] an incident that became the subject of controversy, widespread media attention and Internet memes.[87] That year she won five American Music Awards, including Artist of the Year and Favorite Country Album.[88] Billboard named her 2009\\'s Artist of the Year.[89] She won Video of the Year and Female Video of the Year for \"Love Story\" at the 2009 CMT Music Awards, where she made a parody video of the song with rapper T-Pain called \"Thug Story\".[90] At the 52nd Annual Grammy Awards, Fearless was named Album of the Year and Best Country Album, and \"White Horse\" won Best Country Song and Best Female Country Vocal Performance. Swift was the youngest artist to win Album of the Year.[note 1] At the 2009 Country Music Association Awards, Swift won Album of the Year for Fearless and was named Entertainer of the Year, the youngest person to win the honor.[93]\\nSwift featured on John Mayer\\'s single \"Half of My Heart\" and Boys Like Girls\\' single \"Two Is Better Than One\", the latter of which she co-wrote.[94][95] She co-wrote and recorded \"Best Days of Your Life\" with Kellie Pickler,[96] and wrote two songs for the Hannah Montana: The Movie soundtrack\\xe2\\x80\\x94\"You\\'ll Always Find Your Way Back Home\" and \"Crazier\".[72] She contributed two songs to the Valentine\\'s Day soundtrack, including the single \"Today Was a Fairytale\", which was her first number-one on the Canadian Hot 100 and peaked at number two on the US Hot 100.[97][98] While shooting her film debut Valentine\\'s Day in October 2009, Swift dated co-star Taylor Lautner.[99] In 2009, she made her television debut as a rebellious teenager in an CSI: Crime Scene Investigation episode,[100] and she hosted and performed as the musical guest on Saturday Night Live; she was the first host ever to write their own opening monologue.[101][102]\\n2010\\xe2\\x80\\x932014: Speak Now and Red\\nSwift singing into a mic while playing a banjo\\nSwift performing at the Speak Now World Tour in 2012\\nIn August 2010, Swift released \"Mine\", the lead single from her third studio album, Speak Now. The single entered the Hot 100 at number three.[103] Swift wrote the album alone and co-produced every track.[104] The album was released on October 25, 2010,[105] opening atop the Billboard 200 with over one million copies sold.[106] It became the fastest-selling digital album by a female artist, with 278,000 downloads in a week.[107] Critics appreciated Swift\\'s grown-up perspectives:[108] Rob Sheffield of Rolling Stone wrote, \"in a mere four years, the 20-year-old Nashville firecracker has put her name on three dozen or so of the smartest songs released by anyone in pop, rock or country.\"[109] \"Back to December\", \"Mean\", \"The Story of Us\", \"Sparks Fly\", and \"Ours\" became subsequent singles, with the latter two reaching number one on the Hot Country Songs[80] and the first two peaking in the top ten in Canada.[98] She dated actor Jake Gyllenhaal in 2010.[110]\\nAt the 54th Annual Grammy Awards in 2012, Swift won Best Country Song and Best Country Solo Performance for \"Mean\", which she performed during the ceremony.[111] Swift won other awards for Speak Now, including Songwriter/Artist of the Year by the Nashville Songwriters Association (2010 and 2011),[112][113] Woman of the Year by Billboard (2011),[114] and Entertainer of the Year by the Academy of Country Music (2011 and 2012)[115] and the Country Music Association in 2011.[116] At the American Music Awards of 2011, Swift won Artist of the Year and Favorite Country Album.[117] Rolling Stone named Speak Now amongst its \"50 Best Female Albums of All Time\" (2012), writing: \"She might get played on the country station, but she\\'s one of the few genuine rock stars we\\'ve got these days, with a flawless ear for what makes a song click.\"[118]\\nThe Speak Now World Tour ran from February 2011 to March 2012 and grossed over $123 million,[119] followed up by the live album, Speak Now World Tour: Live.[120] She contributed two original songs to The Hunger Games soundtrack album: \"Eyes Open\" and \"Safe & Sound\", co-written and recorded with the Civil Wars and T-Bone Burnett. \"Safe & Sound\" won the Grammy Award for Best Song Written for Visual Media and was nominated for the Golden Globe Award for Best Original Song.[121][122] Swift featured on B.o.B\\'s single \"Both of Us\", released in May 2012.[123] She dated Conor Kennedy that year.[124]\\nTaylor Swift on the Red Tour\\nSwift on the Red Tour (2013)\\nIn August 2012, Swift released \"We Are Never Ever Getting Back Together\", the lead single from her fourth studio album, Red. It became her first number one single in the US and New Zealand,[125][126] and became the fastest-selling single in digital history.[127] Other singles from the album were \"Begin Again\", \"I Knew You Were Trouble\", \"22\", \"Everything Has Changed\", \"The Last Time\", and \"Red\". \"I Knew You Were Trouble\" reached the top five on charts in Australia, Canada, Denmark, Ireland, New Zealand, the UK and the US.[128] \"Begin Again\", \"22\", and \"Red\" reached the top 20 in the US.[57] On Red, released on October 22, 2012,[129] Swift worked with Chapman and Rose, as well as the new producers Max Martin and Shellback.[130] It incorporated many pop and rock styles such as heartland rock, dubstep and dance-pop.[131] Randall Roberts of Los Angeles Times said Swift \"strives for something much more grand and accomplished\" with Red.[132] It opened at number one on the Billboard 200 with 1.21 million sales.[133] Red was Swift\\'s first number-one album in the UK.[134] It earned several accolades, including four nominations at the 56th Annual Grammy Awards (2014).[135] Swift received American Music Awards for Best Female Country Artist in 2012, Artist of the Year in 2013,[136][137] and the Nashville Songwriters Association\\'s Songwriter/Artist Award for the fifth and sixth consecutive years.[138] The Red Tour ran from March 2013 to June 2014 and grossed over $150 million, becoming the highest-grossing country tour ever.[139] Swift was honored with the Pinnacle Award, making her the second recipient of the accolade after Garth Brooks.[140] During this time, she briefly dated the English singer Harry Styles.[141]\\nIn 2013, Swift recorded \"Sweeter than Fiction\", a song she wrote and produced with Jack Antonoff for the One Chance soundtrack. The song received a Best Original Song nomination at the 71st Golden Globe Awards.[142] She provided guest vocals for Tim McGraw\\'s song \"Highway Don\\'t Care\", also featuring Keith Urban.[143] Swift performed \"As Tears Go By\" with the Rolling Stones in Chicago, Illinois, as part of the band\\'s 50 & Counting tour,[144] and joined Florida Georgia Line at their set at the 2013 Country Radio Seminar to sing \"Cruise\".[145] Swift voiced Audrey in the animated film The Lorax (2012),[146] made a cameo in the sitcom New Girl (2013),[147] and had a supporting role in the dystopian film The Giver (2014).[148]\\n2014\\xe2\\x80\\x932018: 1989 and Reputation\\nSwift performing on a mic, dressed in a blue skirt\\nSwift at the 1989 World Tour, the highest-grossing tour of 2015\\nIn March 2014, Swift began living in New York City.[note 2] She hired Tree Paine as her publicist[151] and worked on her fifth studio album, 1989, with the producers Jack Antonoff, Max Martin, Shellback, Imogen Heap, Ryan Tedder, and Ali Payami.[152] She promoted the album extensively, including inviting fans to secret album-listening sessions.[153] 1989 was released on October 27, 2014, and opened atop the Billboard 200 with 1.28 million copies sold.[154] Its singles \"Shake It Off\", \"Blank Space\" and \"Bad Blood\" reached number one in Australia, Canada and the US, the first two making Swift the first woman to replace herself at the Hot 100 top spot;[155] other singles include \"Style\", \"Wildest Dreams\", \"Out of the Woods\" and \"New Romantics\".[156] The 1989 World Tour (2015) was the highest-grossing tour of the year with $250 million in total revenue.[157]\\nPrior to 1989\\'s release, Swift stressed the importance of albums to artists and fans.[158] In November 2014, she removed her entire catalog from Spotify, arguing that its ad-supported, free service undermined the premium service, which provides higher royalties for songwriters.[159] In a June 2015 open letter, Swift criticized Apple Music for not offering royalties to artists during the streaming service\\'s free three-month trial period and stated that she would pull 1989 from the catalog.[160] The following day, Apple Inc. announced that it would pay artists during the free trial period,[161] and Swift agreed to let 1989 on the streaming service.[162] She then returned her entire catalog plus 1989 to Spotify, Amazon Music and Google Play and other digital streaming platforms in June 2017.[163] Swift was named Billboard\\'s Woman of the Year in 2014, becoming the first artist to win the award twice.[164] At the 2014 American Music Awards, Swift received the inaugural Dick Clark Award for Excellence.[165] On her 25th birthday in 2014, the Grammy Museum at L.A. Live opened an exhibit in her honor in Los Angeles that ran until October 4, 2015, and broke museum attendance records.[166][167] In 2015, Swift won the Brit Award for International Female Solo Artist.[168] The video for \"Bad Blood\" won Video of the Year and Best Collaboration at the 2015 MTV Video Music Awards.[169] At the 58th Grammy Awards (2016), 1989 won Album of the Year and Best Pop Vocal Album, making Swift the first woman and fifth act overall to win Album of the Year twice.[170]\\nSwift wearing a sparkling blazer singing on a mic\\nSwift on her Reputation Stadium Tour (2018), the highest-grossing North American tour ever\\nSwift dated the Scottish DJ Calvin Harris from March 2015 to June 2016.[171] They co-wrote the song \"This Is What You Came For\", featuring vocals from the Barbadian singer Rihanna; Swift was initially credited under the pseudonym Nils Sj\\xc3\\xb6berg.[172] In April 2016, Swift criticized the lyrics of Kanye West\\'s single \"Famous\", in which he sings \"I made that bitch famous\" in reference to his interruption of her acceptance speech at the 2009 MTV Video Music Awards. West claimed he had received her approval for the line, and his then-wife Kim Kardashian released video clips of Swift and West discussing the single amicably over the phone; a full recording leaked in 2020 established that West did not disclose that he would call her a \"bitch\".[173][174]\\nAfter briefly dating the English actor Tom Hiddleston,[175] Swift entered a six-year relationship with the English actor Joe Alwyn in September 2016.[176][177][178] She wrote the song \"Better Man\" for the band Little Big Town, which earned her the Song of the Year award at the 51st CMA Awards.[179] Swift and English singer Zayn Malik released the joint single \"I Don\\'t Wanna Live Forever\" for Fifty Shades Darker: Original Motion Picture Soundtrack (2017). The song reached number two in the US.[180]\\nIn August 2017, Swift successfully countersued David Mueller, a former radio jockey for KYGO-FM, who sued her for damages from loss of employment. Four years earlier, she informed Mueller\\'s bosses that he had sexually assaulted her by groping her at an event.[181] Also that month, after a one-year hiatus from the spotlight, Swift cleared her social media accounts and released \"Look What You Made Me Do\" as the lead single from her sixth album, Reputation.[182][183] The single was Swift\\'s first UK number-one single.[184] It topped charts in Australia, Ireland, New Zealand, and the US.[185] Reputation, released on November 10, 2017,[186] incorporated electropop, hip hop, R&B, and EDM.[187] Reviews praised Swift\\'s mature artistry, but some denounced the themes of fame and gossip.[188] The album opened atop the Billboard 200 with 1.21 million US sales[189] and topped the charts in the UK, Australia, and Canada.[190] Its singles \"...Ready for It?\", \"End Game\" (featuring Ed Sheeran and Future), and \"Delicate\" were released to pop radio.[191] Reputation was nominated for a Grammy Award for Best Pop Vocal Album.[192] Swift featured on the country duo Sugarland\\'s \"Babe\" (2018).[193]\\nAt the 2018 American Music Awards, Swift won four awards, which made her accumulate 23 trophies in total and become the AMAs\\' most awarded female musician, surpassing Whitney Houston.[194] The same year, she embarked on her Reputation Stadium Tour,[195] which became the highest-grossing North American concert tour in history and grossed $345.7 million worldwide.[196]\\n2018\\xe2\\x80\\x932020: Lover, Folklore, and Evermore\\nIn November 2018, she signed a new deal with the Universal Music Group; her subsequent releases were promoted by Republic Records. Swift said the contract included a provision for her to maintain ownership of her masters. In addition, in the event that Universal sold any part of its stake in Spotify, it agreed to distribute a non-recoupable portion of the proceeds among its artists.[197] Vox called it a huge commitment from Universal, which was \"far from assured\" until Swift intervened.[198]\\nA portrait of Swift\\nSwift at the American Music Awards of 2019, where she was named Artist of the Decade\\nSwift\\'s first album with Republic Records, Lover, was released on August 23, 2019.[199] Besides Antonoff, she worked with Louis Bell, Frank Dukes, and Joel Little.[200] Lover was her sixth consecutive album to sell more than 500,000 US copies in one week.[201] Critics commended the album\\'s free-spirited mood and emotional intimacy.[202][203] The singles \"Me!\" and \"You Need to Calm Down\" both peaked at number two on the Hot 100,[204] and other singles were the top-10 single \"Lover\", the top-40 single \"The Man\",[57] and the 2023 resurgent success, chart topper \"Cruel Summer\".[205] Lover was the world\\'s best-selling album by a solo artist of 2019,[206] and along with its singles earned nominations at the 62nd Annual Grammy Awards in 2020.[207] At the 2019 MTV Video Music Awards, \"Me!\" won Best Visual Effects, and \"You Need to Calm Down\" won Video of the Year and Video for Good. Swift was the first female and second artist overall to win Video of the Year for a video that they directed.[208]\\nWhile promoting Lover, Swift became embroiled in a public dispute with the talent manager Scooter Braun and Big Machine over the purchase of the masters of her back catalog.[209][210] Swift said she had been trying to buy the masters, but Big Machine would only allow her to do so if she exchanged one new album for each older one under a new contract, which she refused to sign.[209][211] Swift began re-recording her back catalog in November 2020.[212] Besides music, she played Bombalurina in the film adaptation of Andrew Lloyd Webber\\'s musical Cats (2019), for which she co-wrote and recorded the Golden Globe-nominated original song \"Beautiful Ghosts\".[213][214] Critics panned the film but praised Swift\\'s performance.[215] The documentary Miss Americana, which chronicled parts of Swift\\'s life and career, premiered at the 2020 Sundance Film Festival.[216] Swift signed a global publishing deal with Universal Music Publishing Group in February 2020 after her 16-year contract with Sony/ATV expired.[217]\\nAmidst the COVID-19 pandemic in 2020, Swift surprise-released two \"sister albums\" that she recorded with Antonoff and Aaron Dessner: Folklore on July 24, and Evermore on December 11.[218][219] Alwyn co-wrote and co-produced a few songs under the pseudonym William Bowery.[220] Both explore indie folk with a more muted production compared to her previous upbeat pop songs[221][222] and earned Swift widespread critical acclaim and artistic recognition.[223][224] Each album was supported by three singles catering to US pop, country, and triple A radio formats. The singles were \"Cardigan\", \"Betty\", and \"Exile\" from Folklore, and \"Willow\", \"No Body, No Crime\", and \"Coney Island\" from Evermore.[225] Folklore made Swift the first woman to win the Grammy Award for Album of the Year three times at the 63rd Annual Grammy Awards[226] and was the best-selling album of 2020 in the US.[227] Swift became the first artist to debut a US number-one album and a number-one song at the same time with Folklore\\'s \"Cardigan\".[228] At the 2020 American Music Awards, she won three awards, including Artist of the Year for a record third consecutive time.[229] According to Billboard, she was 2020\\'s highest-paid musician in the US and highest-paid solo musician worldwide.[230]\\n2020\\xe2\\x80\\x932023: Re-recordings and Midnights\\n\\nSwift performing in 2022\\nFollowing the masters dispute, Swift released re-recordings of her first six studio albums, beginning with Fearless (Taylor\\'s Version) and Red (Taylor\\'s Version) in April and November 2021, respectively. Both peaked atop the Billboard 200,[231] and the former was the first re-recorded album to do so.[232] Fearless (Taylor\\'s Version) was preceded by \"Love Story (Taylor\\'s Version)\", which made her the second artist after Dolly Parton to have both the original and re-recorded versions of a song reach number one on the Hot Country Songs chart.[233] Red (Taylor\\'s Version) was supported by \"All Too Well (10 Minute Version)\", which became the longest song in history to top the Hot 100.[234] The song was accompanied by a short film, which won a Grammy Award for Best Music Video[235] and Swift\\'s record third MTV Video Music Award for Video of the Year.[236]\\nSwift\\'s tenth studio album, Midnights, was released on October 21, 2022.[237] Characterized by a restrained electropop[238][239] and synth-pop[240] sound, the album was dubbed by Rolling Stone critics as an instant classic.[241][242] The album was her fifth to open atop the Billboard 200 with first-week sales of over one million copies and broke various sales and streaming records,[243] including the most single-day streams and most single-week streams on Spotify.[244] Its tracks, led by single \"Anti-Hero\", monopolized the top 10 of the Hot 100, making Swift the first artist to do so.[245] Two other singles, \"Lavender Haze\" and \"Karma\", peaked at number two on the Hot 100.[246] Swift won nine awards at the 2023 MTV Video Music Awards, including Video of the Year (\"Anti-Hero\") for a record fourth time.[247] At the 66th Annual Grammy Awards, she received Best Pop Vocal Album, and her fourth Album of the Year\\xe2\\x80\\x94the most for any artist.[248]\\nSwift released the third re-recorded album, Speak Now (Taylor\\'s Version), on July 7, 2023, becoming the woman with the most number-one albums (12) in Billboard 200 history, surpassing Barbra Streisand.[249] 1989 (Taylor\\'s Version), released on October 27, 2023, became Swift\\'s record-extending sixth album to sell one million copies in a single week in the US and surpassed Midnights for her career\\'s largest album sales week.[250] Its single \"Is It Over Now?\" debuted atop the Billboard Hot 100. Swift was 2023\\'s most streamed artist on Spotify,[251] Apple Music,[252] and Amazon Music;[253] the first act to place number one on the year-end Billboard top artists list in three different decades (2009, 2015 and 2023);[254] and the first living artist to simultaneously chart five albums in the top 10 of the Billboard 200.[255] She had five out of the 10 best-selling albums of 2023 in the United States, a record since Luminate began tracking US music sales in 1991.[256][257]\\nBeyond her albums, Swift featured on five songs from 2021 to 2023: \"Renegade\" and \"Birch\" by Big Red Machine,[258] a remix of \"Gasoline\" by Haim,[259] \"The Joker and the Queen\" by Ed Sheeran,[260] and \"The Alcott\" by the National.[261] For the soundtrack of the 2022 film Where the Crawdads Sing, she recorded \"Carolina\", which received nominations for Best Original Song at the Golden Globes and Best Song Written for Visual Media at the Grammy Awards.[262] Outside of music, Swift had a supporting role in the 2022 period comedy film Amsterdam and has signed to direct an upcoming feature film for Searchlight Pictures.[263][264]\\n2023\\xe2\\x80\\x93present: The Eras Tour and The Tortured Poets Department\\nSwift singing into a mic\\nSwift on the Eras Tour in 2023\\nIn March 2023, Swift embarked on the Eras Tour, a retrospective tour covering all her studio albums. Media outlets extensively covered the tour\\'s cultural and economic impact,[265] and its US leg broke the record for the most tickets sold in a day.[245] Ticketmaster received public and political criticisms for mishandling the tour\\'s ticket sales.[266] The Eras Tour became the highest-grossing tour in history, collecting over $1 billion.[267][268] Its concert film, released to theaters worldwide on October 13, 2023, grossed over $250 million to become the highest-grossing concert film, and was nominated for the Golden Globe Award for Cinematic and Box Office Achievement.[269][270] Swift\\'s music releases, touring, and related activities culminated in an unprecedented height of popularity post-pandemic.[271] Music Business Worldwide remarked that she entered a \"new stratosphere of global career success\" in 2023.[272]\\nSwift began dating Kansas City Chiefs\\' tight end Travis Kelce in 2023.[273] In January 2024, AI-generated fake pornographic images portraying Swift were posted to X (formerly Twitter) and spread to other social media platforms, spurring criticism and demands for legal reform.[274][275] At the 66th Grammy Awards, Swift announced her eleventh studio album, The Tortured Poets Department, set for release on April 19, 2024.[248][276]\\nArtistry\\nInfluences\\nOne of Swift\\'s earliest memories of music is listening to her maternal grandmother, Marjorie Finlay, sing in church.[5] As a child, she enjoyed Disney film soundtracks: \"My parents noticed that, once I had run out of words, I would just make up my own.\"[277] Swift said she owes her confidence and \"fascination with writing and storytelling\" to her mother, who helped her prepare for class presentations as a child.[278][279]\\nSwift was drawn to the storytelling aspect of country music,[280] which was introduced to the genre by female country artists of the 1990s: Shania Twain, Faith Hill, and the Dixie Chicks.[281][282] Twain, both as a songwriter and performer, was her biggest musical influence.[283] Hill was Swift\\'s childhood role model, and she would often imitate her.[284] She admired the Chicks\\' defiant attitude and the way they played their instruments,[285] and was also influenced by older country stars like Patsy Cline, Loretta Lynn, Tammy Wynette, and Dolly Parton,[22] the last of whom she believes is exemplary to female songwriters.[114] As a songwriter, Swift was influenced by Joni Mitchell\\'s emotional and autobiographical lyrics, highlighting Mitchell\\'s 1971 album Blue as a favorite \"because it explores somebody\\'s soul so deeply\".[286] She also spoke of influence from 1990s songwriters such as Melissa Etheridge, Sarah McLachlan, and Alanis Morissette,[287][288] and alt-country artists like Patty Griffin[289] and Lori McKenna.[290]\\nVarious pop and rock artists have also influenced Swift. She lists Paul McCartney, Bruce Springsteen, Emmylou Harris, and Kris Kristofferson as her career role models.[12][291] 1989 was influenced by some of her favorite 1980s pop acts, including Peter Gabriel, Annie Lennox, Phil Collins, and Madonna.[292][293] She also cited Keith Urban\\'s musical style and Fall Out Boy\\'s lyrics as major influences.[294][295]\\nGenres\\n\"If there\\'s one thing that Swift has proven throughout her career, it\\'s that she refuses to be put in a box. Her ever-evolving sound took her from country darling to pop phenom to folk\\'s newest raconteur.\"\\n\\xe2\\x80\\x94The Recording Academy, 2021[296]\\nSwift is known for venturing into various music genres and undergoing artistic reinventions,[297][264] having been described as a \"music chameleon\".[298][299] She self-identified as a country musician until 2012, when she released her fourth studio album, Red.[300] Her albums were promoted to country radio, but music critics noted wide-ranging styles of pop and rock[301][302] and said that the melodies of her songs were rooted in pop, and the country music elements were limited to instruments such as banjo, mandolin, and fiddle, and her slight twang.[303][304] Some commented that her country music identity was an indicator of her narrative songwriting rather than musical style.[305][306] Although the Nashville music industry was receptive of Swift\\'s status as a country musician, critics accused her of abandoning her roots in favor of crossover success in mainstream pop.[307][308] Red\\'s eclectic pop, rock, and electronic styles intensified the critical debate, to which Swift responded, \"I leave the genre labeling to other people.\"[309]\\nMusic journalist Jody Rosen commented that by originating her musical career in Nashville, Swift made a \"bait-and-switch maneuver, planting roots in loamy country soil, then pivoting to pop\".[310] She abandoned her country music identity in 2014 with the release of her synth-pop fifth studio album, 1989. Swift described it as her first \"documented, official pop album\".[311] Her subsequent albums Reputation (2017) and Lover (2019) have an upbeat pop production; the former incorporates hip hop, trap, and EDM elements.[312][313][314] Midnights (2022), on the other hand, is distinguished by a more experimental, \"subdued and amorphous pop sound\".[315][316] Although reviews of Swift\\'s pop albums were generally positive, some critics lamented that the pop music production indicated Swift\\'s pursuit of mainstream success, eroding her authenticity as a songwriter nurtured by her country music background\\xe2\\x80\\x94a criticism that has been retrospectively described as rockist.[317][318] Musicologist Nate Sloan remarked that Swift\\'s pop music transition was rather motivated by her need to expand her artistry.[319] Swift eschewed mainstream pop in favor of alternative, folk and indie rock styles with her 2020 studio albums Folklore and Evermore.[320][321] Clash said her career \"has always been one of transcendence and covert boundary-pushing\", reaching a point at which \"Taylor Swift is just Taylor Swift\", not defined by any genre.[322]\\nVoice\\n\\n\"Cardigan\"\\nDuration: 22 seconds.0:22\\nSwift uses her lower register in \"Cardigan\" (2020).[323]\\n\"Lavender Haze\"\\nDuration: 18 seconds.0:18\\n\"Lavender Haze\" (2022) features Swift\\'s falsetto vocals in the refrain.[324]\\nProblems playing these files? See media help.\\nSwift possesses a mezzo-soprano vocal range,[325] and a generally soft but versatile timbre.[326][327] As a country singer, her vocals were criticized by some as weak and strained compared to those of her contemporaries.[328] Swift admitted her vocal ability often concerned her in her early career and has worked hard to improve.[329] Reviews of her vocals remained mixed after she transitioned to pop music with 1989; critics complained that she lacked proper technique but appreciated her usage of her voice to communicate her feelings to the audience, prioritizing \"intimacy over power and nuance\".[330] They also praised her for refraining from correcting her pitch with Auto-Tune.[331]\\nThe Los Angeles Times remarked that Swift\\'s defining vocal feature is her attention to detail to convey an exact feeling\\xe2\\x80\\x94\"the line that slides down like a contented sigh or up like a raised eyebrow\".[332] With Reputation, critics noted she was \"learning how to use her voice as a percussion instrument of its own\",[333] swapping her \"signature\" expressive vocals for \"cool, conversational, detached\" cadences and rhythms similar to hip hop and R&B styles.[334][335][336] Alternative Press stated that her \"evocative\" vocal stylings are more reminiscent of pop-punk and emo genres.[337]\\nReviews of Swift\\'s later albums and performances were more appreciative of her vocals, finding them less nasal, richer, more resonant, and more powerful.[304][338][339] With Folklore and Evermore, Swift received praise for her sharp and agile yet translucent and controlled voice.[340][341][342] Pitchfork described it as \"versatile and expressive\".[343] With her 2021 re-recorded albums, critics began to praise the mature, deeper and \"fuller\" tone of her voice.[344][345][346] An i review said Swift\\'s voice is \"leagues better now\".[347] The Guardian highlighted \"yo-yoing vocal yelps\" and passionate climaxes as the trademarks of Swift\\'s voice,[348] and that her country twang faded away.[349] Midnights received acclaim for Swift\\'s nuanced vocal delivery.[350] She ranked 102nd on the 2023 Rolling Stone list of the 200 Greatest Singers of All Time.[327] In a review of the Eras Tour, The New Yorker critic Amanda Petrusich praised the clarity and tone of Swift\\'s live vocals.[351] Musicologist Alyssa Barna said that Swift\\'s timbre is \"breathy and bright\" in her upper register and \"full and dark\" in the lower.[222]\\nSongwriting\\nFurther information: List of songs by Taylor Swift\\nSwift has been referred to as one of the greatest songwriters ever by several publications.[352][353][354] Literature scholars like Jonathan Bate and Stephanie Burt have noted that her literary and melodic sensibility and writing style are rare amongst her peers.[355][356] Swift\\'s bridges are often noted as one of the best aspects of her songs,[357][358] earning her the title \"Queen of Bridges\" from Time.[359] Mojo described her as \"a sharp narrator with a gift for the extended metaphor\".[360]\\nIn The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\".[12] Her personal experiences were a common inspiration for her early songs, which helped her navigate life.[361][362] Her \"diaristic\" technique began with identifying an emotion, followed by a corresponding melody.[363][364] On her first three studio albums, love, heartbreak, and insecurities, from an adolescent perspective, were dominant themes.[365][366] She delved into the tumult of toxic relationships on Red,[367] and embraced nostalgia and post-romance positivity on 1989.[292] Reputation was inspired by the downsides of Swift\\'s fame,[368] and Lover detailed her realization of the \"full spectrum of love\".[369] Other themes in Swift\\'s music include family dynamics, friendship,[370][371] alienation, self-awareness, and tackling vitriol, especially sexism.[279][372]\\nHer confessional lyrics received positive reviews from critics,[373][12][374] who highlighted their vivid details and emotional engagement, which they found uncommon in pop music.[375][376][377] Critics also praised her melodic compositions; Rolling Stone described Swift as \"a songwriting savant with an intuitive gift for verse-chorus-bridge architecture\".[378][379] NPR dubbed Swift \"a master of the vernacular in her lyrics\",[335] remarking that her songs offer emotional engagement because \"the wit and clarity of her arrangements turn them from standard fare to heartfelt disclosures\".[379] Despite the positive reception, The New Yorker stated she was generally portrayed \"more as a skilled technician than as a Dylanesque visionary\".[12] Tabloid media often speculated and linked the subjects of her songs with her ex-lovers, a practice reviewers and Swift herself criticized as sexist.[380][381][382] Aside from clues in album liner notes, Swift avoided talking about the subjects of her songs.[383]\\nOn her 2020 albums Folklore and Evermore, Swift was inspired by escapism and romanticism to explore fictional narratives.[384] She imposed emotions onto imagined characters and story arcs, which liberated her from tabloid attention and suggested new paths for her artistry.[363] Swift explained that she welcomed the new songwriting direction after she stopped worrying about commercial success.[384] According to Spin, she explored complex emotions with \"precision and devastation\" on Evermore.[385] Consequence stated her 2020 albums convinced skeptics of her songwriting prowess, noting her transformation from \"teenage wunderkind to a confident and careful adult\".[358]\\nSwift divides her writing into three types: \"quill lyrics\", songs rooted in antiquated poeticism; \"fountain pen lyrics\", based on modern and vivid storylines; and \"glitter gel pen lyrics\", which are lively and frivolous.[386] Critics note the fifth track of every Swift album as the most \"emotionally vulnerable\" of the album.[387] Awarding her with the Songwriter Icon Award in 2021, the National Music Publishers\\' Association remarked that \"no one is more influential when it comes to writing music today\".[388] The Week deemed her the foremost female songwriter of modern times,[389] and the Nashville Songwriters Association International named her Songwriter-Artist of the Decade in 2022.[245] Swift has also published two original poems: \"Why She Disappeared\" and \"If You\\'re Anything Like Me\".[390]\\nPerformances\\nFurther information: List of Taylor Swift live performances\\n\\nSwift performing on the Reputation Stadium Tour in Seattle in May 2018\\nJournalists have described Swift as one of the best live performers. Often praised for her showmanship and stage presence,[391][392][393][394][395] Swift commands large audiences,[396][397][398] without having to rely on dance like her contemporaries do.[399] According to V magazine\\'s Greg Krelenstein, she possesses \"a rare gift of turning a stadium spectacle into an intimate setting\", irrespective of whether she is \"plucking a guitar or leading an army of dancers\".[400] In a 2008 review of Swift\\'s early performances, Sasha Frere-Jones of The New Yorker called Swift a \"preternaturally skilled\" entertainer with a vibrant stage presence, adding \"she returned the crowd\\'s energy with the professionalism she has shown since the age of fourteen.\"[401] In 2023, Adrian Horton of The Guardian noted her \"seemingly endless stamina\" on the Eras Tour,[402] and i critic Ilana Kaplan called her showmanship \"unparalleled\".[403]\\nCritics have highlighted Swift\\'s versatility as an entertainer, praising her ability to switch onstage personas and performance styles depending on the varying themes and aesthetics of her albums.[404][405] Her concert productions have been characterized by elaborate Broadway theatricality and high technology,[406] and her performances frequently incorporate a live band, with whom she has played and toured since 2007.[407] Swift also often accompanies herself with musical instruments such as electric guitar;[408] acoustic guitar; piano;[409] and sometimes twelve-string guitar,[410][411] banjo,[412] or ukulele.[413] Interacting frequently with the audience, her solo acoustic performances are considered intimate and emotionally resonant, complementing her story-based lyrics and fan connection.[351][414] Lydia Burgham of The Spinoff opined that this intimacy remains \"integral to her singer-songwriter origins\".[415][409] Chris Willman of Variety called Swift \"pop\\'s most approachable superstar\",[416] and the 21st century\\'s most popular performer.[417]\\nVideo and film\\nFurther information: Taylor Swift videography\\nSwift emphasizes visuals as a key creative component of her music-making process.[418] She has collaborated with different directors to produce her music videos, and over time she has become more involved with writing and directing. She developed the concept and treatment for \"Mean\" in 2011[419] and co-directed the music video for \"Mine\" with Roman White the year before.[420] In an interview, White said that Swift \"was keenly involved in writing the treatment, casting and wardrobe. And she stayed for both the 15-hour shooting days, even when she wasn\\'t in the scenes.\"[421]\\nFrom 2014 to 2018, Swift collaborated with director Joseph Kahn on eight music videos\\xe2\\x80\\x94four each from her albums 1989 and Reputation. Kahn has praised Swift\\'s involvement.[422] She worked with American Express for the \"Blank Space\" music video (which Kahn directed), and served as an executive producer for the interactive app AMEX Unstaged: Taylor Swift Experience, for which she won a Primetime Emmy Award for Outstanding Interactive Program in 2015.[423] Swift produced the music video for \"Bad Blood\" and won a Grammy Award for Best Music Video in 2016.[424]\\nHer production company, Taylor Swift Productions, is credited with producing all of her visual media starting with the 2018 concert documentary Reputation Stadium Tour.[425] She continued to co-direct music videos for the Lover singles \"Me!\" with Dave Meyers, and \"You Need to Calm Down\" (also serving as a co-executive producer) and \"Lover\" with Drew Kirsch,[426] but first ventured into sole direction with the video for \"The Man\" (which won her the MTV Video Music Award for Best Direction).[427] After Folklore: The Long Pond Studio Sessions, Swift debuted as a filmmaker with All Too Well: The Short Film,[245] which made her the first artist to win the Grammy Award for Best Music Video as a sole director.[428] Swift has cited Chlo\\xc3\\xa9 Zhao, Greta Gerwig, Nora Ephron, Guillermo del Toro, John Cassavetes, and Noah Baumbach as filmmaking influences.[418]\\nAccolades and achievements\\nFurther information: List of awards and nominations received by Taylor Swift\\n\\nIn 2009, Swift became the first country singer to win an MTV Video Music Award.\\nSwift\\'s discography is a \"critically hailed songbook\", as per Time\\'s Sam Lansky.[429] She has won 14 Grammy Awards (including four for Album of the Year\\xe2\\x80\\x94the most won by an artist),[430] an Emmy Award,[431] 40 American Music Awards (the most won by an artist),[432] 39 Billboard Music Awards (the most won by an artist\\xe2\\x80\\x94tying with Drake),[433] 118 Guinness World Records,[434] 23 MTV Video Music Awards (including four Video of the Year wins\\xe2\\x80\\x94the most by an act),[247] 12 Country Music Association Awards (including the Pinnacle Award),[435] eight Academy of Country Music Awards,[436] and two Brit Awards.[168] As a songwriter, she has been honored by the Nashville Songwriters Association,[63][437] the Songwriters Hall of Fame, and the National Music Publishers\\' Association and was the youngest person on Rolling Stone\\'s list of the 100 Greatest Songwriters of All Time in 2015.[438][439] At the 64th BMI Awards in 2016, Swift was the first woman to be honored with an award named after its recipient.[440]\\nCommercially, from available data, Swift has amassed over 50 million album sales and 150 million single sales as of 2019,[441][442][443] and 114 million units globally, including 78 billion streams as of 2021.[444][445] The International Federation of the Phonographic Industry ranked her as the Global Recording Artist of the Year for a record three times (2014, 2019 and 2022).[446] Swift has the most number-one albums in the United Kingdom and Ireland for a female artist this millennium,[447][448] earned the highest income for an artist on Chinese digital music platforms (RMB 159,000,000 as of 2021),[449] and is the first artist to replace themselves at the top spot and occupy the entire top five[note 3] of the Australian albums chart.[452][453] Swift remains the world\\'s highest-grossing female touring act ever, with cumulative ticket sales at $1.96 billion as of November 2023 according to Pollstar.[454] The Eras Tour is the highest-grossing tour of all time as of December 2023, and the first to surpass $1 billion in revenue.[455] Beginning with Fearless, each of her studio albums have opened with over one million global units.[456][457] Swift is the most streamed female act on Spotify and Apple Music.[458][459] On Spotify, she is the only artist to have received more than 200 and 250 million streams in one day (260 million on October 27, 2023),[460] and the only female act to reach 100 million monthly listeners.[234] The most entries and the most simultaneous entries for an artist on the Billboard Global 200, with 143 and 31 songs, respectively, are among her feats.[461][462]\\nIn the US, Swift has sold over 37.3 million albums as of 2019,[443] when Billboard placed her eighth on its Greatest of All Time Artists Chart.[463] Eleven of her songs have topped the Billboard Hot 100.[257] She is the longest-reigning act of the Billboard Artist 100 (97 weeks);[464] the soloist with the most cumulative weeks atop the Billboard 200 (68);[465] the woman with the most Billboard 200 number-ones (13),[234] Hot 100 entries (232),[234][156] number-one debuts (6),[note 4] top-ten songs (49),[257] and weeks atop the Top Country Albums chart (101);[467] and the act with the most number-one songs on Pop Airplay (12) and Digital Songs (28).[468][469] Swift is the first woman to simultaneously chart five albums in the top 10 and eleven albums on the entire Billboard 200;[470][471] and the first act to occupy the top four spots and chart seven albums[note 5] in the top 10 on the Top Album Sales chart.[473][474] She is the second highest-certified female digital singles artist (and fifth overall) in the US, with 137.5 million total units certified by the Recording Industry Association of America (RIAA),[475] and the first woman to have both an album (Fearless) and a song (\"Shake It Off\") certified Diamond.[476] Swift is the only artist in Luminate history to have six albums sell over a million copies in a week.[477]\\nSwift has appeared in various power listings. Time included her on its annual list of the 100 most influential people in 2010, 2015, and 2019.[478] She was one of the \"Silence Breakers\" that the magazine spotlighted as Person of the Year in 2017 for speaking up about sexual assault,[479] and received the honor again in 2023 for her cultural domination that year.[429] Time described Swift as the first Person of the Year to be recognized for \"achievement in the arts\", as well as the first woman to be recognized and appear on a Person of the Year cover more than once.[480][481] In 2014, she was named to Forbes\\' 30 Under 30 list in the music category[482] and again in 2017 in its \"All-Star Alumni\" category.[483] Swift became the youngest woman to be included on Forbes\\' list of the 100 most powerful women in 2015, ranked at number 64.[484] In 2023, she was ranked by Forbes as the fifth-most powerful woman in the world, the first entertainer to place in the top five.[485] Swift received an honorary Doctor of Fine Arts degree from New York University and served as its commencement speaker on May 18, 2022.[245]\\nCultural status\\nMain articles: Cultural impact of Taylor Swift and Public image of Taylor Swift\\n\\nSwift at the 2010 Time 100 Gala\\nSwift has been credited with making a profound impact on the music industry, popular culture and the economy.[486][487] She dominates cultural conversations,[488][489] which has led publications to describe her as a cultural \"vitality\" or zeitgeist.[490][491][492] Her music, life and public image are points of attention in global celebrity culture.[297] Initially a teen idol,[493] she has been referred to as a pop icon;[312][494] publications describe her immense popularity and longevity as unwitnessed since the 20th century.[495][496] In 2013, New York magazine\\'s Jody Rosen dubbed Swift the \"world\\'s biggest pop star\" and opined that the trajectory of her stardom has defied established patterns. Rosen added that Swift \"falls between genres, eras, demographics, paradigms, trends\", leaving her contemporaries \"vying for second place\".[310] Critics regard Swift as a rare yet successful combination of the pop star and singer-songwriter archetypes.[497]\\nHer fans are known as Swifties.[266] Billboard noted only few artists have had her chart success, critical acclaim, and fan support.[498] Swift\\'s million-selling albums are considered an anomaly in the streaming-dominated industry following the end of the album era in the 2010s.[499][500] Economist Alan Krueger described Swift as an \"economic genius\".[501]\\nAlthough labeled by the media in her early career as \"America\\'s Sweetheart\" for her girl next door persona,[502][503] Swift has been accused by detractors of being \"calculated\" and manipulative of her image, a narrative bolstered by her 2016 dispute with West.[173][174] Critics have also noted that her personal life and career have been subject to intense misogyny and \"slut-shaming\",[504][505] as well as rampant media scrutiny and tabloid speculation.[506]\\nSwift\\'s private jet use has drawn scrutiny for its carbon emissions.[507][508] In 2023, a spokesperson for Swift stated that she had purchased more than double the required carbon credits to offset all tour travel and personal flights.[509][510] In December 2023, Swift\\'s lawyers sent a cease and desist letter to American programmer Jack Sweeney over tracking her private jet, citing safety concerns and stalking.[511][512] Swift has been a victim of numerous house break-ins and stalkers, some of whom were armed.[513][514]\\nLegacy\\n\"You have different artists dominating different sectors of the industry: Some are huge at streaming, some are big draws on the road. But we\\'re at this moment where there\\'s no one better than Taylor Swift, whether that\\'s on the radio, with streaming, ticket sales or just cultural impact.\"\\n\\xe2\\x80\\x93 Jason Lipshutz, Billboard executive director, 2023[515]\\nSwift helped shape the modern country music scene,[516] having extended her success beyond the Anglosphere,[310][516] pioneered the use of internet (Myspace) as a marketing tool,[33][53] and introduced the genre to a younger generation.[517][310] Country labels have since become interested in signing young singers who write their own music;[518] her guitar performances contributed to the \"Taylor Swift factor\", a phenomenon to which an upsurge in guitar sales to women, a previously ignored demographic, is attributed.[519][520]\\nAccording to publications, Swift changed the music landscape with her genre transitions, a discography that accommodates cultural shifts,[521] and her ability to popularize any sound in mainstream music.[522] Lyrically, in being personal and vulnerable in her songs, music journalist Nick Catucci opined Swift helped make space for later singers like Billie Eilish, Ariana Grande, and Halsey to do the same.[523] Scholars have highlighted the literary sensibility and poptimist implications of Swift.[355][524] She has been credited with legitimizing and popularizing the concept of album \"eras\".[525][526] Swift is a subject of academic study and scholarly media research.[297] Various educational institutions offer courses on Swift in literary, cultural and sociopolitical contexts.[527][297]\\nSwift has influenced numerous music artists, and her albums have inspired a generation of singer-songwriters.[517][320][528] Journalists praise her ability to reform industry practices, noting how her actions changed streaming policies, prompted awareness of intellectual property in new musicians,[529][530] and reshaped ticketing models.[531] Various sources deem Swift\\'s music a paradigm representing the millennial generation;[532] Vox called her the \"millennial Bruce Springsteen\",[533] and The Times named her \"the Bob Dylan of our age\".[534] Swift earned the title Woman of the Decade (2010s) from Billboard,[535] Artist of the Decade (2010s) at the American Music Awards,[536] and Global Icon at the Brit Awards for her impact.[445] Senior artists such as Paul McCartney,[537] Mick Jagger,[538] Madonna,[539] and Dolly Parton have praised her musicianship.[540] Carole King regards Swift her \"professional grand daughter\" and thanked Swift for \"carrying the torch forward\".[541] Springsteen called her a \"tremendous\" writer,[542] while Ringo Starr and Billy Joel considered Swift the Beatles\\' successor.[543][544] Britney Spears labeled Swift \"the most iconic pop woman of our generation\".[545]\\nEntrepreneurship\\nMedia outlets describe Swift as a savvy businesswoman;[546][547] in 2024, she topped Billboard\\'s annual Power 100 ranking of the top music industry executives.[548] Swift is known for her traditional album rollouts, consisting of a variety of promotional activities that Rolling Stone termed as an inescapable \"multimedia bonanza\".[549][550] Easter eggs and cryptic teasers became a common practice in contemporary pop music because of Swift.[551] Publications describe her discography as a music \"universe\" subject to analyses by fans, critics and journalists.[552][553][554] Swift maintains an active presence on social media and a close relationship with fans, to which many journalists attribute her success.[555][487][556] Her in-house management team is called 13 Management.[557]\\nSwift has endorsed many brands and businesses, having launched clothing lines with L.E.I. and Stella McCartney,[558][559] designed American Greetings cards and Jakks Pacific dolls,[560][561] released a number of fragrances with Elizabeth Arden,[562] and signed multi-year deals with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League\\'s Nashville Predators and Sony Cyber-shot digital cameras,[565][566] and became the global ambassador for New York City in 2014 and Record Store Day in 2022.[567][568]\\nSocial activism\\nFurther information: Political impact of Taylor Swift\\nSwift identifies as a pro-choice feminist,[569] and is a founding signatory of the Time\\'s Up movement against sexual harassment.[570] Specifically, she criticized the US Supreme Court\\'s decision to end federal abortion rights in 2022.[571] Swift also advocates for LGBT rights,[572] and has called for the passing of the Equality Act, which prohibits discrimination based on sex, sexual orientation, and gender identity.[573][574] She performed during WorldPride NYC 2019 at the Stonewall Inn, a gay rights monument, and has donated to the LGBT organizations Tennessee Equality Project and GLAAD.[575][576][577]\\nA supporter of the March for Our Lives movement and gun control reform in the US,[578] Swift is a vocal critic of white supremacy, racism, and police brutality.[579][569] Following the George Floyd protests, she donated to the NAACP Legal Defense and Educational Fund and the Black Lives Matter movement,[580] called for the removal of Confederate monuments in Tennessee,[581] and advocated for Juneteenth to become a national holiday.[582] In 2020, Swift urged her fans to check their voter registration ahead of elections, which resulted in 65,000 people registering to vote within one day of her post,[583] and endorsed Joe Biden and Kamala Harris in the US presidential election.[584] She has openly criticized former president Donald Trump.[585]\\nWealth\\nSwift\\'s net worth is estimated by Forbes and Bloomberg News at $1.1 billion as of October 2023, making her the first musician to achieve billionaire status \"solely based on her songs and performances\".[586][587] Forbes named her the annual top-earning female musician in 2016, 2019, 2021, and 2022.[588] She was the highest-paid celebrity of 2016 with $170 million\\xe2\\x80\\x94a feat recognized by the Guinness World Records as the highest annual earnings ever for a female musician,[589] which she herself surpassed with $185 million in 2019.[590] Overall, Forbes listed Swift as the highest-paid female artist of the 2010s, earning $825 million.[591] She has also developed a real estate portfolio worth $150 million as of 2023, with properties in Nashville; Tribeca, Manhattan; Los Angeles (Samuel Goldwyn Estate); and Rhode Island (High Watch).[592]\\nPhilanthropy\\nSwift is known for her philanthropic efforts.[593] She ranked first on DoSomething\\'s 2015 \"Gone Good\" list,[594] having received the Star of Compassion from the Tennessee Disaster Services and the Big Help Award from the Nickelodeon Kids\\' Choice Awards for her \"dedication to helping others\" and \"inspiring others through action\".[595][596] She donated $100,000 to the Red Cross to help the victims of the Iowa flood of 2008.[597] In 2009, she sang at BBC\\'s Children in Need concert and raised \\xc2\\xa313,000 for the cause.[598] Swift has performed at charity relief events, including Sydney\\'s Sound Relief concert.[599] In response to the May 2010 Tennessee floods, Swift donated $500,000.[600] In 2011, Swift used a dress rehearsal of her Speak Now tour as a benefit concert for victims of recent tornadoes in the US, raising more than $750,000.[601] In 2016, she donated $1 million to Louisiana flood relief efforts and $100,000 to the Dolly Parton Fire Fund.[602][603] Swift donated to food banks after Hurricane Harvey struck Houston in 2017 and at every stop of the Eras Tour in 2023;[604][605] she also directly employed local businesses throughout the tour and gave $55 million in bonus payments to her entire crew.[606][607] Swift donated $1 million for Tennessee tornado relief in 2020 and again in 2023.[608][609]\\nShe is a supporter of the arts. A benefactor of the Nashville Songwriters Hall of Fame,[610] Swift has donated $75,000 to Nashville\\'s Hendersonville High School to help refurbish the school auditorium,[611] $4 million to build a new education center at the Country Music Hall of Fame and Museum in Nashville,[612] $60,000 to the music departments of six US colleges,[613] and $100,000 to the Nashville Symphony.[614] Also a promoter of children\\'s literacy, she has donated money and books to schools around the country.[615][616] In 2007, Swift partnered with the Tennessee Association of Chiefs of Police to launch a campaign to protect children from online predators.[617] She has donated items to several charities for auction, including the UNICEF Tap Project and MusiCares.[618] As recipient of the Academy of Country Music\\'s Entertainer of the Year in 2011, Swift donated $25,000 to St. Jude Children\\'s Research Hospital, Tennessee.[619] In 2012, Swift participated in the Stand Up to Cancer telethon, performing the charity single \"Ronan\", which she wrote in memory of a four-year-old boy who died of neuroblastoma.[620] She has also donated $100,000 to the V Foundation for Cancer Research[621] and $50,000 to the Children\\'s Hospital of Philadelphia.[622] Swift has encouraged young people to volunteer in their local communities as part of Global Youth Service Day.[623]\\nSwift donated to fellow singer-songwriter Kesha to help with her legal battles against Dr. Luke and to actress Mariska Hargitay\\'s Joyful Heart Foundation.[593][624] During the COVID-19 pandemic, Swift donated to the World Health Organization and Feeding America,[625] and supported independent record stores.[626][627] Swift performed \"Soon You\\'ll Get Better\" on the One World: Together At Home television special, a benefit concert curated by Lady Gaga for Global Citizen to raise funds for the World Health Organization\\'s COVID-19 Solidarity Response Fund.[628] In 2018 and 2021, Swift donated to the Rape, Abuse & Incest National Network in honor of Sexual Assault Awareness and Prevention Month.[593][629] She has made donations to her fans several times for their medical or academic expenses.[630] In December 2023, Swift attended Ramy Youssef\\'s fundraiser for the Gaza Strip.[631]\\nDiscography\\nMain articles: Taylor Swift albums discography, Taylor Swift singles discography, and List of songs by Taylor Swift\\nStudio albums\\nTaylor Swift (2006)\\nFearless (2008)\\nSpeak Now (2010)\\nRed (2012)\\n1989 (2014)\\nReputation (2017)\\nLover (2019)\\nFolklore (2020)\\nEvermore (2020)\\nMidnights (2022)\\nThe Tortured Poets Department (2024)\\nRe-recorded albums\\nFearless (Taylor\\'s Version) (2021)\\nRed (Taylor\\'s Version) (2021)\\nSpeak Now (Taylor\\'s Version) (2023)\\n1989 (Taylor\\'s Version) (2023)\\nFilmography\\nMain article: Taylor Swift videography\\nThis section lists select works only. Refer to the main article for further information.\\nValentine\\'s Day (2010)\\nThe Lorax (2012)\\nThe Giver (2014)\\nCats (2019)\\nAll Too Well: The Short Film (also director) (2021)\\nAmsterdam (2022)\\nDocumentary and concert films\\nJourney to Fearless (2010)\\nSpeak Now World Tour \\xe2\\x80\\x93 Live (2011)\\nThe 1989 World Tour Live (2015)\\nTaylor Swift: Reputation Stadium Tour (2018)\\nMiss Americana (2020)\\nTaylor Swift: City of Lover (2020)\\nFolklore: The Long Pond Studio Sessions (also director) (2020)\\nTaylor Swift: The Eras Tour (2023)\\nTours\\nMain article: List of Taylor Swift live performances\\nFearless Tour (2009\\xe2\\x80\\x932010)\\nSpeak Now World Tour (2011\\xe2\\x80\\x932012)\\nThe Red Tour (2013\\xe2\\x80\\x932014)\\nThe 1989 World Tour (2015)\\nReputation Stadium Tour (2018)\\nThe Eras Tour (2023\\xe2\\x80\\x932024)\\nSee also\\nList of American Grammy Award winners and nominees\\nList of highest-certified music artists in the United States\\nList of most-followed Instagram accounts\\nList of most-followed Twitter accounts\\nList of most-subscribed YouTube channels\\nFootnotes\\n Swift held the record until the 62nd Annual Grammy Awards in 2020.[91][92]\\n Though Swift has properties throughout the US, she identifies Nashville as her home.[149][150]\\n Swift has occupied the top five of the ARIA Albums Chart twice. She achieved this feat first on the issue published on July 7, 2023,[450] followed by a second time on the issue published on February 9, 2024.[451]\\n In a tie with Ariana Grande.[466]\\n Swift has charted seven titles in the top 10 of the Top Album Sales chart twice\\xe2\\x80\\x94on the issues dated January 6, 2024, and January 20, 2024.[472]\\nReferences\\n \"Taylor Swift: The record-breaking artist in numbers\". Newsround. March 2, 2020. Archived from the original on March 8, 2020. Retrieved April 20, 2020.\\n Sutherland, Mark (May 23, 2015). \"Taylor Swift interview: \\'A relationship? No one\\'s going to sign up for this\\'\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved April 20, 2020.\\n Scott, Walter (June 11, 2015). \"What Famous Pop Star Is Named After James Taylor?\". Parade. Archived from the original on October 15, 2016. Retrieved December 12, 2018.\\n \"Taylor Swift is not an \"underdog\": The real story about her 1 percent upbringing that the New York Times won\\'t tell you\". Salon.com. May 23, 2015. Archived from the original on May 25, 2022. Retrieved December 26, 2020.\\n Jepson 2013, p. 1.\\n Roth, Madeline (May 19, 2015). \"Taylor Swift\\'s Brother Had The Most Epic Graduation Weekend Ever\". MTV News. Archived from the original on July 23, 2016. Retrieved July 25, 2016.\\n McKay, Gabriel (July 6, 2023). \"Taylor Swift Edinburgh: Is star the real queen of Scotland?\". The Herald. Archived from the original on February 1, 2024. Retrieved February 4, 2024.\\n Eleftheriou-Smith, Loulla-Mae (June 24, 2015). \"Taylor Swift tells Scotland: \\'I am one of you\\'\". The Independent. Archived from the original on May 26, 2022. Retrieved July 10, 2019.\\n Vadala, Nick (July 14, 2017). \"Taylor Swift ancestor\\'s home added to Philly Register of Historic Places\". The Philadelphia Inquirer. Archived from the original on May 25, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift\\'s Great-Great-Grandfather\\'s Philly Home Gets Historic Landmark Status\". AP NEWS. July 25, 2017. Archived from the original on May 22, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift stammt aus dem Freistaat\" (in German). BR24. September 17, 2015. Archived from the original on December 31, 2021. Retrieved July 23, 2023.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Raab, Scott (October 20, 2014). \"Taylor Swift Interview\". Esquire. Archived from the original on February 16, 2015. Retrieved April 11, 2015.\\n \"Taylor Swift on Politicians Co-opting Faith: \\'I\\'m a Christian. That\\'s Not What We Stand For\\'\". Relevant. January 31, 2020. Archived from the original on November 25, 2022. Retrieved April 2, 2020.\\n Uhrich, Bill (February 13, 2010). \"Photos Students at Alvernia Montessori School sending Taylor Swift a valentine\". Reading Eagle. Archived from the original on October 16, 2013. Retrieved February 25, 2013.\\n Hatza, George (December 8, 2008). \"Taylor Swift: Growing into superstardom\". Reading Eagle. Archived from the original on April 1, 2012. Retrieved April 17, 2012.\\n Mennen, Lauren (November 12, 2014). \"Taylor Swift\\'s Wyomissing childhood home on the market for $799,500\". Philadelphia Daily News. Archived from the original on October 17, 2016. Retrieved October 13, 2016.\\n Chang, David (February 22, 2016). \"Taylor Swift Returns to Reading Pennsylvania as Maid of Honor in Friend\\'s Wedding\". WCAU. Archived from the original on September 16, 2016. Retrieved August 26, 2016.\\n \"Taylor Swift, Age 12\". New York Daily News. Archived from the original on August 27, 2016. Retrieved August 26, 2016.\\n Cooper, Brittany Joy (April 15, 2012). \"Taylor Swift Opens Up About a Future in Acting and Admiration for Emma Stone\". Taste of Country. Archived from the original on April 17, 2012. Retrieved April 17, 2012.\\n MacPherson, Alex (October 18, 2012). \"Taylor Swift: \\'I want to believe in pretty lies\\'\". The Guardian. Archived from the original on August 26, 2016. Retrieved August 3, 2016.\\n Rolling Stone Interview: The Unabridged Taylor Swift, December 2, 2008\\n Morris, Edward (December 1, 2006). \"When She Thinks \\'Tim McGraw\\', Taylor Swift Savors Payoff: Hardworking Teen to Open for George Strait Next Year\". CMT. Archived from the original on June 26, 2015. Retrieved March 11, 2010.\\n Diu, Nisha Lilia (April 3, 2011). \"Taylor Swift: \\'I won\\'t do sexy shoots\\'\". The Daily Telegraph. Archived from the original on May 6, 2013. Retrieved April 17, 2012.\\n \"News : CMT Insider Interview: Taylor Swift (Part 1 of 2)\". CMT. November 26, 2008. Archived from the original on January 23, 2015. Retrieved July 1, 2012.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. Archived from the original on May 10, 2012. Retrieved May 21, 2012.\\n Qureshi, Hira. \"Visit this Stone Harbor caf\\xc3\\xa9 where Taylor Swift was \\'always coming in to play\\' as a child\". Courier-Post. Archived from the original on October 26, 2023. Retrieved December 12, 2022.\\n Kuperinsky, Amy (July 28, 2020). \"Taylor Swift shouts out Jersey Shore town in video for surprise album\". NJ.com. Archived from the original on December 12, 2022. Retrieved December 12, 2022.\\n Martino, Andy (January 10, 2015). \"EXCLUSIVE: The real story of Taylor Swift\\'s guitar \\'legend\\'\". New York Daily News. Archived from the original on November 22, 2015. Retrieved August 28, 2017.\\n \"Dymtrow v. Swift et al: Federal Civil LawsuitNew York Southern District Court, Case No. 1:07-cv-11277-RJS\" (PDF). American Bar Association. Archived from the original (PDF) on October 11, 2012. Retrieved April 18, 2012.\\n \"On tour with Taylor Swift\". NBC News. May 31, 2009. Archived from the original on October 5, 2013. Retrieved July 1, 2012.\\n Castro, Vicky (February 6, 2015). \"How to Succeed as an Entrepreneur, Taylor Swift Style\". Inc. Archived from the original on June 7, 2016. Retrieved February 9, 2015.\\n Willman, Chris (July 25, 2007). \"Getting to know Taylor Swift\". Entertainment Weekly. Archived from the original on May 24, 2022. Retrieved January 25, 2022.\\n Jo, Nancy (January 2, 2014). \"Taylor Swift and the Growing of a Superstar: Her Men, Her Moods, Her Music\". Vanity Fair. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n \"News : Taylor Swift\\'s High School Names Auditorium in Her Honor\". CMT. September 23, 2010. Archived from the original on November 21, 2014. Retrieved April 18, 2012.\\n Grigoriadis, Vanessa (March 5, 2009). \"The Very Pink, Very Perfect Life of Taylor Swift\". Rolling Stone. Archived from the original on May 3, 2019. Retrieved July 28, 2019.\\n \"Taylor Swift receives her high school diploma\". Houston Chronicle. July 27, 2008. Archived from the original on January 1, 2024. Retrieved January 2, 2024.\\n \"Taylor Swift: The Garden In The Machine\". American Songwriter. May 2, 2011. Archived from the original on August 7, 2013. Retrieved May 21, 2012.\\n \"Songwriter Taylor Swift Signs Publishing Deal With Sony/ATV\". Broadcast Music, Inc. May 12, 2005. Archived from the original on December 4, 2012. Retrieved April 20, 2012.\\n Kosser, Michael (June 3, 2010). \"Liz Rose: Co-Writer to the Stars\". American Songwriter. Archived from the original on December 24, 2011. Retrieved April 19, 2012.\\n Leahey, Andrew (October 24, 2014). \"Songwriter Spotlight: Liz Rose\". Rolling Stone. Archived from the original on September 26, 2016. Retrieved September 24, 2016.\\n DeLuca, Dan (November 11, 2008). \"Focused on \\'great songs\\' Taylor Swift isn\\'t thinking about \\'the next level\\' or Joe Jon as gossip\". Philadelphia Daily News. p. 1. Archived from the original on November 18, 2012. Retrieved April 17, 2012.\\n Preston, John (April 26, 2009). \"Taylor Swift: the 19-year-old country music star conquering America \\xe2\\x80\\x93 and now Britain\". The Daily Telegraph. Archived from the original on January 5, 2012. Retrieved August 30, 2012.\\n Rosa, Christopher (March 24, 2015). \"Opening Acts Who Became Bigger Than The Headliner\". VH1. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n Rapkin, Mickey (July 27, 2017). \"Oral History of Nashville\\'s Bluebird Cafe: Taylor Swift, Maren Morris, Dierks Bentley & More on the Legendary Venue\". Billboard. Archived from the original on July 29, 2017. Retrieved July 28, 2017.\\n Hiatt, Brian (October 25, 2012). \"Taylor Swift in Wonderland\". Rolling Stone. Archived from the original on July 31, 2016. Retrieved August 1, 2016.\\n Greenburg, Zack O\\'Malley (June 26, 2013). \"Toby Keith, Cowboy Capitalist: Country\\'s $500 Million Man\". Forbes. Archived from the original on August 27, 2016. Retrieved August 1, 2016.\\n Taylor Swift (CD). Big Machine Records. 2006. BMR120702.\\n Tamarkin, Jeff. \"Taylor Swift \\xe2\\x80\\x93 Taylor Swift\". AllMusic. Archived from the original on October 20, 2015. Retrieved February 14, 2021.\\n Neal, Chris (December 4, 2006). \"Taylor Swift Review\". Country Weekly. Archived from the original on July 22, 2012. Retrieved March 31, 2010.\\n Trust, Gary (October 29, 2009). \"Chart Beat Thursday: Taylor Swift, Tim McGraw Linked Again\". Billboard. Archived from the original on March 7, 2013. Retrieved November 8, 2016.\\n \"Taylor Swift\". Songwriters\\' Hall of Fame. Archived from the original on February 12, 2021. Retrieved September 21, 2022.\\n Willman, Chris (February 5, 2008). \"Taylor Swift\\'s Road to Fame\". Entertainment Weekly. p. 3. Archived from the original on February 21, 2015. Retrieved April 22, 2012.\\n \"Taylor Swift Joins Rascal Flatts Tour\". CMT. October 18, 2006. Archived from the original on January 7, 2015. Retrieved March 11, 2010.\\n Whitaker, Sterling; Hammar, Ania (May 27, 2019). \"How Eric Church\\'s Rascal Flatts Feud Helped Launch Taylor Swift\\'s Career\". Taste of Country. Townsquare Media. Archived from the original on June 6, 2019. Retrieved June 10, 2019.\\n \"Taylor Swift No. 1 on iTunes\". Great American Country. December 19, 2007. Archived from the original on March 3, 2012. Retrieved July 5, 2010.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart history\". Billboard. Archived from the original on August 9, 2016. Retrieved July 26, 2016.\\n \"Taylor Swift owns top of country chart\". Country Standard Time. July 23, 2008. Archived from the original on July 31, 2008. Retrieved December 26, 2008.\\n \"Wal-Mart \"Eyes\" New Taylor Swift Project\". Great American Country. Archived from the original on July 23, 2008. Retrieved July 24, 2008.\\n \"Taylor Swift Joins George Strait\\'s 2007 Tour\". CMT. November 17, 2006. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Brad Paisley Plans Tour With Three Opening Acts\". CMT. January 9, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Joins Tim McGraw, Faith Hill on Tour\". CMT. June 1, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Youngest Winner of Songwriter/Artist Award\". Great American Country. October 16, 2007. Archived from the original on January 11, 2015. Retrieved February 2, 2015.\\n \"Photos : All Taylor Swift Pictures : Horizon Award Winner Poses in the Pressroom\". CMT. September 7, 2007. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Photos : 43rd Annual ACM Awards \\xe2\\x80\\x93 Onstage: Winners : Acceptance Speech\". CMT. May 18, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Taylor Swift, Rascal Flatts, Carrie Underwood Score at 2008 AMA Awards\" (Blog). Roughstock.com. November 24, 2008. Archived from the original on July 10, 2014. Retrieved May 21, 2012.\\n \"Amy Winehouse Wins Best New Artist, Kanye West Pays Tribute to Mom \\xe2\\x80\\x93 Grammy Awards 2008, Grammy Awards\". People. October 2, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Rascal Flatts Announce Summer Tour With Taylor Swift\". CMT. May 5, 2008. Archived from the original on April 18, 2022. Retrieved June 5, 2019.\\n Caplan, David (September 8, 2008). \"Scoop\". People. Archived from the original on February 2, 2016. Retrieved March 6, 2012.\\n Rizzo, Monica (November 24, 2008). \"Scoop \\xe2\\x80\\x93 Couples, Camilla Belle, Joe Jonas\". People. Archived from the original on March 3, 2016. Retrieved March 6, 2012.\\n Akers, Shelley (June 9, 2008). \"Taylor Swift to Appear in Hannah Montana Movie\". People. Archived from the original on October 27, 2017. Retrieved October 27, 2017.\\n \"Hannah Montana: The Movie (Original Motion Picture Soundtrack) by Hannah Montana\". iTunes Store. January 2009. Archived from the original on May 2, 2016. Retrieved August 2, 2016.\\n \"CD Taylor Swift \\xe2\\x80\\x93 Fearless\" (in Portuguese). Universal Music Group. Archived from the original on January 18, 2021. Retrieved February 14, 2021.\\n Raphael, Amy (February 1, 2009). \"First, she conquered Nashville. Now she\\'s set for world domination\". The Observer. ProQuest 250507223. Archived from the original on December 9, 2022. Retrieved December 9, 2022.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong with Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved July 24, 2014.\\n \"Discography Taylor Swift\". ARIA Charts. Archived from the original on March 21, 2012. Retrieved January 2, 2010.\\n Trust, Gary (December 15, 2009). \"Best of 2009: Part 1\". Billboard. Archived from the original on March 3, 2013. Retrieved September 20, 2022.\\n Ben-Yehuda, Ayala (August 13, 2009). \"Black Eyed Peas, Jason Mraz Tie Records on Billboard Hot 100\". Billboard. Archived from the original on May 8, 2013. Retrieved March 13, 2010.\\n Trust, Gary (September 24, 2009). \"Taylor Swift Climbs Hot 100, Black Eyed Peas Still No. 1\". Billboard. Archived from the original on February 1, 2013. Retrieved September 20, 2022.\\n \"Taylor Swift Chart History (Hot Country Songs)\". Billboard. Archived from the original on January 31, 2021. Retrieved February 14, 2021.\\n Grein, Paul (March 16, 2012). \"Chart Watch Extra: Top Albums Of Last 10 Years\" (Blog). Yahoo! Music. Archived from the original on April 2, 2015. Retrieved June 10, 2011.\\n Mapes, Jillian (November 23, 2010). \"Taylor Swift Announces \\'Speak Now\\' World Tour\". Billboard. Archived from the original on May 8, 2013. Retrieved May 15, 2012.\\n Weiss, Dan (December 12, 2011). \"Taylor Swift: Journey To Fearless DVD\". American Songwriter. Archived from the original on August 15, 2016. Retrieved August 2, 2016.\\n Ryan, Sarah (August 10, 2009). \"Taylor Swift Pranks Keith Urban\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved November 11, 2015.\\n \"Kanye calls Taylor Swift after \\'View\\' appearance\". MSNBC. September 15, 2009. Archived from the original on October 6, 2013. Retrieved September 16, 2009.\\n \"Taylor Swift Thanks \"Gracious\" Beyonce for Inviting Her Onstage After Kanye Stunt at VMAs\". Rolling Stone. September 14, 2009. Archived from the original on June 15, 2012. Retrieved May 15, 2012.\\n Anderson, Kyle (September 16, 2009). \"Kanye West\\'s VMA Interruption Gives Birth To Internet Photo Meme\". MTV. Archived from the original on January 16, 2016. Retrieved October 3, 2009.\\n Ditzian, Eric (2009). \"Taylor Swift, Michael Jackson Big Winners at American Music Awards\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"2009 Artists of the Year\". Billboard. December 10, 2009. Archived from the original on January 6, 2010. Retrieved May 21, 2012.\\n \"Taylor Swift Raps \\'Thug Story\\' With T-Pain On CMT Awards\". MTV. June 17, 2009. Archived from the original on May 25, 2022. Retrieved November 11, 2015.\\n Kreps, Daniel (February 1, 2010). \"Beyonce, Taylor Swift Dominate 2010 Grammy Awards\". Rolling Stone. Archived from the original on February 15, 2012. Retrieved February 13, 2012.\\n \"Billie Eilish replaces Taylor Swift as youngest artist to win a Grammy for Album of the Year\". MSN. January 27, 2020. Archived from the original on November 20, 2020. Retrieved September 1, 2020.\\n Kaufman, Gil (November 12, 2009). \"Taylor Swift Dominates CMA Awards\". MTV News. Archived from the original on March 6, 2016. Retrieved September 13, 2016.\\n Vena, Jocelyn (November 6, 2009). \"John Mayer Talks Taylor Swift Collaboration \\'Half of My Heart\\'\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"Boys Like Girls featuring Taylor Swift, \\'Two Is Better Than One\\'\". Billboard. December 2, 2009. Archived from the original on August 18, 2021. Retrieved December 4, 2020.\\n \"Kellie Pickler Has Her \\'Best Days\\' Thanks To Taylor Swift\". MTV. Archived from the original on January 1, 2016. Retrieved November 11, 2015.\\n Vena, Jocelyn (December 28, 2009). \"New Taylor Swift Song Included In \\'Valentine\\'s Day\\' Featurette\". MTV. Archived from the original on January 16, 2016. Retrieved November 11, 2015.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart history on Canadian Hot 100\". Billboard. Archived from the original on August 9, 2016. Retrieved August 3, 2016.\\n Park, Michael Y.; Sia, Nicole (December 29, 2009). \"Taylor & Taylor Romance Was Overblown, Says Source\". People. Archived from the original on November 13, 2012. Retrieved March 6, 2012.\\n Caramanica, Jon (March 6, 2009). \"OMG! Taylor Swift Does \\'CSI\\'!\". The New York Times (Blog). Archived from the original on August 14, 2011. Retrieved May 7, 2012.\\n Strecker, Erin (January 2, 2015). \"Remember When Taylor Swift Shined as \\'Saturday Night Live\\' Host?\". Billboard. Archived from the original on January 24, 2015. Retrieved January 15, 2015.\\n Dukes, Billy (October 22, 2012). \"10 Things You Didn\\'t Know About Taylor Swift\". Taste of Country. Townsquare Media. Archived from the original on May 22, 2022. Retrieved July 26, 2020.\\n Pietroluongo, Silvio (August 11, 2010). \"Taylor Swift Makes Sparkling Hot 100 Entrance\". Billboard. Archived from the original on January 31, 2016. Retrieved July 25, 2016.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift, Angry on \\'Speak Now\\'\". The New York Times. Archived from the original on October 21, 2010. Retrieved October 23, 2010.\\n \"Taylor Swift\\'s New Album, Speak Now, Set for Oct. 25 Release\". CMT. July 20, 2010. Archived from the original on June 21, 2019. Retrieved February 14, 2020.\\n Kaufman, Gil (November 3, 2010). \"Taylor Swift\\'s Speak Now Tops 1 Million in First Week\". MTV. Archived from the original on August 10, 2016. Retrieved August 8, 2016.\\n \"Fastest-selling digital album in the US by a female artist\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Knopper, Steve (November 25, 2010). \"Taylor Swift\\'s Speak Now Tops the Charts\". Rolling Stone. Archived from the original on March 3, 2021. Retrieved November 25, 2010.\\n Sheffield, Rob (October 26, 2010). \"Speak Now (2010)\". Rolling Stone. Archived from the original on September 11, 2013. Retrieved December 20, 2019.\\n Hammel, Sara (January 4, 2011). \"Taylor Swift & Jake Gyllenhaal Break Up: Source\". People. Archived from the original on May 9, 2012. Retrieved March 6, 2012.\\n Wyland, Sarah (February 12, 2012). \"Taylor Swift Takes Home Two GRAMMYs at Tribute-Filled Show\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved February 13, 2012.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on January 16, 2016. Retrieved November 21, 2015.\\n Smith, Hazel (October 24, 2011). \"News : Hot Dish: Taylor Swift Sings Alan Jackson\\'s Masterpiece at Nashville Songwriters Celebration\". CMT. Archived from the original on November 29, 2014. Retrieved April 22, 2012.\\n Roland, Tom (December 2, 2011). \"Taylor Swift: Billboard\\'s Woman of the Year\". Billboard. Archived from the original on February 3, 2013. Retrieved May 15, 2012.\\n Talbott, Chris; Silva, Cristina (April 2, 2012). \"Taylor Swift wins ACM entertainer of the year\". Yahoo!. Associated Press. Archived from the original on August 23, 2016. Retrieved May 15, 2012.\\n \"CMA Awards 2011: Taylor Swift wins entertainer of the year\". CBS News. November 9, 2011. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n Kellogg, Jane (November 20, 2011). \"AMAs 2011: Winners and Nominees Complete List\". The Hollywood Reporter. Archived from the original on June 27, 2015. Retrieved November 21, 2015.\\n Sheffield, Rob (June 23, 2012). \"Women Who Rock: The 50 Greatest Albums of All Time\". Rolling Stone. Archived from the original on December 10, 2016. Retrieved July 15, 2017.\\n Allen, Bob (March 29, 2012). \"Hot Tours: Taylor Swift, George Strait, Cirque Du Soleil\". Billboard. Archived from the original on February 21, 2013. Retrieved May 10, 2012.\\n \"Taylor Swift News and Blog\". taylorswift.com. September 21, 2011. Archived from the original on October 11, 2011. Retrieved September 21, 2011.\\n Herrera, Monica (March 15, 2012). \"Taylor Swift, Arcade Fire Talk \\'Hunger Games\\'\". Rolling Stone. Archived from the original on June 27, 2015. Retrieved May 10, 2012.\\n \"Nominations 2013 \\xe2\\x80\\x94 Golden Globe Awards\". goldenglobes.org. December 13, 2012. Archived from the original on December 14, 2012. Retrieved December 13, 2012.\\n Horowitz, Steven J. (April 20, 2012). \"B.o.B Explains Origins of Taylor Swift Collaboration \\'Both of Us\\'\". HipHopDX. Archived from the original on September 10, 2015. Retrieved May 15, 2012.\\n Toomedy, Alyssa (October 25, 2012). \"Taylor Swift and Conor Kennedy Breakup: Anatomy of a Split\". E! News. Archived from the original on December 4, 2015. Retrieved November 10, 2015.\\n Trust, Gary (August 22, 2012). \"Taylor Swift Scores First Hot 100 No. 1\". Billboard. Archived from the original on February 13, 2013. Retrieved August 22, 2012.\\n \"Discography Taylor Swift\". New Zealand Charts. Archived from the original on April 20, 2017. Retrieved July 26, 2016.\\n Lynch, Kevin (September 4, 2013). \"Calvin Harris trumps Michael Jackson feat to join Taylor Swift, Rihanna and One Direction in Guinness World Records\\xe2\\x84\\xa2 2014 book\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift \\xe2\\x80\\x93 I Knew You Were Trouble\". ARIA Charts. Archived from the original on May 27, 2016. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Official Singles Charts Top 100\". Official Charts Company. Archived from the original on July 13, 2021. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift Leads Record Breaking Digital Sales Week\". Billboard. January 3, 2013. Archived from the original on April 8, 2017. Retrieved February 14, 2021.\\n Lewis, Randy (October 30, 2012). \"Taylor Swift raises the bar with a savvy \\'Red\\' marketing campaign\". Los Angeles Times. Archived from the original on December 28, 2020. Retrieved December 28, 2020.\\n Mansfield, Brian (October 17, 2012). \"Taylor Swift sees \\'Red\\' all over\". USA Today. Archived from the original on December 21, 2012.\\n English, J. (August 28, 2017). \"Taylor Swift\\'s \\'Red\\': A Canonical Coming-Of-Age Album\". NPR. Archived from the original on April 12, 2021. Retrieved February 14, 2021.\\n Roberts, Randall (October 31, 2012). \"\\'The Last Time\\' connects Taylor Swift with Arcade Fire\". Los Angeles Times. Archived from the original on March 6, 2016. Retrieved December 27, 2020.\\n Greenwald, David (September 6, 2013). \"Taylor Swift, Rihanna, Justin Bieber Among 2014 Guinness Record-Setters\". Billboard. Archived from the original on June 8, 2015. Retrieved July 27, 2016.\\n Sexton, Paul (August 31, 2019). \"Taylor Swift Scores Fourth U.K. No. 1 With \\'Lover\\' Album\". Billboard. Archived from the original on August 16, 2021. Retrieved October 15, 2019.\\n \"Grammys 2014: The complete list of nominees and winners\". Los Angeles Times. January 26, 2014. Archived from the original on March 4, 2015. Retrieved January 25, 2015.\\n Gregoire, Carolyn (November 19, 2012). \"Taylor Swift AMA Awards 2012: Pop Star Performs \\'I Knew You Were Trouble\\' (Video)\". HuffPost. Archived from the original on May 26, 2013. Retrieved June 10, 2013.\\n Payne, Chris (November 25, 2013). \"Taylor Swift & Justin Timberlake Win Big at American Music Awards\". Billboard. Archived from the original on November 24, 2015. Retrieved November 21, 2015.\\n \"NSAI Songwriter/Artists of the Year\". Nashville Songwriters Association International. Archived from the original on August 19, 2016. Retrieved August 2, 2016.\\n Allen, Bob (July 3, 2014). \"Taylor Swift\\'s Red Wraps as All-Time Country Tour\". Billboard. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Caramanica, Jon (November 7, 2013). \"Country Awards Hold Swift Close\". The New York Times. Archived from the original on November 11, 2013. Retrieved April 3, 2014.\\n \"Taylor Swift\\'s Boyfriend Timeline: 10 Relationships & Their Songs\". Billboard. December 30, 2014. Archived from the original on August 26, 2016. Retrieved August 26, 2016.\\n Labrecque, Jeff (December 12, 2013). \"\\'12 Years a Slave\\' and \\'American Hustle\\' lead Golden Globe nominees\". Entertainment Weekly. Archived from the original on March 23, 2015. Retrieved December 12, 2013.\\n Bonaguro, Alison (January 25, 2013). \"News : Offstage: Tim McGraw Wanted to Make Taylor Swift Duet an Event\". CMT. Archived from the original on July 12, 2015. Retrieved February 25, 2013.\\n Blistein, Doyle (June 4, 2013). \"Taylor Swift Joins Rolling Stones for \\'As Tears Go By\\'\". Rolling Stone. Archived from the original on June 9, 2013. Retrieved June 10, 2013.\\n \"Taylor Swift Joins Florida Georgia Line Onstage for \\'Cruise\\'\". Taste of Country. Townsquare Media. March 2, 2013. Archived from the original on March 16, 2013. Retrieved March 29, 2013.\\n Collin, Robbie (July 26, 2012). \"The Lorax, review\". The Daily Telegraph. Archived from the original on September 17, 2016. Retrieved August 3, 2016.\\n Beard, Lanford (May 1, 2013). \"Taylor Swift says \\'I do\\' to \\'New Girl\\'\". Entertainment Weekly. Archived from the original on November 8, 2016. Retrieved August 4, 2016.\\n Busis, Hillary (September 27, 2013). \"Taylor Swift will co-star in long-awaited adaptation of \\'The Giver\\'\". Entertainment Weekly. Archived from the original on December 21, 2016. Retrieved August 2, 2016.\\n Peterson, Price (March 31, 2014). \"Taylor Swift Moves into NYC Apartment Built Over Mysterious River of Pink Slime\". The Atlantic. Archived from the original on September 27, 2016. Retrieved July 31, 2016.\\n Rogers, Alex (March 7, 2014). \"Why Taylor Swift Thinks Nashville Is the Best Place on Earth\". Time. Archived from the original on May 24, 2022. Retrieved September 27, 2016.\\n Milzoff, Rebecca; Brown, Eric Renner; Denis, Kyle (August 24, 2023). \"Taylor Swift and Beyonc\\xc3\\xa9 Are So Big, Even Their Publicists Have Fans\". Billboard. Archived from the original on August 24, 2023. Retrieved August 24, 2023.\\n Zollo, Paul (February 17, 2016). \"The Oral History of Taylor Swift\\'s 1989\". The Recording Academy. Archived from the original on June 3, 2021. Retrieved February 14, 2021.\\n Stutz, Colin (October 16, 2014). \"Watch Taylor Swift\\'s \\'1989\\' Secret Sessions Behind The Scenes Video\". Billboard. Archived from the original on June 5, 2016. Retrieved August 2, 2016.\\n Caulfield, Keith (November 4, 2014). \"Taylor Swift\\'s \"1989\" debuts with 1.287 million copies sold\". Billboard. Archived from the original on November 5, 2014. Retrieved November 4, 2014.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Discography Taylor Swift\". ARIA Charts. Archived from the original on August 26, 2019. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift Chart History (Canadian Hot 100)\". Billboard. Archived from the original on August 9, 2016. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift\\'s \\'Bad Blood\\' Blasts to No. 1 on Hot 100\". Billboard. May 28, 2015. Archived from the original on September 6, 2015. Retrieved May 28, 2015.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart History: Hot 100\". Billboard. Archived from the original on March 8, 2018. Retrieved September 18, 2016.\\n \"Live Music\\'s $20 Billion Year: The Grateful Dead\\'s Fare Thee Well Reunion, Taylor Swift, One Direction Top Boxscore\\'s Year-End\". Billboard. Archived from the original on December 14, 2015.\\n Weissmann, Jordan (July 7, 2014). \"Taylor Swift Has Written an Op-Ed in the Wall Street Journal\". Slate (Blog). Archived from the original on January 23, 2015. Retrieved January 23, 2015.\\n Knopper, Steve (November 8, 2014). \"Taylor Swift\\'s Label Head Explains Spotify Removal\". Rolling Stone. Archived from the original on April 21, 2015. Retrieved April 11, 2015.\\n Peters, Mitchell (June 21, 2015). \"Taylor Swift Pens Open Letter Explaining Why \\'1989\\' Won\\'t Be on Apple Music\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Halperin, Shirley (June 21, 2015). \"Apple Changes Course After Taylor Swift Open Letter: Will Pay Labels During Free Trial\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Rosen, Christopher (June 25, 2015). \"Taylor Swift is putting 1989 on Apple Music\". Entertainment Weekly. Archived from the original on June 26, 2015. Retrieved June 25, 2015.\\n \"Taylor Swift returns to Spotify on the day Katy Perry\\'s album comes out\". BBC News. June 9, 2017. Archived from the original on June 9, 2017.\\n \"Taylor Swift: 2014 Billboard Woman of the Year\". Billboard. October 10, 2014. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Payne, Chris (November 23, 2014). \"Taylor Swift Wins Dick Clark Award of Excellence at 2014, Presented by Diana Ross\". Billboard. Archived from the original on July 4, 2015. Retrieved April 11, 2015.\\n \"The Taylor Swift Experience\". GRAMMY Museum. Archived from the original on November 25, 2022. Retrieved April 22, 2022.\\n Boehrer, Kat (January 7, 2016). \"Watch Taylor Swift\\'s Stunning Acoustic Performance of \\'Blank Space\\' at the Grammy Museum\". Complex. Archived from the original on April 22, 2022. Retrieved April 22, 2022.\\n Jonze, Tim (February 25, 2015). \"Taylor Swift wins international female solo artist at Brit awards 2015\". The Guardian. Archived from the original on January 1, 2016. Retrieved April 11, 2015.\\n Stutz, Colin (July 21, 2015). \"2015 MTV Video Music Awards Nominees Revealed: Taylor Swift, Kendrick Lamar, Ed Sheeran & More\". Billboard. Archived from the original on July 24, 2015. Retrieved August 15, 2016.\\n Lynch, Joe (February 19, 2016). \"Taylor Swift Joins Elite Club to Win Grammy Album of the Year More Than Once: See the Rest\". Billboard. Archived from the original on March 1, 2016. Retrieved July 31, 2016.\\n Chiu, Melody (June 1, 2016). \"Taylor Swift and Calvin Harris Split After 15 Months Together\". People. Archived from the original on June 22, 2016. Retrieved June 1, 2016.\\n Spanos, Brittany (July 13, 2016). \"Taylor Swift Co-Wrote Calvin Harris\\' Smash Hit \\'This Is What You Came For\\'\". Rolling Stone. Archived from the original on July 30, 2016. Retrieved July 31, 2016.\\n Grady, Constance (August 26, 2019). \"How the Taylor Swift-Kanye West VMAs scandal became a perfect American morality tale\". Vox. Archived from the original on December 2, 2022. Retrieved October 12, 2023.\\n Grady, Constance (March 21, 2020). \"Newly leaked footage shows Taylor Swift and Kanye West talking \"Famous\"\". Vox. Archived from the original on March 24, 2022. Retrieved October 12, 2023.\\n Lewis, Anna (July 15, 2016). \"Tom Hiddleston finally tells us the truth about his relationship with Taylor Swift\". Cosmopolitan. Archived from the original on July 17, 2016. Retrieved January 8, 2019.\\n \"Taylor Swift Finally Reveals When She Started Dating Joe Alwyn in Lover Album\". Yahoo!. August 23, 2019. Archived from the original on June 11, 2020. Retrieved June 11, 2020.\\n \"Paul McCartney & Taylor Swift\". Rolling Stone. November 13, 2020. Archived from the original on November 30, 2020. Retrieved September 15, 2021. McCartney: So how does that go? Does your partner sympathize with that and understand? Swift: Oh, absolutely.\\n Irvin, Jack (April 10, 2023). \"Inside Taylor Swift and Joe Alwyn\\'s \\'Differences\\' That Led to Their Breakup: Sources (Exclusive)\". People. Archived from the original on September 25, 2023. Retrieved September 25, 2023.\\n \"Taylor Swift, pop princess, wins song of the year at the CMA Awards\". USA Today. Archived from the original on November 9, 2017. Retrieved November 9, 2017.\\n Trust, Gary (February 21, 2017). \"Ed Sheeran Tops Hot 100, Katy Perry Debuts at No. 4 & Bruno Mars, Rihanna & The Weeknd All Hit Top 10\". Billboard. Archived from the original on February 22, 2017. Retrieved February 22, 2017.\\n Grady, Constance (August 11, 2017). \"Taylor Swift won her day in court. Here\\'s what you need to know\". Vox. Archived from the original on October 17, 2022. Retrieved October 4, 2023.\\n \"Taylor Swift wipes social media profiles, fuelling expectations of new album\". The Daily Telegraph. August 18, 2017. Archived from the original on January 8, 2019. Retrieved August 19, 2019.\\n Aswad, Jem (August 24, 2017). \"Taylor Swift\\'s New Single, \\'Look What You Made Me Do,\\' Arrives (Listen)\". Variety. Archived from the original on August 28, 2017. Retrieved August 29, 2017.\\n White, Jack (September 1, 2017). \"Taylor Swift scores first Number 1 on the Official Singles Chart with \\'LWYMMD\\'\". Official Charts Company. Archived from the original on September 2, 2017. Retrieved September 1, 2017.\\n Peak positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift Scores Fifth No. 1 Single\". Australian Recording Industry Association. September 2, 2017. Archived from the original on September 2, 2017. Retrieved September 2, 2017.\\n \\xe2\\x80\\xa2 \"IRMA \\xe2\\x80\\x93 Irish Charts\". Irish Recorded Music Association. Archived from the original on June 14, 2017. Retrieved September 2, 2017.\\n \\xe2\\x80\\xa2 \"NZ Top 40 Singles Chart\". Recorded Music NZ. September 4, 2017. Archived from the original on September 1, 2017. Retrieved September 1, 2017.\\n \\xe2\\x80\\xa2 \"Taylor Swift at Nos. 1 & 4 on Billboard Hot 100, as Cardi B Moves Up to No. 2\". Billboard. September 11, 2017. Archived from the original on September 21, 2017. Retrieved October 30, 2017.\\n Shaw, Lucas (November 7, 2017). \"Taylor Swift Will Keep New Album From Streaming for a Week\". Bloomberg. Bloomberg News. Archived from the original on November 8, 2017. Retrieved November 7, 2017.\\n Powers, Ann (November 10, 2019). \"The Old Taylor\\'s Not Dead\". NPR. Archived from the original on June 9, 2020. Retrieved June 29, 2020.\\n McDermott, Maeve (October 11, 2017). \"Taylor Swift \\'Reputation\\': Here\\'s what critics are saying\". USA Today. Archived from the original on March 4, 2020. Retrieved October 15, 2020.\\n \"Official: Taylor Swift\\'s \\'Reputation\\' Album Sells 1.2M Copies in US During First Week\". Billboard. Archived from the original on November 30, 2017. Retrieved May 29, 2018.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift\\'s \\'Reputation\\' Rules Australia\\'s Albums Chart\". Billboard. November 20, 2017. Archived from the original on November 20, 2017. Retrieved December 2, 2017.\\n \\xe2\\x80\\xa2 \"Taylor Swift Chart History\". Billboard. Archived from the original on November 22, 2021. Retrieved November 19, 2021.\\n Unterberger, Andrew (July 6, 2018). \"Taylor Swift\\'s \\'Delicate\\' Became the Biggest Reputation Radio Hit While You Weren\\'t Looking\". Billboard. Archived from the original on December 13, 2020. Retrieved April 13, 2020.\\n \"61st Grammy Nominees\". The Recording Academy. December 7, 2018. Archived from the original on December 7, 2018. Retrieved December 7, 2018.\\n Hudak, Joseph (April 12, 2018). \"Sugarland Announce New Album Bigger, Taylor Swift Collaboration\". Rolling Stone. Archived from the original on April 14, 2018. Retrieved April 13, 2018.\\n Havens, Lyndsey (October 9, 2018). \"Taylor Swift Breaks an All-Time AMA Record \\xe2\\x80\\x93 And Urges People to Vote in Midterm Elections\". Billboard. Archived from the original on October 10, 2018. Retrieved October 10, 2018.\\n Stubblebine, Allison (November 13, 2017). \"Taylor Swift Announces First Round of Reputation Stadium Tour Dates\". Billboard. Archived from the original on November 14, 2017. Retrieved November 18, 2017.\\n Frankenberg, Eric (December 6, 2018). \"Taylor Swift Closes Reputation Stadium Tour with $345 Million\". Billboard. Archived from the original on December 9, 2018. Retrieved December 22, 2018.\\n Wang, Amy X. (November 19, 2018). \"Taylor Swift\\'s New Record Deal Affects Thousands of Other Musicians\". Rolling Stone. Archived from the original on November 26, 2018. Retrieved November 26, 2018.\\nWillman, Chris (August 27, 2018). \"Taylor Swift Stands to Make Music Business History as a Free Agent\". Variety. Archived from the original on August 29, 2018. Retrieved August 29, 2018.\\nAswad, Jem; Willman, Chris (November 19, 2018). \"Taylor Swift Signs New Deal With Universal Music Group\". Variety. Archived from the original on November 19, 2018. Retrieved November 19, 2018.\\n Grady, Constance (November 19, 2018). \"What Taylor Swift\\'s new record deal means for the music industry \\xe2\\x80\\x94 and for her image\". Vox. Archived from the original on December 20, 2021. Retrieved December 20, 2021.\\n McKenna, Lyndsey (August 23, 2019). \"Stream Taylor Swift\\'s New Album, \\'Lover\\'\". NPR. Archived from the original on February 19, 2020. Retrieved September 10, 2019.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Archived from the original on August 23, 2019. Retrieved February 15, 2021.\\n Caulfield, Keith (September 1, 2019). \"Official: Taylor Swift\\'s \\'Lover\\' Debuts at No. 1 on Billboard 200 Chart With 867,000 Units Earned in First Week in U.S.\" Billboard. Archived from the original on September 1, 2019. Retrieved September 2, 2019.\\n White, Adam (August 23, 2019). \"Taylor Swift Lover Review Round-Up: Critics Say Album Feels \\'Evolutionary Rather Than Revolutionary\\'\". The Independent. Archived from the original on June 13, 2022. Retrieved June 13, 2022.\\n Moniuszko, Sara M. (August 23, 2019). \"Taylor Swift Lover Reviews: Critics Are Enamored by the \\'Earnest,\\' \\'Romantic\\' New Album\". USA Today. Archived from the original on August 23, 2019. Retrieved November 5, 2020.\\n Trust, Gary (May 6, 2019). \"Lil Nas X\\'s \\'Old Town Road\\' Tops Billboard Hot 100 For Fifth Week, Taylor Swift\\'s \\'Me!\\' Vaults to No. 2\". Billboard. Archived from the original on May 6, 2019. Retrieved June 14, 2019.\\n Trust, Gary (October 23, 2023). \"Taylor Swift\\'s \\'Cruel Summer\\' Hits No. 1 on Billboard Hot 100, Becoming Her 10th Leader\". Billboard. Archived from the original on October 24, 2023. Retrieved October 24, 2023.\\n \"Arashi Best-Of Tops Taylor Swift for IFPI\\'s Best-Selling Album of 2019\". Billboard. March 19, 2020. Archived from the original on March 19, 2020. Retrieved March 21, 2020.\\n \"2020 Grammy Awards: Complete Winners List\". The Recording Academy. November 20, 2019. Archived from the original on May 22, 2020. Retrieved February 15, 2021.\\n Grein, Paul (August 26, 2019). \"12 Records That Were Set at the 2019 VMAs\". Billboard. Archived from the original on January 30, 2020. Retrieved January 11, 2020.\\n Grady, Constance (September 1, 2019). \"The Taylor Swift/Scooter Braun controversy, explained\". Vox. Archived from the original on February 11, 2020. Retrieved August 23, 2019.\\n Beth, John (January 2, 2024). \"Taylor Swift\\'s Chart Triumph\". Square News.\\n \"The Taylor Swift, Scooter Braun, Justin Bieber row explained\". BBC News. July 1, 2019. Archived from the original on December 11, 2021. Retrieved July 18, 2021.\\n Willman, Chris (November 16, 2020). \"Taylor Swift Confirms Sale of Her Masters, Says She Is Already Re-Recording Her Catalog\". Variety. Archived from the original on December 3, 2022. Retrieved November 18, 2020.\\n Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases \\'Beautiful Ghosts,\\' Co-Written With Andrew Lloyd Webber for \\'Cats\\' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.\\n \"Golden Globes 2020: full list of nominations\". The Guardian. December 9, 2019. Archived from the original on December 10, 2019. Retrieved December 20, 2019.\\n Rooney, David (December 18, 2019). \"\\'Cats\\': Film Review\". The Hollywood Reporter. Archived from the original on December 20, 2019. Retrieved December 21, 2019.\\n Mamo, Heran (January 15, 2020). \"Taylor Swift Miss Americana Netflix Doc Has a Release Date & We\\'re So Ready for It\". Billboard. Archived from the original on April 24, 2020. Retrieved January 19, 2020.\\n Willman, Chris (February 6, 2020). \"Taylor Swift Moves to Universal Music Publishing Group with New Pact\". Variety. Archived from the original on February 12, 2020. Retrieved February 6, 2020.\\n Opperman, Jeff (March 12, 2021). \"Taylor Swift Is Singing Us Back to Nature\". The New York Times. Archived from the original on December 28, 2021. Retrieved May 24, 2021.\\n \"Taylor Swift to release surprise ninth album \\'Evermore\\' tonight\". NME. December 10, 2020. Archived from the original on December 10, 2020. Retrieved December 10, 2020.\\n Atkinson, Katie (December 15, 2020). \"Taylor Swift Isn\\'t So Sure She & Joe Alwyn Would Have Made Music Together If It Weren\\'t for Lockdown\". Billboard. Archived from the original on August 11, 2021. Retrieved February 18, 2021.\\n Schaffer, Claire (December 18, 2020). \"Aaron Dessner on How His Collaborative Chemistry With Taylor Swift Led to Evermore\". Rolling Stone. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Barna, Alyssa (December 16, 2020). \"These are the musicological reasons Taylor Swift\\'s new album sounds dull\". The Washington Post. Archived from the original on February 25, 2021. Retrieved November 3, 2021.\\n Snapes, Laura (October 14, 2022). \"\\'Genuine\\': why Taylor Swift can celebrate more than an album release\". The Guardian. Archived from the original on October 14, 2022. Retrieved October 14, 2022.\\n McGrath 2023, p. 79; Fogarty & Arnold 2021, p. 5.\\n Trust, Gary (January 28, 2021). \"Taylor Swift\\'s \\'Coney Island\\' and \\'No Body, No Crime\\' Debut on Airplay Charts, Joining \\'Willow\\'\". Billboard. Archived from the original on August 16, 2021. Retrieved February 2, 2021.\\n Willman, Chris (March 14, 2021). \"Taylor Swift Becomes First Woman to Win Album of the Year Grammy Three Times\". Variety. Archived from the original on December 2, 2021. Retrieved March 15, 2021.\\n Caulfield, Keith (January 7, 2021). \"Lil Baby\\'s My Turn Is MRC Data\\'s Top Album of 2020, Roddy Ricch\\'s \\'The Box\\' Most-Streamed Song\". Billboard. Archived from the original on January 7, 2021. Retrieved January 7, 2021.\\n Trust, Gary (December 21, 2020). \"Taylor Swift\\'s \\'Willow\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Willman, Chris (November 23, 2020). \"Taylor Swift Wins Three American Music Awards, Says She\\'s MIA Because of \\'Recording All of My Old Music\\'\". Variety. Archived from the original on December 2, 2021. Retrieved November 25, 2020.\\n Christman, Ed (July 19, 2021). \"Billboard\\'s U.S. Money Makers: The Top Paid Musicians of 2020\". Billboard. Archived from the original on July 24, 2021. Retrieved July 19, 2021.\\n \\xe2\\x80\\xa2 Christman, Ed (July 19, 2021). \"Billboard\\'s 2020 Global Money Makers: The 5 Top Highest Paid Musicians\". Billboard. Archived from the original on July 23, 2021. Retrieved July 19, 2021.\\n Caulfield, Keith (July 11, 2023). \"Taylor Swift\\'s Re-Recorded Speak Now Already Has 2023\\'s Biggest Week After 4 Days of Release\". Billboard. Archived from the original on July 11, 2023. Retrieved July 11, 2023.\\n Caulfield, Keith (April 18, 2021). \"Taylor Swift\\'s Re-Recorded Fearless Album Debuts at No. 1 on Billboard 200 Chart With Year\\'s Biggest Week\". Billboard. Archived from the original on April 18, 2021. Retrieved April 19, 2021.\\n Asker, Jim; Trust, Gary (February 22, 2021). \"Taylor Swift\\'s \\'Love Story (Taylor\\'s Version)\\' Debuts at No. 1 on Hot Country Songs Chart: \\'I\\'m So Grateful to the Fans\\'\". Billboard. Archived from the original on April 22, 2021. Retrieved February 22, 2021.\\n McCluskey, Megan (December 8, 2023). \"Breaking Down Taylor Swift\\'s 2023 Impact By the Numbers\". Time. Archived from the original on December 26, 2023. Retrieved December 26, 2023.\\n Horton, Adrian; Lee, Benjamin (February 6, 2023). \"Grammy awards 2023: list of winners\". The Guardian. Archived from the original on February 6, 2023. Retrieved February 6, 2023.\\n Corcoran, Nina (August 28, 2022). \"Taylor Swift Announces New Album Midnights, Breaks Record for Most Video of the Year Wins at 2022 VMAs\". Pitchfork. Archived from the original on August 29, 2022. Retrieved August 28, 2022.\\n \"Taylor Swift\\'s new album breaks Spotify streaming record\". The Guardian. October 22, 2022. Archived from the original on October 22, 2022. Retrieved October 22, 2022.\\n Harvilla, Rob (October 25, 2022). \"The Anti-Hero We Deserve: Taylor Swift and Her Polarizing \\'Midnights\\'\". The Ringer. Archived from the original on November 3, 2022. Retrieved November 3, 2022.\\n Light, Alan (October 24, 2022). \"Taylor Swift\\'s Midnights Does Something Astonishing. Even For Her\". Esquire. Archived from the original on January 17, 2023. Retrieved January 17, 2023.\\n Petridis, Alexis (October 21, 2022). \"Taylor Swift: Midnights Review \\xe2\\x80\\x93 Small-Hours Pop Rich with Self-Loathing and Stereotype-Smashing\". The Guardian. Archived from the original on October 22, 2022. Retrieved October 21, 2022.\\n Spanos, Brittany (October 21, 2022). \"Taylor Swift Lets Us Into Her Darkest Dreams On Midnights\". Rolling Stone. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Sheffield, Rob (October 21, 2022). \"Welcome to the Lavender Labyrinth: Taylor Swift\\'s Midnights Is the Mastermind\\'s Ultimate Power Move\". Rolling Stone. Archived from the original on October 22, 2022. Retrieved October 23, 2022.\\n Balasaygun, Kaitlin (November 1, 2022). \"How Taylor Swift went back to the past and turned Midnights into her biggest album success yet\". CNBC. Archived from the original on December 30, 2022. Retrieved December 30, 2022.\\n Shafer, Ellise (October 21, 2022). \"Taylor Swift\\'s Midnights Breaks Spotify Record for Most-Streamed Album in a Single Day\". Variety. Archived from the original on October 21, 2022. Retrieved October 22, 2022.\\n Dailey, Hannah (December 6, 2022). \"Here Are All of Taylor Swift\\'s Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022.\\n Trust, Gary (June 5, 2023). \"Morgan Wallen\\'s \\'Last Night\\' No. 1 on Hot 100 for Ninth Week, Taylor Swift & Ice Spice\\'s \\'Karma\\' Blasts to No. 2\". Billboard. Archived from the original on October 26, 2023. Retrieved August 21, 2023.\\n Blistein, Jon; Guglielmi, Jodi (September 13, 2023). \"Taylor Swift Makes History at 2023 VMAs\". Rolling Stone. Archived from the original on September 13, 2023. Retrieved September 13, 2023.\\n West, Bryan (February 5, 2024). \"Taylor Swift makes Grammys history with fourth album of the year win for Midnights\". USA Today. Archived from the original on February 5, 2024. Retrieved February 5, 2024.\\n Caulfield, Keith (July 16, 2023). \"Taylor Swift\\'s Re-Recorded \\'Speak Now\\' Debuts at No. 1 on Billboard 200 With 2023\\'s Biggest Week\". Billboard. Archived from the original on July 19, 2023. Retrieved July 16, 2023.\\n Caulfield, Keith (November 5, 2023). \"Taylor Swift\\'s 1989 (Taylor\\'s Version) Debuts at No. 1 on Billboard 200 With Biggest Week in Nearly a Decade\". Billboard. Archived from the original on November 5, 2023. Retrieved November 5, 2023.\\n Sherman, Maria (November 29, 2023). \"Taylor Swift is Spotify\\'s most-streamed artist of 2023, ending Bad Bunny\\'s 3-year reign\". ABC News. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n Garcia, Thania (November 28, 2023). \"Taylor Swift Named Apple Music\\'s Artist of the Year; Morgan Wallen Tops Global Songs Chart\". Variety. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n \"Best of 2023 (Taylor\\'s Version) Playlist on Amazon Music\". Curated by Amazon\\'s Music Experts. Amazon Music. Retrieved December 3, 2023. Our most streamed artist of 2023 globally. It\\'s Taylor\\'s world and we are just living for it.\\n Caulfield, Keith (November 21, 2023). \"Taylor Swift Is Billboard\\'s Top Artist of 2023\". Billboard. Archived from the original on November 21, 2023. Retrieved November 21, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Returns to No. 1 on Billboard 200\". Billboard. Retrieved December 3, 2023.\\n Caulfield, Keith (January 10, 2024). \"Morgan Wallen\\'s One Thing at a Time Is Luminate\\'s Top Album of 2023 in U.S.\" Billboard. Archived from the original on January 10, 2024. Retrieved January 12, 2024.\\n Trust, Gary (November 6, 2023). \"Taylor Swift\\'s \\'Is It Over Now? (Taylor\\'s Version)\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved November 6, 2023.\\n Cohen, Jonathan (June 29, 2021). \"Aaron Dessner, Justin Vernon Rev Up Big Red Machine With Help From Taylor Swift\". Variety. Archived from the original on May 23, 2022. Retrieved February 10, 2022.\\n Strauss, Matthew (February 19, 2021). \"HAIM Enlist Taylor Swift for New \"Gasoline\" Remix\". Pitchfork. Archived from the original on February 19, 2021. Retrieved February 10, 2022.\\n Dailey, Hannah (February 11, 2022). \"Ed Sheeran & Taylor Swift Release \\'The Joker and the Queen\\' Remix: Watch the Video\". Billboard. Archived from the original on February 11, 2022. Retrieved February 11, 2022.\\n Rowley, Glenn (January 18, 2023). \"The National Unveils \\'First Two Pages of Frankenstein\\' Tracklist With Taylor Swift, Phoebe Bridgers & Sufjan Stevens\". Billboard. Archived from the original on January 18, 2023. Retrieved January 18, 2023.\\n Davis, Clayton (December 21, 2022). \"Taylor Swift Doesn\\'t Make Oscar Shortlist for All Too Well Short Film, but Advances for \\'Carolina\\' Original Song\". Variety. Archived from the original on December 24, 2022. Retrieved December 24, 2022.\\n Utley, Riley (October 13, 2022). \"Every Taylor Swift Movie Performance, Ranked\". CinemaBlend. Archived from the original on October 13, 2022. Retrieved October 14, 2022.\\n Lang, Brent (December 9, 2022). \"Taylor Swift Making Feature Directing Debut for Searchlight Pictures\". Variety. Archived from the original on December 10, 2022. Retrieved December 9, 2022.\\n Wood, Mikael; Brown, August (August 1, 2023). \"It\\'s a love story, L.A. just says yes: How Taylormania took over the world\". Los Angeles Times. Archived from the original on August 1, 2023. Retrieved August 1, 2023.\\n Mahdawi, Arwa (November 20, 2022). \"Swifties know: the Ticketmaster fiasco shows America has a monopoly problem\". The Guardian. Archived from the original on December 10, 2022. Retrieved November 20, 2022.\\n Murray, Conor (December 8, 2023). \"Taylor Swift\\'s Eras Tour Is First In History To Gross Over $1 Billion, Report Says\". Forbes. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Tapp, Tom (December 8, 2023). \"Taylor Swift\\'s \\'The Eras Tour\\' Grosses Over $1 Billion In 2023, The Biggest Haul For Any Act Ever\". Deadline. Archived from the original on February 6, 2024. Retrieved February 10, 2024.\\n Kaufman, Gil (November 28, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Concert Movie Passes $250 Million in Worldwide Grosses\". Billboard. Archived from the original on December 1, 2023. Retrieved December 1, 2023.\\n Nordyke, Kimberly (January 7, 2024). \"Golden Globes 2024 Winners List\". The Hollywood Reporter. Archived from the original on January 8, 2024. Retrieved January 8, 2024.\\n Adamczyk, Alicia; Abrams, Joseph (July 25, 2023). \"The brilliant marketing synergy of Taylor Swift\\'s Eras Tour and her rerecorded albums\". Fortune. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\\n Ingham, Tim (June 14, 2023). \"Reliving the Taylor Swift Catalog Sale Saga (And Following the Money...)\". Music Business Worldwide. Archived from the original on June 14, 2023. Retrieved June 15, 2023.\\n Blanchet, Brenton (November 20, 2023). \"Travis Kelce Shares the Real Story of How Taylor Swift Romance Began in Wide-Ranging Interview\". People. Archived from the original on November 20, 2023. Retrieved November 20, 2023.\\n Kelly, Samantha Murphy (January 25, 2024). \"Explicit, AI-generated Taylor Swift images spread quickly on social media\". CNN Business. Archived from the original on January 25, 2024. Retrieved January 25, 2024.\\n Phillips, Zoe G. (January 27, 2024). \"SAG-AFTRA and White House Issue Statements on Taylor Swift AI Nudes: \"We Have It in Our Power to Control These Technologies\"\". The Hollywood Reporter. Archived from the original on January 27, 2024. Retrieved January 27, 2024.\\n \"Taylor Swift Announces \\'Brand New Album\\' \\'The Tortured Poets Department\\' with 13th Grammy Win\". Peoplemag. Archived from the original on February 10, 2024. Retrieved February 10, 2024.\\n Cairns, Dan (March 5, 2009). \"Swift rise of the anti-diva\". The Australian. Archived from the original on December 24, 2014. Retrieved July 2, 2012.\\n Bream, Jon (December 7, 2007). \"Music: OMG! Taylor\\'s senior year\". Star Tribune. Archived from the original on July 12, 2015. Retrieved July 1, 2012.\\n Newman, Melinda (December 19, 2008). \"Taylor Swift Sessions Interview\". AOL. Archived from the original on October 9, 2012. Retrieved March 25, 2011.\\n \"Swift starts world tour in Asia, pushes \"Speak Now\\' in NY\". Country Standard Time. October 23, 2007. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n \"News : 20 Questions With Taylor Swift\". Country Music Television. November 12, 2007. Archived from the original on November 17, 2014. Retrieved April 18, 2012.\\n McCafferty, Dennis (April 13, 2008). \"Taylor\\'s Swift rise\". USA Weekend. Archived from the original on November 14, 2012. Retrieved April 17, 2012.\\n \"Interview with Taylor Swift\". Time. April 23, 2009. Archived from the original on October 23, 2013. Retrieved July 1, 2012.\\n \"Taylor Swift Style: Singer Won\\'t Take Her Clothes Off, Wants People To Focus On Music\". HuffPost. October 23, 2012. Archived from the original on January 4, 2015. Retrieved January 4, 2015.\\n \"InStyle meets country singing sensation Taylor Swift\". InStyle UK. October 26, 2010. Archived from the original on May 27, 2013. Retrieved May 29, 2012.\\n \"Joni Mitchell: 15 Great Artists Influenced by the \\'Blue\\' Singer\". Rolling Stone. June 22, 2016. Archived from the original on December 26, 2021. Retrieved December 26, 2020.\\n Jenkins, Sally (September 28, 2023). \"You thought you knew the NFL. Now meet Taylor\\'s Version\". The Washington Post. Archived from the original on October 26, 2023. Retrieved September 29, 2023.\\n Bonaguro, Alison (November 8, 2012). \"Offstage: Taylor Swift Inspired by Female Singer-Songwriters of the \\'90s\". CMT. Archived from the original on October 4, 2023. Retrieved September 29, 2023.\\n \"Taylor Swift\\'s Favorite Music\". The Oprah Winfrey Show. Archived from the original on January 16, 2016. Retrieved October 23, 2012.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Mansfield, Brian (October 23, 2010). \"Taylor Swift learns to \\'Speak Now,\\' reveal her maturity\". USA Today. Archived from the original on November 4, 2012. Retrieved July 1, 2012.\\n Block, Melissa (October 31, 2014). \"\\'Anything That Connects\\': A Conversation With Taylor Swift\". NPR Music. Archived from the original on February 6, 2015. Retrieved October 26, 2019.\\n Eells, Josh (September 8, 2014). \"The Reinvention of Taylor Swift\". Rolling Stone. Archived from the original on June 4, 2016. Retrieved June 8, 2016.\\n Reid, Poppy (November 2, 2021). \"The Curious Case of Keith Urban\". Rolling Stone. Archived from the original on November 19, 2021. Retrieved November 3, 2021.\\n Hiatt, Brian (June 18, 2019). \"Taylor Swift: The Rolling Stone Interview\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved April 14, 2022.\\n Weatherby, Taylor (March 10, 2021). \"Taylor Swift\\'s Road to Folklore\". The Recording Academy. Archived from the original on November 25, 2021. Retrieved November 24, 2021.\\n Franssen, Gaston (January 2, 2022). \"Policing the celebrity of Taylor Swift: introduction\". Celebrity Studies. 13 (1): 90\\xe2\\x80\\x9392. doi:10.1080/19392397.2022.2026148. S2CID 246997248.\\n Savage, Mark (October 19, 2022). \"Midnights: What we know about Taylor Swift\\'s songwriting\". BBC News. Archived from the original on October 19, 2022. Retrieved October 20, 2022.\\n Bruner, Raisa (August 24, 2020). \"Let\\'s Break Down Taylor Swift\\'s Tender New Album Folklore\". Time. Archived from the original on July 31, 2020. Retrieved October 20, 2022.\\n McNutt 2020, p. 77.\\n Hughes 2017, p. 206; Perone 2017, p. 33.\\n \"Taylor Swift: Album Guide\". Rolling Stone. Archived from the original on December 5, 2012. Retrieved December 5, 2012.\\n \"Pop and Rock Listings July 22 \\xe2\\x80\\x93 28\". The New York Times. July 21, 2011. Archived from the original on January 28, 2012. Retrieved July 12, 2012.\\n \"Taylor Swift Remade Fearless as Taylor\\'s Version. Let\\'s Discuss\". The New York Times. April 9, 2021. Archived from the original on April 9, 2021. Retrieved April 21, 2021.\\n Petridis, Alexis (March 6, 2009). \"Taylor Swift: Fearless\". The Guardian. Archived from the original on October 16, 2013. Retrieved August 13, 2022.\\n Jones, Sasha-Frere (November 11, 2008). \"Prodigy\". The New Yorker. Archived from the original on October 21, 2016. Retrieved August 14, 2022.\\n Hughes 2017, p. 206.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. p. 5. Archived from the original on November 20, 2022. Retrieved August 12, 2022.\\n McNutt 2020, p. 78.\\n Rosen, Jody (November 17, 2013). \"Why Taylor Swift Is the Reigning Queen of Pop\". Vulture. Archived from the original on November 19, 2013. Retrieved November 9, 2020.\\n McNutt 2020, p. 79.\\n Levine, Nick (August 21, 2019). \"Taylor Swift\\'s Lover: The struggle to maintain superstardom\". BBC. Archived from the original on March 1, 2021. Retrieved October 29, 2021.\\n da Silva, Michelle (November 13, 2017). \"Taylor Swift Has Changed for the Worse on Reputation\". Now. Archived from the original on July 26, 2020. Retrieved July 27, 2020.\\n Tucker, Ken (November 13, 2017). \"Taylor Swift Pushes Further Into Electro-Pop With \\'Reputation\\'\". NPR. Archived from the original on October 27, 2021. Retrieved April 8, 2023.\\n Moreland, Quinn (October 24, 2022). \"Taylor Swift: Midnights\". Pitchfork. Archived from the original on October 24, 2022. Retrieved April 8, 2023.\\n Ryan, Elise (October 21, 2022). \"Review: Taylor Swift gets dark, electric on \\'Midnights\\'\". Associated Press News. Archived from the original on October 21, 2022. Retrieved April 8, 2023.\\n Winter, Velvet (November 12, 2022). \"Like The Beatles, Madonna and Kylie Minogue before her, Taylor Swift is masterful at pivoting\". ABC News. Archived from the original on November 13, 2022. Retrieved November 13, 2022.\\n McNutt 2020, p. 79; Sloan 2021, p. 17.\\n Sloan 2021, p. 17.\\n Hyden, Steven (March 10, 2021). \"Taylor Swift, Indie-rock star? Long, Long ago, this might have felt strange\". The New York Times. Archived from the original on April 13, 2021. Retrieved April 13, 2021.\\n Caramanica, Jon (July 26, 2020). \"Taylor Swift, A Pop Star Done with Pop\". The New York Times. Archived from the original on September 10, 2020. Retrieved August 14, 2022.\\n Harbron, Lucy (November 11, 2021). \"Why Taylor Swift\\'s \\'Red\\' Is Her Turning Point\". Clash. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Gerber, Brady (July 27, 2020). \"The Story Behind Every Song on Taylor Swift\\'s folklore\". Vulture. Retrieved December 12, 2023.\\n Willman, Chris (October 21, 2022). \"Taylor Swift\\'s Midnights Marks a Return to Electronic, Confessional Pop That\\'s Worth Losing Sleep Over: Album Review\". Variety. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Fulford 2014, p. 192.\\n \"Taylor Swift Deepens Her Goth-Folk Vision on the Excellent \\'Evermore\\'\". Rolling Stone. December 11, 2020. Archived from the original on December 11, 2020. Retrieved August 23, 2022.\\n \"The 200 Greatest Singers of All Time\". Rolling Stone. January 1, 2023. Archived from the original on January 1, 2023. Retrieved January 1, 2023.\\n Provenzano 2018, p. 173.\\n Roland, Tom (October 15, 2010). \"Taylor Swift: The Billboard Cover Story\". Billboard. Archived from the original on October 18, 2010. Retrieved July 3, 2012.\\n Provenzano 2018, pp. 173\\xe2\\x80\\x93174.\\n Provenzano 2018, p. 174.\\n Powers, Ann (October 25, 2010). \"Album review: Taylor Swift\\'s Speak Now\". Los Angeles Times. Archived from the original on October 28, 2010. Retrieved October 25, 2010.\\n Willman, Chris (November 10, 2017). \"Album Review: Taylor Swift\\'s \\'Reputation\\'\". Variety. Retrieved April 8, 2023.\\n Cox, Jamieson (November 13, 2017). \"Taylor Swift: Reputation\". Pitchfork. Retrieved April 8, 2023.\\n Powers, Ann (November 10, 2017). \"The Old Taylor\\'s Not Dead\". NPR. Retrieved April 8, 2023.\\n Wilson, Carl (November 13, 2017). \"On Reputation, the \"Old Taylor\" Is Dead, but the New One Isn\\'t Quite Ready to Come to the Phone\". Slate. Retrieved April 8, 2023.\\n DeCaro, Alessandro (October 19, 2022). \"10 best Taylor Swift scene covers\". Alternative Press. Retrieved July 1, 2023.\\n Barker, Andrew (November 27, 2020). \"Folklore: The Long Pond Studio Sessions Review\". Variety. Retrieved May 31, 2021.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved April 8, 2023.\\n Kornhaber, Spencer (July 28, 2020). \"Taylor Swift Is No Longer Living in the Present\". The Atlantic. Retrieved August 23, 2022.\\n Brehian, Tom (July 24, 2020). \"Review: Taylor Swift\\'s \\'folklore\\' Is An Indie Record Unconcerned With Being Cool\". Stereogum. Retrieved August 23, 2022.\\n Willman, Chris (December 11, 2020). \"Taylor Swift Has Her Second Great Album of 2020 With \\'Evermore\\': Album Review\". Variety. Retrieved August 23, 2022.\\n Sodomsky, Sam (December 15, 2020). \"Taylor Swift: evermore\". Pitchfork. Archived from the original on December 15, 2020. Retrieved December 14, 2020.\\n McCormick, Neil (April 9, 2021). \"Taylor Swift copies her younger self \\xe2\\x80\\x93 and she sounds even more Fearless today\". The Daily Telegraph. Retrieved August 23, 2022.\\n \"Taylor Swift forges ahead with a dreamy throwback in Fearless (Taylor\\'s Version)\". The A.V. Club. April 9, 2021. Archived from the original on April 9, 2021. Retrieved August 23, 2022.\\n Bernstein, Jonathan (April 9, 2021). \"Taylor Swift Carefully Reimagines Her Past on \\'Fearless: Taylor\\'s Version\\'\". Rolling Stone. Retrieved August 23, 2022.\\n Solomon, Kate (November 12, 2021). \"Taylor Swift, Red (Taylor\\'s Version), Review: How Brilliant She Is When Her Heart Is in Tatters\". i. Archived from the original on November 12, 2021. Retrieved November 13, 2021.\\n Snapes, Laura (August 23, 2020). \"Taylor Swift: Folklore review \\xe2\\x80\\x93 bombastic pop makes way for emotional acuity\". The Guardian. Retrieved August 23, 2022.\\n Snapes, Laura (November 12, 2021). \"Taylor Swift: Red (Taylor\\'s Version) Review \\xe2\\x80\\x93 Getting Back Together with a Classic\". The Guardian. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Kelly, Fred (October 21, 2022). \"Taylor Swift\\'s Midnights: what the critics are saying\". The Week. Retrieved October 23, 2022.\\n Petrusich, Amanda (June 12, 2023). \"The Startling Intimacy of Taylor Swift\\'s Eras Tour\". The New Yorker. Retrieved June 12, 2023.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC Music. August 22, 2019. Retrieved November 10, 2021.\\n Emily, Lee (November 5, 2021). \"Here Are Taylor Swift\\'s Best Bridges On \\'Red\\' Ranked\". iHeartRadio. Retrieved November 10, 2021.\\n Eggertsen, Chris (September 20, 2022). \"Taylor Swift\\'s Iconic Songwriting Credits Amplified By Spotify With Dedicated Page\". Billboard. Retrieved September 20, 2022.\\n Bate, Jonathan (April 10, 2023). \"Why Taylor Swift is a literary giant \\xe2\\x80\\x94 by a Shakespeare professor\". The Times. Retrieved April 10, 2023.\\n Pazzanese, Christina (August 2, 2023). \"So what exactly makes Taylor Swift so great?\". Harvard Gazette. Retrieved August 7, 2023.\\n Murphy, Sam (November 10, 2021). \"How \\'Red\\' Became The Most Pivotal Record In Taylor Swift\\'s Career\". Junkee. Retrieved November 10, 2021.\\n Siroky, Mary (November 9, 2021). \"Every Taylor Swift Album Ranked from Worst to Best\". Consequence. Retrieved November 10, 2021.\\n Bruner, Raisa (May 24, 2021). \"How Olivia Rodrigo Become America\\'s Biggest New Pop Star\". Time. Retrieved November 10, 2021.\\n Mulvey, John (September 16, 2023). \"Arctic Monkeys, Taylor Swift, Kendrick Lamar, Lana Del Rey And The 30 Artists Who Will Shape The Next 30 Years\". Mojo. Retrieved September 19, 2023.\\n Farley, Christopher John (October 22, 2010). \"Taylor Swift\\'s Solo Act\". The Wall Street Journal. Archived from the original on February 1, 2015. Retrieved May 24, 2012.\\n Jo Sales, Nancy; Diehl, Jessica (April 2013). \"Taylor Swift\\'s Telltale Heart\". Vanity Fair. Archived from the original on January 30, 2017. Retrieved February 4, 2017.\\n Daly, Rhian (December 13, 2020). \"Taylor Swift says her diaristic songwriting style was \\'unsustainable\\' for her future\". NME. Retrieved February 17, 2021.\\n Gallo, Phil (October 22, 2012). \"Taylor Swift\\'s Red: The Billboard Cover Story\". Billboard. Archived from the original on June 14, 2013.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift Is Angry, Darn It\". The New York Times. Archived from the original on September 11, 2012. Retrieved July 2, 2012.\\n Kelly, James (August 26, 2009). \"Taylor Swift writing her own songs and rules\". The Atlanta Journal-Constitution. Archived from the original on September 8, 2014. Retrieved July 30, 2012.\\n Lansky, Sam (November 8, 2017). \"Why Taylor Swift\\'s Red Is Her Best Album\". Billboard. Retrieved December 27, 2020.\\n Hiatt, Brian (September 30, 2019). \"9 Taylor Swift Moments That Didn\\'t Fit in Our Cover Story\". Rolling Stone. Archived from the original on October 1, 2019. Retrieved December 9, 2019.\\n \"Taylor Swift Talks Newfound \\'Freedom,\\' \\'Lover\\' Tour Plans and So Much More\". On Air with Ryan Seacrest. August 27, 2019. Retrieved March 22, 2020.\\n Yuan, Jada (December 30, 2009). \"Microwaving a tragedy: The marriage of romance and romanticism in \\'00s pop\". Las Vegas Weekly. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n Rotman, Natalie (January 9, 2009). \"Colbie Caillat has \\'Breakthrough\\' with sophomore CD\". Reading Eagle. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC News. Retrieved February 2, 2021.\\n Knibbs, Kate (August 21, 2019). \"Ten Years of Taylor Swift: How the Pop Star Went From Sweetheart to Snake (and Back Again?)\". The Ringer. Retrieved December 13, 2021.\\n Stubbs, Dan (October 9, 2015). \"Taylor Swift: Power, Fame And The Future \\xe2\\x80\\x93 The Full NME Cover Interview\". NME. Retrieved February 17, 2021.\\n Weber, Theon (November 3, 2010). \"The Iceberg Songs of Taylor Swift\". The Village Voice. Archived from the original on November 4, 2015. Retrieved July 30, 2012.\\n Beck, Julia (October 27, 2014). \"Taylor Swift Is So Much More Fun Now That She\\'s Jaded\". The Atlantic. Archived from the original on September 27, 2016. Retrieved October 30, 2021.\\n Willman, Chris (October 10, 2010). \"Princess Crossover\". New York. Archived from the original on July 27, 2013. Retrieved July 1, 2012.\\n Rosen, Jody (November 13, 2008). \"Fearless\". Rolling Stone. Archived from the original on August 15, 2012. Retrieved July 1, 2012.\\n Powers, Ann (October 30, 2014). \"The Many New Voices of Taylor Swift\". NPR. Retrieved June 2, 2022.\\n Stoeffel, Kat (November 16, 2012). \"Stop Asking Taylor Swift to Apologize for Writing Songs About Ex-Boyfriends \\xe2\\x80\\x93 The Cut\". New York. Archived from the original on November 27, 2012. Retrieved February 25, 2013.\\n Raven, Robin (March 16, 2022). \"10 Artists Who Have Stood Up For Women In Music: Taylor Swift, Lizzo & More\". The Recording Academy.\\n \"Cover Preview: Taylor Swift Fights Back About Her Love Life, the Hyannis Port House\\xe2\\x80\\x94and Has Words for Tina Fey and Amy Poehler\". Vanity Fair. March 5, 2013. Archived from the original on August 8, 2016. Retrieved August 3, 2016.\\n Dominus, Susan (November 16, 2012). \"The Many Insecurities of Taylor Swift\". The New York Times. Archived from the original on June 17, 2016.\\n Doyle, Patrick (November 13, 2020). \"Musicians on Musicians: Paul McCartney and Taylor Swift\". Rolling Stone. Archived from the original on November 30, 2020. Retrieved February 7, 2021.\\n Olivier, Bobby (December 11, 2020). \"Taylor Swift\\'s \\'Evermore\\' Is an Undeniable Folk-Pop Masterpiece\". Spin. Retrieved February 1, 2021.\\n Shutler, Ali (October 9, 2022). \"Taylor Swift organises her lyrics into three \\'dorky\\' pen-themed categories\". NME. Retrieved October 18, 2022.\\n Browne, Erin (October 21, 2022). \"All of Taylor Swift\\'s Famously Devastating Track 5\\'s, Ranked\". Vulture. Retrieved October 27, 2022.\\n \"NMPA to Honor Taylor Swift with Songwriter Icon Award Among Other 2021 Annual Meeting Honorees\". National Music Publishers\\' Association. May 24, 2021. Retrieved May 24, 2021.\\n Linker, Damon (November 26, 2021). \"Taylor Swift, Phoebe Bridgers, and Rihanna: How women took over songwriting\". The Week. Retrieved November 28, 2021.\\n Greco, Patti (November 13, 2017). \"A Harvard Professor Critiques Taylor Swift\\'s New Poems\". Cosmopolitan. Retrieved December 21, 2021.\\n Sheffield, Rob (October 13, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Movie Will Make You Sing, Scream, and Sob\". Rolling Stone. Retrieved December 8, 2023.\\n Zacharek, Stephanie (December 14, 2023). \"The Eras Tour Movie Is Irresistible No Matter How Much You Think You Like Taylor Swift\". TIME. Retrieved December 8, 2023.\\n \"Ticketing Shmicketing: Taylor Swift\\'s \\'Eras Tour\\' Debut Slays (And Could Break All-Time Touring Record)\". Pollstar. March 18, 2023. Archived from the original on March 20, 2023. Retrieved June 30, 2023.\\n Aramesh, Waiss David (March 18, 2023). \"Taylor Swift\\'s The Eras Tour Is a 3-Hour Career-Spanning Victory Lap\". Rolling Stone. OCLC 1787396. Archived from the original on March 18, 2023. Retrieved June 30, 2023.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved July 23, 2023.\\n McCormick, Neil (March 18, 2023). \"Taylor Swift: The Eras Tour, review: a roaring spectacle of a comeback\". The Daily Telegraph. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kornhaber, Spencer (March 18, 2023). \"What Made Taylor Swift\\'s Concert Unbelievable\". The Atlantic. Retrieved June 30, 2023.\\n Foggatt, Tyler (June 3, 2023). \"Look What Taylor Made Us Do\". The New Yorker. Retrieved June 30, 2023.\\n Seibert, Brian (August 9, 2023). \"How to Command a Stage Without Great Dance Moves (Taylor\\'s Version)\". The New York Times. Retrieved August 11, 2023.\\n Krelenstein, Greg (May 21, 2018). \"TAYLOR SWIFT\\'S REPUTATION TOUR IS A POP TRIUMPH\". V. Archived from the original on May 22, 2018. Retrieved May 22, 2018.\\n Frere-Jones, Sasha (November 3, 2008). \"Prodigy\". The New Yorker. Retrieved June 30, 2023.\\n Horton, Adrian (March 18, 2023). \"Taylor Swift review \\xe2\\x80\\x93 pop\\'s hardest-working star gives Eras tour her all\". The Guardian. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kaplan, Ilana (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' tour is a thrilling spectacle from a pop mastermind\". i. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Young, Alex (March 27, 2023). \"Taylor Swift\\'s \"The Eras Tour\" Is a Triumph of Spectacle and Stamina: Review\". Consequence. Retrieved June 30, 2023.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved July 1, 2023.\\n O\\'Connor, Roisin (June 8, 2018). \"Taylor Swift \\'reputation\\' stadium tour review: Dazzling pop spectacle from the star who doesn\\'t stand still\". The Independent. Archived from the original on June 30, 2023. Retrieved December 22, 2019.Savage, Mark (March 18, 2023). \"Taylor Swift launches Eras tour with three-hour, 44-song set\". BBC News. Archived from the original on March 18, 2023. Retrieved March 18, 2023. Sisario, Ben (November 5, 2023). \"How Taylor Swift\\'s Eras Tour Conquered the World\". The New York Times. Retrieved August 12, 2023. DeVille, Chris (July 12, 2018). \"Big Reputation: A Trip To Taylor Swift\\'s Hyper-Maximalist Stadium Tour\". Stereogum. Retrieved June 30, 2023.\\n Procell, Carlie; Padilla, Ramon (April 28, 2023). \"Taylor Swift tour has many \\'eras.\\' We tracked her movements to give you the look and feel\". USA Today. Retrieved June 30, 2023.\\n \"Taylor Swift Shares Stunning \\'Wildest Dreams\\' Performance from Grammy Museum\". Billboard. January 5, 2016. Retrieved October 29, 2023.\\n Burgham, Lydia (November 10, 2018). \"Taylor Swift in Auckland, reviewed: Despite the snakes, her Reputation shines on\". The Spinoff. Retrieved December 10, 2019.\\n Dodd, Sophie (November 15, 2023). \"All About Taylor Swift\\'s Parents, Scott and Andrea Swift\". People. Retrieved December 3, 2023.\\n Swift, Taylor (March 15, 2013). \"\"Sparks Fly\" (acoustic) Live on the RED Tour!\" \\xe2\\x80\\x93 via YouTube.\\n Lewis, Randy (April 3, 2011). \"Academy of Country Music Awards: Las Vegas welcomes Miranda Lambert, Taylor Swift with open arms\". Los Angeles Times. Retrieved August 20, 2023.\\n Ritchie, Mike (March 8, 2020). \"Why Taylor Swift is making the ukulele cool again\". The Herald. Retrieved July 1, 2023.\\n Gensler, Andy (August 17, 2023). \"The Showgoer: The Greatest Show On Earth \\xe2\\x80\\x94 Taylor Swift\\'s \\'Eras Tour\\' \\xe2\\x80\\x94 Is All That And Far More\". Pollstar. Retrieved August 19, 2023.\\n Sheffield, Rob (May 9, 2018). \"Why Taylor Swift\\'s \\'Reputation\\' Tour Is Her Finest Yet\". Rolling Stone. Retrieved June 30, 2023.\\n Willman, Chris (May 16, 2018). \"Taylor Swift\\'s \\'Reputation\\' Tour Has Bad Blood, Good Will, Sex Appeal and Serpents\". Variety. Retrieved December 22, 2019.\\n Willman, Chris (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' Show Is a Three-Hour, 44-Song Epic That Leaves \\'Em Wanting More: Concert Review\". Variety. Archived from the original on March 18, 2023. Retrieved June 30, 2018.\\n Ordo\\xc3\\xb1a, Michael (September 9, 2022). \"Taylor Swift wants an Oscar. So she took \\'All Too Well\\' to TIFF\". Los Angeles Times. Retrieved September 10, 2022.\\n CMT.com Staff (May 4, 2011). \"Taylor Swift\\'s \"Mean\" Video Debuts Friday\". CMT. Archived from the original on June 19, 2019. Retrieved June 19, 2019.\\n Anitai, Tamar (August 27, 2010). \"Video Premiere: Taylor Swift, \\'Mine\\'\". MTV News. Archived from the original on April 29, 2019. Retrieved June 19, 2019.\\n Bonaguro, Alison (May 6, 2011). \"OFFSTAGE: Taylor Swift Isn\\'t \\'Mean\\' at All, Director Says\". CMT News. Archived from the original on June 19, 2019. Retrieved June 20, 2019.\\n Tailor, Leena (September 1, 2017). \"Exclusive: Taylor Swift\\'s Director Joseph Kahn on How Her Image Invokes a Double Standard: \\'She\\'s a Genius\\'\". Entertainment Tonight. Archived from the original on June 19, 2019.\\n O\\'Connell, Michael (October 9, 2015). \"Taylor Swift and Jimmy Fallon Among Early Emmy Winners\". The Hollywood Reporter. Archived from the original on June 19, 2019.\\n Forbes, Jihan (May 14, 2015). \"Peep Taylor Swift\\'s Star-Studded Cast for Her \\'Bad Blood\\' Music Video\". The Fashion Spot. Retrieved May 13, 2020.\\n \"9 Things You Might Have Missed in Taylor Swift\\'s Netflix Concert Film\". E! News. December 31, 2018. Retrieved September 10, 2022.\\n Spanos, Brittany (April 25, 2019). \"Watch Taylor Swift, Brendon Urie\\'s Colorful \\'ME!\\' Video\". Rolling Stone. Archived from the original on April 26, 2019. Retrieved April 25, 2019.\\nMoore, Sam (August 23, 2019). \"Watch Taylor Swift\\'s colourful new video for \\'Lover\\'\". NME. Archived from the original on August 27, 2019. Retrieved August 27, 2019.\\nZemler, Emily (June 17, 2019). \"Watch Taylor Swift Reunite With Katy Perry in \\'You Need to Calm Down\\' Video\". Rolling Stone. Archived from the original on June 17, 2019. Retrieved June 17, 2019.\\n Mylrea, Hannah (February 28, 2020). \"Every incredible Easter Egg in Taylor Swift\\'s \\'The Man\\' video\". NME. Retrieved March 9, 2020.\\nSpanos, Brittany; Legaspi, Althea (July 24, 2020). \"Taylor Swift Blends Fantastical With Personal in \\'Cardigan\\' Video\". Rolling Stone. Retrieved July 27, 2020.\\n\"Justin Bieber & Megan Thee Stallion Lead 2021 MTV VMA Nominations\". Billboard. Retrieved August 17, 2021.\\n Weatherby, Taylor (February 5, 2023). \"Taylor Swift Makes GRAMMY History (Again) With Best Music Video Win For \"All Too Well: The Short Film\"\". Grammy Awards. Retrieved February 6, 2023.\\n Lansky, Sam (December 6, 2023). \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift\". The Recording Academy. Archived from the original on August 12, 2016. Retrieved August 3, 2016.\\n Friedlander, Whitney (September 10, 2015). \"Taylor Swift, Jimmy Fallon Among Juried Emmy Award Winners\". Variety. Archived from the original on September 15, 2015. Retrieved August 3, 2016.\\n \"Taylor Swift dominates AMAs with 6 wins, extending lead as show\\'s most-decorated artist\". KTRK-TV. November 21, 2022. Retrieved November 21, 2022.\\n Grein, Paul (November 19, 2023). \"After the 2023 Billboard Music Awards, Who Is the All-Time Biggest Winner?\". Billboard. Retrieved November 19, 2023.\\n See Guinness World Records by Taylor Swift\\n Lewis, Randy (November 4, 2013). \"Taylor Swift to receive rare Pinnacle Award at CMA Awards Nov. 6\". Los Angeles Times. Retrieved May 13, 2020.\\n \"Taylor Swift Nashville Tickets\". Excite. Archived from the original on February 3, 2015. Retrieved February 2, 2015.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on March 14, 2014. Retrieved February 2, 2015.\\n \"Songwriters Hall of Fame\". Songwriters Hall of Fame. Archived from the original on November 29, 2014. Retrieved February 2, 2015.\\n \"The 100 Greatest Songwriters of All Time\". Rolling Stone. Archived from the original on September 2, 2017. Retrieved August 28, 2017.\\n Polanco, Luis (April 5, 2016). \"Taylor Swift to Receive First-Ever Taylor Swift Award From BMI\". Billboard. Retrieved October 21, 2020.\\n Jolly, Nathan (November 17, 2019). \"Why Taylor Swift is to blame for latest twist in music rights drama\". News.com.au. Retrieved November 17, 2019.\\n \"10 Life mantras by Taylor Swift to live by\". India Today. December 13, 2016. Archived from the original on February 9, 2019. Retrieved July 16, 2020.\\n Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Decade Taylor Swift: \\'I Do Want My Music to Live On\\'\". Billboard. Retrieved December 11, 2019.\\n \"Taylor Swift to receive BRITs Global Icon award\". Official Charts Company. May 9, 2021. Retrieved May 10, 2021.\\n \"Taylor Swift to receive Global Icon Award!\". Brit Awards. May 9, 2021. Retrieved May 10, 2021.\\n Paine, Andre (February 22, 2023). \"Taylor Swift wins IFPI\\'s 2022 Global Recording Artist Of The Year Award\". Music Week. Retrieved February 22, 2023.\\n Brandle, Lars (November 5, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Debuts at U.K. No. 1 With \\'Massive\\' Sales\". Billboard. Retrieved November 5, 2023.\\n \"Female artists with the most Irish Number 1 albums since 2000\". Official Charts Company. November 5, 2020. Retrieved February 23, 2021.\\n Wang, Dennis (April 16, 2021). \"Taylor Swift\\'s Fearless hits the right note in China, again\". People\\'s Daily. Retrieved June 26, 2021.\\n Brandle, Lars (July 7, 2023). \"Taylor Swift Sets Chart Record In Australia With Top-Five Sweep\". Billboard. Retrieved February 13, 2024.\\n Brandle, Lars (February 9, 2024). \"Ahead of \\'The Eras Tour\\' of Australia, Taylor Swift Sweeps Top 5\". Billboard. Retrieved February 13, 2024.\\n \"Taylor beats Swift\". Australian Recording Industry Association. July 14, 2023. Archived from the original on July 14, 2023. Retrieved July 19, 2023.\\n \"Taylor sweeps the record\". Australian Recording Industry Association. July 7, 2023. Retrieved July 7, 2023.\\n Cumulative touring gross:\\n\"Top Touring Artist of the Pollstar Era\" (PDF). Pollstar. June 10, 2022. Archived (PDF) from the original on August 5, 2022. Retrieved August 4, 2022.\\nAllen, Bob (September 26, 2023). \"What A Friggin\\' Year! 2023 Boxoffice Results Remain At Record Highs\". Pollstar. Retrieved October 18, 2023.\\n Gensler, Andy (December 8, 2023). \"Taylor Swift Sets All-Time Touring Record With $1 Billion Gross\". Pollstar. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Willman, Chris (December 21, 2020). \"Taylor Swift\\'s \\'Evermore\\' Sells a Million Worldwide in First Week\". Variety. Archived from the original on January 11, 2021. Retrieved December 21, 2020.\\n Paine, Andre (December 22, 2022). \"Taylor Swift Achieves More Than 6 Million Global Units for Midnights and 37 Billion Total Streams in 2022\". Music Week. Archived from the original on December 22, 2022. Retrieved July 19, 2023.\\n Young, Alex (October 31, 2022). \"Taylor Swift broke 73 records with release of new album Midnights\". Consequence. Archived from the original on October 31, 2022. Retrieved July 19, 2023.\\n Grein, Paul (November 8, 2023). \"Taylor Swift Is Apple Music\\'s 2023 Artist of the Year\". Billboard. Retrieved November 8, 2023.\\n Willman, Chris (October 28, 2023). \"Taylor Swift Beats Her Own Spotify Record for Most Single-Day Streams for an Artist With \\'1989 (Taylor\\'s Version)\\' Release\". Variety. Archived from the original on October 29, 2023. Retrieved October 29, 2023.\\n @billboardcharts (January 19, 2022). \"Most entries on the #Global200 chart in a single week\" (Tweet) \\xe2\\x80\\x93 via Twitter.\\n \"Taylor Swift Chart History (Billboard Global 200)\". Billboard. Retrieved July 17, 2023.\\n \"Greatest of All Time Artists\". Billboard. Archived from the original on November 14, 2019. Retrieved November 15, 2019.\\n Zellner, Xander (January 25, 2024). \"Taylor Swift Tallies Record-Extending 95th Week at No. 1 on Artist 100 Chart\". Billboard. Retrieved January 26, 2024.\\n Caulfield, Keith (December 31, 2023). \"Taylor Swift Surpasses Elvis Presley for Most Weeks at No. 1 on Billboard 200 Among Soloists\". Billboard. Retrieved January 3, 2024.\\n Trust, Gary (January 22, 2024). \"Ariana Grande\\'s \\'Yes, And?\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved January 22, 2024.\\n \"Taylor Swift Chart History (Top Country Albums)\". Billboard. Retrieved July 17, 2023.\\n \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved January 14, 2021.\\n Trust, Gary (July 28, 2023). \"Taylor Swift Breaks Record for Most No. 1s on Pop Airplay Chart As \\'Cruel Summer\\' Becomes Her 12th\". Billboard. Archived from the original on September 29, 2023. Retrieved October 19, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift Makes History With Five of the Top 10 Albums on the Billboard 200\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (July 17, 2023). \"Taylor Swift Has 11 Albums on the Billboard 200 Chart for the First Time\". Billboard. Retrieved July 19, 2023.\\n McIntyre, Hugh (January 22, 2024). \"Taylor Swift Made Billboard History\\xe2\\x80\\x93Now Only She Can Match Her Own Feat\". Forbes. Retrieved January 22, 2024.\\n Caulfield, Keith (December 29, 2023). \"Taylor Swift Has the Top Four on the Album Sales Chart for the First Time\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (January 3, 2024). \"Taylor Swift Has 7 of the Top 10 on Billboard\\'s Album Sales Chart\". Billboard. Retrieved January 23, 2024.\\n \"RIAA \\xe2\\x80\\x93 Top Artists (Digital Singles)\". Recording Industry Association of America. Retrieved July 17, 2023.\\n Ahlgrim, Callie (July 12, 2023). \"There are 116 songs in history that have been certified diamond \\xe2\\x80\\x94 here they all are\". Insider Inc. Retrieved July 17, 2023.\\n Caulfield, Keith (October 30, 2022). \"Taylor Swift\\'s Midnights Blasts in at No. 1 on Billboard 200 Chart With Biggest Week for an Album in 7 Years\". Billboard. Retrieved October 30, 2022.\\n Nicks, Stevie (April 29, 2010). \"The 2010 Time 100: Taylor Swift\". Time. Archived from the original on November 29, 2013. Retrieved April 22, 2012.\\nVena, Jocelyn (April 16, 2015). \"Taylor Swift, Kanye West, Kim Kardashian Make Time\\'s 100 Most Influential People List\". Billboard. Archived from the original on September 25, 2016. Retrieved September 1, 2016.\\nJensen, Erin (April 17, 2019). \"Dwayne Johnson, Taylor Swift, Gayle King, more cover Time\\'s 100 most influential people issue\". USA Today. Archived from the original on April 17, 2019. Retrieved April 17, 2019.\\n Dockterman, Eliana (December 6, 2017). \"\\'I Was Angry.\\' Taylor Swift on What Powered Her Sexual Assault Testimony\". Time. Archived from the original on December 6, 2017. Retrieved December 7, 2017.\\n Shah, Simmone (December 6, 2023). \"Taylor Swift Makes History as Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. December 6, 2023. Retrieved December 11, 2023.\\n \"Taylor Swift\". Forbes. Retrieved December 1, 2020.\\n \"30 Under 30 2017: All-Star Alumni\". Forbes. Retrieved December 1, 2020.\\n \"#8 Taylor Swift\". Forbes. Archived from the original on July 28, 2015. Retrieved July 27, 2015.\\n \"Forbes Unveils Its 20th Annual Ranking Of The World\\'s Most Powerful Women\". Forbes. December 5, 2023. Retrieved December 5, 2023.\\n Schneider, Marc (July 24, 2023). \"8 Ways Taylor Swift Has Changed the Music Business\". Billboard. Retrieved July 24, 2023.\\n Khan, Fawzia (June 18, 2021). \"The Might Of Taylor Swift\". Elle. Archived from the original on June 28, 2021. Retrieved October 20, 2021.\\n Pattison, Kate (September 19, 2023). \"How did Taylor Swift get so popular? She never goes out of style\". The Conversation. Retrieved September 22, 2023.\\n Grady, Constance (October 12, 2023). \"The Eras concert movie is Taylor Swift leveling up\". Vox. Retrieved October 13, 2023.\\n Jericho, Greg (October 28, 2022). \"Taylor Swift\\'s incredible success in graphs \\xe2\\x80\\x93 who can blame me for being a Swiftie as a 50-year-old man?\". The Guardian. Retrieved October 29, 2022.\\n Weatherhead, Shaina (April 20, 2023). \"Taylor Swift\\'s \\'Miss Americana\\' Is Required Viewing Right Now\". Collider. Retrieved April 24, 2023.\\n Bartsch, Kayla (September 24, 2023). \"Taylor Swift: Empress of the Zeitgeist\". National Review. Retrieved September 24, 2023.\\n \"Taylor Swift: \\'My Confidence Is Easy To Shake\\'\". NPR. November 2, 2012. Retrieved May 20, 2021.\\nThanki, Juli (September 24, 2015). \"Taylor Swift: Teen idol to \\'biggest pop artist in the world\\'\". The Tennessean. Retrieved May 13, 2021.\\nYahr, Emily (June 16, 2016). \"Taylor Swift\\'s first song came out 10 years ago. Here\\'s what she was like as a teen songwriter\". Arts and Entertainment. The Washington Post. Archived from the original on May 13, 2021. Retrieved May 13, 2021.\\n El Mosselli, Sabrina (December 24, 2022). \"How Taylor Swift rose from \\'Miss Americana\\' to global megastar\". Le Monde. Retrieved December 28, 2022.\\n Cragg, Michael. \"Is Taylor Swift our last remaining real popstar?\". i-D. Archived from the original on May 6, 2023. Retrieved December 3, 2022.\\n Seisdedos, Iker (December 27, 2022). \"Pop music in the era of Taylor Swift: Behind the success of today\\'s biggest star\". El Pa\\xc3\\xads. Retrieved December 28, 2022.\\n Sanders, Sam (October 20, 2022). \"Taylor Swift Is the Greatest Self-Portraitist of Our Time\". Vulture. Retrieved October 21, 2022.\\n \"Taylor Swift\\'s 40 Biggest Hot 100 Hits\". Billboard. March 23, 2022. Retrieved April 12, 2022.\\n Leonard, Devin (November 12, 2014). \"Taylor Swift Is the Music Industry\". Bloomberg News. Archived from the original on March 13, 2016. Retrieved August 11, 2020.\\n Zoladz, Lindsay (December 30, 2019). \"What Were the 2010s?\". New York. Retrieved November 22, 2020.\\n Savaira, Augusta (November 23, 2022). \"Welcome to \\'Swiftonomics\\': What Taylor Swift reveals about the U.S. economy\". Los Angeles Times. Retrieved November 23, 2022.\\n Vincent, Alice (November 3, 2017). \"Taylor Swift: the rise, fall and re-invention of America\\'s sweetheart\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved July 5, 2020.\\n Fischer & Seidman 2016, p. 529.\\n Wahi, Sukriti (March 3, 2021). \"Every Time Taylor Swift Perfectly Shut Down A Sexist Interview Question\". Elle. Retrieved November 4, 2021.\\n Davis, Allison P. (June 28, 2018). \"The Taylor Swift Slut-Shaming Continues\". The Cut. Retrieved November 4, 2021.\\n Meddings, Jacqui (October 31, 2014). \"Taylor Swift is our new cover goddess\". Cosmopolitan. Archived from the original on July 14, 2016. Retrieved August 2, 2016.\\n Chiu, Allyson (August 3, 2022). \"Celebrities use private jets excessively. It\\'s a climate nightmare\". Washington Post. ISSN 0190-8286. Retrieved February 14, 2024.\\n Mendez, Lola (February 14, 2024). \"Taylor Swift claims she offsets her travel carbon footprint - how does that work?\". BBC. Retrieved February 14, 2024.\\n O\\'Malley, Isabella (February 2, 2024). \"Why Taylor Swift\\'s globe-trotting in private jets is getting scrutinized\". Associated Press. Retrieved February 6, 2024.\\n \"Taylor Swift and Travis Kelce romance is bad for the planet. Couple burns a whopping $70,779 jet fuel in the last three months\". The Economic Times. December 21, 2023. Retrieved February 6, 2024.\\n Blistein, Jon (February 6, 2024). \"Taylor Swift\\'s Lawyers Threaten Private Jet Flight Tracker Over \\'Stalking and Harassing Behavior\\'\". Rolling Stone. Retrieved February 6, 2024.\\n Matza, Max (February 7, 2024). \"Taylor Swift threatens to sue student who tracks her private jet\". BBC News. Retrieved February 7, 2024.\\n Benitez-Eves, Tina (July 2, 2022). \"Man Arrested for Harassing Taylor Swift, Attempting to Break Into Her New York City Apartment\". American Songwriter. Retrieved June 10, 2023.\\n Kreps, Daniel (July 2, 2022). \"Brooklyn Man Arrested on Charges of Stalking Taylor Swift\". Rolling Stone. Retrieved June 10, 2023.\\n Della Cava, Marco; Ryan, Patrick (August 31, 2023). \"Taylor Swift is \\'in a class of her own right now,\\' as Eras tour gives way to Eras movie\". USA Today. Archived from the original on September 1, 2023. Retrieved September 1, 2023.\\n Crummy, Colin (March 13, 2014). \"Detwanging country music: how Nashville took the UK\". The Guardian. Retrieved September 23, 2021.\\n Roland, Tom (July 7, 2016). \"Love Story: The Impact of Taylor Swift\\'s First Decade in Music\". Billboard. Archived from the original on July 14, 2016. Retrieved July 7, 2016.\\n Tannembaum, Rob (September 12, 2014). \"4 Ways Billboard Woman of the Year Taylor Swift Changed Country Music\". Billboard. Retrieved December 31, 2019.\\n O\\'Connor, Roisin (October 18, 2018). \"Half of all new guitar players are women, finds study\". The Independent. Archived from the original on May 26, 2022. Retrieved October 20, 2020.\\n Wang, Amy X. (May 22, 2018). \"Guitars Are Getting More Popular. So Why Do We Think They\\'re Dying?\". Rolling Stone. Retrieved October 20, 2020.\\n \"The 200 Most Important Artists of Pitchfork\\'s First 25 Years\". Pitchfork. October 4, 2021. Retrieved October 4, 2021.\\n Lipshutz, Jason (December 16, 2021). \"Billboard\\'s Greatest Pop Stars of 2021: No. 1 \\xe2\\x80\\x94 Taylor Swift\". Billboard. Retrieved December 16, 2021.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Retrieved August 18, 2021.\\n Gormely, Ian (December 3, 2014). \"Taylor Swift leads poptimism\\'s rebirth\". The Guardian. Retrieved August 2, 2021.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved September 7, 2023.\\n Friedman, Vanessa (April 24, 2023). \"Taylor Swift and the Sparkling Trap of Constant Reinvention\". The New York Times. Retrieved September 7, 2023.\\n Rowley, Glenn (March 9, 2023). \"Taylor Swift\\'s \\'All Too Well (10 Minute Version)\\' Is Now a Class at Stanford University\". Billboard. Retrieved March 10, 2023.\\n Ahlgrim, Callie (March 18, 2022). \"How Taylor Swift inspired a new generation of singer-songwriters, in their own words\". Business Insider. Retrieved March 18, 2022.\\n Sutherland, Mark (December 13, 2019). \"\\'I come with opinions about how we can better our industry\\': Taylor Swift\". Music Week. Retrieved April 27, 2020.\\nSisario, Ben (November 6, 2014). \"Sales of Taylor Swift\\'s \\'1989\\' Intensify Streaming Debate\". The New York Times. Retrieved August 11, 2020.\\nRosseinsky, Katie (October 29, 2021). \"How Taylor Swift is changing the music industry a re-record at a time\". Evening Standard. Archived from the original on April 9, 2021. Retrieved April 23, 2021.\\n Smith, Neil (June 22, 2015). \"Five ways Taylor Swift is changing the world\". BBC News. Retrieved December 31, 2019.\\n Brooks, Dave (November 3, 2022). \"How Taylor Swift Changed the Course of the Concert Ticketing Businesses\". Billboard. Retrieved November 4, 2022.\\n Sources on Swift and millennial era\\n \\xe2\\x80\\xa2 McDuling, John (November 3, 2014). \"Why Taylor Swift is the most important artist of the millennial era\". Quartz. Archived from the original on June 2, 2021. Retrieved April 2, 2021.\\n \\xe2\\x80\\xa2 \"What Taylor Swift tells us about millennials\". National Post. October 6, 2015. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Kidd, Joel (March 2, 2020). \"Taylor Swift Is the Surprising Face of Millennial Anxiety\". The Walrus. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Winograd, Morley; D. Hais, Michael (January 29, 2010). \"Move over Kanye West, Taylor Swift and the Millennial generation are taking over music\". The Christian Science Monitor. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 \"The 17 Musicians Gen Z & Millennials Say Represent Their Generations\". YPulse. October 8, 2019. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Nicolau, Elena (November 29, 2022). \"How Taylor Swift is transforming millennial weddings\". Today. Retrieved November 30, 2022.\\n \\xe2\\x80\\xa2 \"Taylor Swift is peak millennial vibes\". NPR. October 21, 2022. Retrieved December 19, 2022.\\n VanDerWerff, Emily (July 31, 2020). \"Taylor Swift is the millennial Bruce Springsteen\". Vox. Retrieved October 30, 2021.\\n Ditum, Sarah (October 8, 2022). \"Taylor Swift: the Bob Dylan of our age\". The Times. Archived from the original on December 24, 2022. Retrieved October 9, 2022.\\n \"Taylor Swift Will Receive First-Ever Woman of the Decade Honor at Billboard\\'s Women in Music\". Billboard. November 18, 2019. Retrieved December 31, 2019.\\n Elizabeth, De (November 24, 2019). \"Taylor Swift Is Named Artist of the Decade at AMAs 2019\". Teen Vogue. Retrieved December 31, 2019.\\n Kemp, Sam (January 27, 2022). \"The surprising Paul McCartney song inspired by Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Golsen, Tyler (May 14, 2023). \"Mick Jagger discusses his performance with Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Brandle, Lars (January 28, 2015). \"Taylor Swift on Madonna Praise: \\'Now I\\'m Dead\\'\". Billboard. Retrieved July 5, 2023.\\n Aniftos, Rania (January 27, 2021). \"Dolly Parton Says Taylor Swift Is \\'Magnificent\\' Following Damon Albarn Feud: \\'She Knows Who She Is\\'\". Billboard. Retrieved July 5, 2023.\\n Graff, Gary (October 31, 2021). \"Taylor Swift Helps Induct Carole King, Sings \"Will You Love Me Tomorrow\" at Rock Hall Ceremony\". The Hollywood Reporter. Retrieved September 20, 2022.\\n Skinner, Tom (November 2, 2022). \"Bruce Springsteen praises Taylor Swift\\'s new album \\'Midnights\\': \"She\\'s a tremendous writer\"\". NME. Retrieved July 5, 2023.\\n Ruggieri, Melissa (November 3, 2021). \"Billy Joel says Taylor Swift is like the Beatles of her generation: \\'She knows music\\'\". USA Today. Retrieved July 5, 2023.\\n Williams, Joe (July 5, 2023). \"Ringo Starr says Taylor Swift is \"biggest star in the world\"\". Far Out. Retrieved July 5, 2023.\\n Iasimone, Ashley (November 12, 2023). \"Britney Spears Shares Throwback Photo With Taylor Swift From 20 Years Ago: \\'Most Iconic Pop Woman of Our Generation\\'\". Billboard. Retrieved November 12, 2023.\\n Lutz, Ashley (October 8, 2022). \"Taylor Swift\\'s new album proves she is a business genius\". Fortune. Retrieved October 8, 2022.\\n Steele, Anne (July 1, 2023). \"How to Succeed in Business Like Taylor Swift\". The Wall Street Journal. Retrieved July 2, 2023.\\n Lynch, Joe (January 31, 2024). \"Taylor Swift Tops Billboard\\'s Annual Power 100 List for the First Time\". Billboard. Retrieved January 31, 2024.\\n Curto, Justin (December 22, 2020). \"Did 2020 Kill the Long, Fancy Pop-Album Rollout for Good?\". Vulture. Archived from the original on December 23, 2020. Retrieved December 27, 2020.\\n Leight, Elias (July 23, 2020). \"Taylor Swift Finally Abandoned the Traditional Album Rollout\". Rolling Stone. Retrieved September 15, 2022.\\n Kheraj, Alim (November 9, 2022). \"The curate\\'s Easter egg: how Taylor Swift turned pop into a multiplayer puzzle\". The Guardian. Retrieved November 9, 2022.\\n Tannenbaum, Emily. \"Taylor Swift Hid a Bunch of Easter Eggs in the All Too Well Short Film\". Glamour. Retrieved May 13, 2022.\\n Suskind, Alex (May 9, 2019). \"New Reputation: Taylor Swift shares intel on TS7, fan theories, and her next era\". Entertainment Weekly. Archived from the original on August 12, 2019. Retrieved May 13, 2022.\\n Yahr, Emily (December 26, 2022). \"2022: The year in review (Taylor\\'s version)\". The Washington Post. Retrieved December 26, 2022.\\n Stein, Jeannine (November 18, 2011). \"Taylor Swift weighs in on being a role model\". Los Angeles Times. Archived from the original on July 15, 2012. Retrieved April 18, 2012.\\n Collins, Katie (June 8, 2018). \"How Taylor Swift flipped online fandom on its head for the better\". CNET. Retrieved March 19, 2023.\\n King, Ashley (July 21, 2023). \"Taylor Swift\\'s 13 Management is Rarely Discussed, Changes Abound\". Digital Music News. Retrieved September 23, 2023.\\n Yotka, Steff (August 20, 2019). \"Taylor Swift and Stella McCartney Reveal Their Lover Fashion Collaboration\". Vogue. Retrieved October 20, 2020.\\n Kaplan, Julee (January 29, 2009). \"Taylor Swift Launches Sundress Line \\xe2\\x80\\x93 Ready-to-Wear and Sportswear \\xe2\\x80\\x93 Media\". Women\\'s Wear Daily. Archived from the original on July 29, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift to have greeting card line\". USA Today. November 18, 2009. Archived from the original on December 14, 2009. Retrieved August 7, 2012.\\n Serpe, Gina (October 29, 2008). \"Taylor Swift Gets All Dolled Up\". E! News (Blog). Archived from the original on March 5, 2011. Retrieved September 20, 2010.\\n Mychaskiw, Marianne (October 1, 2014). \"Taylor Swift Is Launching Her Fourth Fragrance!\". InStyle. Archived from the original on January 7, 2015. Retrieved December 3, 2015.\\n McIntyre, Hugh. \"Taylor Swift Has Signed A Multi-Year Deal With AT&T\". Forbes. Archived from the original on September 11, 2018. Retrieved September 11, 2018.\\n Chiu, Melody (July 26, 2019). \"Watch Taylor Swift Hilariously Moonlight \\xe2\\x80\\x94 and Fail \\xe2\\x80\\x94 as a Bartender and Server in New Commercial\". People. Retrieved May 7, 2022.\\n \"Taylor Swift: NHL\\'s New Spokesperson\". AOL. February 3, 2009. Archived from the original on February 7, 2009. Retrieved September 20, 2010.\\n \"Sony Electronics News and Information\". Sony. April 26, 2010. Retrieved August 7, 2012.\\n Lee, Ashley (October 27, 2014). \"Taylor Swift Named New York City Welcome Ambassador\". Billboard. Retrieved September 26, 2023.\\n Kaufman, Gil (January 20, 2022). \"Taylor Swift Tapped as First Record Store Day Global Ambassador\". Billboard. Retrieved January 20, 2022.\\n Snapes, Laura (August 24, 2019). \"Taylor Swift: \\'Pop music can feel like it\\'s The Hunger Games, and like we\\'re gladiators\\'\". The Guardian. Archived from the original on August 24, 2019. Retrieved August 24, 2019.\\n \"The Original 300+ Time\\'s Up Signatories\". Time\\'s Up. October 21, 2019. Archived from the original on May 3, 2020. Retrieved May 20, 2020.\\n Mier, Tom\\xc3\\xa1s (June 24, 2022). \"Taylor Swift Says She\\'s \\'Absolutely Terrified\\' After Roe v. Wade Decision\". Rolling Stone. Retrieved June 24, 2022.\\n \"Taylor Swift sends \\'love\\' and \\'respect\\' to LGBT fans\". BBC News. June 4, 2018. Archived from the original on November 14, 2018. Retrieved December 15, 2018.\\n Kreps, Daniel (June 1, 2019). \"Taylor Swift Urges Senate to Pass Equality Act in Letter to Tennessee Senator\". Rolling Stone. Retrieved August 12, 2020.\\n Lynch, Joe (June 26, 2020). \"Taylor Swift Calls Out U.S. Census for Transgender Erasure\". Billboard. Retrieved June 29, 2020.\\n Lynch, Joe (June 14, 2019). \"Taylor Swift Wishes NYC\\'s Stonewall Inn \\'Happy Pride\\' at Surprise Set\". Billboard. Retrieved August 7, 2022.\\n Aniftos, Rania (April 8, 2019). \"Taylor Swift Donates $113,000 to Tennessee Equality Project to Fight Anti-LGBTQ Bills\". Billboard. Archived from the original on April 9, 2019. Retrieved April 9, 2019.\\n \"Taylor Swift Makes a Generous Donation to GLAAD in Support of the LGBTQ Community\" (Press release). GLAAD. June 1, 2019. Archived from the original on June 1, 2019. Retrieved June 1, 2019.\\n Lynch, John (March 23, 2018). \"Taylor Swift Shares Support For Gun a Control Reform In Instagram Post\". Business Insider. Retrieved May 20, 2020.\\n Hiatt, Brian (September 18, 2019). \"The Rolling Stone Interview Taylor Swift\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved September 19, 2019.\\n Bailey, Alyssa (June 9, 2020). \"Taylor Swift Tweeted About the Black Lives Matter Movement and Urged People to Vote\". Elle. Retrieved July 27, 2020.\\n Kreps, Daniel (June 12, 2020). \"Taylor Swift Calls for Removal of Monuments That \\'Celebrate Racist Historical Figures\\'\". Rolling Stone. Retrieved August 12, 2020.\\n France, Lisa Respers (June 19, 2020). \"Taylor Swift got educated on Juneteenth and wants you to be, too\". CNN. Retrieved July 27, 2020.\\n Haag, Matthew (October 9, 2018). \"Voter Registrations Spike as Deadlines Loom. Taylor Swift Had Something to Do With It\". The New York Times. Archived from the original on October 9, 2018. Retrieved October 10, 2018.\\n Stevens, Matt (October 7, 2020). \"Taylor Swift, leaning further into politics, endorses Joe Biden and Kamala Harris\". The New York Times. Archived from the original on October 7, 2020. Retrieved October 8, 2020.\\n Schuster, Steve (May 19, 2023). \"Taylor Swift comes out against Trump, again\". Wisconsin Law Journal. Retrieved November 4, 2023.\\n Dellatto, Marisa (October 27, 2023). \"Taylor Swift\\'s New Era: The Pop Star Becomes A Billionaire\". Forbes. Retrieved October 29, 2023.\\n Pendleton, Devon; Ballentine, Claire; Patino, Marie; Whiteaker, Chloe; Li, Diana (October 26, 2023). \"Taylor Swift Vaults Into Billionaire Ranks With Blockbuster Eras Tour\". Bloomberg News. Retrieved October 26, 2023.\\n Greenburg, Zack O\\'Malley (November 2, 2016). \"The World\\'s Highest-Paid Women in Music 2016\". Forbes. Archived from the original on May 25, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (December 6, 2019). \"The World\\'s Top-Earning Musicians Of 2019\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 9, 2022). \"The Highest-Paid Entertainers 2022\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 13, 2023). \"The World\\'s 10 Highest-Paid Entertainers\". Forbes. Retrieved July 29, 2022.\\n \"Highest annual earnings ever for a female pop star\". Guinness World Records. Archived from the original on November 12, 2017. Retrieved November 15, 2017.\\n Mercuri, Monica (July 10, 2019). \"Taylor Swift Is The World\\'s Highest-Paid Celebrity With $185 Million in 2019\". Forbes. Archived from the original on July 12, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (July 11, 2016). \"Taylor Swift Is The World\\'s Top-Earning Celebrity With $170 Million in 2016\". Forbes. Archived from the original on August 3, 2016. Retrieved August 3, 2016.\\n Greenburg, Zack O\\'Malley (December 23, 2019). \"From Taylor Swift To Dr. Dre: The 10 Top-Earning Musicians Of The Decade\". Forbes. Archived from the original on December 30, 2019. Retrieved December 31, 2019.\\n Silva, Rachel; Tonelli, Lucia (October 3, 2023). \"Inside Taylor Swift\\'s 8 Incredible Homes\". Elle Decoration. Retrieved October 12, 2023.\\n Dailey, Hannah; Aniftos, Rania (December 21, 2023). \"A Timeline of Taylor Swift\\'s Generosity\". Billboard. Retrieved December 24, 2023.\\n Couch, Robbie (March 2, 2015). \"Taylor Swift Named Most Charitable Celeb For Supporting Feminist Causes, Education, More\". HuffPost. Archived from the original on October 21, 2016. Retrieved August 2, 2016.\\n Pacella, Megan (June 13, 2012). \"Taylor Swift Receives Star of Compassion Award\". Taste of Country. Archived from the original on July 13, 2015. Retrieved November 27, 2015.\\n Cooper, Brittany Joy (March 27, 2012). \"Taylor Swift to Receive \\'Big\\' Honor From Michelle Obama at Kids\\' Choice Awards\". Taste of Country. Archived from the original on July 12, 2012. Retrieved August 30, 2012.\\n Lee, Ken (August 9, 2009). \"Taylor Swift donates $100,000 to victims of Iowa Flood\". People. Archived from the original on June 7, 2009. Retrieved September 9, 2009.\\n \"How Taylor Swift helps people worldwide\". Borgen. February 2, 2023. Retrieved June 17, 2023.\\n Moran, Jonathon (March 8, 2009). \"Kylie to play at Sound Relief with Coldplay, Midnight Oil\". The Daily Telegraph. Archived from the original on October 10, 2016. Retrieved December 30, 2009.\\n \"Taylor Swift donates $500,000 after Nashville floods\". BBC News. May 7, 2010. Retrieved May 7, 2022.\\n Lewis, Randy (May 23, 2011). \"Taylor Swift benefit concert raises more than $750,000 for tornado victims\". Los Angeles Times (Blog). Archived from the original on April 6, 2012. Retrieved April 18, 2012.\\n \"Taylor Swift gives $1m to help Louisiana flood relief efforts\". The Guardian. August 17, 2016. Archived from the original on August 17, 2016. Retrieved August 17, 2016.\\n Muhammad, Latifah (December 9, 2016). \"Taylor Swift Donates $100,000 to Dolly Parton Fire Fund\". Entertainment Tonight. Archived from the original on January 13, 2017. Retrieved January 11, 2017.\\n Blair, Olivia (September 4, 2019). \"Taylor Swift Makes \\'Very Sizeable Donation\\' To A Food Bank in Houston After Hurricane Harvey\". Elle. Archived from the original on July 2, 2019. Retrieved July 3, 2019.\\n Rowley, Glenn (April 13, 2023). \"Taylor Swift Donated Enough to Feed \\'Over 125,000\\' at Tampa Food Bank Ahead of The Eras Tour Stop\". Billboard. Archived from the original on April 21, 2023. Retrieved May 11, 2023.\\n Dementri, Nikki (May 16, 2023). \"\\'It\\'s a blessing\\': South Philly pizza, cheesesteak shop thankful for T. Swift\\'s business\". CBS News. Archived from the original on May 17, 2023. Retrieved May 17, 2023.\\n Grow, Kory (August 3, 2023). \"Taylor Swift Is Helping Truck Drivers Buy First Homes With $100,000 Bonuses\". Rolling Stone. Retrieved August 3, 2023.\\n Paulson, Dave (March 5, 2020). \"Taylor Swift donates $1 million for Tennessee tornado relief\". The Tennessean. Retrieved March 5, 2020.\\n Ruggieri, Melissa. \"Taylor Swift donates $1 million to help communities ravaged by Tennessee tornadoes\". The Tennessean. Retrieved December 11, 2023.\\n \"Nashville Songwriters Hall of Fame\". Nashville Songwriters Foundation. Retrieved July 2, 2020.\\n \"Taylor Swift becomes namesake of Hendersonville High School auditorium\". The Tennessean. September 23, 2010. Archived from the original on July 12, 2015. Retrieved May 29, 2011.\\n McKinley, James C. (May 17, 2012). \"Taylor Swift to Donate $4 Million to the Country Hall of Fame\". The New York Times. Archived from the original on June 5, 2012. Retrieved July 1, 2012.\\n Golden, Zara (October 1, 2012). \"And The Taylor Swift VH1 Storytellers Contest Winner Is ... Harvey Mudd College!\". VH1. Archived from the original on July 12, 2015. Retrieved April 21, 2014.\\n \"$100,000 Gift from Taylor Swift Provides Vital Support to Nashville Symphony\". Nashville Symphony. December 16, 2013. Retrieved April 7, 2020.\\n Sanz, Cynthia (December 14, 2009). \"Taylor Swift Gives Big as She Turns 20 \\xe2\\x80\\x93 Good Deeds, Taylor Swift\". People. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\nUrban, Mike (October 14, 2011). \"Taylor Swift donates 6,000 books to Reading Library\". Reading Eagle. Archived from the original on May 12, 2012. Retrieved May 29, 2012.\\nZhu, Danielle (November 17, 2015). \"Taylor Swift partners with Scholastic to donate 25,000 books\". Entertainment Weekly. Archived from the original on August 14, 2016. Retrieved August 3, 2016.\\n Cooper, Brittany Joy (February 2, 2012). \"Taylor Swift Donates 14,000 Books to Nashville Public Library\". Taste of Country. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\n \"Taylor Swift Helps With \\'Delete Online Predators\\' Campaign\". Country Weekly. September 18, 2007. Archived from the original on January 16, 2016. Retrieved November 29, 2015.\\n \"Taylor Swift Teams Up With UNICEF Tap Project Initiative\". Taste of Country. March 17, 2011. Archived from the original on May 1, 2013. Retrieved August 7, 2012.\\n\"Taylor Swift Among Participants in MusiCares Back To School Charity Auction\". RTTNews. July 28, 2011. Archived from the original on May 17, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift and ACM Lifting Lives present $50,000 donation to St. Jude\". The Tennessean. June 13, 2011. Archived from the original on July 12, 2015. Retrieved May 29, 2012.\\n \"New and Hot Video: Taylor Swift Debuts \\'Ronan\\' at Stand Up To Cancer Benefit\". Rolling Stone. September 8, 2012. Archived from the original on September 13, 2012. Retrieved September 14, 2012.\\n Knight, Joey (May 17, 2014). \"Dick Vitale holds annual fundraiser for pediatric cancer\". Tampa Bay Times. Archived from the original on July 14, 2014. Retrieved July 30, 2014.\\n George, John. \"Taylor Swift donates $50K to CHOP to help teens with cancer\". The Business Journals (Blog). Archived from the original on July 12, 2015. Retrieved June 5, 2014.\\n Hardiman, Tim (April 20, 2007). \"Taylor Swift Encourages Teen Volunteers\". CMT. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n Vagianos, Alanna (August 17, 2017). \"This Is How Taylor Swift\\'s Donations Will Impact Sexual Assault Survivors\". HuffPost. Retrieved April 7, 2020.\\n Peppin, Hayley (March 24, 2020). \"Taylor Swift responded to the leaked \\'Famous\\' phone call with Kanye West by urging fans to instead focus on coronavirus aid\". Insider Inc. Retrieved May 13, 2020.\\n Grow, Kory (April 1, 2020). \"How Taylor Swift Is Helping a Small Nashville Record Store Survive\". Rolling Stone. Retrieved August 1, 2023.\\n Hissong, Samantha (August 20, 2020). \"Taylor Swift Starts Frenzy at Indie Record Stores With Surprise Signed \\'Folklore\\' CDs\". Rolling Stone. Retrieved August 1, 2023.\\n Havens, Lyndsey (April 18, 2020). \"Taylor Swift Sings Heart-Rending \\'Soon You\\'ll Get Better\\' During \\'One World\\' Concert\". Billboard. Retrieved April 19, 2020.\\n @RAINN (April 8, 2021). \"In 2017, @taylorswift13 fearlessly shared her story of sexual assault in court. In honor of Sexual Assault Awareness & Prevention Month & #RAINNDay, Taylor Swift has made a generous gift to RAINN. Thank you for helping us ensure all survivors are believed and supported! https://t.co/oE7P1BVnNJ\" (Tweet). Retrieved April 21, 2021 \\xe2\\x80\\x93 via Twitter.\\n \"Taylor Swift\\'s cash gift helps student take up degree\". BBC News. August 20, 2020. Retrieved November 9, 2020.\\n M Holloway, Sophie (December 14, 2023). \"Taylor Swift and Selena Gomez among celebrities attending Gaza aid fundraiser\". The Independent.\\nCited literature\\nFischer, Nancy; Seidman, Steven (2016). Introducing the New Sexuality Studies (3rd ed.). Routledge. ISBN 978-1-317-44918-8. OCLC 953030187.\\nFogarty, Mary; Arnold, Gina (2021). \"Are You Ready for It? Re-Evaluating Taylor Swift\". Contemporary Music Review. 40 (1): 1\\xe2\\x80\\x9310. doi:10.1080/07494467.2021.1976586.\\nFulford, Phyllis (2014). An Idiots Guide: Singing Second Edition. Penguin Publishing. ISBN 978-1-61564-622-7.\\nHughes, Charles (2017). \"Country Music and the Recording Industry\". In Stimeling, Travis D. (ed.). The Oxford Handbook of Country Music. Oxford University Press. pp. 205\\xe2\\x80\\x93228. ISBN 978-0-19-024817-8.\\nJepson, Louisa (2013). Taylor Swift. Simon & Schuster. ISBN 978-1-4711-3087-8.\\nMcNutt, Myles (2020). \"From \\'Mine\\' to \\'Ours\\': Gendered Hierarchies of Authorship and the Limits of Taylor Swift\\'s Paratextual Feminism\". Communication, Culture and Critique. 13 (1): 72\\xe2\\x80\\x9391. doi:10.1093/ccc/tcz042.\\nMcGrath, John (2023). \"The Return to Craft: Taylor Swift, Nostalgia, and Covid-19\". Popular Music and Society. 46 (1): 70\\xe2\\x80\\x9384. doi:10.1080/03007766.2022.2156761.\\nPerone, James E. (2017). The Words and Music of Taylor Swift. The Praeger Singer-Songwriter Collection. ABC-Clio. ISBN 978-1-4408-5294-7.\\nProvenzano, Catherine (2018). \"Auto-Tune, Labor, and the Pop-Music Voice\". In Fink, Robert; Latour, Melinda; Wallmark, Zachary (eds.). The Relentless Pursuit of Tone: Timbre in Popular Music. Oxford University Press. pp. 159\\xe2\\x80\\x93182. ISBN 978-0-19-998522-7.\\nSloan, Nate (2021). \"Taylor Swift and the Work of Songwriting\". Contemporary Music Review. Routledge. 40 (1): 11\\xe2\\x80\\x9326. doi:10.1080/07494467.2021.1945226. S2CID 237695045.\\nExternal links\\nTaylor Swift\\nat Wikipedia\\'s sister projects\\nMedia from Commons\\nNews from Wikinews\\nQuotations from Wikiquote\\nData from Wikidata\\nOfficial website Edit this at Wikidata\\nTaylor Swift at AllMusic Edit this at Wikidata\\nTaylor Swift discography at Discogs Edit this at Wikidata\\nTaylor Swift at IMDb Edit this at Wikidata\\nTaylor Swift discography at MusicBrainz Edit this at Wikidata\\nvte\\nTaylor Swift\\nvte\\nTaylor Swift songs\\nAwards for Taylor Swift\\nvte\\nIFPI Global Year-End Charts\\nvte\\nBillboard Year-End number one albums\\nvte\\nBillboard Year-End Top Artist\\nPortals:\\n Biography\\nicon Pop music\\nflag Pennsylvania\\nflag United States\\nAuthority control databases Edit this at Wikidata\\nCategories: Taylor Swift1989 birthsLiving people21st-century American actresses21st-century American philanthropists21st-century American songwriters21st-century American women guitarists21st-century American women pianists21st-century American women singers21st-century women philanthropistsActresses from Nashville, TennesseeActresses from PennsylvaniaAmerican acoustic guitaristsAmerican billionairesAmerican country banjoistsAmerican country guitaristsAmerican country pianistsAmerican country record producersAmerican country singer-songwritersAmerican feministsAmerican film actressesAmerican folk guitaristsAmerican folk musiciansAmerican folk singersAmerican mezzo-sopranosAmerican multi-instrumentalistsAmerican music video directorsAmerican people of English descentAmerican people of German descentAmerican people of Italian descentAmerican people of Scottish descentAmerican pop guitaristsAmerican pop pianistsAmerican synth-pop musiciansAmerican television actressesAmerican voice actressesAmerican women country singersAmerican women guitaristsAmerican women philanthropistsAmerican women pop singersAmerican women record producersAmerican women rock singersAmerican women singer-songwritersAmerican women songwritersBig Machine Records artistsBrit Award winnersChristians from TennesseeCountry musicians from PennsylvaniaCountry musicians from TennesseeCountry pop musiciansFemale billionairesFemale music video directorsFeminist musiciansFilm directors from PennsylvaniaFilm directors from TennesseeGrammy Award winnersGuitarists from PennsylvaniaGuitarists from TennesseeMTV Europe Music Award winnersMTV Video Music Award winnersNME Awards winnersPeople from Bucks County, PennsylvaniaPhilanthropists from PennsylvaniaPhilanthropists from TennesseePrimetime Emmy Award winnersRCA Records artistsRecord producers from PennsylvaniaRecord producers from TennesseeRepublic Records artistsSinger-songwriters from PennsylvaniaSinger-songwriters from TennesseeSingers from Nashville, TennesseeSony Music Publishing artistsSynth-pop singersTime Person of the YearUniversal Music Group artistsBaldi family\\nThis page was last edited on 16 February 2024, at 11:33 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\\xc2\\xae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\\nWikimedia FoundationPowered by MediaWiki'\n185767\n53311\ncompression ratio: 3.48\nmerges: {(101, 32): 256, (44, 32): 257, (100, 32): 258, (46, 32): 259, (114, 32): 260, (50, 48): 261, (115, 32): 262, (105, 110): 263, (111, 110): 264, (114, 105): 265, (116, 32): 266, (116, 104): 267, (101, 258): 268, (257, 261): 269, (97, 110): 270, (97, 114): 271, (101, 260): 272, (121, 32): 273, (97, 108): 274, (267, 256): 275, (118, 268): 276, (119, 105): 277, (101, 114): 278, (264, 32): 279, (277, 102): 280, (82, 101): 281, (83, 280): 282, (111, 260): 283, (99, 104): 284, (269, 49): 285, (111, 109): 286, (98, 272): 287, (32, 275): 288, (97, 121): 289, (101, 110): 290, (111, 114): 291, (274, 32): 292, (101, 109): 293, (46, 10): 294, (265, 101): 295, (263, 103): 296, (269, 50): 297, (116, 105): 298, (289, 108): 299, (34, 259): 300, (108, 108): 301, (84, 299): 302, (116, 295): 303, (294, 32): 304, (116, 111): 305, (259, 281): 306, (306, 303): 307, (307, 276): 308, (302, 283): 309, (101, 115): 310, (309, 282): 311, (117, 115): 312, (114, 286): 313, (293, 287): 314, (41, 259): 315, (65, 114): 316, (102, 313): 317, (315, 34): 318, (270, 258): 319, (114, 101): 320, (111, 117): 321, (111, 265): 322, (111, 102): 323, (103, 263): 324, (296, 32): 325, (284, 105): 326, (93, 32): 327, (324, 292): 328, (317, 288): 329, (322, 328): 330, (104, 256): 331, (316, 326): 332, (332, 276): 333, (329, 330): 334, (333, 334): 335, (335, 279): 336, (259, 336): 337, (97, 32): 338, (115, 116): 339, (105, 99): 340, (46, 91): 341, (101, 99): 342, (105, 301): 343, (39, 262): 344, (311, 266): 345, (111, 118): 346, (97, 116): 347, (97, 262): 348, (101, 262): 349, (74, 117): 350, (323, 32): 351, (305, 32): 352, (117, 109): 353, (84, 331): 354, (271, 100): 355, (263, 32): 356, (270, 32): 357, (101, 108): 358, (297, 51): 359, (271, 273): 360, (267, 32): 361, (97, 109): 362, (108, 273): 363, (111, 112): 364, (311, 116): 365, (116, 114): 366, (105, 115): 367, (104, 272): 368, (111, 32): 369, (117, 360): 370, (78, 346): 371, (312, 340): 372, (371, 314): 373, (101, 119): 374, (97, 266): 375, (108, 32): 376, (58, 32): 377, (98, 111): 378, (282, 266): 379, (68, 342): 380, (105, 116): 381, (105, 103): 382, (66, 343): 383, (49, 48): 384, (97, 115): 385, (264, 103): 386, (79, 99): 387, (97, 298): 388, (83, 116): 389, (387, 305): 390, (390, 287): 391, (97, 99): 392, (111, 119): 393, (380, 314): 394, (383, 378): 395, (97, 100): 396, (108, 101): 397, (117, 114): 398, (102, 283): 399, (32, 40): 400, (297, 50): 401, (117, 103): 402, (284, 32): 403, (115, 266): 404, (321, 110): 405, (98, 353): 406, (111, 108): 407, (312, 266): 408, (101, 98): 409, (77, 97): 410, (350, 363): 411, (318, 345): 412, (107, 32): 413, (278, 115): 414, (93, 91): 415, (65, 402): 416, (416, 408): 417, (105, 100): 418, (297, 49): 419, (109, 101): 420, (101, 112): 421, (261, 49): 422, (50, 51): 423, (285, 50): 424, (101, 271): 425, (269, 261): 426, (73, 110): 427, (102, 105): 428, (110, 256): 429, (395, 355): 430, (265, 116): 431, (104, 105): 432, (372, 32): 433, (304, 34): 434, (78, 374): 435, (100, 105): 436, (65, 112): 437, (285, 57): 438, (114, 111): 439, (39, 32): 440, (115, 257): 441, (350, 429): 442, (323, 288): 443, (99, 291): 444, (50, 49): 445, (49, 57): 446, (105, 109): 447, (290, 32): 448, (409, 114): 449, (290, 116): 450, (111, 301): 451, (77, 271): 452, (265, 99): 453, (277, 361): 454, (44, 91): 455, (70, 449): 456, (456, 370): 457, (365, 344): 458, (300, 430): 459, (101, 97): 460, (285, 54): 461, (421, 116): 462, (410, 273): 463, (285, 53): 464, (437, 265): 465, (465, 376): 466, (108, 256): 467, (65, 119): 468, (388, 264): 469, (83, 462): 470, (470, 314): 471, (114, 97): 472, (274, 406): 473, (67, 104): 474, (118, 256): 475, (310, 266): 476, (74, 270): 477, (50, 50): 478, (477, 370): 479, (405, 366): 480, (382, 104): 481, (300, 354): 482, (359, 304): 483, (49, 51): 484, (65, 108): 485, (101, 116): 486, (310, 115): 487, (452, 403): 488, (117, 116): 489, (119, 431): 490, (108, 111): 491, (115, 386): 492, (226, 128): 493, (271, 258): 494, (48, 32): 495, (117, 108): 496, (50, 52): 497, (105, 262): 498, (298, 99): 499, (97, 103): 500, (34, 32): 501, (65, 110): 502, (49, 56): 503, (102, 291): 504, (480, 273): 505, (65, 420): 506, (506, 453): 507, (32, 84): 508, (115, 296): 509, (119, 348): 510, (49, 50): 511, (111, 99): 512, (364, 32): 513, (49, 55): 514, (108, 105): 515, (77, 433): 516, (101, 257): 517, (49, 52): 518, (117, 110): 519, (93, 10): 520, (435, 32): 521, (118, 105): 522, (468, 355): 523, (269, 48): 524, (83, 331): 525, (116, 278): 526, (98, 273): 527, (115, 331): 528, (97, 105): 529, (271, 116): 530, (101, 301): 531, (49, 49): 532, (111, 116): 533, (97, 98): 534, (444, 100): 535, (83, 112): 536, (121, 257): 537, (308, 411): 538, (308, 417): 539, (321, 114): 540, (270, 100): 541, (285, 55): 542, (507, 357): 543, (101, 101): 544, (115, 117): 545, (327, 319): 546, (49, 53): 547, (50, 54): 548, (111, 103): 549, (101, 118): 550, (50, 57): 551, (419, 304): 552, (87, 105): 553, (32, 77): 554, (320, 99): 555, (83, 386): 556, (50, 53): 557, (50, 55): 558, (261, 48): 559, (389, 264): 560, (101, 105): 561, (308, 394): 562, (290, 266): 563, (401, 304): 564, (321, 260): 565, (114, 404): 566, (111, 100): 567, (40, 373): 568, (105, 264): 569, (32, 319): 570, (115, 115): 571, (82, 451): 572, (459, 337): 573, (418, 101): 574, (290, 110): 575, (101, 120): 576, (278, 257): 577, (347, 268): 578, (504, 109): 579, (270, 99): 580, (572, 325): 581, (581, 560): 582, (51, 48): 583, (308, 373): 584, (104, 97): 585, (34, 257): 586, (99, 264): 587, (112, 32): 588, (97, 263): 589, (101, 100): 590, (485, 406): 591, (271, 298): 592, (71, 114): 593, (105, 279): 594, (341, 53): 595, (84, 447): 596, (359, 318): 597, (49, 54): 598, (388, 279): 599, (290, 100): 600, (105, 339): 601, (282, 116): 602, (112, 108): 603, (310, 116): 604, (308, 457): 605, (308, 391): 606, (99, 286): 607, (110, 32): 608, (264, 256): 609, (298, 264): 610, (117, 99): 611, (105, 270): 612, (278, 262): 613, (297, 52): 614, (395, 494): 615, (338, 40): 616, (341, 51): 617, (263, 288): 618, (50, 56): 619, (393, 32): 620, (341, 52): 621, (308, 466): 622, (493, 147): 623, (310, 262): 624, (73, 262): 625, (104, 348): 626, (109, 273): 627, (267, 375): 628, (257, 74): 629, (285, 51): 630, (111, 266): 631, (116, 262): 632, (443, 89): 633, (460, 260): 634, (582, 101): 635, (327, 525): 636, (327, 379): 637, (115, 112): 638, (108, 117): 639, (112, 112): 640, (87, 104): 641, (279, 275): 642, (427, 32): 643, (285, 52): 644, (273, 40): 645, (109, 433): 646, (428, 566): 647, (116, 272): 648, (83, 104): 649, (291, 108): 650, (256, 40): 651, (300, 635): 652, (76, 105): 653, (261, 423): 654, (97, 107): 655, (112, 104): 656, (112, 114): 657, (385, 104): 658, (522, 374): 659, (285, 56): 660, (109, 111): 661, (281, 535): 662, (317, 32): 663, (105, 114): 664, (84, 104): 665, (434, 345): 666, (300, 67): 667, (290, 99): 668, (308, 463): 669, (85, 83): 670, (72, 272): 671, (257, 319): 672, (397, 385): 673, (65, 301): 674, (109, 270): 675, (553, 361): 676, (426, 304): 677, (316, 298): 678, (99, 505): 679, (602, 344): 680, (109, 343): 681, (337, 373): 682, (281, 112): 683, (473, 32): 684, (109, 372): 685, (48, 48): 686, (424, 304): 687, (574, 369): 688, (97, 260): 689, (268, 275): 690, (98, 101): 691, (346, 272): 692, (262, 40): 693, (115, 104): 694, (521, 89): 695, (695, 291): 696, (99, 265): 697, (285, 48): 698, (337, 394): 699, (105, 108): 700, (118, 343): 701, (119, 432): 702, (267, 272): 703, (112, 278): 704, (415, 51): 705, (308, 471): 706, (72, 631): 707, (102, 102): 708, (32, 83): 709, (544, 107): 710, (113, 117): 711, (308, 442): 712, (401, 318): 713, (41, 10): 714, (492, 490): 715, (98, 108): 716, (98, 342): 717, (320, 673): 718, (89, 321): 719, (291, 256): 720, (69, 114): 721, (119, 104): 722, (556, 490): 723, (278, 256): 724, (300, 86): 725, (56, 57): 726, (268, 527): 727, (271, 266): 728, (111, 115): 729, (67, 505): 730, (524, 57): 731, (67, 264): 732, (77, 372): 733, (320, 362): 734, (110, 353): 735, (98, 278): 736, (261, 261): 737, (71, 320): 738, (263, 99): 739, (702, 403): 740, (262, 351): 741, (77, 84): 742, (49, 32): 743, (97, 339): 744, (78, 658): 745, (745, 701): 746, (268, 368): 747, (112, 513): 748, (257, 379): 749, (593, 362): 750, (97, 118): 751, (274, 108): 752, (367, 266): 753, (337, 391): 754, (69, 110): 755, (65, 32): 756, (111, 111): 757, (302, 291): 758, (362, 256): 759, (377, 354): 760, (298, 109): 761, (97, 273): 762, (101, 273): 763, (105, 118): 764, (71, 117): 765, (51, 49): 766, (435, 115): 767, (116, 398): 768, (446, 726): 769, (274, 256): 770, (87, 650): 771, (750, 627): 772, (444, 258): 773, (348, 338): 774, (474, 265): 775, (321, 266): 776, (100, 257): 777, (66, 476): 778, (461, 304): 779, (337, 442): 780, (472, 656): 781, (97, 257): 782, (375, 275): 783, (270, 105): 784, (312, 366): 785, (69, 118): 786, (107, 256): 787, (721, 348): 788, (59, 32): 789, (74, 111): 790, (285, 49): 791, (101, 300): 792, (112, 101): 793, (121, 108): 794, (66, 382): 795, (116, 257): 796, (84, 565): 797, (109, 32): 798, (604, 45): 799, (274, 376): 800, (491, 98): 801, (633, 634): 802, (105, 420): 803, (117, 292): 804, (367, 104): 805, (384, 48): 806, (101, 266): 807, (263, 110): 808, (101, 103): 809, (458, 39): 810, (365, 257): 811, (347, 256): 812, (65, 99): 813, (310, 257): 814, (392, 413): 815, (683, 489): 816, (758, 344): 817, (99, 278): 818, (256, 351): 819, (268, 352): 820, (78, 111): 821, (415, 53): 822, (512, 105): 823, (112, 439): 824, (516, 523): 825, (119, 286): 826, (592, 339): 827, (87, 343): 828, (117, 260): 829, (293, 770): 830, (70, 291): 831, (591, 32): 832, (111, 312): 833, (271, 256): 834, (101, 344): 835, (296, 288): 836, (270, 110): 837, (308, 479): 838, (86, 688): 839, (117, 716): 840, (396, 105): 841, (77, 101): 842, (262, 319): 843, (681, 594): 844, (381, 273): 845, (704, 579): 846, (103, 104): 847, (108, 121): 848, (661, 404): 849, (44, 686): 850, (87, 710): 851, (116, 101): 852, (337, 411): 853, (337, 471): 854, (337, 417): 855, (115, 300): 856, (596, 310): 857, (270, 103): 858, (374, 32): 859, (340, 32): 860, (460, 413): 861, (396, 256): 862, (271, 107): 863, (305, 288): 864, (115, 369): 865, (73, 32): 866, (101, 102): 867, (392, 107): 868, (468, 494): 869, (286, 263): 870, (85, 110): 871, (414, 569): 872, (337, 479): 873, (359, 412): 874, (438, 304): 875, (575, 487): 876, (425, 108): 877, (381, 262): 878, (121, 110): 879, (97, 117): 880, (418, 110): 881, (678, 404): 882, (268, 279): 883, (696, 413): 884, (592, 404): 885, (697, 499): 886, (52, 327): 887, (65, 571): 888, (97, 104): 889, (337, 463): 890, (821, 259): 891, (675, 257): 892, (101, 347): 893, (382, 110): 894, (41, 32): 895, (111, 257): 896, (70, 877): 897, (319, 34): 898, (102, 648): 899, (104, 481): 900, (275, 647): 901, (53, 327): 902, (100, 264): 903, (103, 476): 904, (117, 266): 905, (103, 290): 906, (290, 298): 907, (481, 116): 908, (116, 121): 909, (86, 872): 910, (438, 318): 911, (419, 318): 912, (320, 256): 913, (115, 10): 914, (84, 540): 915, (100, 611): 916, (40, 422): 917, (509, 467): 918, (415, 52): 919, (80, 101): 920, (455, 53): 921, (70, 105): 922, (97, 496): 923, (97, 295): 924, (365, 377): 925, (436, 338): 926, (114, 310): 927, (585, 475): 928, (100, 310): 929, (473, 262): 930, (719, 32): 931, (97, 112): 932, (536, 861): 933, (933, 78): 934, (356, 422): 935, (831, 98): 936, (77, 111): 937, (639, 100): 938, (340, 107): 939, (327, 643): 940, (746, 256): 941, (111, 45): 942, (82, 111): 943, (405, 100): 944, (524, 56): 945, (399, 275): 946, (850, 495): 947, (84, 114): 948, (817, 910): 949, (77, 99): 950, (623, 32): 951, (549, 781): 952, (263, 100): 953, (112, 364): 954, (97, 258): 955, (108, 349): 956, (384, 495): 957, (116, 325): 958, (586, 34): 959, (66, 491): 960, (278, 109): 961, (320, 535): 962, (71, 801): 963, (739, 938): 964, (358, 112): 965, (455, 52): 966, (529, 258): 967, (119, 279): 968, (888, 823): 969, (502, 110): 970, (428, 358): 971, (291, 273): 972, (110, 859): 973, (385, 115): 974, (474, 728): 975, (32, 767): 976, (461, 539): 977, (365, 300): 978, (72, 620): 979, (83, 296): 980, (263, 256): 981, (339, 734): 982, (77, 881): 983, (97, 119): 984, (439, 571): 985, (291, 101): 986, (341, 54): 987, (67, 271): 988, (121, 425): 989, (262, 352): 990, (277, 267): 991, (493, 148): 992, (116, 589): 993, (84, 803): 994, (472, 367): 995, (97, 265): 996, (337, 488): 997, (40, 391): 998, (400, 391): 999, (40, 394): 1000, (971, 777): 1001, (115, 342): 1002, (367, 99): 1003, (291, 121): 1004, (115, 101): 1005, (10, 379): 1006, (80, 513): 1007, (103, 117): 1008, (117, 436): 1009, (76, 346): 1010, (386, 32): 1011, (108, 270): 1012, (407, 107): 1013, (428, 108): 1014, (717, 759): 1015, (119, 291): 1016, (56, 327): 1017, (274, 865): 1018, (32, 34): 1019, (116, 256): 1020, (559, 57): 1021, (75, 561): 1022, (308, 488): 1023, (464, 304): 1024, (99, 271): 1025, (278, 45): 1026, (824, 916): 1027, (436, 555): 1028, (286, 256): 1029, (261, 384): 1030, (261, 478): 1031, (674, 508): 1032, (305, 829): 1033, (261, 445): 1034, (261, 495): 1035, (351, 368): 1036, (114, 256): 1037, (102, 117): 1038, (115, 341): 1039, (100, 101): 1040, (116, 119): 1041, (991, 288): 1042, (393, 110): 1043, (57, 327): 1044, (707, 806): 1045, (305, 588): 1046, (1022, 361): 1047, (111, 339): 1048, (281, 659): 1049, (270, 107): 1050, (426, 412): 1051, (725, 924): 1052, (1052, 909): 1053, (553, 107): 1054, (545, 98): 1055, (69, 120): 1056, (500, 256): 1057, (84, 876): 1058, (1013, 108): 1059, (121, 259): 1060, (427, 526): 1061, (633, 425): 1062, (936, 310): 1063, (83, 99): 1064, (72, 271): 1065, (407, 100): 1066, (48, 327): 1067, (257, 528): 1068, (321, 262): 1069, (502, 103): 1070, (735, 287): 1071, (109, 720): 1072, (392, 266): 1073, (305, 112): 1074, (771, 258): 1075, (102, 32): 1076, (66, 111): 1077, (66, 114): 1078, (765, 355): 1079, (474, 530): 1080, (66, 67): 1081, (112, 259): 1082, (337, 457): 1083, (920, 364): 1084, (493, 162): 1085, (110, 533): 1086, (342, 116): 1087, (118, 101): 1088, (117, 320): 1089, (115, 105): 1090, (1009, 369): 1091, (519, 100): 1092, (587, 818): 1093, (102, 830): 1094, (97, 640): 1095, (274, 105): 1096, (325, 352): 1097, (66, 265): 1098, (580, 256): 1099, (55, 327): 1100, (34, 400): 1101, (112, 291): 1102, (555, 561): 1103, (65, 109): 1104, (80, 114): 1105, (257, 77): 1106, (614, 304): 1107, (667, 742): 1108, (424, 434): 1109, (891, 743): 1110, (426, 318): 1111, (414, 264): 1112, (101, 256): 1113, (86, 32): 1114, (263, 102): 1115, (262, 356): 1116, (422, 52): 1117, (70, 1059): 1118, (900, 799): 1119, (662, 325): 1120, (339, 45): 1121, (772, 523): 1122, (347, 116): 1123, (396, 293): 1124, (118, 512): 1125, (105, 256): 1126, (325, 368): 1127, (321, 847): 1128, (262, 399): 1129, (267, 357): 1130, (119, 724): 1131, (84, 513): 1132, (105, 122): 1133, (97, 122): 1134, (115, 256): 1135, (257, 65): 1136, (105, 292): 1137, (116, 291): 1138, (281, 773): 1139, (68, 529): 1140, (257, 76): 1141, (482, 1079): 1142, (1142, 612): 1143, (542, 304): 1144, (257, 83): 1145, (99, 108): 1146, (678, 339): 1147, (312, 263): 1148, (1008, 381): 1149, (795, 32): 1150, (897, 624): 1151, (934, 393): 1152, (82, 268): 1153, (103, 985): 1154, (723, 613): 1155, (87, 286): 1156, (320, 773): 1157, (49, 327): 1158, (105, 266): 1159, (323, 102): 1160, (38, 32): 1161, (492, 32): 1162, (262, 375): 1163, (109, 655): 1164, (66, 320): 1165, (50, 327): 1166, (268, 628): 1167, (54, 327): 1168, (116, 270): 1169, (67, 265): 1170, (289, 262): 1171, (481, 266): 1172, (339, 271): 1173, (76, 111): 1174, (39, 266): 1175, (816, 469): 1176, (318, 458): 1177, (337, 466): 1178, (300, 78): 1179, (40, 488): 1180, (482, 884): 1181, (293, 450): 1182, (257, 80): 1183, (89, 425): 1184, (267, 278): 1185, (115, 432): 1186, (268, 454): 1187, (509, 956): 1188, (736, 45): 1189, (640, 291): 1190, (79, 708): 1191, (641, 375): 1192, (481, 632): 1193, (788, 797): 1194, (550, 278): 1195, (264, 258): 1196, (119, 650): 1197, (362, 268): 1198, (469, 292): 1199, (80, 104): 1200, (274, 349): 1201, (398, 325): 1202, (536, 111): 1203, (607, 109): 1204, (270, 115): 1205, (65, 266): 1206, (268, 356): 1207, (273, 80): 1208, (107, 325): 1209, (561, 260): 1210, (109, 563): 1211, (340, 256): 1212, (1041, 369): 1213, (267, 114): 1214, (298, 475): 1215, (99, 942): 1216, (1070, 358): 1217, (347, 349): 1218, (274, 363): 1219, (393, 608): 1220, (97, 284): 1221, (298, 279): 1222, (291, 103): 1223, (384, 32): 1224, (755, 526): 1225, (1225, 993): 1226, (99, 99): 1227, (949, 41): 1228, (409, 489): 1229, (257, 1082): 1230, (400, 373): 1231, (828, 892): 1232, (1232, 775): 1233, (731, 318): 1234, (72, 601): 1235, (1110, 279): 1236, (1181, 857): 1237, (922, 566): 1238, (67, 923): 1239, (1239, 1001): 1240, (1240, 1047): 1241, (1025, 101): 1242, (117, 262): 1243, (117, 112): 1244, (301, 32): 1245, (353, 450): 1246, (410, 284): 1247, (267, 498): 1248, (1115, 639): 1249, (342, 266): 1250, (735, 1189): 1251, (284, 530): 1252, (492, 262): 1253, (73, 266): 1254, (816, 599): 1255, (786, 961): 1256, (270, 121): 1257, (691, 448): 1258, (963, 292): 1259, (364, 290): 1260, (1203, 298): 1261, (1261, 102): 1262, (414, 279): 1263, (771, 100): 1264, (69, 109): 1265, (757, 258): 1266, (99, 450): 1267, (108, 347): 1268, (100, 111): 1269, (114, 293): 1270, (696, 107): 1271, (51, 327): 1272, (455, 51): 1273, (268, 379): 1274, (670, 32): 1275}\n\n\n\nassert tokenizer.decode(tokenizer.encode(taylor_text))==taylor_text, 'enc/dec mismatch'\n\n\ntokenizer.decode(tokenizer.encode(taylor_text))\n\n'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaThe Free Encyclopedia\\n\\nSearch\\nCreate account\\nLog in\\n\\nPersonal tools\\nContents  hide\\n(Top)\\nLife and career\\nToggle Life and career subsection\\nArtistry\\nToggle Artistry subsection\\nAccolades and achievements\\nCultural status\\nToggle Cultural status subsection\\nWealth\\nToggle Wealth subsection\\nDiscography\\nFilmography\\nTours\\nSee also\\nFootnotes\\nReferences\\nToggle References subsection\\nExternal links\\nTaylor Swift\\n\\n136 languages\\nArticle\\nTalk\\nRead\\nView source\\nView history\\n\\nTools\\n Featured article\\nPage semi-protected\\nFrom Wikipedia, the free encyclopedia\\nFor the album, see Taylor Swift (album).\\nTaylor Swift\\nPortrait of Taylor Swift in a cocktail dress\\nSwift at the 2023 MTV Video Music Awards\\nBorn\\tTaylor Alison Swift\\nDecember 13, 1989 (age 34)\\nWest Reading, Pennsylvania, US\\nOccupations\\nSinger-songwriter producer director businesswoman actress\\nYears active\\t2004–present\\nWorks\\nAlbumssinglessongsvideosperformances\\nRelatives\\nAustin Swift (brother)\\nMarjorie Finlay (grandmother)\\nAwards\\tFull list\\nMusical career\\nOrigin\\tNashville, Tennessee, US\\nGenres\\nPop country folk rock alternative\\nInstruments\\nVocals guitar banjo piano ukulele\\nLabels\\nRCA Republic Big Machine\\nWebsite\\twww.taylorswift.com Edit this at Wikidata\\nSignature\\n\\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry, songwriting, and entrepreneurship have influenced the music industry, popular culture, and politics, and her life is a subject of widespread media coverage.\\nSwift began professional songwriting at 14 and signed with Big Machine Records in 2005 to become a country singer. She released six studio albums under the label, four of them to country radio, starting with Taylor Swift (2006). Her next, Fearless (2008), explored country pop, and its singles \"Love Story\" and \"You Belong with Me\" catapulted her to mainstream fame. Speak Now (2010) infused rock influences, while Red (2012) experimented with electronic elements and featured Swift\\'s first Billboard Hot 100 number-one song, \"We Are Never Ever Getting Back Together\". She departed from her country image with 1989 (2014), a synth-pop album supported by the chart-topping songs \"Shake It Off\", \"Blank Space\", and \"Bad Blood\". Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single \"Look What You Made Me Do\".\\nAfter signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylor\\'s Version after a dispute with Big Machine. These albums spawned the number-one songs \"Cruel Summer\", \"Cardigan\", \"Willow\", \"Anti-Hero\", \"All Too Well\", and \"Is It Over Now?\". Her Eras Tour (2023–2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed several music videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021).\\nOne of the world\\'s best-selling musicians, with over 200 million records sold as of 2019, Swift has been named Global Recording Artist of the Year three times by the International Federation of the Phonographic Industry, whereas six of her albums have opened with over a million sales in a week. She is the highest-grossing female touring act, the most-streamed woman on Spotify and Apple Music, and the first billionaire with music as the main source of income. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stone\\'s 100 Greatest Songwriters of All Time, Billboard\\'s Greatest of All Time Artists, and Forbes\\' World\\'s 100 Most Powerful Women. Her accolades include 14 Grammy Awards (featuring a record four Album of the Year wins), a Primetime Emmy Award, 40 American Music Awards, 40 Billboard Music Awards, and 23 MTV Video Music Awards.\\nLife and career\\nEarly life\\n\\nSwift\\'s childhood home in Wyomissing, Pennsylvania\\nTaylor Alison Swift was born on December 13, 1989,[1] in West Reading, Pennsylvania.[2] She is named after singer-songwriter James Taylor.[3] Her father, Scott Kingsley Swift, is a former stockbroker for Merrill Lynch[4] and her mother, Andrea Gardner Swift (née Finlay), worked for a time as a mutual fund marketing executive.[5] Taylor has a younger brother, actor Austin Swift.[6]\\nSwift\\'s mother is of Scottish and German descent, and her father is of Scottish and English descent with distant Italian ancestry.[7][8] Swift\\'s paternal great-great-grandfather, Charles Carmine Antonio Baldi, was an Italian immigrant entrepreneur and community leader who opened several businesses in Philadelphia in the 1800s.[9][10][8] Her maternal grandmother, Marjorie (née Moehlenkamp) Finlay, was an opera singer.[11]\\nSwift spent her early years on a Christmas tree farm that her father had purchased from one of his clients.[12][13] She is a Christian.[14] She attended preschool and kindergarten at Alvernia Montessori School, run by Bernardine Franciscan sisters,[15] before transferring to the Wyndcroft School.[16] The family moved to a rented house in Wyomissing, Pennsylvania,[17] where Swift attended Wyomissing Area Junior/Senior High School.[18]\\nAt age nine, Swift became interested in musical theater and performed in four Berks Youth Theatre Academy productions.[19] She also traveled regularly to New York City for vocal and acting lessons.[20] Swift later shifted her focus toward country music, inspired by Shania Twain\\'s songs, which made her \"want to just run around the block four times and daydream about everything\".[21] She spent weekends performing at local festivals and events.[22][23] After watching a documentary about Faith Hill, Swift felt she needed to move to Nashville, Tennessee, to pursue a career in music.[24] She traveled there with her mother at age eleven to visit record labels and submitted demo tapes of Dolly Parton and Dixie Chicks karaoke covers.[25] She was rejected, however, because \"everyone in that town wanted to do what I wanted to do. So, I kept thinking to myself, I need to figure out a way to be different.\"[26] She spent summers in Stone Harbor, New Jersey until she was 14 years old, performing in a local coffee shop.[27][28]\\nWhen Swift was around 12 years old, musician Ronnie Cremer taught her to play guitar. Cremer helped with her first efforts as a songwriter, leading her to write \"Lucky You\".[29] In 2003, Swift and her parents started working with New York–based talent manager Dan Dymtrow. With his help, Swift modeled for Abercrombie & Fitch as part of their \"Rising Stars\" campaign, had an original song included on a Maybelline compilation CD, and met with major record labels.[30] After performing original songs at an RCA Records showcase, Swift, then 13 years old, was given an artist development deal and began making frequent trips to Nashville with her mother.[31][32][33] To help Swift break into the country music scene, her father transferred to Merrill Lynch\\'s Nashville office when she was 14 years old, and the family relocated to Hendersonville, Tennessee.[12][34] Swift attended Hendersonville High School[35] before transferring to Aaron Academy after two years, which better accommodated her touring schedule through homeschooling. She graduated one year early.[36][37]\\n2004–2008: Career beginnings and first album\\nIn Nashville, Swift worked with experienced Music Row songwriters such as Troy Verges, Brett Beavers, Brett James, Mac McAnally, and the Warren Brothers[38][39] and formed a lasting working relationship with Liz Rose.[40] They began meeting for two-hour writing sessions every Tuesday afternoon after school.[41] Rose called the sessions \"some of the easiest I\\'ve ever done. Basically, I was just her editor. She\\'d write about what happened in school that day. She had such a clear vision of what she was trying to say. And she\\'d come in with the most incredible hooks.\" Swift became the youngest artist signed by the Sony/ATV Tree publishing house,[42] but left then BMG-owned RCA Records (later bought by Sony Music) at the age of 14 due to the label\\'s lack of care and them \"cut[ting] other people\\'s stuff\". She was also concerned that development deals can shelve artists[33][23] and recalled: \"I genuinely felt that I was running out of time. I wanted to capture these years of my life on an album while they still represented what I was going through.\"[43]\\nTaylor Swift singing on a microphone and playing a guitar\\nSwift opening for Brad Paisley in 2007. To promote her first album, she opened tours for other country musicians in 2007 and 2008.[44]\\nAt an industry showcase at Nashville\\'s Bluebird Cafe in 2005, Swift caught the attention of Scott Borchetta, a DreamWorks Records executive who was preparing to form an independent record label, Big Machine Records. She had first met Borchetta in 2004.[45] She was one of Big Machine\\'s first signings,[33] and her father purchased a three-percent stake in the company for an estimated $120,000.[46][47] She began working on her eponymous debut album with Nathan Chapman.[23] Swift wrote or co-wrote all album tracks, and co-writers included Rose, Robert Ellis Orrall, Brian Maher, and Angelo Petraglia.[48] Taylor Swift was released on October 24, 2006.[49] Country Weekly critic Chris Neal deemed Swift better than previous aspiring teenage country singers because of her \"honesty, intelligence and idealism\".[50] The album peaked at number five on the US Billboard 200, on which it spent 157 weeks—the longest stay on the chart by any release in the US in the 2000s decade.[51] Swift became the first female country music artist to write or co-write every track on a US platinum-certified debut album.[52]\\nBig Machine Records was still in its infancy during the June 2006 release of the lead single, \"Tim McGraw\", which Swift and her mother helped promote by packaging and sending copies of the CD single to country radio stations. As there was not enough furniture at the label yet, they would sit on the floor to do so.[53] She spent much of 2006 promoting Taylor Swift with a radio tour and television appearances; she opened for Rascal Flatts on select dates during their 2006 tour,[54] as a replacement for Eric Church.[55] Borchetta said that although record industry peers initially disapproved of his signing a 15-year-old singer-songwriter, Swift tapped into a previously unknown market—teenage girls who listen to country music.[53][12]\\nFollowing \"Tim McGraw\", four more singles were released throughout 2007 and 2008: \"Teardrops on My Guitar\", \"Our Song\", \"Picture to Burn\" and \"Should\\'ve Said No\". All appeared on Billboard\\'s Hot Country Songs, with \"Our Song\" and \"Should\\'ve Said No\" reaching number one. With \"Our Song\", Swift became the youngest person to single-handedly write and sing a number-one song on the chart.[56] \"Teardrops on My Guitar\" reached number thirteen on the US Billboard Hot 100.[57] Swift also released two EPs, The Taylor Swift Holiday Collection in October 2007 and Beautiful Eyes in July 2008.[58][59] She promoted her debut album extensively as the opening act for other country musicians\\' tours in 2006 and 2007, including those by George Strait,[60] Brad Paisley,[61] and Tim McGraw and Faith Hill.[62]\\nSwift won multiple accolades for Taylor Swift. She was one of the recipients of the Nashville Songwriters Association\\'s Songwriter/Artist of the Year in 2007, becoming the youngest person given the title.[63] She also won the Country Music Association\\'s Horizon Award for Best New Artist,[64] the Academy of Country Music Awards\\' Top New Female Vocalist,[65] and the American Music Awards\\' Favorite Country Female Artist honor.[66] She was also nominated for Best New Artist at the 50th Annual Grammy Awards.[67] In 2008, she opened for Rascal Flatts again[68] and briefly dated the singer Joe Jonas.[69][70]\\n2008–2010: Fearless\\nTaylor Swift in 2009\\nSwift at the 2009 premiere of Hannah Montana: The Movie. She had a cameo appearance in the film and wrote two songs for its soundtrack.[71][72]\\nSwift\\'s second studio album, Fearless, was released on November 11, 2008, in North America,[73] and in March 2009 in other markets.[74] Critics lauded Swift\\'s honest and vulnerable songwriting in contrast to other teenage singers.[75] Five singles were released in 2008–2009: \"Love Story\", \"White Horse\", \"You Belong with Me\", \"Fifteen\", and \"Fearless\". The first single peaked at number four on the Billboard Hot 100 and number one in Australia.[57][76] It was the first country song to top Billboard\\'s Pop Songs chart.[77] \"You Belong with Me\" was the album\\'s highest-charting single on the Billboard Hot 100, peaking at number two,[78] and was the first country song to top Billboard\\'s all-genre Radio Songs chart.[79] All five singles were Hot Country Songs top-10 entries, with \"Love Story\" and \"You Belong with Me\" topping the chart.[80] Fearless became her first number-one album on the Billboard 200 and 2009\\'s top-selling album in the US.[81] The Fearless Tour, Swift\\'s first headlining concert tour, grossed over $63 million.[82] Journey to Fearless, a documentary miniseries, aired on television and was later released on DVD and Blu-ray.[83] Swift performed as a supporting act for Keith Urban\\'s Escape Together World Tour in 2009.[84]\\nIn 2009, the music video for \"You Belong with Me\" was named Best Female Video at the 2009 MTV Video Music Awards.[85] Her acceptance speech was interrupted by rapper Kanye West,[86] an incident that became the subject of controversy, widespread media attention and Internet memes.[87] That year she won five American Music Awards, including Artist of the Year and Favorite Country Album.[88] Billboard named her 2009\\'s Artist of the Year.[89] She won Video of the Year and Female Video of the Year for \"Love Story\" at the 2009 CMT Music Awards, where she made a parody video of the song with rapper T-Pain called \"Thug Story\".[90] At the 52nd Annual Grammy Awards, Fearless was named Album of the Year and Best Country Album, and \"White Horse\" won Best Country Song and Best Female Country Vocal Performance. Swift was the youngest artist to win Album of the Year.[note 1] At the 2009 Country Music Association Awards, Swift won Album of the Year for Fearless and was named Entertainer of the Year, the youngest person to win the honor.[93]\\nSwift featured on John Mayer\\'s single \"Half of My Heart\" and Boys Like Girls\\' single \"Two Is Better Than One\", the latter of which she co-wrote.[94][95] She co-wrote and recorded \"Best Days of Your Life\" with Kellie Pickler,[96] and wrote two songs for the Hannah Montana: The Movie soundtrack—\"You\\'ll Always Find Your Way Back Home\" and \"Crazier\".[72] She contributed two songs to the Valentine\\'s Day soundtrack, including the single \"Today Was a Fairytale\", which was her first number-one on the Canadian Hot 100 and peaked at number two on the US Hot 100.[97][98] While shooting her film debut Valentine\\'s Day in October 2009, Swift dated co-star Taylor Lautner.[99] In 2009, she made her television debut as a rebellious teenager in an CSI: Crime Scene Investigation episode,[100] and she hosted and performed as the musical guest on Saturday Night Live; she was the first host ever to write their own opening monologue.[101][102]\\n2010–2014: Speak Now and Red\\nSwift singing into a mic while playing a banjo\\nSwift performing at the Speak Now World Tour in 2012\\nIn August 2010, Swift released \"Mine\", the lead single from her third studio album, Speak Now. The single entered the Hot 100 at number three.[103] Swift wrote the album alone and co-produced every track.[104] The album was released on October 25, 2010,[105] opening atop the Billboard 200 with over one million copies sold.[106] It became the fastest-selling digital album by a female artist, with 278,000 downloads in a week.[107] Critics appreciated Swift\\'s grown-up perspectives:[108] Rob Sheffield of Rolling Stone wrote, \"in a mere four years, the 20-year-old Nashville firecracker has put her name on three dozen or so of the smartest songs released by anyone in pop, rock or country.\"[109] \"Back to December\", \"Mean\", \"The Story of Us\", \"Sparks Fly\", and \"Ours\" became subsequent singles, with the latter two reaching number one on the Hot Country Songs[80] and the first two peaking in the top ten in Canada.[98] She dated actor Jake Gyllenhaal in 2010.[110]\\nAt the 54th Annual Grammy Awards in 2012, Swift won Best Country Song and Best Country Solo Performance for \"Mean\", which she performed during the ceremony.[111] Swift won other awards for Speak Now, including Songwriter/Artist of the Year by the Nashville Songwriters Association (2010 and 2011),[112][113] Woman of the Year by Billboard (2011),[114] and Entertainer of the Year by the Academy of Country Music (2011 and 2012)[115] and the Country Music Association in 2011.[116] At the American Music Awards of 2011, Swift won Artist of the Year and Favorite Country Album.[117] Rolling Stone named Speak Now amongst its \"50 Best Female Albums of All Time\" (2012), writing: \"She might get played on the country station, but she\\'s one of the few genuine rock stars we\\'ve got these days, with a flawless ear for what makes a song click.\"[118]\\nThe Speak Now World Tour ran from February 2011 to March 2012 and grossed over $123 million,[119] followed up by the live album, Speak Now World Tour: Live.[120] She contributed two original songs to The Hunger Games soundtrack album: \"Eyes Open\" and \"Safe & Sound\", co-written and recorded with the Civil Wars and T-Bone Burnett. \"Safe & Sound\" won the Grammy Award for Best Song Written for Visual Media and was nominated for the Golden Globe Award for Best Original Song.[121][122] Swift featured on B.o.B\\'s single \"Both of Us\", released in May 2012.[123] She dated Conor Kennedy that year.[124]\\nTaylor Swift on the Red Tour\\nSwift on the Red Tour (2013)\\nIn August 2012, Swift released \"We Are Never Ever Getting Back Together\", the lead single from her fourth studio album, Red. It became her first number one single in the US and New Zealand,[125][126] and became the fastest-selling single in digital history.[127] Other singles from the album were \"Begin Again\", \"I Knew You Were Trouble\", \"22\", \"Everything Has Changed\", \"The Last Time\", and \"Red\". \"I Knew You Were Trouble\" reached the top five on charts in Australia, Canada, Denmark, Ireland, New Zealand, the UK and the US.[128] \"Begin Again\", \"22\", and \"Red\" reached the top 20 in the US.[57] On Red, released on October 22, 2012,[129] Swift worked with Chapman and Rose, as well as the new producers Max Martin and Shellback.[130] It incorporated many pop and rock styles such as heartland rock, dubstep and dance-pop.[131] Randall Roberts of Los Angeles Times said Swift \"strives for something much more grand and accomplished\" with Red.[132] It opened at number one on the Billboard 200 with 1.21 million sales.[133] Red was Swift\\'s first number-one album in the UK.[134] It earned several accolades, including four nominations at the 56th Annual Grammy Awards (2014).[135] Swift received American Music Awards for Best Female Country Artist in 2012, Artist of the Year in 2013,[136][137] and the Nashville Songwriters Association\\'s Songwriter/Artist Award for the fifth and sixth consecutive years.[138] The Red Tour ran from March 2013 to June 2014 and grossed over $150 million, becoming the highest-grossing country tour ever.[139] Swift was honored with the Pinnacle Award, making her the second recipient of the accolade after Garth Brooks.[140] During this time, she briefly dated the English singer Harry Styles.[141]\\nIn 2013, Swift recorded \"Sweeter than Fiction\", a song she wrote and produced with Jack Antonoff for the One Chance soundtrack. The song received a Best Original Song nomination at the 71st Golden Globe Awards.[142] She provided guest vocals for Tim McGraw\\'s song \"Highway Don\\'t Care\", also featuring Keith Urban.[143] Swift performed \"As Tears Go By\" with the Rolling Stones in Chicago, Illinois, as part of the band\\'s 50 & Counting tour,[144] and joined Florida Georgia Line at their set at the 2013 Country Radio Seminar to sing \"Cruise\".[145] Swift voiced Audrey in the animated film The Lorax (2012),[146] made a cameo in the sitcom New Girl (2013),[147] and had a supporting role in the dystopian film The Giver (2014).[148]\\n2014–2018: 1989 and Reputation\\nSwift performing on a mic, dressed in a blue skirt\\nSwift at the 1989 World Tour, the highest-grossing tour of 2015\\nIn March 2014, Swift began living in New York City.[note 2] She hired Tree Paine as her publicist[151] and worked on her fifth studio album, 1989, with the producers Jack Antonoff, Max Martin, Shellback, Imogen Heap, Ryan Tedder, and Ali Payami.[152] She promoted the album extensively, including inviting fans to secret album-listening sessions.[153] 1989 was released on October 27, 2014, and opened atop the Billboard 200 with 1.28 million copies sold.[154] Its singles \"Shake It Off\", \"Blank Space\" and \"Bad Blood\" reached number one in Australia, Canada and the US, the first two making Swift the first woman to replace herself at the Hot 100 top spot;[155] other singles include \"Style\", \"Wildest Dreams\", \"Out of the Woods\" and \"New Romantics\".[156] The 1989 World Tour (2015) was the highest-grossing tour of the year with $250 million in total revenue.[157]\\nPrior to 1989\\'s release, Swift stressed the importance of albums to artists and fans.[158] In November 2014, she removed her entire catalog from Spotify, arguing that its ad-supported, free service undermined the premium service, which provides higher royalties for songwriters.[159] In a June 2015 open letter, Swift criticized Apple Music for not offering royalties to artists during the streaming service\\'s free three-month trial period and stated that she would pull 1989 from the catalog.[160] The following day, Apple Inc. announced that it would pay artists during the free trial period,[161] and Swift agreed to let 1989 on the streaming service.[162] She then returned her entire catalog plus 1989 to Spotify, Amazon Music and Google Play and other digital streaming platforms in June 2017.[163] Swift was named Billboard\\'s Woman of the Year in 2014, becoming the first artist to win the award twice.[164] At the 2014 American Music Awards, Swift received the inaugural Dick Clark Award for Excellence.[165] On her 25th birthday in 2014, the Grammy Museum at L.A. Live opened an exhibit in her honor in Los Angeles that ran until October 4, 2015, and broke museum attendance records.[166][167] In 2015, Swift won the Brit Award for International Female Solo Artist.[168] The video for \"Bad Blood\" won Video of the Year and Best Collaboration at the 2015 MTV Video Music Awards.[169] At the 58th Grammy Awards (2016), 1989 won Album of the Year and Best Pop Vocal Album, making Swift the first woman and fifth act overall to win Album of the Year twice.[170]\\nSwift wearing a sparkling blazer singing on a mic\\nSwift on her Reputation Stadium Tour (2018), the highest-grossing North American tour ever\\nSwift dated the Scottish DJ Calvin Harris from March 2015 to June 2016.[171] They co-wrote the song \"This Is What You Came For\", featuring vocals from the Barbadian singer Rihanna; Swift was initially credited under the pseudonym Nils Sjöberg.[172] In April 2016, Swift criticized the lyrics of Kanye West\\'s single \"Famous\", in which he sings \"I made that bitch famous\" in reference to his interruption of her acceptance speech at the 2009 MTV Video Music Awards. West claimed he had received her approval for the line, and his then-wife Kim Kardashian released video clips of Swift and West discussing the single amicably over the phone; a full recording leaked in 2020 established that West did not disclose that he would call her a \"bitch\".[173][174]\\nAfter briefly dating the English actor Tom Hiddleston,[175] Swift entered a six-year relationship with the English actor Joe Alwyn in September 2016.[176][177][178] She wrote the song \"Better Man\" for the band Little Big Town, which earned her the Song of the Year award at the 51st CMA Awards.[179] Swift and English singer Zayn Malik released the joint single \"I Don\\'t Wanna Live Forever\" for Fifty Shades Darker: Original Motion Picture Soundtrack (2017). The song reached number two in the US.[180]\\nIn August 2017, Swift successfully countersued David Mueller, a former radio jockey for KYGO-FM, who sued her for damages from loss of employment. Four years earlier, she informed Mueller\\'s bosses that he had sexually assaulted her by groping her at an event.[181] Also that month, after a one-year hiatus from the spotlight, Swift cleared her social media accounts and released \"Look What You Made Me Do\" as the lead single from her sixth album, Reputation.[182][183] The single was Swift\\'s first UK number-one single.[184] It topped charts in Australia, Ireland, New Zealand, and the US.[185] Reputation, released on November 10, 2017,[186] incorporated electropop, hip hop, R&B, and EDM.[187] Reviews praised Swift\\'s mature artistry, but some denounced the themes of fame and gossip.[188] The album opened atop the Billboard 200 with 1.21 million US sales[189] and topped the charts in the UK, Australia, and Canada.[190] Its singles \"...Ready for It?\", \"End Game\" (featuring Ed Sheeran and Future), and \"Delicate\" were released to pop radio.[191] Reputation was nominated for a Grammy Award for Best Pop Vocal Album.[192] Swift featured on the country duo Sugarland\\'s \"Babe\" (2018).[193]\\nAt the 2018 American Music Awards, Swift won four awards, which made her accumulate 23 trophies in total and become the AMAs\\' most awarded female musician, surpassing Whitney Houston.[194] The same year, she embarked on her Reputation Stadium Tour,[195] which became the highest-grossing North American concert tour in history and grossed $345.7 million worldwide.[196]\\n2018–2020: Lover, Folklore, and Evermore\\nIn November 2018, she signed a new deal with the Universal Music Group; her subsequent releases were promoted by Republic Records. Swift said the contract included a provision for her to maintain ownership of her masters. In addition, in the event that Universal sold any part of its stake in Spotify, it agreed to distribute a non-recoupable portion of the proceeds among its artists.[197] Vox called it a huge commitment from Universal, which was \"far from assured\" until Swift intervened.[198]\\nA portrait of Swift\\nSwift at the American Music Awards of 2019, where she was named Artist of the Decade\\nSwift\\'s first album with Republic Records, Lover, was released on August 23, 2019.[199] Besides Antonoff, she worked with Louis Bell, Frank Dukes, and Joel Little.[200] Lover was her sixth consecutive album to sell more than 500,000 US copies in one week.[201] Critics commended the album\\'s free-spirited mood and emotional intimacy.[202][203] The singles \"Me!\" and \"You Need to Calm Down\" both peaked at number two on the Hot 100,[204] and other singles were the top-10 single \"Lover\", the top-40 single \"The Man\",[57] and the 2023 resurgent success, chart topper \"Cruel Summer\".[205] Lover was the world\\'s best-selling album by a solo artist of 2019,[206] and along with its singles earned nominations at the 62nd Annual Grammy Awards in 2020.[207] At the 2019 MTV Video Music Awards, \"Me!\" won Best Visual Effects, and \"You Need to Calm Down\" won Video of the Year and Video for Good. Swift was the first female and second artist overall to win Video of the Year for a video that they directed.[208]\\nWhile promoting Lover, Swift became embroiled in a public dispute with the talent manager Scooter Braun and Big Machine over the purchase of the masters of her back catalog.[209][210] Swift said she had been trying to buy the masters, but Big Machine would only allow her to do so if she exchanged one new album for each older one under a new contract, which she refused to sign.[209][211] Swift began re-recording her back catalog in November 2020.[212] Besides music, she played Bombalurina in the film adaptation of Andrew Lloyd Webber\\'s musical Cats (2019), for which she co-wrote and recorded the Golden Globe-nominated original song \"Beautiful Ghosts\".[213][214] Critics panned the film but praised Swift\\'s performance.[215] The documentary Miss Americana, which chronicled parts of Swift\\'s life and career, premiered at the 2020 Sundance Film Festival.[216] Swift signed a global publishing deal with Universal Music Publishing Group in February 2020 after her 16-year contract with Sony/ATV expired.[217]\\nAmidst the COVID-19 pandemic in 2020, Swift surprise-released two \"sister albums\" that she recorded with Antonoff and Aaron Dessner: Folklore on July 24, and Evermore on December 11.[218][219] Alwyn co-wrote and co-produced a few songs under the pseudonym William Bowery.[220] Both explore indie folk with a more muted production compared to her previous upbeat pop songs[221][222] and earned Swift widespread critical acclaim and artistic recognition.[223][224] Each album was supported by three singles catering to US pop, country, and triple A radio formats. The singles were \"Cardigan\", \"Betty\", and \"Exile\" from Folklore, and \"Willow\", \"No Body, No Crime\", and \"Coney Island\" from Evermore.[225] Folklore made Swift the first woman to win the Grammy Award for Album of the Year three times at the 63rd Annual Grammy Awards[226] and was the best-selling album of 2020 in the US.[227] Swift became the first artist to debut a US number-one album and a number-one song at the same time with Folklore\\'s \"Cardigan\".[228] At the 2020 American Music Awards, she won three awards, including Artist of the Year for a record third consecutive time.[229] According to Billboard, she was 2020\\'s highest-paid musician in the US and highest-paid solo musician worldwide.[230]\\n2020–2023: Re-recordings and Midnights\\n\\nSwift performing in 2022\\nFollowing the masters dispute, Swift released re-recordings of her first six studio albums, beginning with Fearless (Taylor\\'s Version) and Red (Taylor\\'s Version) in April and November 2021, respectively. Both peaked atop the Billboard 200,[231] and the former was the first re-recorded album to do so.[232] Fearless (Taylor\\'s Version) was preceded by \"Love Story (Taylor\\'s Version)\", which made her the second artist after Dolly Parton to have both the original and re-recorded versions of a song reach number one on the Hot Country Songs chart.[233] Red (Taylor\\'s Version) was supported by \"All Too Well (10 Minute Version)\", which became the longest song in history to top the Hot 100.[234] The song was accompanied by a short film, which won a Grammy Award for Best Music Video[235] and Swift\\'s record third MTV Video Music Award for Video of the Year.[236]\\nSwift\\'s tenth studio album, Midnights, was released on October 21, 2022.[237] Characterized by a restrained electropop[238][239] and synth-pop[240] sound, the album was dubbed by Rolling Stone critics as an instant classic.[241][242] The album was her fifth to open atop the Billboard 200 with first-week sales of over one million copies and broke various sales and streaming records,[243] including the most single-day streams and most single-week streams on Spotify.[244] Its tracks, led by single \"Anti-Hero\", monopolized the top 10 of the Hot 100, making Swift the first artist to do so.[245] Two other singles, \"Lavender Haze\" and \"Karma\", peaked at number two on the Hot 100.[246] Swift won nine awards at the 2023 MTV Video Music Awards, including Video of the Year (\"Anti-Hero\") for a record fourth time.[247] At the 66th Annual Grammy Awards, she received Best Pop Vocal Album, and her fourth Album of the Year—the most for any artist.[248]\\nSwift released the third re-recorded album, Speak Now (Taylor\\'s Version), on July 7, 2023, becoming the woman with the most number-one albums (12) in Billboard 200 history, surpassing Barbra Streisand.[249] 1989 (Taylor\\'s Version), released on October 27, 2023, became Swift\\'s record-extending sixth album to sell one million copies in a single week in the US and surpassed Midnights for her career\\'s largest album sales week.[250] Its single \"Is It Over Now?\" debuted atop the Billboard Hot 100. Swift was 2023\\'s most streamed artist on Spotify,[251] Apple Music,[252] and Amazon Music;[253] the first act to place number one on the year-end Billboard top artists list in three different decades (2009, 2015 and 2023);[254] and the first living artist to simultaneously chart five albums in the top 10 of the Billboard 200.[255] She had five out of the 10 best-selling albums of 2023 in the United States, a record since Luminate began tracking US music sales in 1991.[256][257]\\nBeyond her albums, Swift featured on five songs from 2021 to 2023: \"Renegade\" and \"Birch\" by Big Red Machine,[258] a remix of \"Gasoline\" by Haim,[259] \"The Joker and the Queen\" by Ed Sheeran,[260] and \"The Alcott\" by the National.[261] For the soundtrack of the 2022 film Where the Crawdads Sing, she recorded \"Carolina\", which received nominations for Best Original Song at the Golden Globes and Best Song Written for Visual Media at the Grammy Awards.[262] Outside of music, Swift had a supporting role in the 2022 period comedy film Amsterdam and has signed to direct an upcoming feature film for Searchlight Pictures.[263][264]\\n2023–present: The Eras Tour and The Tortured Poets Department\\nSwift singing into a mic\\nSwift on the Eras Tour in 2023\\nIn March 2023, Swift embarked on the Eras Tour, a retrospective tour covering all her studio albums. Media outlets extensively covered the tour\\'s cultural and economic impact,[265] and its US leg broke the record for the most tickets sold in a day.[245] Ticketmaster received public and political criticisms for mishandling the tour\\'s ticket sales.[266] The Eras Tour became the highest-grossing tour in history, collecting over $1 billion.[267][268] Its concert film, released to theaters worldwide on October 13, 2023, grossed over $250 million to become the highest-grossing concert film, and was nominated for the Golden Globe Award for Cinematic and Box Office Achievement.[269][270] Swift\\'s music releases, touring, and related activities culminated in an unprecedented height of popularity post-pandemic.[271] Music Business Worldwide remarked that she entered a \"new stratosphere of global career success\" in 2023.[272]\\nSwift began dating Kansas City Chiefs\\' tight end Travis Kelce in 2023.[273] In January 2024, AI-generated fake pornographic images portraying Swift were posted to X (formerly Twitter) and spread to other social media platforms, spurring criticism and demands for legal reform.[274][275] At the 66th Grammy Awards, Swift announced her eleventh studio album, The Tortured Poets Department, set for release on April 19, 2024.[248][276]\\nArtistry\\nInfluences\\nOne of Swift\\'s earliest memories of music is listening to her maternal grandmother, Marjorie Finlay, sing in church.[5] As a child, she enjoyed Disney film soundtracks: \"My parents noticed that, once I had run out of words, I would just make up my own.\"[277] Swift said she owes her confidence and \"fascination with writing and storytelling\" to her mother, who helped her prepare for class presentations as a child.[278][279]\\nSwift was drawn to the storytelling aspect of country music,[280] which was introduced to the genre by female country artists of the 1990s: Shania Twain, Faith Hill, and the Dixie Chicks.[281][282] Twain, both as a songwriter and performer, was her biggest musical influence.[283] Hill was Swift\\'s childhood role model, and she would often imitate her.[284] She admired the Chicks\\' defiant attitude and the way they played their instruments,[285] and was also influenced by older country stars like Patsy Cline, Loretta Lynn, Tammy Wynette, and Dolly Parton,[22] the last of whom she believes is exemplary to female songwriters.[114] As a songwriter, Swift was influenced by Joni Mitchell\\'s emotional and autobiographical lyrics, highlighting Mitchell\\'s 1971 album Blue as a favorite \"because it explores somebody\\'s soul so deeply\".[286] She also spoke of influence from 1990s songwriters such as Melissa Etheridge, Sarah McLachlan, and Alanis Morissette,[287][288] and alt-country artists like Patty Griffin[289] and Lori McKenna.[290]\\nVarious pop and rock artists have also influenced Swift. She lists Paul McCartney, Bruce Springsteen, Emmylou Harris, and Kris Kristofferson as her career role models.[12][291] 1989 was influenced by some of her favorite 1980s pop acts, including Peter Gabriel, Annie Lennox, Phil Collins, and Madonna.[292][293] She also cited Keith Urban\\'s musical style and Fall Out Boy\\'s lyrics as major influences.[294][295]\\nGenres\\n\"If there\\'s one thing that Swift has proven throughout her career, it\\'s that she refuses to be put in a box. Her ever-evolving sound took her from country darling to pop phenom to folk\\'s newest raconteur.\"\\n—The Recording Academy, 2021[296]\\nSwift is known for venturing into various music genres and undergoing artistic reinventions,[297][264] having been described as a \"music chameleon\".[298][299] She self-identified as a country musician until 2012, when she released her fourth studio album, Red.[300] Her albums were promoted to country radio, but music critics noted wide-ranging styles of pop and rock[301][302] and said that the melodies of her songs were rooted in pop, and the country music elements were limited to instruments such as banjo, mandolin, and fiddle, and her slight twang.[303][304] Some commented that her country music identity was an indicator of her narrative songwriting rather than musical style.[305][306] Although the Nashville music industry was receptive of Swift\\'s status as a country musician, critics accused her of abandoning her roots in favor of crossover success in mainstream pop.[307][308] Red\\'s eclectic pop, rock, and electronic styles intensified the critical debate, to which Swift responded, \"I leave the genre labeling to other people.\"[309]\\nMusic journalist Jody Rosen commented that by originating her musical career in Nashville, Swift made a \"bait-and-switch maneuver, planting roots in loamy country soil, then pivoting to pop\".[310] She abandoned her country music identity in 2014 with the release of her synth-pop fifth studio album, 1989. Swift described it as her first \"documented, official pop album\".[311] Her subsequent albums Reputation (2017) and Lover (2019) have an upbeat pop production; the former incorporates hip hop, trap, and EDM elements.[312][313][314] Midnights (2022), on the other hand, is distinguished by a more experimental, \"subdued and amorphous pop sound\".[315][316] Although reviews of Swift\\'s pop albums were generally positive, some critics lamented that the pop music production indicated Swift\\'s pursuit of mainstream success, eroding her authenticity as a songwriter nurtured by her country music background—a criticism that has been retrospectively described as rockist.[317][318] Musicologist Nate Sloan remarked that Swift\\'s pop music transition was rather motivated by her need to expand her artistry.[319] Swift eschewed mainstream pop in favor of alternative, folk and indie rock styles with her 2020 studio albums Folklore and Evermore.[320][321] Clash said her career \"has always been one of transcendence and covert boundary-pushing\", reaching a point at which \"Taylor Swift is just Taylor Swift\", not defined by any genre.[322]\\nVoice\\n\\n\"Cardigan\"\\nDuration: 22 seconds.0:22\\nSwift uses her lower register in \"Cardigan\" (2020).[323]\\n\"Lavender Haze\"\\nDuration: 18 seconds.0:18\\n\"Lavender Haze\" (2022) features Swift\\'s falsetto vocals in the refrain.[324]\\nProblems playing these files? See media help.\\nSwift possesses a mezzo-soprano vocal range,[325] and a generally soft but versatile timbre.[326][327] As a country singer, her vocals were criticized by some as weak and strained compared to those of her contemporaries.[328] Swift admitted her vocal ability often concerned her in her early career and has worked hard to improve.[329] Reviews of her vocals remained mixed after she transitioned to pop music with 1989; critics complained that she lacked proper technique but appreciated her usage of her voice to communicate her feelings to the audience, prioritizing \"intimacy over power and nuance\".[330] They also praised her for refraining from correcting her pitch with Auto-Tune.[331]\\nThe Los Angeles Times remarked that Swift\\'s defining vocal feature is her attention to detail to convey an exact feeling—\"the line that slides down like a contented sigh or up like a raised eyebrow\".[332] With Reputation, critics noted she was \"learning how to use her voice as a percussion instrument of its own\",[333] swapping her \"signature\" expressive vocals for \"cool, conversational, detached\" cadences and rhythms similar to hip hop and R&B styles.[334][335][336] Alternative Press stated that her \"evocative\" vocal stylings are more reminiscent of pop-punk and emo genres.[337]\\nReviews of Swift\\'s later albums and performances were more appreciative of her vocals, finding them less nasal, richer, more resonant, and more powerful.[304][338][339] With Folklore and Evermore, Swift received praise for her sharp and agile yet translucent and controlled voice.[340][341][342] Pitchfork described it as \"versatile and expressive\".[343] With her 2021 re-recorded albums, critics began to praise the mature, deeper and \"fuller\" tone of her voice.[344][345][346] An i review said Swift\\'s voice is \"leagues better now\".[347] The Guardian highlighted \"yo-yoing vocal yelps\" and passionate climaxes as the trademarks of Swift\\'s voice,[348] and that her country twang faded away.[349] Midnights received acclaim for Swift\\'s nuanced vocal delivery.[350] She ranked 102nd on the 2023 Rolling Stone list of the 200 Greatest Singers of All Time.[327] In a review of the Eras Tour, The New Yorker critic Amanda Petrusich praised the clarity and tone of Swift\\'s live vocals.[351] Musicologist Alyssa Barna said that Swift\\'s timbre is \"breathy and bright\" in her upper register and \"full and dark\" in the lower.[222]\\nSongwriting\\nFurther information: List of songs by Taylor Swift\\nSwift has been referred to as one of the greatest songwriters ever by several publications.[352][353][354] Literature scholars like Jonathan Bate and Stephanie Burt have noted that her literary and melodic sensibility and writing style are rare amongst her peers.[355][356] Swift\\'s bridges are often noted as one of the best aspects of her songs,[357][358] earning her the title \"Queen of Bridges\" from Time.[359] Mojo described her as \"a sharp narrator with a gift for the extended metaphor\".[360]\\nIn The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\".[12] Her personal experiences were a common inspiration for her early songs, which helped her navigate life.[361][362] Her \"diaristic\" technique began with identifying an emotion, followed by a corresponding melody.[363][364] On her first three studio albums, love, heartbreak, and insecurities, from an adolescent perspective, were dominant themes.[365][366] She delved into the tumult of toxic relationships on Red,[367] and embraced nostalgia and post-romance positivity on 1989.[292] Reputation was inspired by the downsides of Swift\\'s fame,[368] and Lover detailed her realization of the \"full spectrum of love\".[369] Other themes in Swift\\'s music include family dynamics, friendship,[370][371] alienation, self-awareness, and tackling vitriol, especially sexism.[279][372]\\nHer confessional lyrics received positive reviews from critics,[373][12][374] who highlighted their vivid details and emotional engagement, which they found uncommon in pop music.[375][376][377] Critics also praised her melodic compositions; Rolling Stone described Swift as \"a songwriting savant with an intuitive gift for verse-chorus-bridge architecture\".[378][379] NPR dubbed Swift \"a master of the vernacular in her lyrics\",[335] remarking that her songs offer emotional engagement because \"the wit and clarity of her arrangements turn them from standard fare to heartfelt disclosures\".[379] Despite the positive reception, The New Yorker stated she was generally portrayed \"more as a skilled technician than as a Dylanesque visionary\".[12] Tabloid media often speculated and linked the subjects of her songs with her ex-lovers, a practice reviewers and Swift herself criticized as sexist.[380][381][382] Aside from clues in album liner notes, Swift avoided talking about the subjects of her songs.[383]\\nOn her 2020 albums Folklore and Evermore, Swift was inspired by escapism and romanticism to explore fictional narratives.[384] She imposed emotions onto imagined characters and story arcs, which liberated her from tabloid attention and suggested new paths for her artistry.[363] Swift explained that she welcomed the new songwriting direction after she stopped worrying about commercial success.[384] According to Spin, she explored complex emotions with \"precision and devastation\" on Evermore.[385] Consequence stated her 2020 albums convinced skeptics of her songwriting prowess, noting her transformation from \"teenage wunderkind to a confident and careful adult\".[358]\\nSwift divides her writing into three types: \"quill lyrics\", songs rooted in antiquated poeticism; \"fountain pen lyrics\", based on modern and vivid storylines; and \"glitter gel pen lyrics\", which are lively and frivolous.[386] Critics note the fifth track of every Swift album as the most \"emotionally vulnerable\" of the album.[387] Awarding her with the Songwriter Icon Award in 2021, the National Music Publishers\\' Association remarked that \"no one is more influential when it comes to writing music today\".[388] The Week deemed her the foremost female songwriter of modern times,[389] and the Nashville Songwriters Association International named her Songwriter-Artist of the Decade in 2022.[245] Swift has also published two original poems: \"Why She Disappeared\" and \"If You\\'re Anything Like Me\".[390]\\nPerformances\\nFurther information: List of Taylor Swift live performances\\n\\nSwift performing on the Reputation Stadium Tour in Seattle in May 2018\\nJournalists have described Swift as one of the best live performers. Often praised for her showmanship and stage presence,[391][392][393][394][395] Swift commands large audiences,[396][397][398] without having to rely on dance like her contemporaries do.[399] According to V magazine\\'s Greg Krelenstein, she possesses \"a rare gift of turning a stadium spectacle into an intimate setting\", irrespective of whether she is \"plucking a guitar or leading an army of dancers\".[400] In a 2008 review of Swift\\'s early performances, Sasha Frere-Jones of The New Yorker called Swift a \"preternaturally skilled\" entertainer with a vibrant stage presence, adding \"she returned the crowd\\'s energy with the professionalism she has shown since the age of fourteen.\"[401] In 2023, Adrian Horton of The Guardian noted her \"seemingly endless stamina\" on the Eras Tour,[402] and i critic Ilana Kaplan called her showmanship \"unparalleled\".[403]\\nCritics have highlighted Swift\\'s versatility as an entertainer, praising her ability to switch onstage personas and performance styles depending on the varying themes and aesthetics of her albums.[404][405] Her concert productions have been characterized by elaborate Broadway theatricality and high technology,[406] and her performances frequently incorporate a live band, with whom she has played and toured since 2007.[407] Swift also often accompanies herself with musical instruments such as electric guitar;[408] acoustic guitar; piano;[409] and sometimes twelve-string guitar,[410][411] banjo,[412] or ukulele.[413] Interacting frequently with the audience, her solo acoustic performances are considered intimate and emotionally resonant, complementing her story-based lyrics and fan connection.[351][414] Lydia Burgham of The Spinoff opined that this intimacy remains \"integral to her singer-songwriter origins\".[415][409] Chris Willman of Variety called Swift \"pop\\'s most approachable superstar\",[416] and the 21st century\\'s most popular performer.[417]\\nVideo and film\\nFurther information: Taylor Swift videography\\nSwift emphasizes visuals as a key creative component of her music-making process.[418] She has collaborated with different directors to produce her music videos, and over time she has become more involved with writing and directing. She developed the concept and treatment for \"Mean\" in 2011[419] and co-directed the music video for \"Mine\" with Roman White the year before.[420] In an interview, White said that Swift \"was keenly involved in writing the treatment, casting and wardrobe. And she stayed for both the 15-hour shooting days, even when she wasn\\'t in the scenes.\"[421]\\nFrom 2014 to 2018, Swift collaborated with director Joseph Kahn on eight music videos—four each from her albums 1989 and Reputation. Kahn has praised Swift\\'s involvement.[422] She worked with American Express for the \"Blank Space\" music video (which Kahn directed), and served as an executive producer for the interactive app AMEX Unstaged: Taylor Swift Experience, for which she won a Primetime Emmy Award for Outstanding Interactive Program in 2015.[423] Swift produced the music video for \"Bad Blood\" and won a Grammy Award for Best Music Video in 2016.[424]\\nHer production company, Taylor Swift Productions, is credited with producing all of her visual media starting with the 2018 concert documentary Reputation Stadium Tour.[425] She continued to co-direct music videos for the Lover singles \"Me!\" with Dave Meyers, and \"You Need to Calm Down\" (also serving as a co-executive producer) and \"Lover\" with Drew Kirsch,[426] but first ventured into sole direction with the video for \"The Man\" (which won her the MTV Video Music Award for Best Direction).[427] After Folklore: The Long Pond Studio Sessions, Swift debuted as a filmmaker with All Too Well: The Short Film,[245] which made her the first artist to win the Grammy Award for Best Music Video as a sole director.[428] Swift has cited Chloé Zhao, Greta Gerwig, Nora Ephron, Guillermo del Toro, John Cassavetes, and Noah Baumbach as filmmaking influences.[418]\\nAccolades and achievements\\nFurther information: List of awards and nominations received by Taylor Swift\\n\\nIn 2009, Swift became the first country singer to win an MTV Video Music Award.\\nSwift\\'s discography is a \"critically hailed songbook\", as per Time\\'s Sam Lansky.[429] She has won 14 Grammy Awards (including four for Album of the Year—the most won by an artist),[430] an Emmy Award,[431] 40 American Music Awards (the most won by an artist),[432] 39 Billboard Music Awards (the most won by an artist—tying with Drake),[433] 118 Guinness World Records,[434] 23 MTV Video Music Awards (including four Video of the Year wins—the most by an act),[247] 12 Country Music Association Awards (including the Pinnacle Award),[435] eight Academy of Country Music Awards,[436] and two Brit Awards.[168] As a songwriter, she has been honored by the Nashville Songwriters Association,[63][437] the Songwriters Hall of Fame, and the National Music Publishers\\' Association and was the youngest person on Rolling Stone\\'s list of the 100 Greatest Songwriters of All Time in 2015.[438][439] At the 64th BMI Awards in 2016, Swift was the first woman to be honored with an award named after its recipient.[440]\\nCommercially, from available data, Swift has amassed over 50 million album sales and 150 million single sales as of 2019,[441][442][443] and 114 million units globally, including 78 billion streams as of 2021.[444][445] The International Federation of the Phonographic Industry ranked her as the Global Recording Artist of the Year for a record three times (2014, 2019 and 2022).[446] Swift has the most number-one albums in the United Kingdom and Ireland for a female artist this millennium,[447][448] earned the highest income for an artist on Chinese digital music platforms (RMB 159,000,000 as of 2021),[449] and is the first artist to replace themselves at the top spot and occupy the entire top five[note 3] of the Australian albums chart.[452][453] Swift remains the world\\'s highest-grossing female touring act ever, with cumulative ticket sales at $1.96 billion as of November 2023 according to Pollstar.[454] The Eras Tour is the highest-grossing tour of all time as of December 2023, and the first to surpass $1 billion in revenue.[455] Beginning with Fearless, each of her studio albums have opened with over one million global units.[456][457] Swift is the most streamed female act on Spotify and Apple Music.[458][459] On Spotify, she is the only artist to have received more than 200 and 250 million streams in one day (260 million on October 27, 2023),[460] and the only female act to reach 100 million monthly listeners.[234] The most entries and the most simultaneous entries for an artist on the Billboard Global 200, with 143 and 31 songs, respectively, are among her feats.[461][462]\\nIn the US, Swift has sold over 37.3 million albums as of 2019,[443] when Billboard placed her eighth on its Greatest of All Time Artists Chart.[463] Eleven of her songs have topped the Billboard Hot 100.[257] She is the longest-reigning act of the Billboard Artist 100 (97 weeks);[464] the soloist with the most cumulative weeks atop the Billboard 200 (68);[465] the woman with the most Billboard 200 number-ones (13),[234] Hot 100 entries (232),[234][156] number-one debuts (6),[note 4] top-ten songs (49),[257] and weeks atop the Top Country Albums chart (101);[467] and the act with the most number-one songs on Pop Airplay (12) and Digital Songs (28).[468][469] Swift is the first woman to simultaneously chart five albums in the top 10 and eleven albums on the entire Billboard 200;[470][471] and the first act to occupy the top four spots and chart seven albums[note 5] in the top 10 on the Top Album Sales chart.[473][474] She is the second highest-certified female digital singles artist (and fifth overall) in the US, with 137.5 million total units certified by the Recording Industry Association of America (RIAA),[475] and the first woman to have both an album (Fearless) and a song (\"Shake It Off\") certified Diamond.[476] Swift is the only artist in Luminate history to have six albums sell over a million copies in a week.[477]\\nSwift has appeared in various power listings. Time included her on its annual list of the 100 most influential people in 2010, 2015, and 2019.[478] She was one of the \"Silence Breakers\" that the magazine spotlighted as Person of the Year in 2017 for speaking up about sexual assault,[479] and received the honor again in 2023 for her cultural domination that year.[429] Time described Swift as the first Person of the Year to be recognized for \"achievement in the arts\", as well as the first woman to be recognized and appear on a Person of the Year cover more than once.[480][481] In 2014, she was named to Forbes\\' 30 Under 30 list in the music category[482] and again in 2017 in its \"All-Star Alumni\" category.[483] Swift became the youngest woman to be included on Forbes\\' list of the 100 most powerful women in 2015, ranked at number 64.[484] In 2023, she was ranked by Forbes as the fifth-most powerful woman in the world, the first entertainer to place in the top five.[485] Swift received an honorary Doctor of Fine Arts degree from New York University and served as its commencement speaker on May 18, 2022.[245]\\nCultural status\\nMain articles: Cultural impact of Taylor Swift and Public image of Taylor Swift\\n\\nSwift at the 2010 Time 100 Gala\\nSwift has been credited with making a profound impact on the music industry, popular culture and the economy.[486][487] She dominates cultural conversations,[488][489] which has led publications to describe her as a cultural \"vitality\" or zeitgeist.[490][491][492] Her music, life and public image are points of attention in global celebrity culture.[297] Initially a teen idol,[493] she has been referred to as a pop icon;[312][494] publications describe her immense popularity and longevity as unwitnessed since the 20th century.[495][496] In 2013, New York magazine\\'s Jody Rosen dubbed Swift the \"world\\'s biggest pop star\" and opined that the trajectory of her stardom has defied established patterns. Rosen added that Swift \"falls between genres, eras, demographics, paradigms, trends\", leaving her contemporaries \"vying for second place\".[310] Critics regard Swift as a rare yet successful combination of the pop star and singer-songwriter archetypes.[497]\\nHer fans are known as Swifties.[266] Billboard noted only few artists have had her chart success, critical acclaim, and fan support.[498] Swift\\'s million-selling albums are considered an anomaly in the streaming-dominated industry following the end of the album era in the 2010s.[499][500] Economist Alan Krueger described Swift as an \"economic genius\".[501]\\nAlthough labeled by the media in her early career as \"America\\'s Sweetheart\" for her girl next door persona,[502][503] Swift has been accused by detractors of being \"calculated\" and manipulative of her image, a narrative bolstered by her 2016 dispute with West.[173][174] Critics have also noted that her personal life and career have been subject to intense misogyny and \"slut-shaming\",[504][505] as well as rampant media scrutiny and tabloid speculation.[506]\\nSwift\\'s private jet use has drawn scrutiny for its carbon emissions.[507][508] In 2023, a spokesperson for Swift stated that she had purchased more than double the required carbon credits to offset all tour travel and personal flights.[509][510] In December 2023, Swift\\'s lawyers sent a cease and desist letter to American programmer Jack Sweeney over tracking her private jet, citing safety concerns and stalking.[511][512] Swift has been a victim of numerous house break-ins and stalkers, some of whom were armed.[513][514]\\nLegacy\\n\"You have different artists dominating different sectors of the industry: Some are huge at streaming, some are big draws on the road. But we\\'re at this moment where there\\'s no one better than Taylor Swift, whether that\\'s on the radio, with streaming, ticket sales or just cultural impact.\"\\n– Jason Lipshutz, Billboard executive director, 2023[515]\\nSwift helped shape the modern country music scene,[516] having extended her success beyond the Anglosphere,[310][516] pioneered the use of internet (Myspace) as a marketing tool,[33][53] and introduced the genre to a younger generation.[517][310] Country labels have since become interested in signing young singers who write their own music;[518] her guitar performances contributed to the \"Taylor Swift factor\", a phenomenon to which an upsurge in guitar sales to women, a previously ignored demographic, is attributed.[519][520]\\nAccording to publications, Swift changed the music landscape with her genre transitions, a discography that accommodates cultural shifts,[521] and her ability to popularize any sound in mainstream music.[522] Lyrically, in being personal and vulnerable in her songs, music journalist Nick Catucci opined Swift helped make space for later singers like Billie Eilish, Ariana Grande, and Halsey to do the same.[523] Scholars have highlighted the literary sensibility and poptimist implications of Swift.[355][524] She has been credited with legitimizing and popularizing the concept of album \"eras\".[525][526] Swift is a subject of academic study and scholarly media research.[297] Various educational institutions offer courses on Swift in literary, cultural and sociopolitical contexts.[527][297]\\nSwift has influenced numerous music artists, and her albums have inspired a generation of singer-songwriters.[517][320][528] Journalists praise her ability to reform industry practices, noting how her actions changed streaming policies, prompted awareness of intellectual property in new musicians,[529][530] and reshaped ticketing models.[531] Various sources deem Swift\\'s music a paradigm representing the millennial generation;[532] Vox called her the \"millennial Bruce Springsteen\",[533] and The Times named her \"the Bob Dylan of our age\".[534] Swift earned the title Woman of the Decade (2010s) from Billboard,[535] Artist of the Decade (2010s) at the American Music Awards,[536] and Global Icon at the Brit Awards for her impact.[445] Senior artists such as Paul McCartney,[537] Mick Jagger,[538] Madonna,[539] and Dolly Parton have praised her musicianship.[540] Carole King regards Swift her \"professional grand daughter\" and thanked Swift for \"carrying the torch forward\".[541] Springsteen called her a \"tremendous\" writer,[542] while Ringo Starr and Billy Joel considered Swift the Beatles\\' successor.[543][544] Britney Spears labeled Swift \"the most iconic pop woman of our generation\".[545]\\nEntrepreneurship\\nMedia outlets describe Swift as a savvy businesswoman;[546][547] in 2024, she topped Billboard\\'s annual Power 100 ranking of the top music industry executives.[548] Swift is known for her traditional album rollouts, consisting of a variety of promotional activities that Rolling Stone termed as an inescapable \"multimedia bonanza\".[549][550] Easter eggs and cryptic teasers became a common practice in contemporary pop music because of Swift.[551] Publications describe her discography as a music \"universe\" subject to analyses by fans, critics and journalists.[552][553][554] Swift maintains an active presence on social media and a close relationship with fans, to which many journalists attribute her success.[555][487][556] Her in-house management team is called 13 Management.[557]\\nSwift has endorsed many brands and businesses, having launched clothing lines with L.E.I. and Stella McCartney,[558][559] designed American Greetings cards and Jakks Pacific dolls,[560][561] released a number of fragrances with Elizabeth Arden,[562] and signed multi-year deals with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League\\'s Nashville Predators and Sony Cyber-shot digital cameras,[565][566] and became the global ambassador for New York City in 2014 and Record Store Day in 2022.[567][568]\\nSocial activism\\nFurther information: Political impact of Taylor Swift\\nSwift identifies as a pro-choice feminist,[569] and is a founding signatory of the Time\\'s Up movement against sexual harassment.[570] Specifically, she criticized the US Supreme Court\\'s decision to end federal abortion rights in 2022.[571] Swift also advocates for LGBT rights,[572] and has called for the passing of the Equality Act, which prohibits discrimination based on sex, sexual orientation, and gender identity.[573][574] She performed during WorldPride NYC 2019 at the Stonewall Inn, a gay rights monument, and has donated to the LGBT organizations Tennessee Equality Project and GLAAD.[575][576][577]\\nA supporter of the March for Our Lives movement and gun control reform in the US,[578] Swift is a vocal critic of white supremacy, racism, and police brutality.[579][569] Following the George Floyd protests, she donated to the NAACP Legal Defense and Educational Fund and the Black Lives Matter movement,[580] called for the removal of Confederate monuments in Tennessee,[581] and advocated for Juneteenth to become a national holiday.[582] In 2020, Swift urged her fans to check their voter registration ahead of elections, which resulted in 65,000 people registering to vote within one day of her post,[583] and endorsed Joe Biden and Kamala Harris in the US presidential election.[584] She has openly criticized former president Donald Trump.[585]\\nWealth\\nSwift\\'s net worth is estimated by Forbes and Bloomberg News at $1.1 billion as of October 2023, making her the first musician to achieve billionaire status \"solely based on her songs and performances\".[586][587] Forbes named her the annual top-earning female musician in 2016, 2019, 2021, and 2022.[588] She was the highest-paid celebrity of 2016 with $170 million—a feat recognized by the Guinness World Records as the highest annual earnings ever for a female musician,[589] which she herself surpassed with $185 million in 2019.[590] Overall, Forbes listed Swift as the highest-paid female artist of the 2010s, earning $825 million.[591] She has also developed a real estate portfolio worth $150 million as of 2023, with properties in Nashville; Tribeca, Manhattan; Los Angeles (Samuel Goldwyn Estate); and Rhode Island (High Watch).[592]\\nPhilanthropy\\nSwift is known for her philanthropic efforts.[593] She ranked first on DoSomething\\'s 2015 \"Gone Good\" list,[594] having received the Star of Compassion from the Tennessee Disaster Services and the Big Help Award from the Nickelodeon Kids\\' Choice Awards for her \"dedication to helping others\" and \"inspiring others through action\".[595][596] She donated $100,000 to the Red Cross to help the victims of the Iowa flood of 2008.[597] In 2009, she sang at BBC\\'s Children in Need concert and raised £13,000 for the cause.[598] Swift has performed at charity relief events, including Sydney\\'s Sound Relief concert.[599] In response to the May 2010 Tennessee floods, Swift donated $500,000.[600] In 2011, Swift used a dress rehearsal of her Speak Now tour as a benefit concert for victims of recent tornadoes in the US, raising more than $750,000.[601] In 2016, she donated $1 million to Louisiana flood relief efforts and $100,000 to the Dolly Parton Fire Fund.[602][603] Swift donated to food banks after Hurricane Harvey struck Houston in 2017 and at every stop of the Eras Tour in 2023;[604][605] she also directly employed local businesses throughout the tour and gave $55 million in bonus payments to her entire crew.[606][607] Swift donated $1 million for Tennessee tornado relief in 2020 and again in 2023.[608][609]\\nShe is a supporter of the arts. A benefactor of the Nashville Songwriters Hall of Fame,[610] Swift has donated $75,000 to Nashville\\'s Hendersonville High School to help refurbish the school auditorium,[611] $4 million to build a new education center at the Country Music Hall of Fame and Museum in Nashville,[612] $60,000 to the music departments of six US colleges,[613] and $100,000 to the Nashville Symphony.[614] Also a promoter of children\\'s literacy, she has donated money and books to schools around the country.[615][616] In 2007, Swift partnered with the Tennessee Association of Chiefs of Police to launch a campaign to protect children from online predators.[617] She has donated items to several charities for auction, including the UNICEF Tap Project and MusiCares.[618] As recipient of the Academy of Country Music\\'s Entertainer of the Year in 2011, Swift donated $25,000 to St. Jude Children\\'s Research Hospital, Tennessee.[619] In 2012, Swift participated in the Stand Up to Cancer telethon, performing the charity single \"Ronan\", which she wrote in memory of a four-year-old boy who died of neuroblastoma.[620] She has also donated $100,000 to the V Foundation for Cancer Research[621] and $50,000 to the Children\\'s Hospital of Philadelphia.[622] Swift has encouraged young people to volunteer in their local communities as part of Global Youth Service Day.[623]\\nSwift donated to fellow singer-songwriter Kesha to help with her legal battles against Dr. Luke and to actress Mariska Hargitay\\'s Joyful Heart Foundation.[593][624] During the COVID-19 pandemic, Swift donated to the World Health Organization and Feeding America,[625] and supported independent record stores.[626][627] Swift performed \"Soon You\\'ll Get Better\" on the One World: Together At Home television special, a benefit concert curated by Lady Gaga for Global Citizen to raise funds for the World Health Organization\\'s COVID-19 Solidarity Response Fund.[628] In 2018 and 2021, Swift donated to the Rape, Abuse & Incest National Network in honor of Sexual Assault Awareness and Prevention Month.[593][629] She has made donations to her fans several times for their medical or academic expenses.[630] In December 2023, Swift attended Ramy Youssef\\'s fundraiser for the Gaza Strip.[631]\\nDiscography\\nMain articles: Taylor Swift albums discography, Taylor Swift singles discography, and List of songs by Taylor Swift\\nStudio albums\\nTaylor Swift (2006)\\nFearless (2008)\\nSpeak Now (2010)\\nRed (2012)\\n1989 (2014)\\nReputation (2017)\\nLover (2019)\\nFolklore (2020)\\nEvermore (2020)\\nMidnights (2022)\\nThe Tortured Poets Department (2024)\\nRe-recorded albums\\nFearless (Taylor\\'s Version) (2021)\\nRed (Taylor\\'s Version) (2021)\\nSpeak Now (Taylor\\'s Version) (2023)\\n1989 (Taylor\\'s Version) (2023)\\nFilmography\\nMain article: Taylor Swift videography\\nThis section lists select works only. Refer to the main article for further information.\\nValentine\\'s Day (2010)\\nThe Lorax (2012)\\nThe Giver (2014)\\nCats (2019)\\nAll Too Well: The Short Film (also director) (2021)\\nAmsterdam (2022)\\nDocumentary and concert films\\nJourney to Fearless (2010)\\nSpeak Now World Tour – Live (2011)\\nThe 1989 World Tour Live (2015)\\nTaylor Swift: Reputation Stadium Tour (2018)\\nMiss Americana (2020)\\nTaylor Swift: City of Lover (2020)\\nFolklore: The Long Pond Studio Sessions (also director) (2020)\\nTaylor Swift: The Eras Tour (2023)\\nTours\\nMain article: List of Taylor Swift live performances\\nFearless Tour (2009–2010)\\nSpeak Now World Tour (2011–2012)\\nThe Red Tour (2013–2014)\\nThe 1989 World Tour (2015)\\nReputation Stadium Tour (2018)\\nThe Eras Tour (2023–2024)\\nSee also\\nList of American Grammy Award winners and nominees\\nList of highest-certified music artists in the United States\\nList of most-followed Instagram accounts\\nList of most-followed Twitter accounts\\nList of most-subscribed YouTube channels\\nFootnotes\\n Swift held the record until the 62nd Annual Grammy Awards in 2020.[91][92]\\n Though Swift has properties throughout the US, she identifies Nashville as her home.[149][150]\\n Swift has occupied the top five of the ARIA Albums Chart twice. She achieved this feat first on the issue published on July 7, 2023,[450] followed by a second time on the issue published on February 9, 2024.[451]\\n In a tie with Ariana Grande.[466]\\n Swift has charted seven titles in the top 10 of the Top Album Sales chart twice—on the issues dated January 6, 2024, and January 20, 2024.[472]\\nReferences\\n \"Taylor Swift: The record-breaking artist in numbers\". Newsround. March 2, 2020. Archived from the original on March 8, 2020. Retrieved April 20, 2020.\\n Sutherland, Mark (May 23, 2015). \"Taylor Swift interview: \\'A relationship? No one\\'s going to sign up for this\\'\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved April 20, 2020.\\n Scott, Walter (June 11, 2015). \"What Famous Pop Star Is Named After James Taylor?\". Parade. Archived from the original on October 15, 2016. Retrieved December 12, 2018.\\n \"Taylor Swift is not an \"underdog\": The real story about her 1 percent upbringing that the New York Times won\\'t tell you\". Salon.com. May 23, 2015. Archived from the original on May 25, 2022. Retrieved December 26, 2020.\\n Jepson 2013, p. 1.\\n Roth, Madeline (May 19, 2015). \"Taylor Swift\\'s Brother Had The Most Epic Graduation Weekend Ever\". MTV News. Archived from the original on July 23, 2016. Retrieved July 25, 2016.\\n McKay, Gabriel (July 6, 2023). \"Taylor Swift Edinburgh: Is star the real queen of Scotland?\". The Herald. Archived from the original on February 1, 2024. Retrieved February 4, 2024.\\n Eleftheriou-Smith, Loulla-Mae (June 24, 2015). \"Taylor Swift tells Scotland: \\'I am one of you\\'\". The Independent. Archived from the original on May 26, 2022. Retrieved July 10, 2019.\\n Vadala, Nick (July 14, 2017). \"Taylor Swift ancestor\\'s home added to Philly Register of Historic Places\". The Philadelphia Inquirer. Archived from the original on May 25, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift\\'s Great-Great-Grandfather\\'s Philly Home Gets Historic Landmark Status\". AP NEWS. July 25, 2017. Archived from the original on May 22, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift stammt aus dem Freistaat\" (in German). BR24. September 17, 2015. Archived from the original on December 31, 2021. Retrieved July 23, 2023.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Raab, Scott (October 20, 2014). \"Taylor Swift Interview\". Esquire. Archived from the original on February 16, 2015. Retrieved April 11, 2015.\\n \"Taylor Swift on Politicians Co-opting Faith: \\'I\\'m a Christian. That\\'s Not What We Stand For\\'\". Relevant. January 31, 2020. Archived from the original on November 25, 2022. Retrieved April 2, 2020.\\n Uhrich, Bill (February 13, 2010). \"Photos Students at Alvernia Montessori School sending Taylor Swift a valentine\". Reading Eagle. Archived from the original on October 16, 2013. Retrieved February 25, 2013.\\n Hatza, George (December 8, 2008). \"Taylor Swift: Growing into superstardom\". Reading Eagle. Archived from the original on April 1, 2012. Retrieved April 17, 2012.\\n Mennen, Lauren (November 12, 2014). \"Taylor Swift\\'s Wyomissing childhood home on the market for $799,500\". Philadelphia Daily News. Archived from the original on October 17, 2016. Retrieved October 13, 2016.\\n Chang, David (February 22, 2016). \"Taylor Swift Returns to Reading Pennsylvania as Maid of Honor in Friend\\'s Wedding\". WCAU. Archived from the original on September 16, 2016. Retrieved August 26, 2016.\\n \"Taylor Swift, Age 12\". New York Daily News. Archived from the original on August 27, 2016. Retrieved August 26, 2016.\\n Cooper, Brittany Joy (April 15, 2012). \"Taylor Swift Opens Up About a Future in Acting and Admiration for Emma Stone\". Taste of Country. Archived from the original on April 17, 2012. Retrieved April 17, 2012.\\n MacPherson, Alex (October 18, 2012). \"Taylor Swift: \\'I want to believe in pretty lies\\'\". The Guardian. Archived from the original on August 26, 2016. Retrieved August 3, 2016.\\n Rolling Stone Interview: The Unabridged Taylor Swift, December 2, 2008\\n Morris, Edward (December 1, 2006). \"When She Thinks \\'Tim McGraw\\', Taylor Swift Savors Payoff: Hardworking Teen to Open for George Strait Next Year\". CMT. Archived from the original on June 26, 2015. Retrieved March 11, 2010.\\n Diu, Nisha Lilia (April 3, 2011). \"Taylor Swift: \\'I won\\'t do sexy shoots\\'\". The Daily Telegraph. Archived from the original on May 6, 2013. Retrieved April 17, 2012.\\n \"News : CMT Insider Interview: Taylor Swift (Part 1 of 2)\". CMT. November 26, 2008. Archived from the original on January 23, 2015. Retrieved July 1, 2012.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. Archived from the original on May 10, 2012. Retrieved May 21, 2012.\\n Qureshi, Hira. \"Visit this Stone Harbor café where Taylor Swift was \\'always coming in to play\\' as a child\". Courier-Post. Archived from the original on October 26, 2023. Retrieved December 12, 2022.\\n Kuperinsky, Amy (July 28, 2020). \"Taylor Swift shouts out Jersey Shore town in video for surprise album\". NJ.com. Archived from the original on December 12, 2022. Retrieved December 12, 2022.\\n Martino, Andy (January 10, 2015). \"EXCLUSIVE: The real story of Taylor Swift\\'s guitar \\'legend\\'\". New York Daily News. Archived from the original on November 22, 2015. Retrieved August 28, 2017.\\n \"Dymtrow v. Swift et al: Federal Civil LawsuitNew York Southern District Court, Case No. 1:07-cv-11277-RJS\" (PDF). American Bar Association. Archived from the original (PDF) on October 11, 2012. Retrieved April 18, 2012.\\n \"On tour with Taylor Swift\". NBC News. May 31, 2009. Archived from the original on October 5, 2013. Retrieved July 1, 2012.\\n Castro, Vicky (February 6, 2015). \"How to Succeed as an Entrepreneur, Taylor Swift Style\". Inc. Archived from the original on June 7, 2016. Retrieved February 9, 2015.\\n Willman, Chris (July 25, 2007). \"Getting to know Taylor Swift\". Entertainment Weekly. Archived from the original on May 24, 2022. Retrieved January 25, 2022.\\n Jo, Nancy (January 2, 2014). \"Taylor Swift and the Growing of a Superstar: Her Men, Her Moods, Her Music\". Vanity Fair. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n \"News : Taylor Swift\\'s High School Names Auditorium in Her Honor\". CMT. September 23, 2010. Archived from the original on November 21, 2014. Retrieved April 18, 2012.\\n Grigoriadis, Vanessa (March 5, 2009). \"The Very Pink, Very Perfect Life of Taylor Swift\". Rolling Stone. Archived from the original on May 3, 2019. Retrieved July 28, 2019.\\n \"Taylor Swift receives her high school diploma\". Houston Chronicle. July 27, 2008. Archived from the original on January 1, 2024. Retrieved January 2, 2024.\\n \"Taylor Swift: The Garden In The Machine\". American Songwriter. May 2, 2011. Archived from the original on August 7, 2013. Retrieved May 21, 2012.\\n \"Songwriter Taylor Swift Signs Publishing Deal With Sony/ATV\". Broadcast Music, Inc. May 12, 2005. Archived from the original on December 4, 2012. Retrieved April 20, 2012.\\n Kosser, Michael (June 3, 2010). \"Liz Rose: Co-Writer to the Stars\". American Songwriter. Archived from the original on December 24, 2011. Retrieved April 19, 2012.\\n Leahey, Andrew (October 24, 2014). \"Songwriter Spotlight: Liz Rose\". Rolling Stone. Archived from the original on September 26, 2016. Retrieved September 24, 2016.\\n DeLuca, Dan (November 11, 2008). \"Focused on \\'great songs\\' Taylor Swift isn\\'t thinking about \\'the next level\\' or Joe Jon as gossip\". Philadelphia Daily News. p. 1. Archived from the original on November 18, 2012. Retrieved April 17, 2012.\\n Preston, John (April 26, 2009). \"Taylor Swift: the 19-year-old country music star conquering America – and now Britain\". The Daily Telegraph. Archived from the original on January 5, 2012. Retrieved August 30, 2012.\\n Rosa, Christopher (March 24, 2015). \"Opening Acts Who Became Bigger Than The Headliner\". VH1. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n Rapkin, Mickey (July 27, 2017). \"Oral History of Nashville\\'s Bluebird Cafe: Taylor Swift, Maren Morris, Dierks Bentley & More on the Legendary Venue\". Billboard. Archived from the original on July 29, 2017. Retrieved July 28, 2017.\\n Hiatt, Brian (October 25, 2012). \"Taylor Swift in Wonderland\". Rolling Stone. Archived from the original on July 31, 2016. Retrieved August 1, 2016.\\n Greenburg, Zack O\\'Malley (June 26, 2013). \"Toby Keith, Cowboy Capitalist: Country\\'s $500 Million Man\". Forbes. Archived from the original on August 27, 2016. Retrieved August 1, 2016.\\n Taylor Swift (CD). Big Machine Records. 2006. BMR120702.\\n Tamarkin, Jeff. \"Taylor Swift – Taylor Swift\". AllMusic. Archived from the original on October 20, 2015. Retrieved February 14, 2021.\\n Neal, Chris (December 4, 2006). \"Taylor Swift Review\". Country Weekly. Archived from the original on July 22, 2012. Retrieved March 31, 2010.\\n Trust, Gary (October 29, 2009). \"Chart Beat Thursday: Taylor Swift, Tim McGraw Linked Again\". Billboard. Archived from the original on March 7, 2013. Retrieved November 8, 2016.\\n \"Taylor Swift\". Songwriters\\' Hall of Fame. Archived from the original on February 12, 2021. Retrieved September 21, 2022.\\n Willman, Chris (February 5, 2008). \"Taylor Swift\\'s Road to Fame\". Entertainment Weekly. p. 3. Archived from the original on February 21, 2015. Retrieved April 22, 2012.\\n \"Taylor Swift Joins Rascal Flatts Tour\". CMT. October 18, 2006. Archived from the original on January 7, 2015. Retrieved March 11, 2010.\\n Whitaker, Sterling; Hammar, Ania (May 27, 2019). \"How Eric Church\\'s Rascal Flatts Feud Helped Launch Taylor Swift\\'s Career\". Taste of Country. Townsquare Media. Archived from the original on June 6, 2019. Retrieved June 10, 2019.\\n \"Taylor Swift No. 1 on iTunes\". Great American Country. December 19, 2007. Archived from the original on March 3, 2012. Retrieved July 5, 2010.\\n \"Taylor Swift – Chart history\". Billboard. Archived from the original on August 9, 2016. Retrieved July 26, 2016.\\n \"Taylor Swift owns top of country chart\". Country Standard Time. July 23, 2008. Archived from the original on July 31, 2008. Retrieved December 26, 2008.\\n \"Wal-Mart \"Eyes\" New Taylor Swift Project\". Great American Country. Archived from the original on July 23, 2008. Retrieved July 24, 2008.\\n \"Taylor Swift Joins George Strait\\'s 2007 Tour\". CMT. November 17, 2006. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Brad Paisley Plans Tour With Three Opening Acts\". CMT. January 9, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Joins Tim McGraw, Faith Hill on Tour\". CMT. June 1, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Youngest Winner of Songwriter/Artist Award\". Great American Country. October 16, 2007. Archived from the original on January 11, 2015. Retrieved February 2, 2015.\\n \"Photos : All Taylor Swift Pictures : Horizon Award Winner Poses in the Pressroom\". CMT. September 7, 2007. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Photos : 43rd Annual ACM Awards – Onstage: Winners : Acceptance Speech\". CMT. May 18, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Taylor Swift, Rascal Flatts, Carrie Underwood Score at 2008 AMA Awards\" (Blog). Roughstock.com. November 24, 2008. Archived from the original on July 10, 2014. Retrieved May 21, 2012.\\n \"Amy Winehouse Wins Best New Artist, Kanye West Pays Tribute to Mom – Grammy Awards 2008, Grammy Awards\". People. October 2, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Rascal Flatts Announce Summer Tour With Taylor Swift\". CMT. May 5, 2008. Archived from the original on April 18, 2022. Retrieved June 5, 2019.\\n Caplan, David (September 8, 2008). \"Scoop\". People. Archived from the original on February 2, 2016. Retrieved March 6, 2012.\\n Rizzo, Monica (November 24, 2008). \"Scoop – Couples, Camilla Belle, Joe Jonas\". People. Archived from the original on March 3, 2016. Retrieved March 6, 2012.\\n Akers, Shelley (June 9, 2008). \"Taylor Swift to Appear in Hannah Montana Movie\". People. Archived from the original on October 27, 2017. Retrieved October 27, 2017.\\n \"Hannah Montana: The Movie (Original Motion Picture Soundtrack) by Hannah Montana\". iTunes Store. January 2009. Archived from the original on May 2, 2016. Retrieved August 2, 2016.\\n \"CD Taylor Swift – Fearless\" (in Portuguese). Universal Music Group. Archived from the original on January 18, 2021. Retrieved February 14, 2021.\\n Raphael, Amy (February 1, 2009). \"First, she conquered Nashville. Now she\\'s set for world domination\". The Observer. ProQuest 250507223. Archived from the original on December 9, 2022. Retrieved December 9, 2022.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong with Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved July 24, 2014.\\n \"Discography Taylor Swift\". ARIA Charts. Archived from the original on March 21, 2012. Retrieved January 2, 2010.\\n Trust, Gary (December 15, 2009). \"Best of 2009: Part 1\". Billboard. Archived from the original on March 3, 2013. Retrieved September 20, 2022.\\n Ben-Yehuda, Ayala (August 13, 2009). \"Black Eyed Peas, Jason Mraz Tie Records on Billboard Hot 100\". Billboard. Archived from the original on May 8, 2013. Retrieved March 13, 2010.\\n Trust, Gary (September 24, 2009). \"Taylor Swift Climbs Hot 100, Black Eyed Peas Still No. 1\". Billboard. Archived from the original on February 1, 2013. Retrieved September 20, 2022.\\n \"Taylor Swift Chart History (Hot Country Songs)\". Billboard. Archived from the original on January 31, 2021. Retrieved February 14, 2021.\\n Grein, Paul (March 16, 2012). \"Chart Watch Extra: Top Albums Of Last 10 Years\" (Blog). Yahoo! Music. Archived from the original on April 2, 2015. Retrieved June 10, 2011.\\n Mapes, Jillian (November 23, 2010). \"Taylor Swift Announces \\'Speak Now\\' World Tour\". Billboard. Archived from the original on May 8, 2013. Retrieved May 15, 2012.\\n Weiss, Dan (December 12, 2011). \"Taylor Swift: Journey To Fearless DVD\". American Songwriter. Archived from the original on August 15, 2016. Retrieved August 2, 2016.\\n Ryan, Sarah (August 10, 2009). \"Taylor Swift Pranks Keith Urban\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved November 11, 2015.\\n \"Kanye calls Taylor Swift after \\'View\\' appearance\". MSNBC. September 15, 2009. Archived from the original on October 6, 2013. Retrieved September 16, 2009.\\n \"Taylor Swift Thanks \"Gracious\" Beyonce for Inviting Her Onstage After Kanye Stunt at VMAs\". Rolling Stone. September 14, 2009. Archived from the original on June 15, 2012. Retrieved May 15, 2012.\\n Anderson, Kyle (September 16, 2009). \"Kanye West\\'s VMA Interruption Gives Birth To Internet Photo Meme\". MTV. Archived from the original on January 16, 2016. Retrieved October 3, 2009.\\n Ditzian, Eric (2009). \"Taylor Swift, Michael Jackson Big Winners at American Music Awards\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"2009 Artists of the Year\". Billboard. December 10, 2009. Archived from the original on January 6, 2010. Retrieved May 21, 2012.\\n \"Taylor Swift Raps \\'Thug Story\\' With T-Pain On CMT Awards\". MTV. June 17, 2009. Archived from the original on May 25, 2022. Retrieved November 11, 2015.\\n Kreps, Daniel (February 1, 2010). \"Beyonce, Taylor Swift Dominate 2010 Grammy Awards\". Rolling Stone. Archived from the original on February 15, 2012. Retrieved February 13, 2012.\\n \"Billie Eilish replaces Taylor Swift as youngest artist to win a Grammy for Album of the Year\". MSN. January 27, 2020. Archived from the original on November 20, 2020. Retrieved September 1, 2020.\\n Kaufman, Gil (November 12, 2009). \"Taylor Swift Dominates CMA Awards\". MTV News. Archived from the original on March 6, 2016. Retrieved September 13, 2016.\\n Vena, Jocelyn (November 6, 2009). \"John Mayer Talks Taylor Swift Collaboration \\'Half of My Heart\\'\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"Boys Like Girls featuring Taylor Swift, \\'Two Is Better Than One\\'\". Billboard. December 2, 2009. Archived from the original on August 18, 2021. Retrieved December 4, 2020.\\n \"Kellie Pickler Has Her \\'Best Days\\' Thanks To Taylor Swift\". MTV. Archived from the original on January 1, 2016. Retrieved November 11, 2015.\\n Vena, Jocelyn (December 28, 2009). \"New Taylor Swift Song Included In \\'Valentine\\'s Day\\' Featurette\". MTV. Archived from the original on January 16, 2016. Retrieved November 11, 2015.\\n \"Taylor Swift – Chart history on Canadian Hot 100\". Billboard. Archived from the original on August 9, 2016. Retrieved August 3, 2016.\\n Park, Michael Y.; Sia, Nicole (December 29, 2009). \"Taylor & Taylor Romance Was Overblown, Says Source\". People. Archived from the original on November 13, 2012. Retrieved March 6, 2012.\\n Caramanica, Jon (March 6, 2009). \"OMG! Taylor Swift Does \\'CSI\\'!\". The New York Times (Blog). Archived from the original on August 14, 2011. Retrieved May 7, 2012.\\n Strecker, Erin (January 2, 2015). \"Remember When Taylor Swift Shined as \\'Saturday Night Live\\' Host?\". Billboard. Archived from the original on January 24, 2015. Retrieved January 15, 2015.\\n Dukes, Billy (October 22, 2012). \"10 Things You Didn\\'t Know About Taylor Swift\". Taste of Country. Townsquare Media. Archived from the original on May 22, 2022. Retrieved July 26, 2020.\\n Pietroluongo, Silvio (August 11, 2010). \"Taylor Swift Makes Sparkling Hot 100 Entrance\". Billboard. Archived from the original on January 31, 2016. Retrieved July 25, 2016.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift, Angry on \\'Speak Now\\'\". The New York Times. Archived from the original on October 21, 2010. Retrieved October 23, 2010.\\n \"Taylor Swift\\'s New Album, Speak Now, Set for Oct. 25 Release\". CMT. July 20, 2010. Archived from the original on June 21, 2019. Retrieved February 14, 2020.\\n Kaufman, Gil (November 3, 2010). \"Taylor Swift\\'s Speak Now Tops 1 Million in First Week\". MTV. Archived from the original on August 10, 2016. Retrieved August 8, 2016.\\n \"Fastest-selling digital album in the US by a female artist\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Knopper, Steve (November 25, 2010). \"Taylor Swift\\'s Speak Now Tops the Charts\". Rolling Stone. Archived from the original on March 3, 2021. Retrieved November 25, 2010.\\n Sheffield, Rob (October 26, 2010). \"Speak Now (2010)\". Rolling Stone. Archived from the original on September 11, 2013. Retrieved December 20, 2019.\\n Hammel, Sara (January 4, 2011). \"Taylor Swift & Jake Gyllenhaal Break Up: Source\". People. Archived from the original on May 9, 2012. Retrieved March 6, 2012.\\n Wyland, Sarah (February 12, 2012). \"Taylor Swift Takes Home Two GRAMMYs at Tribute-Filled Show\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved February 13, 2012.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on January 16, 2016. Retrieved November 21, 2015.\\n Smith, Hazel (October 24, 2011). \"News : Hot Dish: Taylor Swift Sings Alan Jackson\\'s Masterpiece at Nashville Songwriters Celebration\". CMT. Archived from the original on November 29, 2014. Retrieved April 22, 2012.\\n Roland, Tom (December 2, 2011). \"Taylor Swift: Billboard\\'s Woman of the Year\". Billboard. Archived from the original on February 3, 2013. Retrieved May 15, 2012.\\n Talbott, Chris; Silva, Cristina (April 2, 2012). \"Taylor Swift wins ACM entertainer of the year\". Yahoo!. Associated Press. Archived from the original on August 23, 2016. Retrieved May 15, 2012.\\n \"CMA Awards 2011: Taylor Swift wins entertainer of the year\". CBS News. November 9, 2011. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n Kellogg, Jane (November 20, 2011). \"AMAs 2011: Winners and Nominees Complete List\". The Hollywood Reporter. Archived from the original on June 27, 2015. Retrieved November 21, 2015.\\n Sheffield, Rob (June 23, 2012). \"Women Who Rock: The 50 Greatest Albums of All Time\". Rolling Stone. Archived from the original on December 10, 2016. Retrieved July 15, 2017.\\n Allen, Bob (March 29, 2012). \"Hot Tours: Taylor Swift, George Strait, Cirque Du Soleil\". Billboard. Archived from the original on February 21, 2013. Retrieved May 10, 2012.\\n \"Taylor Swift News and Blog\". taylorswift.com. September 21, 2011. Archived from the original on October 11, 2011. Retrieved September 21, 2011.\\n Herrera, Monica (March 15, 2012). \"Taylor Swift, Arcade Fire Talk \\'Hunger Games\\'\". Rolling Stone. Archived from the original on June 27, 2015. Retrieved May 10, 2012.\\n \"Nominations 2013 — Golden Globe Awards\". goldenglobes.org. December 13, 2012. Archived from the original on December 14, 2012. Retrieved December 13, 2012.\\n Horowitz, Steven J. (April 20, 2012). \"B.o.B Explains Origins of Taylor Swift Collaboration \\'Both of Us\\'\". HipHopDX. Archived from the original on September 10, 2015. Retrieved May 15, 2012.\\n Toomedy, Alyssa (October 25, 2012). \"Taylor Swift and Conor Kennedy Breakup: Anatomy of a Split\". E! News. Archived from the original on December 4, 2015. Retrieved November 10, 2015.\\n Trust, Gary (August 22, 2012). \"Taylor Swift Scores First Hot 100 No. 1\". Billboard. Archived from the original on February 13, 2013. Retrieved August 22, 2012.\\n \"Discography Taylor Swift\". New Zealand Charts. Archived from the original on April 20, 2017. Retrieved July 26, 2016.\\n Lynch, Kevin (September 4, 2013). \"Calvin Harris trumps Michael Jackson feat to join Taylor Swift, Rihanna and One Direction in Guinness World Records™ 2014 book\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Chart positions:\\n • \"Taylor Swift – I Knew You Were Trouble\". ARIA Charts. Archived from the original on May 27, 2016. Retrieved February 14, 2021.\\n • \"Official Singles Charts Top 100\". Official Charts Company. Archived from the original on July 13, 2021. Retrieved February 14, 2021.\\n • \"Taylor Swift Leads Record Breaking Digital Sales Week\". Billboard. January 3, 2013. Archived from the original on April 8, 2017. Retrieved February 14, 2021.\\n Lewis, Randy (October 30, 2012). \"Taylor Swift raises the bar with a savvy \\'Red\\' marketing campaign\". Los Angeles Times. Archived from the original on December 28, 2020. Retrieved December 28, 2020.\\n Mansfield, Brian (October 17, 2012). \"Taylor Swift sees \\'Red\\' all over\". USA Today. Archived from the original on December 21, 2012.\\n English, J. (August 28, 2017). \"Taylor Swift\\'s \\'Red\\': A Canonical Coming-Of-Age Album\". NPR. Archived from the original on April 12, 2021. Retrieved February 14, 2021.\\n Roberts, Randall (October 31, 2012). \"\\'The Last Time\\' connects Taylor Swift with Arcade Fire\". Los Angeles Times. Archived from the original on March 6, 2016. Retrieved December 27, 2020.\\n Greenwald, David (September 6, 2013). \"Taylor Swift, Rihanna, Justin Bieber Among 2014 Guinness Record-Setters\". Billboard. Archived from the original on June 8, 2015. Retrieved July 27, 2016.\\n Sexton, Paul (August 31, 2019). \"Taylor Swift Scores Fourth U.K. No. 1 With \\'Lover\\' Album\". Billboard. Archived from the original on August 16, 2021. Retrieved October 15, 2019.\\n \"Grammys 2014: The complete list of nominees and winners\". Los Angeles Times. January 26, 2014. Archived from the original on March 4, 2015. Retrieved January 25, 2015.\\n Gregoire, Carolyn (November 19, 2012). \"Taylor Swift AMA Awards 2012: Pop Star Performs \\'I Knew You Were Trouble\\' (Video)\". HuffPost. Archived from the original on May 26, 2013. Retrieved June 10, 2013.\\n Payne, Chris (November 25, 2013). \"Taylor Swift & Justin Timberlake Win Big at American Music Awards\". Billboard. Archived from the original on November 24, 2015. Retrieved November 21, 2015.\\n \"NSAI Songwriter/Artists of the Year\". Nashville Songwriters Association International. Archived from the original on August 19, 2016. Retrieved August 2, 2016.\\n Allen, Bob (July 3, 2014). \"Taylor Swift\\'s Red Wraps as All-Time Country Tour\". Billboard. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Caramanica, Jon (November 7, 2013). \"Country Awards Hold Swift Close\". The New York Times. Archived from the original on November 11, 2013. Retrieved April 3, 2014.\\n \"Taylor Swift\\'s Boyfriend Timeline: 10 Relationships & Their Songs\". Billboard. December 30, 2014. Archived from the original on August 26, 2016. Retrieved August 26, 2016.\\n Labrecque, Jeff (December 12, 2013). \"\\'12 Years a Slave\\' and \\'American Hustle\\' lead Golden Globe nominees\". Entertainment Weekly. Archived from the original on March 23, 2015. Retrieved December 12, 2013.\\n Bonaguro, Alison (January 25, 2013). \"News : Offstage: Tim McGraw Wanted to Make Taylor Swift Duet an Event\". CMT. Archived from the original on July 12, 2015. Retrieved February 25, 2013.\\n Blistein, Doyle (June 4, 2013). \"Taylor Swift Joins Rolling Stones for \\'As Tears Go By\\'\". Rolling Stone. Archived from the original on June 9, 2013. Retrieved June 10, 2013.\\n \"Taylor Swift Joins Florida Georgia Line Onstage for \\'Cruise\\'\". Taste of Country. Townsquare Media. March 2, 2013. Archived from the original on March 16, 2013. Retrieved March 29, 2013.\\n Collin, Robbie (July 26, 2012). \"The Lorax, review\". The Daily Telegraph. Archived from the original on September 17, 2016. Retrieved August 3, 2016.\\n Beard, Lanford (May 1, 2013). \"Taylor Swift says \\'I do\\' to \\'New Girl\\'\". Entertainment Weekly. Archived from the original on November 8, 2016. Retrieved August 4, 2016.\\n Busis, Hillary (September 27, 2013). \"Taylor Swift will co-star in long-awaited adaptation of \\'The Giver\\'\". Entertainment Weekly. Archived from the original on December 21, 2016. Retrieved August 2, 2016.\\n Peterson, Price (March 31, 2014). \"Taylor Swift Moves into NYC Apartment Built Over Mysterious River of Pink Slime\". The Atlantic. Archived from the original on September 27, 2016. Retrieved July 31, 2016.\\n Rogers, Alex (March 7, 2014). \"Why Taylor Swift Thinks Nashville Is the Best Place on Earth\". Time. Archived from the original on May 24, 2022. Retrieved September 27, 2016.\\n Milzoff, Rebecca; Brown, Eric Renner; Denis, Kyle (August 24, 2023). \"Taylor Swift and Beyoncé Are So Big, Even Their Publicists Have Fans\". Billboard. Archived from the original on August 24, 2023. Retrieved August 24, 2023.\\n Zollo, Paul (February 17, 2016). \"The Oral History of Taylor Swift\\'s 1989\". The Recording Academy. Archived from the original on June 3, 2021. Retrieved February 14, 2021.\\n Stutz, Colin (October 16, 2014). \"Watch Taylor Swift\\'s \\'1989\\' Secret Sessions Behind The Scenes Video\". Billboard. Archived from the original on June 5, 2016. Retrieved August 2, 2016.\\n Caulfield, Keith (November 4, 2014). \"Taylor Swift\\'s \"1989\" debuts with 1.287 million copies sold\". Billboard. Archived from the original on November 5, 2014. Retrieved November 4, 2014.\\n Chart positions:\\n • \"Discography Taylor Swift\". ARIA Charts. Archived from the original on August 26, 2019. Retrieved February 14, 2021.\\n • \"Taylor Swift Chart History (Canadian Hot 100)\". Billboard. Archived from the original on August 9, 2016. Retrieved February 14, 2021.\\n • \"Taylor Swift\\'s \\'Bad Blood\\' Blasts to No. 1 on Hot 100\". Billboard. May 28, 2015. Archived from the original on September 6, 2015. Retrieved May 28, 2015.\\n \"Taylor Swift – Chart History: Hot 100\". Billboard. Archived from the original on March 8, 2018. Retrieved September 18, 2016.\\n \"Live Music\\'s $20 Billion Year: The Grateful Dead\\'s Fare Thee Well Reunion, Taylor Swift, One Direction Top Boxscore\\'s Year-End\". Billboard. Archived from the original on December 14, 2015.\\n Weissmann, Jordan (July 7, 2014). \"Taylor Swift Has Written an Op-Ed in the Wall Street Journal\". Slate (Blog). Archived from the original on January 23, 2015. Retrieved January 23, 2015.\\n Knopper, Steve (November 8, 2014). \"Taylor Swift\\'s Label Head Explains Spotify Removal\". Rolling Stone. Archived from the original on April 21, 2015. Retrieved April 11, 2015.\\n Peters, Mitchell (June 21, 2015). \"Taylor Swift Pens Open Letter Explaining Why \\'1989\\' Won\\'t Be on Apple Music\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Halperin, Shirley (June 21, 2015). \"Apple Changes Course After Taylor Swift Open Letter: Will Pay Labels During Free Trial\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Rosen, Christopher (June 25, 2015). \"Taylor Swift is putting 1989 on Apple Music\". Entertainment Weekly. Archived from the original on June 26, 2015. Retrieved June 25, 2015.\\n \"Taylor Swift returns to Spotify on the day Katy Perry\\'s album comes out\". BBC News. June 9, 2017. Archived from the original on June 9, 2017.\\n \"Taylor Swift: 2014 Billboard Woman of the Year\". Billboard. October 10, 2014. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Payne, Chris (November 23, 2014). \"Taylor Swift Wins Dick Clark Award of Excellence at 2014, Presented by Diana Ross\". Billboard. Archived from the original on July 4, 2015. Retrieved April 11, 2015.\\n \"The Taylor Swift Experience\". GRAMMY Museum. Archived from the original on November 25, 2022. Retrieved April 22, 2022.\\n Boehrer, Kat (January 7, 2016). \"Watch Taylor Swift\\'s Stunning Acoustic Performance of \\'Blank Space\\' at the Grammy Museum\". Complex. Archived from the original on April 22, 2022. Retrieved April 22, 2022.\\n Jonze, Tim (February 25, 2015). \"Taylor Swift wins international female solo artist at Brit awards 2015\". The Guardian. Archived from the original on January 1, 2016. Retrieved April 11, 2015.\\n Stutz, Colin (July 21, 2015). \"2015 MTV Video Music Awards Nominees Revealed: Taylor Swift, Kendrick Lamar, Ed Sheeran & More\". Billboard. Archived from the original on July 24, 2015. Retrieved August 15, 2016.\\n Lynch, Joe (February 19, 2016). \"Taylor Swift Joins Elite Club to Win Grammy Album of the Year More Than Once: See the Rest\". Billboard. Archived from the original on March 1, 2016. Retrieved July 31, 2016.\\n Chiu, Melody (June 1, 2016). \"Taylor Swift and Calvin Harris Split After 15 Months Together\". People. Archived from the original on June 22, 2016. Retrieved June 1, 2016.\\n Spanos, Brittany (July 13, 2016). \"Taylor Swift Co-Wrote Calvin Harris\\' Smash Hit \\'This Is What You Came For\\'\". Rolling Stone. Archived from the original on July 30, 2016. Retrieved July 31, 2016.\\n Grady, Constance (August 26, 2019). \"How the Taylor Swift-Kanye West VMAs scandal became a perfect American morality tale\". Vox. Archived from the original on December 2, 2022. Retrieved October 12, 2023.\\n Grady, Constance (March 21, 2020). \"Newly leaked footage shows Taylor Swift and Kanye West talking \"Famous\"\". Vox. Archived from the original on March 24, 2022. Retrieved October 12, 2023.\\n Lewis, Anna (July 15, 2016). \"Tom Hiddleston finally tells us the truth about his relationship with Taylor Swift\". Cosmopolitan. Archived from the original on July 17, 2016. Retrieved January 8, 2019.\\n \"Taylor Swift Finally Reveals When She Started Dating Joe Alwyn in Lover Album\". Yahoo!. August 23, 2019. Archived from the original on June 11, 2020. Retrieved June 11, 2020.\\n \"Paul McCartney & Taylor Swift\". Rolling Stone. November 13, 2020. Archived from the original on November 30, 2020. Retrieved September 15, 2021. McCartney: So how does that go? Does your partner sympathize with that and understand? Swift: Oh, absolutely.\\n Irvin, Jack (April 10, 2023). \"Inside Taylor Swift and Joe Alwyn\\'s \\'Differences\\' That Led to Their Breakup: Sources (Exclusive)\". People. Archived from the original on September 25, 2023. Retrieved September 25, 2023.\\n \"Taylor Swift, pop princess, wins song of the year at the CMA Awards\". USA Today. Archived from the original on November 9, 2017. Retrieved November 9, 2017.\\n Trust, Gary (February 21, 2017). \"Ed Sheeran Tops Hot 100, Katy Perry Debuts at No. 4 & Bruno Mars, Rihanna & The Weeknd All Hit Top 10\". Billboard. Archived from the original on February 22, 2017. Retrieved February 22, 2017.\\n Grady, Constance (August 11, 2017). \"Taylor Swift won her day in court. Here\\'s what you need to know\". Vox. Archived from the original on October 17, 2022. Retrieved October 4, 2023.\\n \"Taylor Swift wipes social media profiles, fuelling expectations of new album\". The Daily Telegraph. August 18, 2017. Archived from the original on January 8, 2019. Retrieved August 19, 2019.\\n Aswad, Jem (August 24, 2017). \"Taylor Swift\\'s New Single, \\'Look What You Made Me Do,\\' Arrives (Listen)\". Variety. Archived from the original on August 28, 2017. Retrieved August 29, 2017.\\n White, Jack (September 1, 2017). \"Taylor Swift scores first Number 1 on the Official Singles Chart with \\'LWYMMD\\'\". Official Charts Company. Archived from the original on September 2, 2017. Retrieved September 1, 2017.\\n Peak positions:\\n • \"Taylor Swift Scores Fifth No. 1 Single\". Australian Recording Industry Association. September 2, 2017. Archived from the original on September 2, 2017. Retrieved September 2, 2017.\\n • \"IRMA – Irish Charts\". Irish Recorded Music Association. Archived from the original on June 14, 2017. Retrieved September 2, 2017.\\n • \"NZ Top 40 Singles Chart\". Recorded Music NZ. September 4, 2017. Archived from the original on September 1, 2017. Retrieved September 1, 2017.\\n • \"Taylor Swift at Nos. 1 & 4 on Billboard Hot 100, as Cardi B Moves Up to No. 2\". Billboard. September 11, 2017. Archived from the original on September 21, 2017. Retrieved October 30, 2017.\\n Shaw, Lucas (November 7, 2017). \"Taylor Swift Will Keep New Album From Streaming for a Week\". Bloomberg. Bloomberg News. Archived from the original on November 8, 2017. Retrieved November 7, 2017.\\n Powers, Ann (November 10, 2019). \"The Old Taylor\\'s Not Dead\". NPR. Archived from the original on June 9, 2020. Retrieved June 29, 2020.\\n McDermott, Maeve (October 11, 2017). \"Taylor Swift \\'Reputation\\': Here\\'s what critics are saying\". USA Today. Archived from the original on March 4, 2020. Retrieved October 15, 2020.\\n \"Official: Taylor Swift\\'s \\'Reputation\\' Album Sells 1.2M Copies in US During First Week\". Billboard. Archived from the original on November 30, 2017. Retrieved May 29, 2018.\\n Chart positions:\\n • \"Taylor Swift\\'s \\'Reputation\\' Rules Australia\\'s Albums Chart\". Billboard. November 20, 2017. Archived from the original on November 20, 2017. Retrieved December 2, 2017.\\n • \"Taylor Swift Chart History\". Billboard. Archived from the original on November 22, 2021. Retrieved November 19, 2021.\\n Unterberger, Andrew (July 6, 2018). \"Taylor Swift\\'s \\'Delicate\\' Became the Biggest Reputation Radio Hit While You Weren\\'t Looking\". Billboard. Archived from the original on December 13, 2020. Retrieved April 13, 2020.\\n \"61st Grammy Nominees\". The Recording Academy. December 7, 2018. Archived from the original on December 7, 2018. Retrieved December 7, 2018.\\n Hudak, Joseph (April 12, 2018). \"Sugarland Announce New Album Bigger, Taylor Swift Collaboration\". Rolling Stone. Archived from the original on April 14, 2018. Retrieved April 13, 2018.\\n Havens, Lyndsey (October 9, 2018). \"Taylor Swift Breaks an All-Time AMA Record – And Urges People to Vote in Midterm Elections\". Billboard. Archived from the original on October 10, 2018. Retrieved October 10, 2018.\\n Stubblebine, Allison (November 13, 2017). \"Taylor Swift Announces First Round of Reputation Stadium Tour Dates\". Billboard. Archived from the original on November 14, 2017. Retrieved November 18, 2017.\\n Frankenberg, Eric (December 6, 2018). \"Taylor Swift Closes Reputation Stadium Tour with $345 Million\". Billboard. Archived from the original on December 9, 2018. Retrieved December 22, 2018.\\n Wang, Amy X. (November 19, 2018). \"Taylor Swift\\'s New Record Deal Affects Thousands of Other Musicians\". Rolling Stone. Archived from the original on November 26, 2018. Retrieved November 26, 2018.\\nWillman, Chris (August 27, 2018). \"Taylor Swift Stands to Make Music Business History as a Free Agent\". Variety. Archived from the original on August 29, 2018. Retrieved August 29, 2018.\\nAswad, Jem; Willman, Chris (November 19, 2018). \"Taylor Swift Signs New Deal With Universal Music Group\". Variety. Archived from the original on November 19, 2018. Retrieved November 19, 2018.\\n Grady, Constance (November 19, 2018). \"What Taylor Swift\\'s new record deal means for the music industry — and for her image\". Vox. Archived from the original on December 20, 2021. Retrieved December 20, 2021.\\n McKenna, Lyndsey (August 23, 2019). \"Stream Taylor Swift\\'s New Album, \\'Lover\\'\". NPR. Archived from the original on February 19, 2020. Retrieved September 10, 2019.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Archived from the original on August 23, 2019. Retrieved February 15, 2021.\\n Caulfield, Keith (September 1, 2019). \"Official: Taylor Swift\\'s \\'Lover\\' Debuts at No. 1 on Billboard 200 Chart With 867,000 Units Earned in First Week in U.S.\" Billboard. Archived from the original on September 1, 2019. Retrieved September 2, 2019.\\n White, Adam (August 23, 2019). \"Taylor Swift Lover Review Round-Up: Critics Say Album Feels \\'Evolutionary Rather Than Revolutionary\\'\". The Independent. Archived from the original on June 13, 2022. Retrieved June 13, 2022.\\n Moniuszko, Sara M. (August 23, 2019). \"Taylor Swift Lover Reviews: Critics Are Enamored by the \\'Earnest,\\' \\'Romantic\\' New Album\". USA Today. Archived from the original on August 23, 2019. Retrieved November 5, 2020.\\n Trust, Gary (May 6, 2019). \"Lil Nas X\\'s \\'Old Town Road\\' Tops Billboard Hot 100 For Fifth Week, Taylor Swift\\'s \\'Me!\\' Vaults to No. 2\". Billboard. Archived from the original on May 6, 2019. Retrieved June 14, 2019.\\n Trust, Gary (October 23, 2023). \"Taylor Swift\\'s \\'Cruel Summer\\' Hits No. 1 on Billboard Hot 100, Becoming Her 10th Leader\". Billboard. Archived from the original on October 24, 2023. Retrieved October 24, 2023.\\n \"Arashi Best-Of Tops Taylor Swift for IFPI\\'s Best-Selling Album of 2019\". Billboard. March 19, 2020. Archived from the original on March 19, 2020. Retrieved March 21, 2020.\\n \"2020 Grammy Awards: Complete Winners List\". The Recording Academy. November 20, 2019. Archived from the original on May 22, 2020. Retrieved February 15, 2021.\\n Grein, Paul (August 26, 2019). \"12 Records That Were Set at the 2019 VMAs\". Billboard. Archived from the original on January 30, 2020. Retrieved January 11, 2020.\\n Grady, Constance (September 1, 2019). \"The Taylor Swift/Scooter Braun controversy, explained\". Vox. Archived from the original on February 11, 2020. Retrieved August 23, 2019.\\n Beth, John (January 2, 2024). \"Taylor Swift\\'s Chart Triumph\". Square News.\\n \"The Taylor Swift, Scooter Braun, Justin Bieber row explained\". BBC News. July 1, 2019. Archived from the original on December 11, 2021. Retrieved July 18, 2021.\\n Willman, Chris (November 16, 2020). \"Taylor Swift Confirms Sale of Her Masters, Says She Is Already Re-Recording Her Catalog\". Variety. Archived from the original on December 3, 2022. Retrieved November 18, 2020.\\n Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases \\'Beautiful Ghosts,\\' Co-Written With Andrew Lloyd Webber for \\'Cats\\' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.\\n \"Golden Globes 2020: full list of nominations\". The Guardian. December 9, 2019. Archived from the original on December 10, 2019. Retrieved December 20, 2019.\\n Rooney, David (December 18, 2019). \"\\'Cats\\': Film Review\". The Hollywood Reporter. Archived from the original on December 20, 2019. Retrieved December 21, 2019.\\n Mamo, Heran (January 15, 2020). \"Taylor Swift Miss Americana Netflix Doc Has a Release Date & We\\'re So Ready for It\". Billboard. Archived from the original on April 24, 2020. Retrieved January 19, 2020.\\n Willman, Chris (February 6, 2020). \"Taylor Swift Moves to Universal Music Publishing Group with New Pact\". Variety. Archived from the original on February 12, 2020. Retrieved February 6, 2020.\\n Opperman, Jeff (March 12, 2021). \"Taylor Swift Is Singing Us Back to Nature\". The New York Times. Archived from the original on December 28, 2021. Retrieved May 24, 2021.\\n \"Taylor Swift to release surprise ninth album \\'Evermore\\' tonight\". NME. December 10, 2020. Archived from the original on December 10, 2020. Retrieved December 10, 2020.\\n Atkinson, Katie (December 15, 2020). \"Taylor Swift Isn\\'t So Sure She & Joe Alwyn Would Have Made Music Together If It Weren\\'t for Lockdown\". Billboard. Archived from the original on August 11, 2021. Retrieved February 18, 2021.\\n Schaffer, Claire (December 18, 2020). \"Aaron Dessner on How His Collaborative Chemistry With Taylor Swift Led to Evermore\". Rolling Stone. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Barna, Alyssa (December 16, 2020). \"These are the musicological reasons Taylor Swift\\'s new album sounds dull\". The Washington Post. Archived from the original on February 25, 2021. Retrieved November 3, 2021.\\n Snapes, Laura (October 14, 2022). \"\\'Genuine\\': why Taylor Swift can celebrate more than an album release\". The Guardian. Archived from the original on October 14, 2022. Retrieved October 14, 2022.\\n McGrath 2023, p. 79; Fogarty & Arnold 2021, p. 5.\\n Trust, Gary (January 28, 2021). \"Taylor Swift\\'s \\'Coney Island\\' and \\'No Body, No Crime\\' Debut on Airplay Charts, Joining \\'Willow\\'\". Billboard. Archived from the original on August 16, 2021. Retrieved February 2, 2021.\\n Willman, Chris (March 14, 2021). \"Taylor Swift Becomes First Woman to Win Album of the Year Grammy Three Times\". Variety. Archived from the original on December 2, 2021. Retrieved March 15, 2021.\\n Caulfield, Keith (January 7, 2021). \"Lil Baby\\'s My Turn Is MRC Data\\'s Top Album of 2020, Roddy Ricch\\'s \\'The Box\\' Most-Streamed Song\". Billboard. Archived from the original on January 7, 2021. Retrieved January 7, 2021.\\n Trust, Gary (December 21, 2020). \"Taylor Swift\\'s \\'Willow\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Willman, Chris (November 23, 2020). \"Taylor Swift Wins Three American Music Awards, Says She\\'s MIA Because of \\'Recording All of My Old Music\\'\". Variety. Archived from the original on December 2, 2021. Retrieved November 25, 2020.\\n Christman, Ed (July 19, 2021). \"Billboard\\'s U.S. Money Makers: The Top Paid Musicians of 2020\". Billboard. Archived from the original on July 24, 2021. Retrieved July 19, 2021.\\n • Christman, Ed (July 19, 2021). \"Billboard\\'s 2020 Global Money Makers: The 5 Top Highest Paid Musicians\". Billboard. Archived from the original on July 23, 2021. Retrieved July 19, 2021.\\n Caulfield, Keith (July 11, 2023). \"Taylor Swift\\'s Re-Recorded Speak Now Already Has 2023\\'s Biggest Week After 4 Days of Release\". Billboard. Archived from the original on July 11, 2023. Retrieved July 11, 2023.\\n Caulfield, Keith (April 18, 2021). \"Taylor Swift\\'s Re-Recorded Fearless Album Debuts at No. 1 on Billboard 200 Chart With Year\\'s Biggest Week\". Billboard. Archived from the original on April 18, 2021. Retrieved April 19, 2021.\\n Asker, Jim; Trust, Gary (February 22, 2021). \"Taylor Swift\\'s \\'Love Story (Taylor\\'s Version)\\' Debuts at No. 1 on Hot Country Songs Chart: \\'I\\'m So Grateful to the Fans\\'\". Billboard. Archived from the original on April 22, 2021. Retrieved February 22, 2021.\\n McCluskey, Megan (December 8, 2023). \"Breaking Down Taylor Swift\\'s 2023 Impact By the Numbers\". Time. Archived from the original on December 26, 2023. Retrieved December 26, 2023.\\n Horton, Adrian; Lee, Benjamin (February 6, 2023). \"Grammy awards 2023: list of winners\". The Guardian. Archived from the original on February 6, 2023. Retrieved February 6, 2023.\\n Corcoran, Nina (August 28, 2022). \"Taylor Swift Announces New Album Midnights, Breaks Record for Most Video of the Year Wins at 2022 VMAs\". Pitchfork. Archived from the original on August 29, 2022. Retrieved August 28, 2022.\\n \"Taylor Swift\\'s new album breaks Spotify streaming record\". The Guardian. October 22, 2022. Archived from the original on October 22, 2022. Retrieved October 22, 2022.\\n Harvilla, Rob (October 25, 2022). \"The Anti-Hero We Deserve: Taylor Swift and Her Polarizing \\'Midnights\\'\". The Ringer. Archived from the original on November 3, 2022. Retrieved November 3, 2022.\\n Light, Alan (October 24, 2022). \"Taylor Swift\\'s Midnights Does Something Astonishing. Even For Her\". Esquire. Archived from the original on January 17, 2023. Retrieved January 17, 2023.\\n Petridis, Alexis (October 21, 2022). \"Taylor Swift: Midnights Review – Small-Hours Pop Rich with Self-Loathing and Stereotype-Smashing\". The Guardian. Archived from the original on October 22, 2022. Retrieved October 21, 2022.\\n Spanos, Brittany (October 21, 2022). \"Taylor Swift Lets Us Into Her Darkest Dreams On Midnights\". Rolling Stone. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Sheffield, Rob (October 21, 2022). \"Welcome to the Lavender Labyrinth: Taylor Swift\\'s Midnights Is the Mastermind\\'s Ultimate Power Move\". Rolling Stone. Archived from the original on October 22, 2022. Retrieved October 23, 2022.\\n Balasaygun, Kaitlin (November 1, 2022). \"How Taylor Swift went back to the past and turned Midnights into her biggest album success yet\". CNBC. Archived from the original on December 30, 2022. Retrieved December 30, 2022.\\n Shafer, Ellise (October 21, 2022). \"Taylor Swift\\'s Midnights Breaks Spotify Record for Most-Streamed Album in a Single Day\". Variety. Archived from the original on October 21, 2022. Retrieved October 22, 2022.\\n Dailey, Hannah (December 6, 2022). \"Here Are All of Taylor Swift\\'s Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022.\\n Trust, Gary (June 5, 2023). \"Morgan Wallen\\'s \\'Last Night\\' No. 1 on Hot 100 for Ninth Week, Taylor Swift & Ice Spice\\'s \\'Karma\\' Blasts to No. 2\". Billboard. Archived from the original on October 26, 2023. Retrieved August 21, 2023.\\n Blistein, Jon; Guglielmi, Jodi (September 13, 2023). \"Taylor Swift Makes History at 2023 VMAs\". Rolling Stone. Archived from the original on September 13, 2023. Retrieved September 13, 2023.\\n West, Bryan (February 5, 2024). \"Taylor Swift makes Grammys history with fourth album of the year win for Midnights\". USA Today. Archived from the original on February 5, 2024. Retrieved February 5, 2024.\\n Caulfield, Keith (July 16, 2023). \"Taylor Swift\\'s Re-Recorded \\'Speak Now\\' Debuts at No. 1 on Billboard 200 With 2023\\'s Biggest Week\". Billboard. Archived from the original on July 19, 2023. Retrieved July 16, 2023.\\n Caulfield, Keith (November 5, 2023). \"Taylor Swift\\'s 1989 (Taylor\\'s Version) Debuts at No. 1 on Billboard 200 With Biggest Week in Nearly a Decade\". Billboard. Archived from the original on November 5, 2023. Retrieved November 5, 2023.\\n Sherman, Maria (November 29, 2023). \"Taylor Swift is Spotify\\'s most-streamed artist of 2023, ending Bad Bunny\\'s 3-year reign\". ABC News. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n Garcia, Thania (November 28, 2023). \"Taylor Swift Named Apple Music\\'s Artist of the Year; Morgan Wallen Tops Global Songs Chart\". Variety. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n \"Best of 2023 (Taylor\\'s Version) Playlist on Amazon Music\". Curated by Amazon\\'s Music Experts. Amazon Music. Retrieved December 3, 2023. Our most streamed artist of 2023 globally. It\\'s Taylor\\'s world and we are just living for it.\\n Caulfield, Keith (November 21, 2023). \"Taylor Swift Is Billboard\\'s Top Artist of 2023\". Billboard. Archived from the original on November 21, 2023. Retrieved November 21, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Returns to No. 1 on Billboard 200\". Billboard. Retrieved December 3, 2023.\\n Caulfield, Keith (January 10, 2024). \"Morgan Wallen\\'s One Thing at a Time Is Luminate\\'s Top Album of 2023 in U.S.\" Billboard. Archived from the original on January 10, 2024. Retrieved January 12, 2024.\\n Trust, Gary (November 6, 2023). \"Taylor Swift\\'s \\'Is It Over Now? (Taylor\\'s Version)\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved November 6, 2023.\\n Cohen, Jonathan (June 29, 2021). \"Aaron Dessner, Justin Vernon Rev Up Big Red Machine With Help From Taylor Swift\". Variety. Archived from the original on May 23, 2022. Retrieved February 10, 2022.\\n Strauss, Matthew (February 19, 2021). \"HAIM Enlist Taylor Swift for New \"Gasoline\" Remix\". Pitchfork. Archived from the original on February 19, 2021. Retrieved February 10, 2022.\\n Dailey, Hannah (February 11, 2022). \"Ed Sheeran & Taylor Swift Release \\'The Joker and the Queen\\' Remix: Watch the Video\". Billboard. Archived from the original on February 11, 2022. Retrieved February 11, 2022.\\n Rowley, Glenn (January 18, 2023). \"The National Unveils \\'First Two Pages of Frankenstein\\' Tracklist With Taylor Swift, Phoebe Bridgers & Sufjan Stevens\". Billboard. Archived from the original on January 18, 2023. Retrieved January 18, 2023.\\n Davis, Clayton (December 21, 2022). \"Taylor Swift Doesn\\'t Make Oscar Shortlist for All Too Well Short Film, but Advances for \\'Carolina\\' Original Song\". Variety. Archived from the original on December 24, 2022. Retrieved December 24, 2022.\\n Utley, Riley (October 13, 2022). \"Every Taylor Swift Movie Performance, Ranked\". CinemaBlend. Archived from the original on October 13, 2022. Retrieved October 14, 2022.\\n Lang, Brent (December 9, 2022). \"Taylor Swift Making Feature Directing Debut for Searchlight Pictures\". Variety. Archived from the original on December 10, 2022. Retrieved December 9, 2022.\\n Wood, Mikael; Brown, August (August 1, 2023). \"It\\'s a love story, L.A. just says yes: How Taylormania took over the world\". Los Angeles Times. Archived from the original on August 1, 2023. Retrieved August 1, 2023.\\n Mahdawi, Arwa (November 20, 2022). \"Swifties know: the Ticketmaster fiasco shows America has a monopoly problem\". The Guardian. Archived from the original on December 10, 2022. Retrieved November 20, 2022.\\n Murray, Conor (December 8, 2023). \"Taylor Swift\\'s Eras Tour Is First In History To Gross Over $1 Billion, Report Says\". Forbes. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Tapp, Tom (December 8, 2023). \"Taylor Swift\\'s \\'The Eras Tour\\' Grosses Over $1 Billion In 2023, The Biggest Haul For Any Act Ever\". Deadline. Archived from the original on February 6, 2024. Retrieved February 10, 2024.\\n Kaufman, Gil (November 28, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Concert Movie Passes $250 Million in Worldwide Grosses\". Billboard. Archived from the original on December 1, 2023. Retrieved December 1, 2023.\\n Nordyke, Kimberly (January 7, 2024). \"Golden Globes 2024 Winners List\". The Hollywood Reporter. Archived from the original on January 8, 2024. Retrieved January 8, 2024.\\n Adamczyk, Alicia; Abrams, Joseph (July 25, 2023). \"The brilliant marketing synergy of Taylor Swift\\'s Eras Tour and her rerecorded albums\". Fortune. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\\n Ingham, Tim (June 14, 2023). \"Reliving the Taylor Swift Catalog Sale Saga (And Following the Money...)\". Music Business Worldwide. Archived from the original on June 14, 2023. Retrieved June 15, 2023.\\n Blanchet, Brenton (November 20, 2023). \"Travis Kelce Shares the Real Story of How Taylor Swift Romance Began in Wide-Ranging Interview\". People. Archived from the original on November 20, 2023. Retrieved November 20, 2023.\\n Kelly, Samantha Murphy (January 25, 2024). \"Explicit, AI-generated Taylor Swift images spread quickly on social media\". CNN Business. Archived from the original on January 25, 2024. Retrieved January 25, 2024.\\n Phillips, Zoe G. (January 27, 2024). \"SAG-AFTRA and White House Issue Statements on Taylor Swift AI Nudes: \"We Have It in Our Power to Control These Technologies\"\". The Hollywood Reporter. Archived from the original on January 27, 2024. Retrieved January 27, 2024.\\n \"Taylor Swift Announces \\'Brand New Album\\' \\'The Tortured Poets Department\\' with 13th Grammy Win\". Peoplemag. Archived from the original on February 10, 2024. Retrieved February 10, 2024.\\n Cairns, Dan (March 5, 2009). \"Swift rise of the anti-diva\". The Australian. Archived from the original on December 24, 2014. Retrieved July 2, 2012.\\n Bream, Jon (December 7, 2007). \"Music: OMG! Taylor\\'s senior year\". Star Tribune. Archived from the original on July 12, 2015. Retrieved July 1, 2012.\\n Newman, Melinda (December 19, 2008). \"Taylor Swift Sessions Interview\". AOL. Archived from the original on October 9, 2012. Retrieved March 25, 2011.\\n \"Swift starts world tour in Asia, pushes \"Speak Now\\' in NY\". Country Standard Time. October 23, 2007. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n \"News : 20 Questions With Taylor Swift\". Country Music Television. November 12, 2007. Archived from the original on November 17, 2014. Retrieved April 18, 2012.\\n McCafferty, Dennis (April 13, 2008). \"Taylor\\'s Swift rise\". USA Weekend. Archived from the original on November 14, 2012. Retrieved April 17, 2012.\\n \"Interview with Taylor Swift\". Time. April 23, 2009. Archived from the original on October 23, 2013. Retrieved July 1, 2012.\\n \"Taylor Swift Style: Singer Won\\'t Take Her Clothes Off, Wants People To Focus On Music\". HuffPost. October 23, 2012. Archived from the original on January 4, 2015. Retrieved January 4, 2015.\\n \"InStyle meets country singing sensation Taylor Swift\". InStyle UK. October 26, 2010. Archived from the original on May 27, 2013. Retrieved May 29, 2012.\\n \"Joni Mitchell: 15 Great Artists Influenced by the \\'Blue\\' Singer\". Rolling Stone. June 22, 2016. Archived from the original on December 26, 2021. Retrieved December 26, 2020.\\n Jenkins, Sally (September 28, 2023). \"You thought you knew the NFL. Now meet Taylor\\'s Version\". The Washington Post. Archived from the original on October 26, 2023. Retrieved September 29, 2023.\\n Bonaguro, Alison (November 8, 2012). \"Offstage: Taylor Swift Inspired by Female Singer-Songwriters of the \\'90s\". CMT. Archived from the original on October 4, 2023. Retrieved September 29, 2023.\\n \"Taylor Swift\\'s Favorite Music\". The Oprah Winfrey Show. Archived from the original on January 16, 2016. Retrieved October 23, 2012.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Mansfield, Brian (October 23, 2010). \"Taylor Swift learns to \\'Speak Now,\\' reveal her maturity\". USA Today. Archived from the original on November 4, 2012. Retrieved July 1, 2012.\\n Block, Melissa (October 31, 2014). \"\\'Anything That Connects\\': A Conversation With Taylor Swift\". NPR Music. Archived from the original on February 6, 2015. Retrieved October 26, 2019.\\n Eells, Josh (September 8, 2014). \"The Reinvention of Taylor Swift\". Rolling Stone. Archived from the original on June 4, 2016. Retrieved June 8, 2016.\\n Reid, Poppy (November 2, 2021). \"The Curious Case of Keith Urban\". Rolling Stone. Archived from the original on November 19, 2021. Retrieved November 3, 2021.\\n Hiatt, Brian (June 18, 2019). \"Taylor Swift: The Rolling Stone Interview\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved April 14, 2022.\\n Weatherby, Taylor (March 10, 2021). \"Taylor Swift\\'s Road to Folklore\". The Recording Academy. Archived from the original on November 25, 2021. Retrieved November 24, 2021.\\n Franssen, Gaston (January 2, 2022). \"Policing the celebrity of Taylor Swift: introduction\". Celebrity Studies. 13 (1): 90–92. doi:10.1080/19392397.2022.2026148. S2CID 246997248.\\n Savage, Mark (October 19, 2022). \"Midnights: What we know about Taylor Swift\\'s songwriting\". BBC News. Archived from the original on October 19, 2022. Retrieved October 20, 2022.\\n Bruner, Raisa (August 24, 2020). \"Let\\'s Break Down Taylor Swift\\'s Tender New Album Folklore\". Time. Archived from the original on July 31, 2020. Retrieved October 20, 2022.\\n McNutt 2020, p. 77.\\n Hughes 2017, p. 206; Perone 2017, p. 33.\\n \"Taylor Swift: Album Guide\". Rolling Stone. Archived from the original on December 5, 2012. Retrieved December 5, 2012.\\n \"Pop and Rock Listings July 22 – 28\". The New York Times. July 21, 2011. Archived from the original on January 28, 2012. Retrieved July 12, 2012.\\n \"Taylor Swift Remade Fearless as Taylor\\'s Version. Let\\'s Discuss\". The New York Times. April 9, 2021. Archived from the original on April 9, 2021. Retrieved April 21, 2021.\\n Petridis, Alexis (March 6, 2009). \"Taylor Swift: Fearless\". The Guardian. Archived from the original on October 16, 2013. Retrieved August 13, 2022.\\n Jones, Sasha-Frere (November 11, 2008). \"Prodigy\". The New Yorker. Archived from the original on October 21, 2016. Retrieved August 14, 2022.\\n Hughes 2017, p. 206.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. p. 5. Archived from the original on November 20, 2022. Retrieved August 12, 2022.\\n McNutt 2020, p. 78.\\n Rosen, Jody (November 17, 2013). \"Why Taylor Swift Is the Reigning Queen of Pop\". Vulture. Archived from the original on November 19, 2013. Retrieved November 9, 2020.\\n McNutt 2020, p. 79.\\n Levine, Nick (August 21, 2019). \"Taylor Swift\\'s Lover: The struggle to maintain superstardom\". BBC. Archived from the original on March 1, 2021. Retrieved October 29, 2021.\\n da Silva, Michelle (November 13, 2017). \"Taylor Swift Has Changed for the Worse on Reputation\". Now. Archived from the original on July 26, 2020. Retrieved July 27, 2020.\\n Tucker, Ken (November 13, 2017). \"Taylor Swift Pushes Further Into Electro-Pop With \\'Reputation\\'\". NPR. Archived from the original on October 27, 2021. Retrieved April 8, 2023.\\n Moreland, Quinn (October 24, 2022). \"Taylor Swift: Midnights\". Pitchfork. Archived from the original on October 24, 2022. Retrieved April 8, 2023.\\n Ryan, Elise (October 21, 2022). \"Review: Taylor Swift gets dark, electric on \\'Midnights\\'\". Associated Press News. Archived from the original on October 21, 2022. Retrieved April 8, 2023.\\n Winter, Velvet (November 12, 2022). \"Like The Beatles, Madonna and Kylie Minogue before her, Taylor Swift is masterful at pivoting\". ABC News. Archived from the original on November 13, 2022. Retrieved November 13, 2022.\\n McNutt 2020, p. 79; Sloan 2021, p. 17.\\n Sloan 2021, p. 17.\\n Hyden, Steven (March 10, 2021). \"Taylor Swift, Indie-rock star? Long, Long ago, this might have felt strange\". The New York Times. Archived from the original on April 13, 2021. Retrieved April 13, 2021.\\n Caramanica, Jon (July 26, 2020). \"Taylor Swift, A Pop Star Done with Pop\". The New York Times. Archived from the original on September 10, 2020. Retrieved August 14, 2022.\\n Harbron, Lucy (November 11, 2021). \"Why Taylor Swift\\'s \\'Red\\' Is Her Turning Point\". Clash. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Gerber, Brady (July 27, 2020). \"The Story Behind Every Song on Taylor Swift\\'s folklore\". Vulture. Retrieved December 12, 2023.\\n Willman, Chris (October 21, 2022). \"Taylor Swift\\'s Midnights Marks a Return to Electronic, Confessional Pop That\\'s Worth Losing Sleep Over: Album Review\". Variety. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Fulford 2014, p. 192.\\n \"Taylor Swift Deepens Her Goth-Folk Vision on the Excellent \\'Evermore\\'\". Rolling Stone. December 11, 2020. Archived from the original on December 11, 2020. Retrieved August 23, 2022.\\n \"The 200 Greatest Singers of All Time\". Rolling Stone. January 1, 2023. Archived from the original on January 1, 2023. Retrieved January 1, 2023.\\n Provenzano 2018, p. 173.\\n Roland, Tom (October 15, 2010). \"Taylor Swift: The Billboard Cover Story\". Billboard. Archived from the original on October 18, 2010. Retrieved July 3, 2012.\\n Provenzano 2018, pp. 173–174.\\n Provenzano 2018, p. 174.\\n Powers, Ann (October 25, 2010). \"Album review: Taylor Swift\\'s Speak Now\". Los Angeles Times. Archived from the original on October 28, 2010. Retrieved October 25, 2010.\\n Willman, Chris (November 10, 2017). \"Album Review: Taylor Swift\\'s \\'Reputation\\'\". Variety. Retrieved April 8, 2023.\\n Cox, Jamieson (November 13, 2017). \"Taylor Swift: Reputation\". Pitchfork. Retrieved April 8, 2023.\\n Powers, Ann (November 10, 2017). \"The Old Taylor\\'s Not Dead\". NPR. Retrieved April 8, 2023.\\n Wilson, Carl (November 13, 2017). \"On Reputation, the \"Old Taylor\" Is Dead, but the New One Isn\\'t Quite Ready to Come to the Phone\". Slate. Retrieved April 8, 2023.\\n DeCaro, Alessandro (October 19, 2022). \"10 best Taylor Swift scene covers\". Alternative Press. Retrieved July 1, 2023.\\n Barker, Andrew (November 27, 2020). \"Folklore: The Long Pond Studio Sessions Review\". Variety. Retrieved May 31, 2021.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved April 8, 2023.\\n Kornhaber, Spencer (July 28, 2020). \"Taylor Swift Is No Longer Living in the Present\". The Atlantic. Retrieved August 23, 2022.\\n Brehian, Tom (July 24, 2020). \"Review: Taylor Swift\\'s \\'folklore\\' Is An Indie Record Unconcerned With Being Cool\". Stereogum. Retrieved August 23, 2022.\\n Willman, Chris (December 11, 2020). \"Taylor Swift Has Her Second Great Album of 2020 With \\'Evermore\\': Album Review\". Variety. Retrieved August 23, 2022.\\n Sodomsky, Sam (December 15, 2020). \"Taylor Swift: evermore\". Pitchfork. Archived from the original on December 15, 2020. Retrieved December 14, 2020.\\n McCormick, Neil (April 9, 2021). \"Taylor Swift copies her younger self – and she sounds even more Fearless today\". The Daily Telegraph. Retrieved August 23, 2022.\\n \"Taylor Swift forges ahead with a dreamy throwback in Fearless (Taylor\\'s Version)\". The A.V. Club. April 9, 2021. Archived from the original on April 9, 2021. Retrieved August 23, 2022.\\n Bernstein, Jonathan (April 9, 2021). \"Taylor Swift Carefully Reimagines Her Past on \\'Fearless: Taylor\\'s Version\\'\". Rolling Stone. Retrieved August 23, 2022.\\n Solomon, Kate (November 12, 2021). \"Taylor Swift, Red (Taylor\\'s Version), Review: How Brilliant She Is When Her Heart Is in Tatters\". i. Archived from the original on November 12, 2021. Retrieved November 13, 2021.\\n Snapes, Laura (August 23, 2020). \"Taylor Swift: Folklore review – bombastic pop makes way for emotional acuity\". The Guardian. Retrieved August 23, 2022.\\n Snapes, Laura (November 12, 2021). \"Taylor Swift: Red (Taylor\\'s Version) Review – Getting Back Together with a Classic\". The Guardian. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Kelly, Fred (October 21, 2022). \"Taylor Swift\\'s Midnights: what the critics are saying\". The Week. Retrieved October 23, 2022.\\n Petrusich, Amanda (June 12, 2023). \"The Startling Intimacy of Taylor Swift\\'s Eras Tour\". The New Yorker. Retrieved June 12, 2023.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC Music. August 22, 2019. Retrieved November 10, 2021.\\n Emily, Lee (November 5, 2021). \"Here Are Taylor Swift\\'s Best Bridges On \\'Red\\' Ranked\". iHeartRadio. Retrieved November 10, 2021.\\n Eggertsen, Chris (September 20, 2022). \"Taylor Swift\\'s Iconic Songwriting Credits Amplified By Spotify With Dedicated Page\". Billboard. Retrieved September 20, 2022.\\n Bate, Jonathan (April 10, 2023). \"Why Taylor Swift is a literary giant — by a Shakespeare professor\". The Times. Retrieved April 10, 2023.\\n Pazzanese, Christina (August 2, 2023). \"So what exactly makes Taylor Swift so great?\". Harvard Gazette. Retrieved August 7, 2023.\\n Murphy, Sam (November 10, 2021). \"How \\'Red\\' Became The Most Pivotal Record In Taylor Swift\\'s Career\". Junkee. Retrieved November 10, 2021.\\n Siroky, Mary (November 9, 2021). \"Every Taylor Swift Album Ranked from Worst to Best\". Consequence. Retrieved November 10, 2021.\\n Bruner, Raisa (May 24, 2021). \"How Olivia Rodrigo Become America\\'s Biggest New Pop Star\". Time. Retrieved November 10, 2021.\\n Mulvey, John (September 16, 2023). \"Arctic Monkeys, Taylor Swift, Kendrick Lamar, Lana Del Rey And The 30 Artists Who Will Shape The Next 30 Years\". Mojo. Retrieved September 19, 2023.\\n Farley, Christopher John (October 22, 2010). \"Taylor Swift\\'s Solo Act\". The Wall Street Journal. Archived from the original on February 1, 2015. Retrieved May 24, 2012.\\n Jo Sales, Nancy; Diehl, Jessica (April 2013). \"Taylor Swift\\'s Telltale Heart\". Vanity Fair. Archived from the original on January 30, 2017. Retrieved February 4, 2017.\\n Daly, Rhian (December 13, 2020). \"Taylor Swift says her diaristic songwriting style was \\'unsustainable\\' for her future\". NME. Retrieved February 17, 2021.\\n Gallo, Phil (October 22, 2012). \"Taylor Swift\\'s Red: The Billboard Cover Story\". Billboard. Archived from the original on June 14, 2013.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift Is Angry, Darn It\". The New York Times. Archived from the original on September 11, 2012. Retrieved July 2, 2012.\\n Kelly, James (August 26, 2009). \"Taylor Swift writing her own songs and rules\". The Atlanta Journal-Constitution. Archived from the original on September 8, 2014. Retrieved July 30, 2012.\\n Lansky, Sam (November 8, 2017). \"Why Taylor Swift\\'s Red Is Her Best Album\". Billboard. Retrieved December 27, 2020.\\n Hiatt, Brian (September 30, 2019). \"9 Taylor Swift Moments That Didn\\'t Fit in Our Cover Story\". Rolling Stone. Archived from the original on October 1, 2019. Retrieved December 9, 2019.\\n \"Taylor Swift Talks Newfound \\'Freedom,\\' \\'Lover\\' Tour Plans and So Much More\". On Air with Ryan Seacrest. August 27, 2019. Retrieved March 22, 2020.\\n Yuan, Jada (December 30, 2009). \"Microwaving a tragedy: The marriage of romance and romanticism in \\'00s pop\". Las Vegas Weekly. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n Rotman, Natalie (January 9, 2009). \"Colbie Caillat has \\'Breakthrough\\' with sophomore CD\". Reading Eagle. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC News. Retrieved February 2, 2021.\\n Knibbs, Kate (August 21, 2019). \"Ten Years of Taylor Swift: How the Pop Star Went From Sweetheart to Snake (and Back Again?)\". The Ringer. Retrieved December 13, 2021.\\n Stubbs, Dan (October 9, 2015). \"Taylor Swift: Power, Fame And The Future – The Full NME Cover Interview\". NME. Retrieved February 17, 2021.\\n Weber, Theon (November 3, 2010). \"The Iceberg Songs of Taylor Swift\". The Village Voice. Archived from the original on November 4, 2015. Retrieved July 30, 2012.\\n Beck, Julia (October 27, 2014). \"Taylor Swift Is So Much More Fun Now That She\\'s Jaded\". The Atlantic. Archived from the original on September 27, 2016. Retrieved October 30, 2021.\\n Willman, Chris (October 10, 2010). \"Princess Crossover\". New York. Archived from the original on July 27, 2013. Retrieved July 1, 2012.\\n Rosen, Jody (November 13, 2008). \"Fearless\". Rolling Stone. Archived from the original on August 15, 2012. Retrieved July 1, 2012.\\n Powers, Ann (October 30, 2014). \"The Many New Voices of Taylor Swift\". NPR. Retrieved June 2, 2022.\\n Stoeffel, Kat (November 16, 2012). \"Stop Asking Taylor Swift to Apologize for Writing Songs About Ex-Boyfriends – The Cut\". New York. Archived from the original on November 27, 2012. Retrieved February 25, 2013.\\n Raven, Robin (March 16, 2022). \"10 Artists Who Have Stood Up For Women In Music: Taylor Swift, Lizzo & More\". The Recording Academy.\\n \"Cover Preview: Taylor Swift Fights Back About Her Love Life, the Hyannis Port House—and Has Words for Tina Fey and Amy Poehler\". Vanity Fair. March 5, 2013. Archived from the original on August 8, 2016. Retrieved August 3, 2016.\\n Dominus, Susan (November 16, 2012). \"The Many Insecurities of Taylor Swift\". The New York Times. Archived from the original on June 17, 2016.\\n Doyle, Patrick (November 13, 2020). \"Musicians on Musicians: Paul McCartney and Taylor Swift\". Rolling Stone. Archived from the original on November 30, 2020. Retrieved February 7, 2021.\\n Olivier, Bobby (December 11, 2020). \"Taylor Swift\\'s \\'Evermore\\' Is an Undeniable Folk-Pop Masterpiece\". Spin. Retrieved February 1, 2021.\\n Shutler, Ali (October 9, 2022). \"Taylor Swift organises her lyrics into three \\'dorky\\' pen-themed categories\". NME. Retrieved October 18, 2022.\\n Browne, Erin (October 21, 2022). \"All of Taylor Swift\\'s Famously Devastating Track 5\\'s, Ranked\". Vulture. Retrieved October 27, 2022.\\n \"NMPA to Honor Taylor Swift with Songwriter Icon Award Among Other 2021 Annual Meeting Honorees\". National Music Publishers\\' Association. May 24, 2021. Retrieved May 24, 2021.\\n Linker, Damon (November 26, 2021). \"Taylor Swift, Phoebe Bridgers, and Rihanna: How women took over songwriting\". The Week. Retrieved November 28, 2021.\\n Greco, Patti (November 13, 2017). \"A Harvard Professor Critiques Taylor Swift\\'s New Poems\". Cosmopolitan. Retrieved December 21, 2021.\\n Sheffield, Rob (October 13, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Movie Will Make You Sing, Scream, and Sob\". Rolling Stone. Retrieved December 8, 2023.\\n Zacharek, Stephanie (December 14, 2023). \"The Eras Tour Movie Is Irresistible No Matter How Much You Think You Like Taylor Swift\". TIME. Retrieved December 8, 2023.\\n \"Ticketing Shmicketing: Taylor Swift\\'s \\'Eras Tour\\' Debut Slays (And Could Break All-Time Touring Record)\". Pollstar. March 18, 2023. Archived from the original on March 20, 2023. Retrieved June 30, 2023.\\n Aramesh, Waiss David (March 18, 2023). \"Taylor Swift\\'s The Eras Tour Is a 3-Hour Career-Spanning Victory Lap\". Rolling Stone. OCLC 1787396. Archived from the original on March 18, 2023. Retrieved June 30, 2023.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved July 23, 2023.\\n McCormick, Neil (March 18, 2023). \"Taylor Swift: The Eras Tour, review: a roaring spectacle of a comeback\". The Daily Telegraph. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kornhaber, Spencer (March 18, 2023). \"What Made Taylor Swift\\'s Concert Unbelievable\". The Atlantic. Retrieved June 30, 2023.\\n Foggatt, Tyler (June 3, 2023). \"Look What Taylor Made Us Do\". The New Yorker. Retrieved June 30, 2023.\\n Seibert, Brian (August 9, 2023). \"How to Command a Stage Without Great Dance Moves (Taylor\\'s Version)\". The New York Times. Retrieved August 11, 2023.\\n Krelenstein, Greg (May 21, 2018). \"TAYLOR SWIFT\\'S REPUTATION TOUR IS A POP TRIUMPH\". V. Archived from the original on May 22, 2018. Retrieved May 22, 2018.\\n Frere-Jones, Sasha (November 3, 2008). \"Prodigy\". The New Yorker. Retrieved June 30, 2023.\\n Horton, Adrian (March 18, 2023). \"Taylor Swift review – pop\\'s hardest-working star gives Eras tour her all\". The Guardian. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kaplan, Ilana (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' tour is a thrilling spectacle from a pop mastermind\". i. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Young, Alex (March 27, 2023). \"Taylor Swift\\'s \"The Eras Tour\" Is a Triumph of Spectacle and Stamina: Review\". Consequence. Retrieved June 30, 2023.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved July 1, 2023.\\n O\\'Connor, Roisin (June 8, 2018). \"Taylor Swift \\'reputation\\' stadium tour review: Dazzling pop spectacle from the star who doesn\\'t stand still\". The Independent. Archived from the original on June 30, 2023. Retrieved December 22, 2019.Savage, Mark (March 18, 2023). \"Taylor Swift launches Eras tour with three-hour, 44-song set\". BBC News. Archived from the original on March 18, 2023. Retrieved March 18, 2023. Sisario, Ben (November 5, 2023). \"How Taylor Swift\\'s Eras Tour Conquered the World\". The New York Times. Retrieved August 12, 2023. DeVille, Chris (July 12, 2018). \"Big Reputation: A Trip To Taylor Swift\\'s Hyper-Maximalist Stadium Tour\". Stereogum. Retrieved June 30, 2023.\\n Procell, Carlie; Padilla, Ramon (April 28, 2023). \"Taylor Swift tour has many \\'eras.\\' We tracked her movements to give you the look and feel\". USA Today. Retrieved June 30, 2023.\\n \"Taylor Swift Shares Stunning \\'Wildest Dreams\\' Performance from Grammy Museum\". Billboard. January 5, 2016. Retrieved October 29, 2023.\\n Burgham, Lydia (November 10, 2018). \"Taylor Swift in Auckland, reviewed: Despite the snakes, her Reputation shines on\". The Spinoff. Retrieved December 10, 2019.\\n Dodd, Sophie (November 15, 2023). \"All About Taylor Swift\\'s Parents, Scott and Andrea Swift\". People. Retrieved December 3, 2023.\\n Swift, Taylor (March 15, 2013). \"\"Sparks Fly\" (acoustic) Live on the RED Tour!\" – via YouTube.\\n Lewis, Randy (April 3, 2011). \"Academy of Country Music Awards: Las Vegas welcomes Miranda Lambert, Taylor Swift with open arms\". Los Angeles Times. Retrieved August 20, 2023.\\n Ritchie, Mike (March 8, 2020). \"Why Taylor Swift is making the ukulele cool again\". The Herald. Retrieved July 1, 2023.\\n Gensler, Andy (August 17, 2023). \"The Showgoer: The Greatest Show On Earth — Taylor Swift\\'s \\'Eras Tour\\' — Is All That And Far More\". Pollstar. Retrieved August 19, 2023.\\n Sheffield, Rob (May 9, 2018). \"Why Taylor Swift\\'s \\'Reputation\\' Tour Is Her Finest Yet\". Rolling Stone. Retrieved June 30, 2023.\\n Willman, Chris (May 16, 2018). \"Taylor Swift\\'s \\'Reputation\\' Tour Has Bad Blood, Good Will, Sex Appeal and Serpents\". Variety. Retrieved December 22, 2019.\\n Willman, Chris (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' Show Is a Three-Hour, 44-Song Epic That Leaves \\'Em Wanting More: Concert Review\". Variety. Archived from the original on March 18, 2023. Retrieved June 30, 2018.\\n Ordoña, Michael (September 9, 2022). \"Taylor Swift wants an Oscar. So she took \\'All Too Well\\' to TIFF\". Los Angeles Times. Retrieved September 10, 2022.\\n CMT.com Staff (May 4, 2011). \"Taylor Swift\\'s \"Mean\" Video Debuts Friday\". CMT. Archived from the original on June 19, 2019. Retrieved June 19, 2019.\\n Anitai, Tamar (August 27, 2010). \"Video Premiere: Taylor Swift, \\'Mine\\'\". MTV News. Archived from the original on April 29, 2019. Retrieved June 19, 2019.\\n Bonaguro, Alison (May 6, 2011). \"OFFSTAGE: Taylor Swift Isn\\'t \\'Mean\\' at All, Director Says\". CMT News. Archived from the original on June 19, 2019. Retrieved June 20, 2019.\\n Tailor, Leena (September 1, 2017). \"Exclusive: Taylor Swift\\'s Director Joseph Kahn on How Her Image Invokes a Double Standard: \\'She\\'s a Genius\\'\". Entertainment Tonight. Archived from the original on June 19, 2019.\\n O\\'Connell, Michael (October 9, 2015). \"Taylor Swift and Jimmy Fallon Among Early Emmy Winners\". The Hollywood Reporter. Archived from the original on June 19, 2019.\\n Forbes, Jihan (May 14, 2015). \"Peep Taylor Swift\\'s Star-Studded Cast for Her \\'Bad Blood\\' Music Video\". The Fashion Spot. Retrieved May 13, 2020.\\n \"9 Things You Might Have Missed in Taylor Swift\\'s Netflix Concert Film\". E! News. December 31, 2018. Retrieved September 10, 2022.\\n Spanos, Brittany (April 25, 2019). \"Watch Taylor Swift, Brendon Urie\\'s Colorful \\'ME!\\' Video\". Rolling Stone. Archived from the original on April 26, 2019. Retrieved April 25, 2019.\\nMoore, Sam (August 23, 2019). \"Watch Taylor Swift\\'s colourful new video for \\'Lover\\'\". NME. Archived from the original on August 27, 2019. Retrieved August 27, 2019.\\nZemler, Emily (June 17, 2019). \"Watch Taylor Swift Reunite With Katy Perry in \\'You Need to Calm Down\\' Video\". Rolling Stone. Archived from the original on June 17, 2019. Retrieved June 17, 2019.\\n Mylrea, Hannah (February 28, 2020). \"Every incredible Easter Egg in Taylor Swift\\'s \\'The Man\\' video\". NME. Retrieved March 9, 2020.\\nSpanos, Brittany; Legaspi, Althea (July 24, 2020). \"Taylor Swift Blends Fantastical With Personal in \\'Cardigan\\' Video\". Rolling Stone. Retrieved July 27, 2020.\\n\"Justin Bieber & Megan Thee Stallion Lead 2021 MTV VMA Nominations\". Billboard. Retrieved August 17, 2021.\\n Weatherby, Taylor (February 5, 2023). \"Taylor Swift Makes GRAMMY History (Again) With Best Music Video Win For \"All Too Well: The Short Film\"\". Grammy Awards. Retrieved February 6, 2023.\\n Lansky, Sam (December 6, 2023). \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift\". The Recording Academy. Archived from the original on August 12, 2016. Retrieved August 3, 2016.\\n Friedlander, Whitney (September 10, 2015). \"Taylor Swift, Jimmy Fallon Among Juried Emmy Award Winners\". Variety. Archived from the original on September 15, 2015. Retrieved August 3, 2016.\\n \"Taylor Swift dominates AMAs with 6 wins, extending lead as show\\'s most-decorated artist\". KTRK-TV. November 21, 2022. Retrieved November 21, 2022.\\n Grein, Paul (November 19, 2023). \"After the 2023 Billboard Music Awards, Who Is the All-Time Biggest Winner?\". Billboard. Retrieved November 19, 2023.\\n See Guinness World Records by Taylor Swift\\n Lewis, Randy (November 4, 2013). \"Taylor Swift to receive rare Pinnacle Award at CMA Awards Nov. 6\". Los Angeles Times. Retrieved May 13, 2020.\\n \"Taylor Swift Nashville Tickets\". Excite. Archived from the original on February 3, 2015. Retrieved February 2, 2015.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on March 14, 2014. Retrieved February 2, 2015.\\n \"Songwriters Hall of Fame\". Songwriters Hall of Fame. Archived from the original on November 29, 2014. Retrieved February 2, 2015.\\n \"The 100 Greatest Songwriters of All Time\". Rolling Stone. Archived from the original on September 2, 2017. Retrieved August 28, 2017.\\n Polanco, Luis (April 5, 2016). \"Taylor Swift to Receive First-Ever Taylor Swift Award From BMI\". Billboard. Retrieved October 21, 2020.\\n Jolly, Nathan (November 17, 2019). \"Why Taylor Swift is to blame for latest twist in music rights drama\". News.com.au. Retrieved November 17, 2019.\\n \"10 Life mantras by Taylor Swift to live by\". India Today. December 13, 2016. Archived from the original on February 9, 2019. Retrieved July 16, 2020.\\n Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Decade Taylor Swift: \\'I Do Want My Music to Live On\\'\". Billboard. Retrieved December 11, 2019.\\n \"Taylor Swift to receive BRITs Global Icon award\". Official Charts Company. May 9, 2021. Retrieved May 10, 2021.\\n \"Taylor Swift to receive Global Icon Award!\". Brit Awards. May 9, 2021. Retrieved May 10, 2021.\\n Paine, Andre (February 22, 2023). \"Taylor Swift wins IFPI\\'s 2022 Global Recording Artist Of The Year Award\". Music Week. Retrieved February 22, 2023.\\n Brandle, Lars (November 5, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Debuts at U.K. No. 1 With \\'Massive\\' Sales\". Billboard. Retrieved November 5, 2023.\\n \"Female artists with the most Irish Number 1 albums since 2000\". Official Charts Company. November 5, 2020. Retrieved February 23, 2021.\\n Wang, Dennis (April 16, 2021). \"Taylor Swift\\'s Fearless hits the right note in China, again\". People\\'s Daily. Retrieved June 26, 2021.\\n Brandle, Lars (July 7, 2023). \"Taylor Swift Sets Chart Record In Australia With Top-Five Sweep\". Billboard. Retrieved February 13, 2024.\\n Brandle, Lars (February 9, 2024). \"Ahead of \\'The Eras Tour\\' of Australia, Taylor Swift Sweeps Top 5\". Billboard. Retrieved February 13, 2024.\\n \"Taylor beats Swift\". Australian Recording Industry Association. July 14, 2023. Archived from the original on July 14, 2023. Retrieved July 19, 2023.\\n \"Taylor sweeps the record\". Australian Recording Industry Association. July 7, 2023. Retrieved July 7, 2023.\\n Cumulative touring gross:\\n\"Top Touring Artist of the Pollstar Era\" (PDF). Pollstar. June 10, 2022. Archived (PDF) from the original on August 5, 2022. Retrieved August 4, 2022.\\nAllen, Bob (September 26, 2023). \"What A Friggin\\' Year! 2023 Boxoffice Results Remain At Record Highs\". Pollstar. Retrieved October 18, 2023.\\n Gensler, Andy (December 8, 2023). \"Taylor Swift Sets All-Time Touring Record With $1 Billion Gross\". Pollstar. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Willman, Chris (December 21, 2020). \"Taylor Swift\\'s \\'Evermore\\' Sells a Million Worldwide in First Week\". Variety. Archived from the original on January 11, 2021. Retrieved December 21, 2020.\\n Paine, Andre (December 22, 2022). \"Taylor Swift Achieves More Than 6 Million Global Units for Midnights and 37 Billion Total Streams in 2022\". Music Week. Archived from the original on December 22, 2022. Retrieved July 19, 2023.\\n Young, Alex (October 31, 2022). \"Taylor Swift broke 73 records with release of new album Midnights\". Consequence. Archived from the original on October 31, 2022. Retrieved July 19, 2023.\\n Grein, Paul (November 8, 2023). \"Taylor Swift Is Apple Music\\'s 2023 Artist of the Year\". Billboard. Retrieved November 8, 2023.\\n Willman, Chris (October 28, 2023). \"Taylor Swift Beats Her Own Spotify Record for Most Single-Day Streams for an Artist With \\'1989 (Taylor\\'s Version)\\' Release\". Variety. Archived from the original on October 29, 2023. Retrieved October 29, 2023.\\n @billboardcharts (January 19, 2022). \"Most entries on the #Global200 chart in a single week\" (Tweet) – via Twitter.\\n \"Taylor Swift Chart History (Billboard Global 200)\". Billboard. Retrieved July 17, 2023.\\n \"Greatest of All Time Artists\". Billboard. Archived from the original on November 14, 2019. Retrieved November 15, 2019.\\n Zellner, Xander (January 25, 2024). \"Taylor Swift Tallies Record-Extending 95th Week at No. 1 on Artist 100 Chart\". Billboard. Retrieved January 26, 2024.\\n Caulfield, Keith (December 31, 2023). \"Taylor Swift Surpasses Elvis Presley for Most Weeks at No. 1 on Billboard 200 Among Soloists\". Billboard. Retrieved January 3, 2024.\\n Trust, Gary (January 22, 2024). \"Ariana Grande\\'s \\'Yes, And?\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved January 22, 2024.\\n \"Taylor Swift Chart History (Top Country Albums)\". Billboard. Retrieved July 17, 2023.\\n \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved January 14, 2021.\\n Trust, Gary (July 28, 2023). \"Taylor Swift Breaks Record for Most No. 1s on Pop Airplay Chart As \\'Cruel Summer\\' Becomes Her 12th\". Billboard. Archived from the original on September 29, 2023. Retrieved October 19, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift Makes History With Five of the Top 10 Albums on the Billboard 200\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (July 17, 2023). \"Taylor Swift Has 11 Albums on the Billboard 200 Chart for the First Time\". Billboard. Retrieved July 19, 2023.\\n McIntyre, Hugh (January 22, 2024). \"Taylor Swift Made Billboard History–Now Only She Can Match Her Own Feat\". Forbes. Retrieved January 22, 2024.\\n Caulfield, Keith (December 29, 2023). \"Taylor Swift Has the Top Four on the Album Sales Chart for the First Time\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (January 3, 2024). \"Taylor Swift Has 7 of the Top 10 on Billboard\\'s Album Sales Chart\". Billboard. Retrieved January 23, 2024.\\n \"RIAA – Top Artists (Digital Singles)\". Recording Industry Association of America. Retrieved July 17, 2023.\\n Ahlgrim, Callie (July 12, 2023). \"There are 116 songs in history that have been certified diamond — here they all are\". Insider Inc. Retrieved July 17, 2023.\\n Caulfield, Keith (October 30, 2022). \"Taylor Swift\\'s Midnights Blasts in at No. 1 on Billboard 200 Chart With Biggest Week for an Album in 7 Years\". Billboard. Retrieved October 30, 2022.\\n Nicks, Stevie (April 29, 2010). \"The 2010 Time 100: Taylor Swift\". Time. Archived from the original on November 29, 2013. Retrieved April 22, 2012.\\nVena, Jocelyn (April 16, 2015). \"Taylor Swift, Kanye West, Kim Kardashian Make Time\\'s 100 Most Influential People List\". Billboard. Archived from the original on September 25, 2016. Retrieved September 1, 2016.\\nJensen, Erin (April 17, 2019). \"Dwayne Johnson, Taylor Swift, Gayle King, more cover Time\\'s 100 most influential people issue\". USA Today. Archived from the original on April 17, 2019. Retrieved April 17, 2019.\\n Dockterman, Eliana (December 6, 2017). \"\\'I Was Angry.\\' Taylor Swift on What Powered Her Sexual Assault Testimony\". Time. Archived from the original on December 6, 2017. Retrieved December 7, 2017.\\n Shah, Simmone (December 6, 2023). \"Taylor Swift Makes History as Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. December 6, 2023. Retrieved December 11, 2023.\\n \"Taylor Swift\". Forbes. Retrieved December 1, 2020.\\n \"30 Under 30 2017: All-Star Alumni\". Forbes. Retrieved December 1, 2020.\\n \"#8 Taylor Swift\". Forbes. Archived from the original on July 28, 2015. Retrieved July 27, 2015.\\n \"Forbes Unveils Its 20th Annual Ranking Of The World\\'s Most Powerful Women\". Forbes. December 5, 2023. Retrieved December 5, 2023.\\n Schneider, Marc (July 24, 2023). \"8 Ways Taylor Swift Has Changed the Music Business\". Billboard. Retrieved July 24, 2023.\\n Khan, Fawzia (June 18, 2021). \"The Might Of Taylor Swift\". Elle. Archived from the original on June 28, 2021. Retrieved October 20, 2021.\\n Pattison, Kate (September 19, 2023). \"How did Taylor Swift get so popular? She never goes out of style\". The Conversation. Retrieved September 22, 2023.\\n Grady, Constance (October 12, 2023). \"The Eras concert movie is Taylor Swift leveling up\". Vox. Retrieved October 13, 2023.\\n Jericho, Greg (October 28, 2022). \"Taylor Swift\\'s incredible success in graphs – who can blame me for being a Swiftie as a 50-year-old man?\". The Guardian. Retrieved October 29, 2022.\\n Weatherhead, Shaina (April 20, 2023). \"Taylor Swift\\'s \\'Miss Americana\\' Is Required Viewing Right Now\". Collider. Retrieved April 24, 2023.\\n Bartsch, Kayla (September 24, 2023). \"Taylor Swift: Empress of the Zeitgeist\". National Review. Retrieved September 24, 2023.\\n \"Taylor Swift: \\'My Confidence Is Easy To Shake\\'\". NPR. November 2, 2012. Retrieved May 20, 2021.\\nThanki, Juli (September 24, 2015). \"Taylor Swift: Teen idol to \\'biggest pop artist in the world\\'\". The Tennessean. Retrieved May 13, 2021.\\nYahr, Emily (June 16, 2016). \"Taylor Swift\\'s first song came out 10 years ago. Here\\'s what she was like as a teen songwriter\". Arts and Entertainment. The Washington Post. Archived from the original on May 13, 2021. Retrieved May 13, 2021.\\n El Mosselli, Sabrina (December 24, 2022). \"How Taylor Swift rose from \\'Miss Americana\\' to global megastar\". Le Monde. Retrieved December 28, 2022.\\n Cragg, Michael. \"Is Taylor Swift our last remaining real popstar?\". i-D. Archived from the original on May 6, 2023. Retrieved December 3, 2022.\\n Seisdedos, Iker (December 27, 2022). \"Pop music in the era of Taylor Swift: Behind the success of today\\'s biggest star\". El País. Retrieved December 28, 2022.\\n Sanders, Sam (October 20, 2022). \"Taylor Swift Is the Greatest Self-Portraitist of Our Time\". Vulture. Retrieved October 21, 2022.\\n \"Taylor Swift\\'s 40 Biggest Hot 100 Hits\". Billboard. March 23, 2022. Retrieved April 12, 2022.\\n Leonard, Devin (November 12, 2014). \"Taylor Swift Is the Music Industry\". Bloomberg News. Archived from the original on March 13, 2016. Retrieved August 11, 2020.\\n Zoladz, Lindsay (December 30, 2019). \"What Were the 2010s?\". New York. Retrieved November 22, 2020.\\n Savaira, Augusta (November 23, 2022). \"Welcome to \\'Swiftonomics\\': What Taylor Swift reveals about the U.S. economy\". Los Angeles Times. Retrieved November 23, 2022.\\n Vincent, Alice (November 3, 2017). \"Taylor Swift: the rise, fall and re-invention of America\\'s sweetheart\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved July 5, 2020.\\n Fischer & Seidman 2016, p. 529.\\n Wahi, Sukriti (March 3, 2021). \"Every Time Taylor Swift Perfectly Shut Down A Sexist Interview Question\". Elle. Retrieved November 4, 2021.\\n Davis, Allison P. (June 28, 2018). \"The Taylor Swift Slut-Shaming Continues\". The Cut. Retrieved November 4, 2021.\\n Meddings, Jacqui (October 31, 2014). \"Taylor Swift is our new cover goddess\". Cosmopolitan. Archived from the original on July 14, 2016. Retrieved August 2, 2016.\\n Chiu, Allyson (August 3, 2022). \"Celebrities use private jets excessively. It\\'s a climate nightmare\". Washington Post. ISSN 0190-8286. Retrieved February 14, 2024.\\n Mendez, Lola (February 14, 2024). \"Taylor Swift claims she offsets her travel carbon footprint - how does that work?\". BBC. Retrieved February 14, 2024.\\n O\\'Malley, Isabella (February 2, 2024). \"Why Taylor Swift\\'s globe-trotting in private jets is getting scrutinized\". Associated Press. Retrieved February 6, 2024.\\n \"Taylor Swift and Travis Kelce romance is bad for the planet. Couple burns a whopping $70,779 jet fuel in the last three months\". The Economic Times. December 21, 2023. Retrieved February 6, 2024.\\n Blistein, Jon (February 6, 2024). \"Taylor Swift\\'s Lawyers Threaten Private Jet Flight Tracker Over \\'Stalking and Harassing Behavior\\'\". Rolling Stone. Retrieved February 6, 2024.\\n Matza, Max (February 7, 2024). \"Taylor Swift threatens to sue student who tracks her private jet\". BBC News. Retrieved February 7, 2024.\\n Benitez-Eves, Tina (July 2, 2022). \"Man Arrested for Harassing Taylor Swift, Attempting to Break Into Her New York City Apartment\". American Songwriter. Retrieved June 10, 2023.\\n Kreps, Daniel (July 2, 2022). \"Brooklyn Man Arrested on Charges of Stalking Taylor Swift\". Rolling Stone. Retrieved June 10, 2023.\\n Della Cava, Marco; Ryan, Patrick (August 31, 2023). \"Taylor Swift is \\'in a class of her own right now,\\' as Eras tour gives way to Eras movie\". USA Today. Archived from the original on September 1, 2023. Retrieved September 1, 2023.\\n Crummy, Colin (March 13, 2014). \"Detwanging country music: how Nashville took the UK\". The Guardian. Retrieved September 23, 2021.\\n Roland, Tom (July 7, 2016). \"Love Story: The Impact of Taylor Swift\\'s First Decade in Music\". Billboard. Archived from the original on July 14, 2016. Retrieved July 7, 2016.\\n Tannembaum, Rob (September 12, 2014). \"4 Ways Billboard Woman of the Year Taylor Swift Changed Country Music\". Billboard. Retrieved December 31, 2019.\\n O\\'Connor, Roisin (October 18, 2018). \"Half of all new guitar players are women, finds study\". The Independent. Archived from the original on May 26, 2022. Retrieved October 20, 2020.\\n Wang, Amy X. (May 22, 2018). \"Guitars Are Getting More Popular. So Why Do We Think They\\'re Dying?\". Rolling Stone. Retrieved October 20, 2020.\\n \"The 200 Most Important Artists of Pitchfork\\'s First 25 Years\". Pitchfork. October 4, 2021. Retrieved October 4, 2021.\\n Lipshutz, Jason (December 16, 2021). \"Billboard\\'s Greatest Pop Stars of 2021: No. 1 — Taylor Swift\". Billboard. Retrieved December 16, 2021.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Retrieved August 18, 2021.\\n Gormely, Ian (December 3, 2014). \"Taylor Swift leads poptimism\\'s rebirth\". The Guardian. Retrieved August 2, 2021.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved September 7, 2023.\\n Friedman, Vanessa (April 24, 2023). \"Taylor Swift and the Sparkling Trap of Constant Reinvention\". The New York Times. Retrieved September 7, 2023.\\n Rowley, Glenn (March 9, 2023). \"Taylor Swift\\'s \\'All Too Well (10 Minute Version)\\' Is Now a Class at Stanford University\". Billboard. Retrieved March 10, 2023.\\n Ahlgrim, Callie (March 18, 2022). \"How Taylor Swift inspired a new generation of singer-songwriters, in their own words\". Business Insider. Retrieved March 18, 2022.\\n Sutherland, Mark (December 13, 2019). \"\\'I come with opinions about how we can better our industry\\': Taylor Swift\". Music Week. Retrieved April 27, 2020.\\nSisario, Ben (November 6, 2014). \"Sales of Taylor Swift\\'s \\'1989\\' Intensify Streaming Debate\". The New York Times. Retrieved August 11, 2020.\\nRosseinsky, Katie (October 29, 2021). \"How Taylor Swift is changing the music industry a re-record at a time\". Evening Standard. Archived from the original on April 9, 2021. Retrieved April 23, 2021.\\n Smith, Neil (June 22, 2015). \"Five ways Taylor Swift is changing the world\". BBC News. Retrieved December 31, 2019.\\n Brooks, Dave (November 3, 2022). \"How Taylor Swift Changed the Course of the Concert Ticketing Businesses\". Billboard. Retrieved November 4, 2022.\\n Sources on Swift and millennial era\\n • McDuling, John (November 3, 2014). \"Why Taylor Swift is the most important artist of the millennial era\". Quartz. Archived from the original on June 2, 2021. Retrieved April 2, 2021.\\n • \"What Taylor Swift tells us about millennials\". National Post. October 6, 2015. Retrieved October 30, 2021.\\n • Kidd, Joel (March 2, 2020). \"Taylor Swift Is the Surprising Face of Millennial Anxiety\". The Walrus. Retrieved October 30, 2021.\\n • Winograd, Morley; D. Hais, Michael (January 29, 2010). \"Move over Kanye West, Taylor Swift and the Millennial generation are taking over music\". The Christian Science Monitor. Retrieved October 30, 2021.\\n • \"The 17 Musicians Gen Z & Millennials Say Represent Their Generations\". YPulse. October 8, 2019. Retrieved October 30, 2021.\\n • Nicolau, Elena (November 29, 2022). \"How Taylor Swift is transforming millennial weddings\". Today. Retrieved November 30, 2022.\\n • \"Taylor Swift is peak millennial vibes\". NPR. October 21, 2022. Retrieved December 19, 2022.\\n VanDerWerff, Emily (July 31, 2020). \"Taylor Swift is the millennial Bruce Springsteen\". Vox. Retrieved October 30, 2021.\\n Ditum, Sarah (October 8, 2022). \"Taylor Swift: the Bob Dylan of our age\". The Times. Archived from the original on December 24, 2022. Retrieved October 9, 2022.\\n \"Taylor Swift Will Receive First-Ever Woman of the Decade Honor at Billboard\\'s Women in Music\". Billboard. November 18, 2019. Retrieved December 31, 2019.\\n Elizabeth, De (November 24, 2019). \"Taylor Swift Is Named Artist of the Decade at AMAs 2019\". Teen Vogue. Retrieved December 31, 2019.\\n Kemp, Sam (January 27, 2022). \"The surprising Paul McCartney song inspired by Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Golsen, Tyler (May 14, 2023). \"Mick Jagger discusses his performance with Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Brandle, Lars (January 28, 2015). \"Taylor Swift on Madonna Praise: \\'Now I\\'m Dead\\'\". Billboard. Retrieved July 5, 2023.\\n Aniftos, Rania (January 27, 2021). \"Dolly Parton Says Taylor Swift Is \\'Magnificent\\' Following Damon Albarn Feud: \\'She Knows Who She Is\\'\". Billboard. Retrieved July 5, 2023.\\n Graff, Gary (October 31, 2021). \"Taylor Swift Helps Induct Carole King, Sings \"Will You Love Me Tomorrow\" at Rock Hall Ceremony\". The Hollywood Reporter. Retrieved September 20, 2022.\\n Skinner, Tom (November 2, 2022). \"Bruce Springsteen praises Taylor Swift\\'s new album \\'Midnights\\': \"She\\'s a tremendous writer\"\". NME. Retrieved July 5, 2023.\\n Ruggieri, Melissa (November 3, 2021). \"Billy Joel says Taylor Swift is like the Beatles of her generation: \\'She knows music\\'\". USA Today. Retrieved July 5, 2023.\\n Williams, Joe (July 5, 2023). \"Ringo Starr says Taylor Swift is \"biggest star in the world\"\". Far Out. Retrieved July 5, 2023.\\n Iasimone, Ashley (November 12, 2023). \"Britney Spears Shares Throwback Photo With Taylor Swift From 20 Years Ago: \\'Most Iconic Pop Woman of Our Generation\\'\". Billboard. Retrieved November 12, 2023.\\n Lutz, Ashley (October 8, 2022). \"Taylor Swift\\'s new album proves she is a business genius\". Fortune. Retrieved October 8, 2022.\\n Steele, Anne (July 1, 2023). \"How to Succeed in Business Like Taylor Swift\". The Wall Street Journal. Retrieved July 2, 2023.\\n Lynch, Joe (January 31, 2024). \"Taylor Swift Tops Billboard\\'s Annual Power 100 List for the First Time\". Billboard. Retrieved January 31, 2024.\\n Curto, Justin (December 22, 2020). \"Did 2020 Kill the Long, Fancy Pop-Album Rollout for Good?\". Vulture. Archived from the original on December 23, 2020. Retrieved December 27, 2020.\\n Leight, Elias (July 23, 2020). \"Taylor Swift Finally Abandoned the Traditional Album Rollout\". Rolling Stone. Retrieved September 15, 2022.\\n Kheraj, Alim (November 9, 2022). \"The curate\\'s Easter egg: how Taylor Swift turned pop into a multiplayer puzzle\". The Guardian. Retrieved November 9, 2022.\\n Tannenbaum, Emily. \"Taylor Swift Hid a Bunch of Easter Eggs in the All Too Well Short Film\". Glamour. Retrieved May 13, 2022.\\n Suskind, Alex (May 9, 2019). \"New Reputation: Taylor Swift shares intel on TS7, fan theories, and her next era\". Entertainment Weekly. Archived from the original on August 12, 2019. Retrieved May 13, 2022.\\n Yahr, Emily (December 26, 2022). \"2022: The year in review (Taylor\\'s version)\". The Washington Post. Retrieved December 26, 2022.\\n Stein, Jeannine (November 18, 2011). \"Taylor Swift weighs in on being a role model\". Los Angeles Times. Archived from the original on July 15, 2012. Retrieved April 18, 2012.\\n Collins, Katie (June 8, 2018). \"How Taylor Swift flipped online fandom on its head for the better\". CNET. Retrieved March 19, 2023.\\n King, Ashley (July 21, 2023). \"Taylor Swift\\'s 13 Management is Rarely Discussed, Changes Abound\". Digital Music News. Retrieved September 23, 2023.\\n Yotka, Steff (August 20, 2019). \"Taylor Swift and Stella McCartney Reveal Their Lover Fashion Collaboration\". Vogue. Retrieved October 20, 2020.\\n Kaplan, Julee (January 29, 2009). \"Taylor Swift Launches Sundress Line – Ready-to-Wear and Sportswear – Media\". Women\\'s Wear Daily. Archived from the original on July 29, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift to have greeting card line\". USA Today. November 18, 2009. Archived from the original on December 14, 2009. Retrieved August 7, 2012.\\n Serpe, Gina (October 29, 2008). \"Taylor Swift Gets All Dolled Up\". E! News (Blog). Archived from the original on March 5, 2011. Retrieved September 20, 2010.\\n Mychaskiw, Marianne (October 1, 2014). \"Taylor Swift Is Launching Her Fourth Fragrance!\". InStyle. Archived from the original on January 7, 2015. Retrieved December 3, 2015.\\n McIntyre, Hugh. \"Taylor Swift Has Signed A Multi-Year Deal With AT&T\". Forbes. Archived from the original on September 11, 2018. Retrieved September 11, 2018.\\n Chiu, Melody (July 26, 2019). \"Watch Taylor Swift Hilariously Moonlight — and Fail — as a Bartender and Server in New Commercial\". People. Retrieved May 7, 2022.\\n \"Taylor Swift: NHL\\'s New Spokesperson\". AOL. February 3, 2009. Archived from the original on February 7, 2009. Retrieved September 20, 2010.\\n \"Sony Electronics News and Information\". Sony. April 26, 2010. Retrieved August 7, 2012.\\n Lee, Ashley (October 27, 2014). \"Taylor Swift Named New York City Welcome Ambassador\". Billboard. Retrieved September 26, 2023.\\n Kaufman, Gil (January 20, 2022). \"Taylor Swift Tapped as First Record Store Day Global Ambassador\". Billboard. Retrieved January 20, 2022.\\n Snapes, Laura (August 24, 2019). \"Taylor Swift: \\'Pop music can feel like it\\'s The Hunger Games, and like we\\'re gladiators\\'\". The Guardian. Archived from the original on August 24, 2019. Retrieved August 24, 2019.\\n \"The Original 300+ Time\\'s Up Signatories\". Time\\'s Up. October 21, 2019. Archived from the original on May 3, 2020. Retrieved May 20, 2020.\\n Mier, Tomás (June 24, 2022). \"Taylor Swift Says She\\'s \\'Absolutely Terrified\\' After Roe v. Wade Decision\". Rolling Stone. Retrieved June 24, 2022.\\n \"Taylor Swift sends \\'love\\' and \\'respect\\' to LGBT fans\". BBC News. June 4, 2018. Archived from the original on November 14, 2018. Retrieved December 15, 2018.\\n Kreps, Daniel (June 1, 2019). \"Taylor Swift Urges Senate to Pass Equality Act in Letter to Tennessee Senator\". Rolling Stone. Retrieved August 12, 2020.\\n Lynch, Joe (June 26, 2020). \"Taylor Swift Calls Out U.S. Census for Transgender Erasure\". Billboard. Retrieved June 29, 2020.\\n Lynch, Joe (June 14, 2019). \"Taylor Swift Wishes NYC\\'s Stonewall Inn \\'Happy Pride\\' at Surprise Set\". Billboard. Retrieved August 7, 2022.\\n Aniftos, Rania (April 8, 2019). \"Taylor Swift Donates $113,000 to Tennessee Equality Project to Fight Anti-LGBTQ Bills\". Billboard. Archived from the original on April 9, 2019. Retrieved April 9, 2019.\\n \"Taylor Swift Makes a Generous Donation to GLAAD in Support of the LGBTQ Community\" (Press release). GLAAD. June 1, 2019. Archived from the original on June 1, 2019. Retrieved June 1, 2019.\\n Lynch, John (March 23, 2018). \"Taylor Swift Shares Support For Gun a Control Reform In Instagram Post\". Business Insider. Retrieved May 20, 2020.\\n Hiatt, Brian (September 18, 2019). \"The Rolling Stone Interview Taylor Swift\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved September 19, 2019.\\n Bailey, Alyssa (June 9, 2020). \"Taylor Swift Tweeted About the Black Lives Matter Movement and Urged People to Vote\". Elle. Retrieved July 27, 2020.\\n Kreps, Daniel (June 12, 2020). \"Taylor Swift Calls for Removal of Monuments That \\'Celebrate Racist Historical Figures\\'\". Rolling Stone. Retrieved August 12, 2020.\\n France, Lisa Respers (June 19, 2020). \"Taylor Swift got educated on Juneteenth and wants you to be, too\". CNN. Retrieved July 27, 2020.\\n Haag, Matthew (October 9, 2018). \"Voter Registrations Spike as Deadlines Loom. Taylor Swift Had Something to Do With It\". The New York Times. Archived from the original on October 9, 2018. Retrieved October 10, 2018.\\n Stevens, Matt (October 7, 2020). \"Taylor Swift, leaning further into politics, endorses Joe Biden and Kamala Harris\". The New York Times. Archived from the original on October 7, 2020. Retrieved October 8, 2020.\\n Schuster, Steve (May 19, 2023). \"Taylor Swift comes out against Trump, again\". Wisconsin Law Journal. Retrieved November 4, 2023.\\n Dellatto, Marisa (October 27, 2023). \"Taylor Swift\\'s New Era: The Pop Star Becomes A Billionaire\". Forbes. Retrieved October 29, 2023.\\n Pendleton, Devon; Ballentine, Claire; Patino, Marie; Whiteaker, Chloe; Li, Diana (October 26, 2023). \"Taylor Swift Vaults Into Billionaire Ranks With Blockbuster Eras Tour\". Bloomberg News. Retrieved October 26, 2023.\\n Greenburg, Zack O\\'Malley (November 2, 2016). \"The World\\'s Highest-Paid Women in Music 2016\". Forbes. Archived from the original on May 25, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (December 6, 2019). \"The World\\'s Top-Earning Musicians Of 2019\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 9, 2022). \"The Highest-Paid Entertainers 2022\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 13, 2023). \"The World\\'s 10 Highest-Paid Entertainers\". Forbes. Retrieved July 29, 2022.\\n \"Highest annual earnings ever for a female pop star\". Guinness World Records. Archived from the original on November 12, 2017. Retrieved November 15, 2017.\\n Mercuri, Monica (July 10, 2019). \"Taylor Swift Is The World\\'s Highest-Paid Celebrity With $185 Million in 2019\". Forbes. Archived from the original on July 12, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (July 11, 2016). \"Taylor Swift Is The World\\'s Top-Earning Celebrity With $170 Million in 2016\". Forbes. Archived from the original on August 3, 2016. Retrieved August 3, 2016.\\n Greenburg, Zack O\\'Malley (December 23, 2019). \"From Taylor Swift To Dr. Dre: The 10 Top-Earning Musicians Of The Decade\". Forbes. Archived from the original on December 30, 2019. Retrieved December 31, 2019.\\n Silva, Rachel; Tonelli, Lucia (October 3, 2023). \"Inside Taylor Swift\\'s 8 Incredible Homes\". Elle Decoration. Retrieved October 12, 2023.\\n Dailey, Hannah; Aniftos, Rania (December 21, 2023). \"A Timeline of Taylor Swift\\'s Generosity\". Billboard. Retrieved December 24, 2023.\\n Couch, Robbie (March 2, 2015). \"Taylor Swift Named Most Charitable Celeb For Supporting Feminist Causes, Education, More\". HuffPost. Archived from the original on October 21, 2016. Retrieved August 2, 2016.\\n Pacella, Megan (June 13, 2012). \"Taylor Swift Receives Star of Compassion Award\". Taste of Country. Archived from the original on July 13, 2015. Retrieved November 27, 2015.\\n Cooper, Brittany Joy (March 27, 2012). \"Taylor Swift to Receive \\'Big\\' Honor From Michelle Obama at Kids\\' Choice Awards\". Taste of Country. Archived from the original on July 12, 2012. Retrieved August 30, 2012.\\n Lee, Ken (August 9, 2009). \"Taylor Swift donates $100,000 to victims of Iowa Flood\". People. Archived from the original on June 7, 2009. Retrieved September 9, 2009.\\n \"How Taylor Swift helps people worldwide\". Borgen. February 2, 2023. Retrieved June 17, 2023.\\n Moran, Jonathon (March 8, 2009). \"Kylie to play at Sound Relief with Coldplay, Midnight Oil\". The Daily Telegraph. Archived from the original on October 10, 2016. Retrieved December 30, 2009.\\n \"Taylor Swift donates $500,000 after Nashville floods\". BBC News. May 7, 2010. Retrieved May 7, 2022.\\n Lewis, Randy (May 23, 2011). \"Taylor Swift benefit concert raises more than $750,000 for tornado victims\". Los Angeles Times (Blog). Archived from the original on April 6, 2012. Retrieved April 18, 2012.\\n \"Taylor Swift gives $1m to help Louisiana flood relief efforts\". The Guardian. August 17, 2016. Archived from the original on August 17, 2016. Retrieved August 17, 2016.\\n Muhammad, Latifah (December 9, 2016). \"Taylor Swift Donates $100,000 to Dolly Parton Fire Fund\". Entertainment Tonight. Archived from the original on January 13, 2017. Retrieved January 11, 2017.\\n Blair, Olivia (September 4, 2019). \"Taylor Swift Makes \\'Very Sizeable Donation\\' To A Food Bank in Houston After Hurricane Harvey\". Elle. Archived from the original on July 2, 2019. Retrieved July 3, 2019.\\n Rowley, Glenn (April 13, 2023). \"Taylor Swift Donated Enough to Feed \\'Over 125,000\\' at Tampa Food Bank Ahead of The Eras Tour Stop\". Billboard. Archived from the original on April 21, 2023. Retrieved May 11, 2023.\\n Dementri, Nikki (May 16, 2023). \"\\'It\\'s a blessing\\': South Philly pizza, cheesesteak shop thankful for T. Swift\\'s business\". CBS News. Archived from the original on May 17, 2023. Retrieved May 17, 2023.\\n Grow, Kory (August 3, 2023). \"Taylor Swift Is Helping Truck Drivers Buy First Homes With $100,000 Bonuses\". Rolling Stone. Retrieved August 3, 2023.\\n Paulson, Dave (March 5, 2020). \"Taylor Swift donates $1 million for Tennessee tornado relief\". The Tennessean. Retrieved March 5, 2020.\\n Ruggieri, Melissa. \"Taylor Swift donates $1 million to help communities ravaged by Tennessee tornadoes\". The Tennessean. Retrieved December 11, 2023.\\n \"Nashville Songwriters Hall of Fame\". Nashville Songwriters Foundation. Retrieved July 2, 2020.\\n \"Taylor Swift becomes namesake of Hendersonville High School auditorium\". The Tennessean. September 23, 2010. Archived from the original on July 12, 2015. Retrieved May 29, 2011.\\n McKinley, James C. (May 17, 2012). \"Taylor Swift to Donate $4 Million to the Country Hall of Fame\". The New York Times. Archived from the original on June 5, 2012. Retrieved July 1, 2012.\\n Golden, Zara (October 1, 2012). \"And The Taylor Swift VH1 Storytellers Contest Winner Is ... Harvey Mudd College!\". VH1. Archived from the original on July 12, 2015. Retrieved April 21, 2014.\\n \"$100,000 Gift from Taylor Swift Provides Vital Support to Nashville Symphony\". Nashville Symphony. December 16, 2013. Retrieved April 7, 2020.\\n Sanz, Cynthia (December 14, 2009). \"Taylor Swift Gives Big as She Turns 20 – Good Deeds, Taylor Swift\". People. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\nUrban, Mike (October 14, 2011). \"Taylor Swift donates 6,000 books to Reading Library\". Reading Eagle. Archived from the original on May 12, 2012. Retrieved May 29, 2012.\\nZhu, Danielle (November 17, 2015). \"Taylor Swift partners with Scholastic to donate 25,000 books\". Entertainment Weekly. Archived from the original on August 14, 2016. Retrieved August 3, 2016.\\n Cooper, Brittany Joy (February 2, 2012). \"Taylor Swift Donates 14,000 Books to Nashville Public Library\". Taste of Country. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\n \"Taylor Swift Helps With \\'Delete Online Predators\\' Campaign\". Country Weekly. September 18, 2007. Archived from the original on January 16, 2016. Retrieved November 29, 2015.\\n \"Taylor Swift Teams Up With UNICEF Tap Project Initiative\". Taste of Country. March 17, 2011. Archived from the original on May 1, 2013. Retrieved August 7, 2012.\\n\"Taylor Swift Among Participants in MusiCares Back To School Charity Auction\". RTTNews. July 28, 2011. Archived from the original on May 17, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift and ACM Lifting Lives present $50,000 donation to St. Jude\". The Tennessean. June 13, 2011. Archived from the original on July 12, 2015. Retrieved May 29, 2012.\\n \"New and Hot Video: Taylor Swift Debuts \\'Ronan\\' at Stand Up To Cancer Benefit\". Rolling Stone. September 8, 2012. Archived from the original on September 13, 2012. Retrieved September 14, 2012.\\n Knight, Joey (May 17, 2014). \"Dick Vitale holds annual fundraiser for pediatric cancer\". Tampa Bay Times. Archived from the original on July 14, 2014. Retrieved July 30, 2014.\\n George, John. \"Taylor Swift donates $50K to CHOP to help teens with cancer\". The Business Journals (Blog). Archived from the original on July 12, 2015. Retrieved June 5, 2014.\\n Hardiman, Tim (April 20, 2007). \"Taylor Swift Encourages Teen Volunteers\". CMT. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n Vagianos, Alanna (August 17, 2017). \"This Is How Taylor Swift\\'s Donations Will Impact Sexual Assault Survivors\". HuffPost. Retrieved April 7, 2020.\\n Peppin, Hayley (March 24, 2020). \"Taylor Swift responded to the leaked \\'Famous\\' phone call with Kanye West by urging fans to instead focus on coronavirus aid\". Insider Inc. Retrieved May 13, 2020.\\n Grow, Kory (April 1, 2020). \"How Taylor Swift Is Helping a Small Nashville Record Store Survive\". Rolling Stone. Retrieved August 1, 2023.\\n Hissong, Samantha (August 20, 2020). \"Taylor Swift Starts Frenzy at Indie Record Stores With Surprise Signed \\'Folklore\\' CDs\". Rolling Stone. Retrieved August 1, 2023.\\n Havens, Lyndsey (April 18, 2020). \"Taylor Swift Sings Heart-Rending \\'Soon You\\'ll Get Better\\' During \\'One World\\' Concert\". Billboard. Retrieved April 19, 2020.\\n @RAINN (April 8, 2021). \"In 2017, @taylorswift13 fearlessly shared her story of sexual assault in court. In honor of Sexual Assault Awareness & Prevention Month & #RAINNDay, Taylor Swift has made a generous gift to RAINN. Thank you for helping us ensure all survivors are believed and supported! https://t.co/oE7P1BVnNJ\" (Tweet). Retrieved April 21, 2021 – via Twitter.\\n \"Taylor Swift\\'s cash gift helps student take up degree\". BBC News. August 20, 2020. Retrieved November 9, 2020.\\n M Holloway, Sophie (December 14, 2023). \"Taylor Swift and Selena Gomez among celebrities attending Gaza aid fundraiser\". The Independent.\\nCited literature\\nFischer, Nancy; Seidman, Steven (2016). Introducing the New Sexuality Studies (3rd ed.). Routledge. ISBN 978-1-317-44918-8. OCLC 953030187.\\nFogarty, Mary; Arnold, Gina (2021). \"Are You Ready for It? Re-Evaluating Taylor Swift\". Contemporary Music Review. 40 (1): 1–10. doi:10.1080/07494467.2021.1976586.\\nFulford, Phyllis (2014). An Idiots Guide: Singing Second Edition. Penguin Publishing. ISBN 978-1-61564-622-7.\\nHughes, Charles (2017). \"Country Music and the Recording Industry\". In Stimeling, Travis D. (ed.). The Oxford Handbook of Country Music. Oxford University Press. pp. 205–228. ISBN 978-0-19-024817-8.\\nJepson, Louisa (2013). Taylor Swift. Simon & Schuster. ISBN 978-1-4711-3087-8.\\nMcNutt, Myles (2020). \"From \\'Mine\\' to \\'Ours\\': Gendered Hierarchies of Authorship and the Limits of Taylor Swift\\'s Paratextual Feminism\". Communication, Culture and Critique. 13 (1): 72–91. doi:10.1093/ccc/tcz042.\\nMcGrath, John (2023). \"The Return to Craft: Taylor Swift, Nostalgia, and Covid-19\". Popular Music and Society. 46 (1): 70–84. doi:10.1080/03007766.2022.2156761.\\nPerone, James E. (2017). The Words and Music of Taylor Swift. The Praeger Singer-Songwriter Collection. ABC-Clio. ISBN 978-1-4408-5294-7.\\nProvenzano, Catherine (2018). \"Auto-Tune, Labor, and the Pop-Music Voice\". In Fink, Robert; Latour, Melinda; Wallmark, Zachary (eds.). The Relentless Pursuit of Tone: Timbre in Popular Music. Oxford University Press. pp. 159–182. ISBN 978-0-19-998522-7.\\nSloan, Nate (2021). \"Taylor Swift and the Work of Songwriting\". Contemporary Music Review. Routledge. 40 (1): 11–26. doi:10.1080/07494467.2021.1945226. S2CID 237695045.\\nExternal links\\nTaylor Swift\\nat Wikipedia\\'s sister projects\\nMedia from Commons\\nNews from Wikinews\\nQuotations from Wikiquote\\nData from Wikidata\\nOfficial website Edit this at Wikidata\\nTaylor Swift at AllMusic Edit this at Wikidata\\nTaylor Swift discography at Discogs Edit this at Wikidata\\nTaylor Swift at IMDb Edit this at Wikidata\\nTaylor Swift discography at MusicBrainz Edit this at Wikidata\\nvte\\nTaylor Swift\\nvte\\nTaylor Swift songs\\nAwards for Taylor Swift\\nvte\\nIFPI Global Year-End Charts\\nvte\\nBillboard Year-End number one albums\\nvte\\nBillboard Year-End Top Artist\\nPortals:\\n Biography\\nicon Pop music\\nflag Pennsylvania\\nflag United States\\nAuthority control databases Edit this at Wikidata\\nCategories: Taylor Swift1989 birthsLiving people21st-century American actresses21st-century American philanthropists21st-century American songwriters21st-century American women guitarists21st-century American women pianists21st-century American women singers21st-century women philanthropistsActresses from Nashville, TennesseeActresses from PennsylvaniaAmerican acoustic guitaristsAmerican billionairesAmerican country banjoistsAmerican country guitaristsAmerican country pianistsAmerican country record producersAmerican country singer-songwritersAmerican feministsAmerican film actressesAmerican folk guitaristsAmerican folk musiciansAmerican folk singersAmerican mezzo-sopranosAmerican multi-instrumentalistsAmerican music video directorsAmerican people of English descentAmerican people of German descentAmerican people of Italian descentAmerican people of Scottish descentAmerican pop guitaristsAmerican pop pianistsAmerican synth-pop musiciansAmerican television actressesAmerican voice actressesAmerican women country singersAmerican women guitaristsAmerican women philanthropistsAmerican women pop singersAmerican women record producersAmerican women rock singersAmerican women singer-songwritersAmerican women songwritersBig Machine Records artistsBrit Award winnersChristians from TennesseeCountry musicians from PennsylvaniaCountry musicians from TennesseeCountry pop musiciansFemale billionairesFemale music video directorsFeminist musiciansFilm directors from PennsylvaniaFilm directors from TennesseeGrammy Award winnersGuitarists from PennsylvaniaGuitarists from TennesseeMTV Europe Music Award winnersMTV Video Music Award winnersNME Awards winnersPeople from Bucks County, PennsylvaniaPhilanthropists from PennsylvaniaPhilanthropists from TennesseePrimetime Emmy Award winnersRCA Records artistsRecord producers from PennsylvaniaRecord producers from TennesseeRepublic Records artistsSinger-songwriters from PennsylvaniaSinger-songwriters from TennesseeSingers from Nashville, TennesseeSony Music Publishing artistsSynth-pop singersTime Person of the YearUniversal Music Group artistsBaldi family\\nThis page was last edited on 16 February 2024, at 11:33 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\\nWikimedia FoundationPowered by MediaWiki'\n\n\n\n\n\n\nThe ‘naive’ BPE implementation would merge common words occuring with different punctuation into single tokens: For example, ‘dog.’, ‘dog!’, and ‘dog?’ are likely to become individual tokens. This would combine semantics with punctuation, which is sub-optimal according to studies done by Radford et al. (2019).\nThe regex patterns used by GPT-2 and GPT-4 are below:\n\n\nimport regex\nGPT4_SPLIT_PATTERN = regex.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\nGPT2_SPLIT_PATTERN = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n\nLooking at the example below,\n\n\nprint(regex.findall(GPT2_SPLIT_PATTERN,\"Hello're,     world! Here are the cats.\"))\n\n['Hello', \"'re\", ',', '    ', ' world', '!', ' Here', ' are', ' the', ' cats', '.']\n\n\nwe can make the following observations: - By breaking up the text this way, we’ll not be merging ‘e’, for example, as they are now parts of separate elements and we’ll only perform merges within elements.\n- GPT-2 pattern instructs the tokenizer not to merge across letters, numbers, punctuation, and select apostrophes. - re.IGNORECASE was not used in the original GPT-2 tokenizer, which means tokenization will not be consistent for uppercase and lowercase words containing apostrophes! - +(?!) is a negative lookahead assertion. It’s matching white spaces up to but not including the last white space. This consistently tokenizes start-of-word tokens with white space prepended to them.\nThere are additional rules other than chunking and BPE that are not clear from the original OpenAI implementation. For example 4 spaces in a succession would be likely merged by a tokenizer trained on Python code, which was likely the case. However, these spaces are not merged by the default GPT-2 tokenizer:\n\n\n\nSource: https://tiktokenizer.vercel.app/?model=gpt2\n\n\n\nMy implementation of the BPE tokenizer with RegEx is below:\n\n\nfrom tqdm import tqdm\nimport regex\nclass RegexTokenizer(BasicTokenizer):\n    def __init__(self,pattern) -&gt; None:\n        super().__init__()\n        self.pattern = pattern\n    \n    def train(self, text, vocab_size, verbose=False):\n        num_merges = vocab_size - 256\n        #--- Start: main regex modification ---#\n        raw_tokens = [token for token in regex.findall(self.pattern, text)]\n        encoded_tokens = [s.encode('utf-8') for s in raw_tokens]\n        tokens = b\"\".join(encoded_tokens)\n        #---   End: main regex modification ---#\n\n        init_num_tokens = len(tokens)\n        for i in tqdm(range(num_merges)):\n            stats = RegexTokenizer.get_stats(tokens)\n            #top_pair = max(stats, key=stats.get)\n            max_freq = max(stats.values())\n            top_pairs = [pair for pair, freq in stats.items() if freq == max_freq]\n            top_pair = min(top_pairs)  # lexicographically smallest pair\n            tokens = RegexTokenizer.merge(tokens, top_pair, 256+i)\n            self.merges[top_pair] = 256+i\n        final_num_tokens = len(tokens)\n        if verbose:\n            print(init_num_tokens)\n            print(final_num_tokens)\n            print(f\"compression ratio: {init_num_tokens/final_num_tokens:.2f}\")\n            print(f\"merges: {self.merges}\")\n\n\nBreaking down the main new lines above, it appears that the encoding checks out:\n\n\nraw_tokens = [token for token in regex.findall(GPT2_SPLIT_PATTERN, text)]\nencoded_tokens = [s.encode('utf-8') for s in raw_tokens]\ntokens = b\"\".join(encoded_tokens) \n\n\ntext = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view. I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\"\ntokenizer = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\ntokenizer.train(text,vocab_size=276,verbose=True)\n\n100%|██████████| 20/20 [00:00&lt;00:00, 8480.19it/s]\n\n\n1116\n847\ncompression ratio: 1.32\nmerges: {(101, 32): 256, (105, 110): 257, (115, 32): 258, (226, 128): 259, (32, 116): 260, (97, 110): 261, (240, 159): 262, (116, 32): 263, (97, 114): 264, (257, 103): 265, (100, 32): 266, (101, 114): 267, (44, 32): 268, (111, 114): 269, (105, 99): 270, (111, 110): 271, (260, 104): 272, (32, 115): 273, (46, 32): 274, (100, 101): 275}\n\n\n\n\n\n\nAndrej has a more sophisticated implementation with handling of special tokens, saving, and loading methods. His train method is below for reference:\n\n\n\nclass RegexTokenizerAK(Tokenizer):\n    ...\n    def train(self, text, vocab_size, verbose=False):\n        assert vocab_size &gt;= 256\n        num_merges = vocab_size - 256\n\n        # split the text up into text chunks\n        text_chunks = re.findall(self.compiled_pattern, text)\n\n        # input text preprocessing\n        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n\n        # iteratively merge the most common pairs to create new tokens\n        merges = {} # (int, int) -&gt; int\n        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -&gt; bytes\n        for i in range(num_merges):\n            # count the number of times every consecutive pair appears\n            stats = {}\n            for chunk_ids in ids:\n                # passing in stats will update it in place, adding up counts\n                get_stats(chunk_ids, stats)\n            # find the pair with the highest count\n            pair = max(stats, key=stats.get)\n            # mint a new token: assign it the next available id\n            idx = 256 + i\n            # replace all occurrences of pair in ids with idx\n            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n            # save the merge\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            # prints\n            if verbose:\n                print(f\"merge {i+1}/{num_merges}: {pair} -&gt; {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n\n        # save class variables\n        self.merges = merges # used in encode()\n        self.vocab = vocab   # used in decode()\n    ...\n\n\nAs expected, Andrej’s tokenizer works flawlessly:\n\n\nfrom pathlib import Path\ntokenizer = RegexTokenizerAK(pattern=GPT2_SPLIT_PATTERN)\ntaylor_text = Path('taylorswift.txt').read_text().strip()\ntokenizer.train(taylor_text,vocab_size=1276,verbose=False)\nassert tokenizer.decode(tokenizer.encode(taylor_text))==taylor_text, 'enc/dec mismatch'\n\n\nAs a side note, I could be wrong, but I think the following line may help make the tokenizer deterministic: pair = max(stats.items(), key=lambda item: (item[1], -item[0][0], -item[0][1]))[0]\n\n\n\n\n\nNow let’s train and compare the tokenizers on subsets on the TinyStories dataset.\n\n\nfrom datasets import load_dataset\ntiny_stories_ds=load_dataset(\"roneneldan/TinyStories\")\n\n/home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nsample_list = tiny_stories_ds['train'].shuffle(seed=1337).select(range(1000))['text']\n\n\nsample_list[:3]\n\n['Once upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to her mother.\\n\\nHer mother said, \"Wow, Lily! That\\'s a beautiful horse. Can you kneel down and show me how it runs?\"\\n\\nLily knelt down and made the horse run with her hands. Her mother smiled and said, \"You\\'re so creative, Lily. Keep making more things with clay.\" \\n\\nLily smiled and said, \"I will, Mommy. And maybe next time, I\\'ll make a good-looking dog for you.\"',\n 'Once upon a time, there was a mum and her two kids. The two kids were Joe and Sam. Joe and Sam did not always agree with each other. One day, the two boys had a big fight over a toy. Joe wanted to use the toy first, but Sam did not agree.\\n\\nSo, mum decided to take the boys to see a lawyer. The lawyer was very calm. The lawyer helped them talk with each other and figure out a solution. Eventually, the lawyer helped Joe and Sam agree on which one should use the toy first. \\n\\nThe boys were proud of themselves for sorting their problem out. Ever since then, they have become very good at using the law when they disagree, and they often keep calm when talking with each other.',\n 'One day, a curious little girl named Lucy wanted to have a picnic. She asked her mom, \"Can we have a picnic today?\" Her mom said, \"Yes, but we must be careful.\" Lucy was very happy and started to pack a big basket with food.\\n\\nThey went to the park and found a nice spot under a big tree. They put a blanket on the grass and started eating their sandwiches. Lucy saw a little bird and wanted to give it some bread. Her mom said, \"Be careful, Lucy. Don\\'t go too far.\"\\n\\nBut Lucy didn\\'t listen. She walked away from the picnic to find the little bird. She looked and looked but couldn\\'t find it. She was lost and didn\\'t know how to go back to her mom. Lucy started to cry.\\n\\nHer mom looked for her but couldn\\'t find her. She was very sad and scared. The picnic was not fun anymore. They didn\\'t know that going too far away from the picnic would make them feel so bad.\\n\\nIn the end, Lucy and her mom were both very sad. They wished they had listened to each other and stayed close during the picnic.']\n\n\n\ntraining_corpus='&lt;--&gt;'.join(sample_list) # choose to use this since my basic tokenizer does not handle the special tokens\n#training_corpus='&lt;|endoftext|&gt;'.join(sample_list)\n\n\nprint(training_corpus[:500])\n\nOnce upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to her mother.\n\nHer mother said, \"Wow, Lily! That's a beautiful horse. Can you kneel down and show me how it runs?\"\n\nLily knelt down and made the horse run with her hands. Her mother smiled and said, \"You're so creative, Lily. Keep making more things with clay.\" \n\nLily smiled and said, \"I will, Mommy. A\n\n\n\ntokenizer_dpv = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\ntokenizer_dpv.train(training_corpus,vocab_size=6400,verbose=False)\n\n100%|██████████| 6144/6144 [03:09&lt;00:00, 32.38it/s]\n\n\n\nassert tokenizer_dpv.decode(tokenizer_dpv.encode(training_corpus[:200]))==training_corpus[:200], 'enc/dec mismatch'\n\n\ntokenizer_ak = RegexTokenizerAK(pattern=GPT2_SPLIT_PATTERN)\ntokenizer_ak.train(training_corpus,vocab_size=6400,verbose=False)\n\n\nassert tokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))==training_corpus[:200], 'enc/dec mismatch'\n\n\nassert tokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))==tokenizer_dpv.decode(tokenizer_dpv.encode(training_corpus[:200])), 'ak/dpv mismatch'\n\n\ntokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))\n\n'Once upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to h'\n\n\n\ntext_sample = training_corpus[:1024]\n\n\nNow let’s train the GPT-2 tokenizer using HuggingFace tokenizers:\n\n\nfrom transformers import GPT2TokenizerFast\nfrom tokenizers import trainers, Tokenizer\n\n\n# Load the GPT-2 tokenizer\noriginal_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n\n# Get the settings from the original tokenizer\nmodel = original_tokenizer._tokenizer.model\npre_tokenizer = original_tokenizer._tokenizer.pre_tokenizer\npost_processor = original_tokenizer._tokenizer.post_processor\n\n# Initialize a new tokenizer with the same settings\ngpt2_custom = Tokenizer(model)\ngpt2_custom.pre_tokenizer = pre_tokenizer\ngpt2_custom.post_processor = post_processor\n\n# Train the new tokenizer\ntrainer = trainers.BpeTrainer(vocab_size=6400, show_progress=True)\ngpt2_custom.train_from_iterator(sample_list, trainer=trainer)\n\n/home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\nFor reference, let’s also train a ‘standard’ BPE tokenizer where all implementation details are known:\n\n\nfrom tokenizers import Tokenizer, trainers, models, pre_tokenizers, normalizers\n\n# Initialize a new ByteLevelBPETokenizer\nbpe_tokenizer = Tokenizer(models.BPE())\n\n# Normalize the text\nbpe_tokenizer.normalizer = normalizers.Sequence([\n    normalizers.NFKC(),\n    normalizers.Lowercase()\n])\n\n# Pre-tokenize the text\nbpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Train the tokenizer\ntrainer = trainers.BpeTrainer(vocab_size=6400, show_progress=True)\nbpe_tokenizer.train_from_iterator(sample_list, trainer=trainer)\n\n\n\n\n\n\n\nBefore the lexicographic adjustment:\n\n\ntokenizer_dpv.encode(text) \n#[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 435, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 435, 240, 159, 152, 132, 32, 389, 378, 2443, 5152, 3301, 3655, 274, 97, 669, 937, 655, 697, 452, 1481, 1022, 331, 109, 2477, 2632, 119, 1268, 866, 637, 1111, 669, 273, 567, 275, 226, 128, 156, 1656, 337, 288, 261, 85, 2159, 111, 732, 226, 128, 157, 32, 549, 6378, 3206, 270, 397, 40, 3598, 1831, 356, 1541, 115, 226, 128, 148, 534, 782, 316, 119, 99, 681, 95, 261, 343, 1320, 2267, 283, 859, 1183, 63, 41, 619, 85, 2159, 111, 557, 2090, 97, 98, 308, 1310, 367, 494, 805, 1467, 937, 719, 1859, 45, 552, 1013, 85, 2159, 111, 557, 83, 116, 935, 2041, 112, 708, 5540, 456, 122, 276, 262, 1890, 117, 337, 323, 4443, 282, 2440, 1226, 783, 294, 112, 1139, 1510, 820, 458, 2090, 821, 738, 4221, 269, 380, 298, 398, 283, 839, 100, 280, 226, 128, 153, 261, 2112, 1030, 1481, 1022, 331, 109, 2477, 343, 4582, 2604, 803, 3402, 589, 2636, 1071, 859, 1085, 51, 48, 32, 1303, 115, 3316, 85, 2159, 111, 732, 226, 128, 153, 262, 2761, 535, 4733, 2701, 2234, 1569, 258, 115, 259, 395, 2763, 676, 2658, 4462, 85, 2159, 111, 100, 477, 712, 4294, 641, 1077, 753, 316, 821, 2841, 313, 549, 732, 2053, 369, 1403, 2318, 1434, 116, 469, 323, 272, 73, 226, 128, 153, 904, 1204, 371, 1322, 412, 100, 902, 4627, 275, 313, 4569, 1481, 1022, 331, 4442, 226, 128, 153, 262, 3348, 261, 452, 5107, 119, 369, 226, 128, 153, 713, 2002, 1858, 957, 4459, 1447, 3213, 460, 281, 1640, 3598, 226, 128, 153, 262, 269, 118, 467, 709, 549, 896, 316, 333, 1573, 283, 421, 4717, 458, 452, 85, 2159, 111, 557, 116, 466, 116, 5154, 1277, 272, 549, 2318, 1434, 116, 469, 339, 73, 226, 128, 153, 713, 438, 2002, 1611, 961, 102, 280, 116, 859, 116, 466, 261, 307, 347, 116, 47, 2716, 283, 47, 114, 402, 264, 1498, 1036, 297, 528, 320, 122, 3732, 549, 732, 2053, 226, 128, 148, 5112, 1059, 367, 506, 1736, 455, 1656, 783, 338, 3599, 265, 420, 2669, 513, 256, 40, 304, 1061, 2379, 415, 41, 4573, 46]\nlen(tokenizer_dpv.encode(text)) \n#454\n\n\n454\n\n\n\ntokenizer_ak.encode(text) \n#[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 299, 396, 1253, 2342, 778, 2407, 264, 6081, 696, 262, 3703, 345, 282, 1312, 114, 333, 109, 667, 1371, 119, 433, 46, 750, 495, 713, 366, 32, 611, 266, 32, 226, 128, 156, 115, 117, 340, 811, 4089, 474, 4759, 226, 128, 157, 315, 1097, 1431, 119, 491, 32, 40, 119, 578, 1426, 353, 2900, 226, 128, 148, 108, 1636, 3290, 260, 382, 289, 95, 116, 365, 495, 262, 733, 566, 44, 957, 63, 41, 46, 552, 4089, 474, 4759, 483, 324, 689, 362, 114, 117, 387, 44, 264, 3897, 847, 696, 262, 305, 613, 787, 45, 112, 945, 4089, 474, 4759, 2341, 787, 462, 356, 585, 883, 417, 122, 2951, 345, 4497, 304, 1837, 1198, 414, 3062, 120, 310, 44, 393, 112, 3054, 44, 264, 5469, 483, 324, 677, 645, 258, 388, 630, 300, 292, 2508, 46, 335, 877, 226, 128, 153, 116, 595, 539, 282, 1312, 114, 333, 109, 667, 365, 901, 3602, 262, 2245, 1188, 2552, 44, 850, 2032, 48, 1390, 1149, 4089, 474, 4759, 226, 128, 153, 115, 315, 336, 885, 751, 46, 469, 1543, 5633, 535, 111, 44, 335, 587, 3903, 315, 4089, 474, 4759, 264, 642, 266, 2200, 499, 392, 2851, 677, 700, 302, 315, 278, 311, 2309, 46, 1395, 762, 3236, 474, 304, 44, 335, 226, 128, 153, 306, 1136, 414, 4042, 4077, 751, 266, 302, 583, 258, 282, 1312, 114, 333, 1195, 226, 128, 153, 115, 5903, 345, 3179, 46, 335, 226, 128, 153, 109, 925, 266, 1255, 99, 585, 349, 262, 5953, 419, 1263, 264, 563, 226, 128, 153, 115, 2284, 1189, 420, 315, 2645, 339, 733, 566, 264, 3009, 310, 345, 4089, 474, 4759, 1139, 976, 46, 3963, 44, 315, 762, 3236, 474, 304, 335, 226, 128, 153, 109, 400, 925, 266, 1594, 700, 272, 911, 115, 44, 1139, 976, 2634, 389, 47, 1367, 669, 287, 47, 270, 686, 287, 44, 716, 432, 99, 406, 735, 1577, 315, 278, 311, 2309, 226, 128, 148, 1014, 1752, 453, 5149, 402, 115, 117, 310, 44, 324, 121, 2387, 660, 532, 529, 101, 32, 40, 787, 4511, 41, 1324, 46]\nlen(tokenizer_ak.encode(text)) \n#454\n\n454\n\n\n\nAfter the lexicographic adjustment:\n\n\ntokenizer_ak.encode(text) #[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 303, 366, 1285, 357, 324, 1355, 2631, 264, 258, 119, 101, 980, 262, 2098, 115, 338, 1827, 2010, 327, 109, 711, 1172, 119, 429, 46, 1015, 477, 746, 384, 32, 634, 267, 32, 226, 2556, 115, 117, 345, 654, 1469, 986, 111, 100, 101, 226, 128, 157, 312, 1335, 1784, 119, 493, 32, 40, 119, 104, 726, 313, 360, 1835, 115, 226, 128, 148, 108, 2506, 2293, 260, 402, 293, 95, 116, 365, 477, 262, 357, 2037, 44, 1114, 63, 41, 46, 638, 1469, 986, 111, 100, 101, 507, 328, 722, 575, 114, 117, 361, 44, 264, 281, 105, 783, 980, 262, 305, 816, 1480, 45, 112, 905, 1469, 986, 111, 100, 101, 298, 116, 1480, 502, 359, 462, 790, 440, 122, 290, 115, 338, 259, 117, 345, 310, 2024, 293, 121, 428, 110, 101, 120, 306, 44, 446, 782, 115, 44, 264, 450, 306, 507, 328, 650, 764, 258, 390, 850, 304, 292, 291, 287, 46, 337, 998, 226, 128, 153, 116, 626, 553, 1827, 2010, 327, 109, 711, 365, 1266, 635, 287, 262, 714, 310, 508, 1856, 2018, 44, 820, 1971, 48, 1317, 990, 1469, 986, 111, 100, 101, 226, 128, 153, 115, 312, 1258, 755, 46, 524, 2891, 910, 1130, 115, 562, 111, 44, 337, 543, 2995, 575, 263, 312, 1469, 986, 111, 100, 101, 264, 682, 267, 1813, 469, 388, 2930, 650, 827, 301, 312, 281, 314, 1481, 46, 1153, 741, 435, 116, 480, 310, 44, 337, 226, 128, 153, 289, 885, 428, 2997, 116, 755, 267, 301, 578, 258, 1827, 2010, 327, 109, 276, 226, 128, 153, 115, 2104, 116, 338, 3395, 46, 337, 226, 128, 153, 109, 1164, 267, 273, 111, 2474, 347, 262, 607, 849, 99, 393, 1365, 264, 537, 226, 128, 153, 115, 312, 118, 806, 394, 312, 2878, 334, 357, 2037, 264, 273, 296, 306, 338, 1469, 986, 111, 100, 101, 3005, 46, 2139, 1487, 44, 312, 741, 435, 116, 480, 310, 337, 226, 128, 153, 109, 450, 1164, 267, 1447, 827, 273, 2716, 115, 44, 3005, 2644, 385, 47, 115, 104, 1038, 287, 47, 268, 1052, 287, 44, 788, 414, 99, 370, 620, 291, 755, 312, 281, 314, 1481, 226, 128, 148, 1130, 2033, 472, 259, 803, 293, 726, 415, 115, 117, 306, 44, 328, 121, 2525, 712, 515, 541, 101, 32, 40, 1480, 746, 1126, 594, 41, 1384, 46]\nlen(tokenizer_ak.encode(text)) #497\n\n454\n\n\n\ntokenizer_dpv.encode('&lt;--&gt;'),tokenizer_ak.encode('&lt;--&gt;') # Ok, encoded early on by both tokenizers\n\n([374], [375])\n\n\n\nMy tokenizer behaves similarly to Andrej’s, with most merges happening around the same time. Both appear to possess the same compression ratio (tokenized the sample text to 454 tokens to represent text of length 1024) on the chosen sample, and both successfully recover the original sequence. The main difference seems to come from Andrej’s tokenizer using get_stats on each chunk separately and merging pairs within the chunks separately, while mine merges pairs within the entire concatenated byte string. While this is slower (13:57 vs 03:18 on my machine), it is a more careful (and likely more ‘correct’) handling of the individual chunks. I would imagine the modern highly optimized tokenizers to perform tokenization on multiple chunks of text in parallel, aligning best with Andrej’s implementation.\nHowever, GPT-2 tokenizer appears to yield different tokens even with same vocabulary size. It also differs from the standard BPE tokenizer available from Hugging Face. Since not all details of GPT-2 tokenizer implementation are known (referring to 4-spaces not being merged for Python code, for example), this could be due to a custom implementation detail of GPT-2.\n\n\nprint(gpt2_custom.encode(text_sample).ids)\nprint(len(gpt2_custom.encode(text)))\n\n[248, 261, 81, 212, 4, 216, 103, 81, 209, 260, 321, 182, 6, 136, 309, 89, 195, 162, 4781, 87, 395, 1036, 1676, 6, 333, 178, 4, 155, 370, 81, 419, 5, 3323, 1709, 162, 85, 4781, 6, 136, 103, 215, 227, 87, 999, 125, 89, 135, 1062, 6, 72, 72, 1080, 1062, 145, 4, 154, 1188, 4, 182, 0, 1032, 190, 81, 610, 1709, 6, 1174, 175, 3679, 470, 87, 528, 354, 541, 125, 1703, 295, 72, 72, 164, 3973, 470, 87, 370, 85, 1709, 631, 162, 135, 799, 6, 677, 1062, 307, 87, 145, 4, 154, 341, 1207, 170, 3040, 4, 182, 6, 4921, 1011, 496, 464, 162, 4781, 235, 228, 72, 164, 307, 87, 145, 4, 154, 23, 500, 4, 944, 6, 792, 2025, 894, 212, 4, 158, 1466, 395, 81, 419, 5, 3323, 536, 188, 175, 235, 5155, 248, 261, 81, 212, 4, 216, 103, 81, 808, 87, 135, 729, 1049, 6, 122, 729, 1049, 253, 813, 87, 455, 6, 813, 87, 455, 287, 219, 452, 3404, 162, 628, 412, 6, 333, 178, 4, 85, 729, 1772, 186, 81, 220, 1515, 582, 81, 314, 6, 813, 267, 89, 1072, 85, 314, 1117, 4, 234, 455, 287, 219, 3404, 6, 72, 72, 762, 4, 808, 462, 89, 492, 85, 1772, 89, 343, 81, 2612, 6, 122, 2612, 103, 215, 2006, 6, 122, 2612, 1065, 278, 1428, 162, 628, 412, 87, 5314, 327, 264, 81, 4973, 6, 4359, 4, 85, 2612, 1065, 813, 87, 455, 3404, 172, 3178, 483, 828, 1072, 85]\n344\n\n\n\nprint(bpe_tokenizer.encode(text_sample).ids)\nprint(len(bpe_tokenizer.encode(text)))\n\n[211, 228, 56, 182, 4, 183, 79, 56, 177, 226, 282, 150, 6, 86, 277, 64, 162, 132, 4495, 63, 360, 971, 1574, 6, 220, 148, 4, 86, 334, 56, 376, 5, 3147, 1606, 132, 58, 4495, 6, 86, 79, 185, 196, 63, 934, 90, 64, 105, 966, 6, 47, 47, 217, 966, 119, 4, 125, 1110, 4, 150, 0, 145, 158, 56, 558, 1606, 6, 249, 126, 3479, 424, 63, 474, 313, 462, 90, 1601, 260, 47, 47, 393, 3752, 424, 63, 334, 58, 1606, 586, 132, 105, 742, 6, 105, 966, 275, 63, 119, 4, 125, 697, 1125, 131, 2871, 4, 150, 6, 755, 945, 448, 420, 132, 4495, 201, 197, 47, 393, 275, 63, 119, 4, 125, 23, 447, 4, 369, 6, 63, 1029, 811, 182, 4, 174, 1372, 360, 56, 376, 5, 3147, 489, 156, 126, 201, 4857, 3410, 228, 56, 182, 4, 183, 79, 56, 579, 63, 105, 666, 981, 6, 58, 666, 981, 222, 748, 63, 379, 6, 748, 63, 379, 253, 189, 407, 3230, 132, 575, 373, 6, 220, 148, 4, 58, 666, 1664, 153, 56, 188, 1400, 529, 56, 461, 6, 748, 233, 64, 1002, 58, 461, 1000, 4, 154, 379, 253, 189, 3230, 6, 47, 47, 468, 4, 579, 419, 64, 443, 58, 1664, 64, 303, 56, 2468, 6, 58, 2468, 79, 185, 1887, 6, 58, 2468, 995, 244, 1341, 132, 575, 373, 63, 4995, 291, 230, 56, 4690, 6, 1908, 4, 58, 2468, 995, 748, 63, 379, 3230, 103, 2658, 220, 766, 1002, 58]\n314\n\n\n\nIt is noteworthy that tokenizer trained directly from GPT-2 tokenizer using the Hugging Face library yielded a smaller compression ratio than the BPE tokenizer (344 vs 314 tokens to represent text of length 1024), and the compression ratio of these tokenizers was larger than that of our custom tokenizers.\nAlso, both GPT-2 and BPE tokenizer appear to be highly optimized, taking less than a second to train on our sample compared to minutes, implying that it’s the best move to understand the tokenization process and use an optimized off-the-shelf implementation.\n\n\n\n\n\nAndrej makes a number or important points in the second half of the Let’s build the GPT Tokenizer video.\n\n\n\n\nGPT-4 tokenizer fixed the case sensitivity pointed out above by making the match pattern case insensitive.\nIn additional to the encoder and decoder, GPT-2 also has byte_encoder and byte_decoder methods that don’t add substantially to the implementation.\n\nGPT-2 tokenizer’s vocab size of 50257 comes from 256 raw byte tokens, 50000 merges, and the &lt;|endoftext|&gt; token.\n\nModern tokenizers, such as the gpt-3.5-turbo tokenizer, also have other special tokens such as &lt;|imstart|&gt;, &lt;|imend|&gt;, etc, where ‘im’ stands for ‘imaginary monologue’ to delimit chat conversations, for example. tiktoken library allows addition of these special tokens.\nAdding special tokens necessitates transformer model surgery to add the tokens in the embedding matrix and the classifier layer.\nAs noted above, tiktoken does not provide the train function, just the inference code.\n\n\n\n\n\nsentencepiece provices a train function and works on the level of code points, not bytes, falling back to UTF-8 if it encounters unknown tokens.\nIt also tries to split out sentences, which could be challenging since many syntactic units do not squarely fall within sentence/no-sentence category.\nBy default sentencepiece adds white spaces in front of strings that are not at the start of sentences for consistency of treating words occurring at the start and in the middle of sentences.\nIn Andrej’s experience, sentencepiece documentation is lacking a bit at the moment.\n\n\n\n\n\nWith larger vocabulary size, more tokens can be compressed and included within a given context length, but there would also be fewer of a given token for a language model to train on.\nWhile the greater amount of information included within a context window is generally beneficial, too much of it due to a large vocab size is likely not give the model enough time to ‘think’ as it’s attending to the tokens. Thus vocab size can be viewed as a hyperparameter of the model.\nIf we’re finetuning and want to add special tokens (eg, for using a browser), we would resize the embedding and the weights, then could freeze the original weights and finetune just the new parameters.\nThere is a gist tokens compression technique where new tokens are introduced and the model is trained by distillation, training just the embedding representations of the new tokens such that the behavior of this language model is identical to that of a model with a very long prompt. This is shown in the following diagram:\n\n\n\n\nSource: Mu, J., Li, X. L., & Goodman, N. (2023).\n\n\n\nIt is also possible to tokenize non-language data (eg, images), and use the transformer architecture for autoregressive or diffusion models (Sora is an example).\n\n\n\n\n\nAndrej’s answers to the following questions are below.\nWhy can’t LLM spell words?\n\nAK: Characters are chunked up into tokens, some of which are long (a model with a ‘:DefaultCellStyle’ token will not be able to answer spelling questions about it). Also, if ‘:DefaultCellSty’ is passed to the model, the model generation will likely stop since it has never seen just a chunk of the token in it’s vocabulary and such chunks are outside of the model’s distribution.\n\nWhy can’t LLM do super simple string processing tasks like reversing a string?\n\nAK: Similarly, reversing a single long token would be challenging to a model unless it’s first prompted to list out the individual characters.\n\n\nWhy is LLM worse at non-English languages (e.g. Japanese)?\n\nAK: Tokenizers are not sufficiently trained on non-English data and as a consequence do not compress it sufficiently.\n\nWhy is LLM bad at simple arithmetic?\n\nAK: Addition algorithm is character-level, yet integers are not split up consistently (sometimes into 1, or 2, or 3 digits).\n\nWhy did GPT-2 have more than necessary trouble coding in Python?\n\nAK: Encoding efficiency of the GPT-2 tokenizer was not great since tabs did not get encoded into single tokens, for example.\n\nWhy did my LLM abruptly halt when it sees the string “&lt;|endoftext|&gt;”?\n\nAK: Something is breaking as this token does not seem to be handled properly: questions to LLMs containing it end up unanswered as this token appears to be parsed when it shouldn’t be when coming from a user that could attack the system.\n\nWhat is this weird warning I get about a “trailing whitespace”?\n\nAK: Prompting ending in trailing space sent to instruct models cause sampling inefficiency since tokens at the beginning of words already start with a space and adding an extra space is out of distribution for the model.\n\n\nWhy did the LLM break if I ask it about “SolidGoldMagikarp”?\n\nAK: There’s an internet-famous blog examining clusters of tokens based on their embeddings. When these tokens are used in questions to an LLM that had these tokens in its tokenizer, the model may respond with evasion, hallucination, insults, etc. The likely cause of this is the discrepancy between tokenization and training datasets. In the tokenization dataset, tokens for popular Reddit users such as SolidGoldMagikarp got merged into their own tokens. However, the token likely never appeared in the training data, and the token never got sampled and ‘trained’. Then at test time, giving this token to the model leads to undefined behavior.\n\n\nWhy should I prefer to use YAML over JSON with LLMs?\n\nAK: YAMLs generally end up being tokenized into fewer tokens than JSONs and hence are more economical.\n\n\nWhy is LLM not actually end-to-end language modeling?\n\nAK: Tokenization.\n\nWhat is the real root of suffering?\n\nAK: Andrej really dislikes tokenization ( ͡° ͜ʖ ͡°)\n\n\n\n\n\n\nMu, J., Li, X. L., & Goodman, N. (2023). Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467. doi: 10.48550/arXiv.2304.08467\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. Technical report, OpenAI."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#basic-tokenizer",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#basic-tokenizer",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "Following Andrej’s suggestion, I implemented the basic BPE tokenizer as an exercise:\n\n\nclass BasicTokenizer:\n    def __init__(self) -&gt; None:\n        self.merges = dict()\n        #self.vocab_size = vocab_size\n        \n    @staticmethod\n    def get_stats(ids, counts=None):\n        \"\"\"\n        Given a list of integers, return a dictionary of counts of consecutive pairs\n        Example: [1, 2, 3, 1, 2] -&gt; {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n        Optionally allows to update an existing dictionary of counts\n        \"\"\"\n        counts = {} if counts is None else counts\n        for pair in zip(ids, ids[1:]): # iterate consecutive elements\n            counts[pair] = counts.get(pair, 0) + 1\n        return counts\n    \n    @staticmethod\n    def merge(ids, pair, idx):\n        \"\"\"Andrej's/DPV's implementations match\"\"\"\n        i = 0\n        out = []\n        while i &lt; len(ids):\n            if (i&lt;len(ids)-1) and(ids[i],ids[i+1])==pair:\n                out.append(idx)\n                i+=2 # merging!\n            else:\n                out.append(ids[i])\n                i+=1 #moving by one\n        return out\n    \n    def train(self, text, vocab_size, verbose=False):\n        #vocab_size = 276\n        num_merges = vocab_size - 256\n        tokens = text.encode(\"utf-8\")\n        print(f\"tokens: {tokens}\")\n        init_num_tokens = len(tokens)\n        for i in range(num_merges):\n            stats = BasicTokenizer.get_stats(tokens)\n            top_pair = max(stats, key=stats.get)\n            tokens = BasicTokenizer.merge(tokens, top_pair, 256+i)\n            self.merges[top_pair] = 256+i\n        final_num_tokens = len(tokens)\n        if verbose:\n            print(init_num_tokens)\n            print(final_num_tokens)\n            print(f\"compression ratio: {init_num_tokens/final_num_tokens:.2f}\")\n            print(f\"merges: {self.merges}\")\n\n    def encode(self, text):\n        # Given a string, return list of integers (tokens)\n        tokens = list(text.encode(\"utf-8\"))\n        while len(tokens)&gt;=2:\n            stats = BasicTokenizer.get_stats(tokens)\n            top_pair = min(stats,key=lambda pair: self.merges.get(pair,float('inf')))\n            if top_pair not in self.merges: break\n            idx = self.merges[top_pair]\n            tokens = BasicTokenizer.merge(tokens,top_pair,idx)\n            #del self.merges[top_pair]\n        return tokens       \n    def decode(self, ids):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0,p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n\n        # given ids (lists of ints), return a Python string\n        tokens = b\"\".join(vocab[idx] for idx in ids)\n        text = tokens.decode(\"utf-8\",errors='replace')        \n        return text \n\n\nI tested it to make sure it works on both the original Unicode text sample and on Taylor Swift’s Wikipedia page (which is on the longer spectrum of Wikipedia pages):\n\n\ntext = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view. I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\"\ntokenizer = BasicTokenizer()\ntokenizer.train(text,vocab_size=276,verbose=True)\n\ntokens: b'\\xef\\xbc\\xb5\\xef\\xbd\\x8e\\xef\\xbd\\x89\\xef\\xbd\\x83\\xef\\xbd\\x8f\\xef\\xbd\\x84\\xef\\xbd\\x85! \\xf0\\x9f\\x85\\xa4\\xf0\\x9f\\x85\\x9d\\xf0\\x9f\\x85\\x98\\xf0\\x9f\\x85\\x92\\xf0\\x9f\\x85\\x9e\\xf0\\x9f\\x85\\x93\\xf0\\x9f\\x85\\x94\\xe2\\x80\\xbd \\xf0\\x9f\\x87\\xba\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb3\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xae\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa8\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xb4\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xa9\\xe2\\x80\\x8c\\xf0\\x9f\\x87\\xaa! \\xf0\\x9f\\x98\\x84 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \\xe2\\x80\\x9csupport Unicode\\xe2\\x80\\x9d in our software (whatever that means\\xe2\\x80\\x94like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don\\xe2\\x80\\x99t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode\\xe2\\x80\\x99s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I\\xe2\\x80\\x99ll give an introduction to it from a programmer\\xe2\\x80\\x99s point of view. I\\xe2\\x80\\x99m going to focus on the character set and what\\xe2\\x80\\x99s involved in working with strings and files of Unicode text. However, in this article I\\xe2\\x80\\x99m not going to talk about fonts, text layout/shaping/rendering, or localization in detail\\xe2\\x80\\x94those are separate issues, beyond my scope (and knowledge) here.'\n1116\n847\ncompression ratio: 1.32\nmerges: {(101, 32): 256, (105, 110): 257, (226, 128): 258, (115, 32): 259, (240, 159): 260, (97, 110): 261, (32, 116): 262, (116, 32): 263, (97, 114): 264, (257, 103): 265, (101, 114): 266, (100, 32): 267, (44, 32): 268, (111, 114): 269, (262, 104): 270, (105, 99): 271, (111, 110): 272, (260, 133): 273, (260, 135): 274, (115, 116): 275}\n\n\n\nfrom pathlib import Path\ntokenizer = BasicTokenizer()\ntaylor_text = Path('taylorswift.txt').read_text().strip()\ntokenizer.train(taylor_text,vocab_size=1276,verbose=True)\n\ntokens: b'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaThe Free Encyclopedia\\n\\nSearch\\nCreate account\\nLog in\\n\\nPersonal tools\\nContents  hide\\n(Top)\\nLife and career\\nToggle Life and career subsection\\nArtistry\\nToggle Artistry subsection\\nAccolades and achievements\\nCultural status\\nToggle Cultural status subsection\\nWealth\\nToggle Wealth subsection\\nDiscography\\nFilmography\\nTours\\nSee also\\nFootnotes\\nReferences\\nToggle References subsection\\nExternal links\\nTaylor Swift\\n\\n136 languages\\nArticle\\nTalk\\nRead\\nView source\\nView history\\n\\nTools\\n Featured article\\nPage semi-protected\\nFrom Wikipedia, the free encyclopedia\\nFor the album, see Taylor Swift (album).\\nTaylor Swift\\nPortrait of Taylor Swift in a cocktail dress\\nSwift at the 2023 MTV Video Music Awards\\nBorn\\tTaylor Alison Swift\\nDecember 13, 1989 (age 34)\\nWest Reading, Pennsylvania, US\\nOccupations\\nSinger-songwriter producer director businesswoman actress\\nYears active\\t2004\\xe2\\x80\\x93present\\nWorks\\nAlbumssinglessongsvideosperformances\\nRelatives\\nAustin Swift (brother)\\nMarjorie Finlay (grandmother)\\nAwards\\tFull list\\nMusical career\\nOrigin\\tNashville, Tennessee, US\\nGenres\\nPop country folk rock alternative\\nInstruments\\nVocals guitar banjo piano ukulele\\nLabels\\nRCA Republic Big Machine\\nWebsite\\twww.taylorswift.com Edit this at Wikidata\\nSignature\\n\\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry, songwriting, and entrepreneurship have influenced the music industry, popular culture, and politics, and her life is a subject of widespread media coverage.\\nSwift began professional songwriting at 14 and signed with Big Machine Records in 2005 to become a country singer. She released six studio albums under the label, four of them to country radio, starting with Taylor Swift (2006). Her next, Fearless (2008), explored country pop, and its singles \"Love Story\" and \"You Belong with Me\" catapulted her to mainstream fame. Speak Now (2010) infused rock influences, while Red (2012) experimented with electronic elements and featured Swift\\'s first Billboard Hot 100 number-one song, \"We Are Never Ever Getting Back Together\". She departed from her country image with 1989 (2014), a synth-pop album supported by the chart-topping songs \"Shake It Off\", \"Blank Space\", and \"Bad Blood\". Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single \"Look What You Made Me Do\".\\nAfter signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylor\\'s Version after a dispute with Big Machine. These albums spawned the number-one songs \"Cruel Summer\", \"Cardigan\", \"Willow\", \"Anti-Hero\", \"All Too Well\", and \"Is It Over Now?\". Her Eras Tour (2023\\xe2\\x80\\x932024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed several music videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021).\\nOne of the world\\'s best-selling musicians, with over 200 million records sold as of 2019, Swift has been named Global Recording Artist of the Year three times by the International Federation of the Phonographic Industry, whereas six of her albums have opened with over a million sales in a week. She is the highest-grossing female touring act, the most-streamed woman on Spotify and Apple Music, and the first billionaire with music as the main source of income. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stone\\'s 100 Greatest Songwriters of All Time, Billboard\\'s Greatest of All Time Artists, and Forbes\\' World\\'s 100 Most Powerful Women. Her accolades include 14 Grammy Awards (featuring a record four Album of the Year wins), a Primetime Emmy Award, 40 American Music Awards, 40 Billboard Music Awards, and 23 MTV Video Music Awards.\\nLife and career\\nEarly life\\n\\nSwift\\'s childhood home in Wyomissing, Pennsylvania\\nTaylor Alison Swift was born on December 13, 1989,[1] in West Reading, Pennsylvania.[2] She is named after singer-songwriter James Taylor.[3] Her father, Scott Kingsley Swift, is a former stockbroker for Merrill Lynch[4] and her mother, Andrea Gardner Swift (n\\xc3\\xa9e Finlay), worked for a time as a mutual fund marketing executive.[5] Taylor has a younger brother, actor Austin Swift.[6]\\nSwift\\'s mother is of Scottish and German descent, and her father is of Scottish and English descent with distant Italian ancestry.[7][8] Swift\\'s paternal great-great-grandfather, Charles Carmine Antonio Baldi, was an Italian immigrant entrepreneur and community leader who opened several businesses in Philadelphia in the 1800s.[9][10][8] Her maternal grandmother, Marjorie (n\\xc3\\xa9e Moehlenkamp) Finlay, was an opera singer.[11]\\nSwift spent her early years on a Christmas tree farm that her father had purchased from one of his clients.[12][13] She is a Christian.[14] She attended preschool and kindergarten at Alvernia Montessori School, run by Bernardine Franciscan sisters,[15] before transferring to the Wyndcroft School.[16] The family moved to a rented house in Wyomissing, Pennsylvania,[17] where Swift attended Wyomissing Area Junior/Senior High School.[18]\\nAt age nine, Swift became interested in musical theater and performed in four Berks Youth Theatre Academy productions.[19] She also traveled regularly to New York City for vocal and acting lessons.[20] Swift later shifted her focus toward country music, inspired by Shania Twain\\'s songs, which made her \"want to just run around the block four times and daydream about everything\".[21] She spent weekends performing at local festivals and events.[22][23] After watching a documentary about Faith Hill, Swift felt she needed to move to Nashville, Tennessee, to pursue a career in music.[24] She traveled there with her mother at age eleven to visit record labels and submitted demo tapes of Dolly Parton and Dixie Chicks karaoke covers.[25] She was rejected, however, because \"everyone in that town wanted to do what I wanted to do. So, I kept thinking to myself, I need to figure out a way to be different.\"[26] She spent summers in Stone Harbor, New Jersey until she was 14 years old, performing in a local coffee shop.[27][28]\\nWhen Swift was around 12 years old, musician Ronnie Cremer taught her to play guitar. Cremer helped with her first efforts as a songwriter, leading her to write \"Lucky You\".[29] In 2003, Swift and her parents started working with New York\\xe2\\x80\\x93based talent manager Dan Dymtrow. With his help, Swift modeled for Abercrombie & Fitch as part of their \"Rising Stars\" campaign, had an original song included on a Maybelline compilation CD, and met with major record labels.[30] After performing original songs at an RCA Records showcase, Swift, then 13 years old, was given an artist development deal and began making frequent trips to Nashville with her mother.[31][32][33] To help Swift break into the country music scene, her father transferred to Merrill Lynch\\'s Nashville office when she was 14 years old, and the family relocated to Hendersonville, Tennessee.[12][34] Swift attended Hendersonville High School[35] before transferring to Aaron Academy after two years, which better accommodated her touring schedule through homeschooling. She graduated one year early.[36][37]\\n2004\\xe2\\x80\\x932008: Career beginnings and first album\\nIn Nashville, Swift worked with experienced Music Row songwriters such as Troy Verges, Brett Beavers, Brett James, Mac McAnally, and the Warren Brothers[38][39] and formed a lasting working relationship with Liz Rose.[40] They began meeting for two-hour writing sessions every Tuesday afternoon after school.[41] Rose called the sessions \"some of the easiest I\\'ve ever done. Basically, I was just her editor. She\\'d write about what happened in school that day. She had such a clear vision of what she was trying to say. And she\\'d come in with the most incredible hooks.\" Swift became the youngest artist signed by the Sony/ATV Tree publishing house,[42] but left then BMG-owned RCA Records (later bought by Sony Music) at the age of 14 due to the label\\'s lack of care and them \"cut[ting] other people\\'s stuff\". She was also concerned that development deals can shelve artists[33][23] and recalled: \"I genuinely felt that I was running out of time. I wanted to capture these years of my life on an album while they still represented what I was going through.\"[43]\\nTaylor Swift singing on a microphone and playing a guitar\\nSwift opening for Brad Paisley in 2007. To promote her first album, she opened tours for other country musicians in 2007 and 2008.[44]\\nAt an industry showcase at Nashville\\'s Bluebird Cafe in 2005, Swift caught the attention of Scott Borchetta, a DreamWorks Records executive who was preparing to form an independent record label, Big Machine Records. She had first met Borchetta in 2004.[45] She was one of Big Machine\\'s first signings,[33] and her father purchased a three-percent stake in the company for an estimated $120,000.[46][47] She began working on her eponymous debut album with Nathan Chapman.[23] Swift wrote or co-wrote all album tracks, and co-writers included Rose, Robert Ellis Orrall, Brian Maher, and Angelo Petraglia.[48] Taylor Swift was released on October 24, 2006.[49] Country Weekly critic Chris Neal deemed Swift better than previous aspiring teenage country singers because of her \"honesty, intelligence and idealism\".[50] The album peaked at number five on the US Billboard 200, on which it spent 157 weeks\\xe2\\x80\\x94the longest stay on the chart by any release in the US in the 2000s decade.[51] Swift became the first female country music artist to write or co-write every track on a US platinum-certified debut album.[52]\\nBig Machine Records was still in its infancy during the June 2006 release of the lead single, \"Tim McGraw\", which Swift and her mother helped promote by packaging and sending copies of the CD single to country radio stations. As there was not enough furniture at the label yet, they would sit on the floor to do so.[53] She spent much of 2006 promoting Taylor Swift with a radio tour and television appearances; she opened for Rascal Flatts on select dates during their 2006 tour,[54] as a replacement for Eric Church.[55] Borchetta said that although record industry peers initially disapproved of his signing a 15-year-old singer-songwriter, Swift tapped into a previously unknown market\\xe2\\x80\\x94teenage girls who listen to country music.[53][12]\\nFollowing \"Tim McGraw\", four more singles were released throughout 2007 and 2008: \"Teardrops on My Guitar\", \"Our Song\", \"Picture to Burn\" and \"Should\\'ve Said No\". All appeared on Billboard\\'s Hot Country Songs, with \"Our Song\" and \"Should\\'ve Said No\" reaching number one. With \"Our Song\", Swift became the youngest person to single-handedly write and sing a number-one song on the chart.[56] \"Teardrops on My Guitar\" reached number thirteen on the US Billboard Hot 100.[57] Swift also released two EPs, The Taylor Swift Holiday Collection in October 2007 and Beautiful Eyes in July 2008.[58][59] She promoted her debut album extensively as the opening act for other country musicians\\' tours in 2006 and 2007, including those by George Strait,[60] Brad Paisley,[61] and Tim McGraw and Faith Hill.[62]\\nSwift won multiple accolades for Taylor Swift. She was one of the recipients of the Nashville Songwriters Association\\'s Songwriter/Artist of the Year in 2007, becoming the youngest person given the title.[63] She also won the Country Music Association\\'s Horizon Award for Best New Artist,[64] the Academy of Country Music Awards\\' Top New Female Vocalist,[65] and the American Music Awards\\' Favorite Country Female Artist honor.[66] She was also nominated for Best New Artist at the 50th Annual Grammy Awards.[67] In 2008, she opened for Rascal Flatts again[68] and briefly dated the singer Joe Jonas.[69][70]\\n2008\\xe2\\x80\\x932010: Fearless\\nTaylor Swift in 2009\\nSwift at the 2009 premiere of Hannah Montana: The Movie. She had a cameo appearance in the film and wrote two songs for its soundtrack.[71][72]\\nSwift\\'s second studio album, Fearless, was released on November 11, 2008, in North America,[73] and in March 2009 in other markets.[74] Critics lauded Swift\\'s honest and vulnerable songwriting in contrast to other teenage singers.[75] Five singles were released in 2008\\xe2\\x80\\x932009: \"Love Story\", \"White Horse\", \"You Belong with Me\", \"Fifteen\", and \"Fearless\". The first single peaked at number four on the Billboard Hot 100 and number one in Australia.[57][76] It was the first country song to top Billboard\\'s Pop Songs chart.[77] \"You Belong with Me\" was the album\\'s highest-charting single on the Billboard Hot 100, peaking at number two,[78] and was the first country song to top Billboard\\'s all-genre Radio Songs chart.[79] All five singles were Hot Country Songs top-10 entries, with \"Love Story\" and \"You Belong with Me\" topping the chart.[80] Fearless became her first number-one album on the Billboard 200 and 2009\\'s top-selling album in the US.[81] The Fearless Tour, Swift\\'s first headlining concert tour, grossed over $63 million.[82] Journey to Fearless, a documentary miniseries, aired on television and was later released on DVD and Blu-ray.[83] Swift performed as a supporting act for Keith Urban\\'s Escape Together World Tour in 2009.[84]\\nIn 2009, the music video for \"You Belong with Me\" was named Best Female Video at the 2009 MTV Video Music Awards.[85] Her acceptance speech was interrupted by rapper Kanye West,[86] an incident that became the subject of controversy, widespread media attention and Internet memes.[87] That year she won five American Music Awards, including Artist of the Year and Favorite Country Album.[88] Billboard named her 2009\\'s Artist of the Year.[89] She won Video of the Year and Female Video of the Year for \"Love Story\" at the 2009 CMT Music Awards, where she made a parody video of the song with rapper T-Pain called \"Thug Story\".[90] At the 52nd Annual Grammy Awards, Fearless was named Album of the Year and Best Country Album, and \"White Horse\" won Best Country Song and Best Female Country Vocal Performance. Swift was the youngest artist to win Album of the Year.[note 1] At the 2009 Country Music Association Awards, Swift won Album of the Year for Fearless and was named Entertainer of the Year, the youngest person to win the honor.[93]\\nSwift featured on John Mayer\\'s single \"Half of My Heart\" and Boys Like Girls\\' single \"Two Is Better Than One\", the latter of which she co-wrote.[94][95] She co-wrote and recorded \"Best Days of Your Life\" with Kellie Pickler,[96] and wrote two songs for the Hannah Montana: The Movie soundtrack\\xe2\\x80\\x94\"You\\'ll Always Find Your Way Back Home\" and \"Crazier\".[72] She contributed two songs to the Valentine\\'s Day soundtrack, including the single \"Today Was a Fairytale\", which was her first number-one on the Canadian Hot 100 and peaked at number two on the US Hot 100.[97][98] While shooting her film debut Valentine\\'s Day in October 2009, Swift dated co-star Taylor Lautner.[99] In 2009, she made her television debut as a rebellious teenager in an CSI: Crime Scene Investigation episode,[100] and she hosted and performed as the musical guest on Saturday Night Live; she was the first host ever to write their own opening monologue.[101][102]\\n2010\\xe2\\x80\\x932014: Speak Now and Red\\nSwift singing into a mic while playing a banjo\\nSwift performing at the Speak Now World Tour in 2012\\nIn August 2010, Swift released \"Mine\", the lead single from her third studio album, Speak Now. The single entered the Hot 100 at number three.[103] Swift wrote the album alone and co-produced every track.[104] The album was released on October 25, 2010,[105] opening atop the Billboard 200 with over one million copies sold.[106] It became the fastest-selling digital album by a female artist, with 278,000 downloads in a week.[107] Critics appreciated Swift\\'s grown-up perspectives:[108] Rob Sheffield of Rolling Stone wrote, \"in a mere four years, the 20-year-old Nashville firecracker has put her name on three dozen or so of the smartest songs released by anyone in pop, rock or country.\"[109] \"Back to December\", \"Mean\", \"The Story of Us\", \"Sparks Fly\", and \"Ours\" became subsequent singles, with the latter two reaching number one on the Hot Country Songs[80] and the first two peaking in the top ten in Canada.[98] She dated actor Jake Gyllenhaal in 2010.[110]\\nAt the 54th Annual Grammy Awards in 2012, Swift won Best Country Song and Best Country Solo Performance for \"Mean\", which she performed during the ceremony.[111] Swift won other awards for Speak Now, including Songwriter/Artist of the Year by the Nashville Songwriters Association (2010 and 2011),[112][113] Woman of the Year by Billboard (2011),[114] and Entertainer of the Year by the Academy of Country Music (2011 and 2012)[115] and the Country Music Association in 2011.[116] At the American Music Awards of 2011, Swift won Artist of the Year and Favorite Country Album.[117] Rolling Stone named Speak Now amongst its \"50 Best Female Albums of All Time\" (2012), writing: \"She might get played on the country station, but she\\'s one of the few genuine rock stars we\\'ve got these days, with a flawless ear for what makes a song click.\"[118]\\nThe Speak Now World Tour ran from February 2011 to March 2012 and grossed over $123 million,[119] followed up by the live album, Speak Now World Tour: Live.[120] She contributed two original songs to The Hunger Games soundtrack album: \"Eyes Open\" and \"Safe & Sound\", co-written and recorded with the Civil Wars and T-Bone Burnett. \"Safe & Sound\" won the Grammy Award for Best Song Written for Visual Media and was nominated for the Golden Globe Award for Best Original Song.[121][122] Swift featured on B.o.B\\'s single \"Both of Us\", released in May 2012.[123] She dated Conor Kennedy that year.[124]\\nTaylor Swift on the Red Tour\\nSwift on the Red Tour (2013)\\nIn August 2012, Swift released \"We Are Never Ever Getting Back Together\", the lead single from her fourth studio album, Red. It became her first number one single in the US and New Zealand,[125][126] and became the fastest-selling single in digital history.[127] Other singles from the album were \"Begin Again\", \"I Knew You Were Trouble\", \"22\", \"Everything Has Changed\", \"The Last Time\", and \"Red\". \"I Knew You Were Trouble\" reached the top five on charts in Australia, Canada, Denmark, Ireland, New Zealand, the UK and the US.[128] \"Begin Again\", \"22\", and \"Red\" reached the top 20 in the US.[57] On Red, released on October 22, 2012,[129] Swift worked with Chapman and Rose, as well as the new producers Max Martin and Shellback.[130] It incorporated many pop and rock styles such as heartland rock, dubstep and dance-pop.[131] Randall Roberts of Los Angeles Times said Swift \"strives for something much more grand and accomplished\" with Red.[132] It opened at number one on the Billboard 200 with 1.21 million sales.[133] Red was Swift\\'s first number-one album in the UK.[134] It earned several accolades, including four nominations at the 56th Annual Grammy Awards (2014).[135] Swift received American Music Awards for Best Female Country Artist in 2012, Artist of the Year in 2013,[136][137] and the Nashville Songwriters Association\\'s Songwriter/Artist Award for the fifth and sixth consecutive years.[138] The Red Tour ran from March 2013 to June 2014 and grossed over $150 million, becoming the highest-grossing country tour ever.[139] Swift was honored with the Pinnacle Award, making her the second recipient of the accolade after Garth Brooks.[140] During this time, she briefly dated the English singer Harry Styles.[141]\\nIn 2013, Swift recorded \"Sweeter than Fiction\", a song she wrote and produced with Jack Antonoff for the One Chance soundtrack. The song received a Best Original Song nomination at the 71st Golden Globe Awards.[142] She provided guest vocals for Tim McGraw\\'s song \"Highway Don\\'t Care\", also featuring Keith Urban.[143] Swift performed \"As Tears Go By\" with the Rolling Stones in Chicago, Illinois, as part of the band\\'s 50 & Counting tour,[144] and joined Florida Georgia Line at their set at the 2013 Country Radio Seminar to sing \"Cruise\".[145] Swift voiced Audrey in the animated film The Lorax (2012),[146] made a cameo in the sitcom New Girl (2013),[147] and had a supporting role in the dystopian film The Giver (2014).[148]\\n2014\\xe2\\x80\\x932018: 1989 and Reputation\\nSwift performing on a mic, dressed in a blue skirt\\nSwift at the 1989 World Tour, the highest-grossing tour of 2015\\nIn March 2014, Swift began living in New York City.[note 2] She hired Tree Paine as her publicist[151] and worked on her fifth studio album, 1989, with the producers Jack Antonoff, Max Martin, Shellback, Imogen Heap, Ryan Tedder, and Ali Payami.[152] She promoted the album extensively, including inviting fans to secret album-listening sessions.[153] 1989 was released on October 27, 2014, and opened atop the Billboard 200 with 1.28 million copies sold.[154] Its singles \"Shake It Off\", \"Blank Space\" and \"Bad Blood\" reached number one in Australia, Canada and the US, the first two making Swift the first woman to replace herself at the Hot 100 top spot;[155] other singles include \"Style\", \"Wildest Dreams\", \"Out of the Woods\" and \"New Romantics\".[156] The 1989 World Tour (2015) was the highest-grossing tour of the year with $250 million in total revenue.[157]\\nPrior to 1989\\'s release, Swift stressed the importance of albums to artists and fans.[158] In November 2014, she removed her entire catalog from Spotify, arguing that its ad-supported, free service undermined the premium service, which provides higher royalties for songwriters.[159] In a June 2015 open letter, Swift criticized Apple Music for not offering royalties to artists during the streaming service\\'s free three-month trial period and stated that she would pull 1989 from the catalog.[160] The following day, Apple Inc. announced that it would pay artists during the free trial period,[161] and Swift agreed to let 1989 on the streaming service.[162] She then returned her entire catalog plus 1989 to Spotify, Amazon Music and Google Play and other digital streaming platforms in June 2017.[163] Swift was named Billboard\\'s Woman of the Year in 2014, becoming the first artist to win the award twice.[164] At the 2014 American Music Awards, Swift received the inaugural Dick Clark Award for Excellence.[165] On her 25th birthday in 2014, the Grammy Museum at L.A. Live opened an exhibit in her honor in Los Angeles that ran until October 4, 2015, and broke museum attendance records.[166][167] In 2015, Swift won the Brit Award for International Female Solo Artist.[168] The video for \"Bad Blood\" won Video of the Year and Best Collaboration at the 2015 MTV Video Music Awards.[169] At the 58th Grammy Awards (2016), 1989 won Album of the Year and Best Pop Vocal Album, making Swift the first woman and fifth act overall to win Album of the Year twice.[170]\\nSwift wearing a sparkling blazer singing on a mic\\nSwift on her Reputation Stadium Tour (2018), the highest-grossing North American tour ever\\nSwift dated the Scottish DJ Calvin Harris from March 2015 to June 2016.[171] They co-wrote the song \"This Is What You Came For\", featuring vocals from the Barbadian singer Rihanna; Swift was initially credited under the pseudonym Nils Sj\\xc3\\xb6berg.[172] In April 2016, Swift criticized the lyrics of Kanye West\\'s single \"Famous\", in which he sings \"I made that bitch famous\" in reference to his interruption of her acceptance speech at the 2009 MTV Video Music Awards. West claimed he had received her approval for the line, and his then-wife Kim Kardashian released video clips of Swift and West discussing the single amicably over the phone; a full recording leaked in 2020 established that West did not disclose that he would call her a \"bitch\".[173][174]\\nAfter briefly dating the English actor Tom Hiddleston,[175] Swift entered a six-year relationship with the English actor Joe Alwyn in September 2016.[176][177][178] She wrote the song \"Better Man\" for the band Little Big Town, which earned her the Song of the Year award at the 51st CMA Awards.[179] Swift and English singer Zayn Malik released the joint single \"I Don\\'t Wanna Live Forever\" for Fifty Shades Darker: Original Motion Picture Soundtrack (2017). The song reached number two in the US.[180]\\nIn August 2017, Swift successfully countersued David Mueller, a former radio jockey for KYGO-FM, who sued her for damages from loss of employment. Four years earlier, she informed Mueller\\'s bosses that he had sexually assaulted her by groping her at an event.[181] Also that month, after a one-year hiatus from the spotlight, Swift cleared her social media accounts and released \"Look What You Made Me Do\" as the lead single from her sixth album, Reputation.[182][183] The single was Swift\\'s first UK number-one single.[184] It topped charts in Australia, Ireland, New Zealand, and the US.[185] Reputation, released on November 10, 2017,[186] incorporated electropop, hip hop, R&B, and EDM.[187] Reviews praised Swift\\'s mature artistry, but some denounced the themes of fame and gossip.[188] The album opened atop the Billboard 200 with 1.21 million US sales[189] and topped the charts in the UK, Australia, and Canada.[190] Its singles \"...Ready for It?\", \"End Game\" (featuring Ed Sheeran and Future), and \"Delicate\" were released to pop radio.[191] Reputation was nominated for a Grammy Award for Best Pop Vocal Album.[192] Swift featured on the country duo Sugarland\\'s \"Babe\" (2018).[193]\\nAt the 2018 American Music Awards, Swift won four awards, which made her accumulate 23 trophies in total and become the AMAs\\' most awarded female musician, surpassing Whitney Houston.[194] The same year, she embarked on her Reputation Stadium Tour,[195] which became the highest-grossing North American concert tour in history and grossed $345.7 million worldwide.[196]\\n2018\\xe2\\x80\\x932020: Lover, Folklore, and Evermore\\nIn November 2018, she signed a new deal with the Universal Music Group; her subsequent releases were promoted by Republic Records. Swift said the contract included a provision for her to maintain ownership of her masters. In addition, in the event that Universal sold any part of its stake in Spotify, it agreed to distribute a non-recoupable portion of the proceeds among its artists.[197] Vox called it a huge commitment from Universal, which was \"far from assured\" until Swift intervened.[198]\\nA portrait of Swift\\nSwift at the American Music Awards of 2019, where she was named Artist of the Decade\\nSwift\\'s first album with Republic Records, Lover, was released on August 23, 2019.[199] Besides Antonoff, she worked with Louis Bell, Frank Dukes, and Joel Little.[200] Lover was her sixth consecutive album to sell more than 500,000 US copies in one week.[201] Critics commended the album\\'s free-spirited mood and emotional intimacy.[202][203] The singles \"Me!\" and \"You Need to Calm Down\" both peaked at number two on the Hot 100,[204] and other singles were the top-10 single \"Lover\", the top-40 single \"The Man\",[57] and the 2023 resurgent success, chart topper \"Cruel Summer\".[205] Lover was the world\\'s best-selling album by a solo artist of 2019,[206] and along with its singles earned nominations at the 62nd Annual Grammy Awards in 2020.[207] At the 2019 MTV Video Music Awards, \"Me!\" won Best Visual Effects, and \"You Need to Calm Down\" won Video of the Year and Video for Good. Swift was the first female and second artist overall to win Video of the Year for a video that they directed.[208]\\nWhile promoting Lover, Swift became embroiled in a public dispute with the talent manager Scooter Braun and Big Machine over the purchase of the masters of her back catalog.[209][210] Swift said she had been trying to buy the masters, but Big Machine would only allow her to do so if she exchanged one new album for each older one under a new contract, which she refused to sign.[209][211] Swift began re-recording her back catalog in November 2020.[212] Besides music, she played Bombalurina in the film adaptation of Andrew Lloyd Webber\\'s musical Cats (2019), for which she co-wrote and recorded the Golden Globe-nominated original song \"Beautiful Ghosts\".[213][214] Critics panned the film but praised Swift\\'s performance.[215] The documentary Miss Americana, which chronicled parts of Swift\\'s life and career, premiered at the 2020 Sundance Film Festival.[216] Swift signed a global publishing deal with Universal Music Publishing Group in February 2020 after her 16-year contract with Sony/ATV expired.[217]\\nAmidst the COVID-19 pandemic in 2020, Swift surprise-released two \"sister albums\" that she recorded with Antonoff and Aaron Dessner: Folklore on July 24, and Evermore on December 11.[218][219] Alwyn co-wrote and co-produced a few songs under the pseudonym William Bowery.[220] Both explore indie folk with a more muted production compared to her previous upbeat pop songs[221][222] and earned Swift widespread critical acclaim and artistic recognition.[223][224] Each album was supported by three singles catering to US pop, country, and triple A radio formats. The singles were \"Cardigan\", \"Betty\", and \"Exile\" from Folklore, and \"Willow\", \"No Body, No Crime\", and \"Coney Island\" from Evermore.[225] Folklore made Swift the first woman to win the Grammy Award for Album of the Year three times at the 63rd Annual Grammy Awards[226] and was the best-selling album of 2020 in the US.[227] Swift became the first artist to debut a US number-one album and a number-one song at the same time with Folklore\\'s \"Cardigan\".[228] At the 2020 American Music Awards, she won three awards, including Artist of the Year for a record third consecutive time.[229] According to Billboard, she was 2020\\'s highest-paid musician in the US and highest-paid solo musician worldwide.[230]\\n2020\\xe2\\x80\\x932023: Re-recordings and Midnights\\n\\nSwift performing in 2022\\nFollowing the masters dispute, Swift released re-recordings of her first six studio albums, beginning with Fearless (Taylor\\'s Version) and Red (Taylor\\'s Version) in April and November 2021, respectively. Both peaked atop the Billboard 200,[231] and the former was the first re-recorded album to do so.[232] Fearless (Taylor\\'s Version) was preceded by \"Love Story (Taylor\\'s Version)\", which made her the second artist after Dolly Parton to have both the original and re-recorded versions of a song reach number one on the Hot Country Songs chart.[233] Red (Taylor\\'s Version) was supported by \"All Too Well (10 Minute Version)\", which became the longest song in history to top the Hot 100.[234] The song was accompanied by a short film, which won a Grammy Award for Best Music Video[235] and Swift\\'s record third MTV Video Music Award for Video of the Year.[236]\\nSwift\\'s tenth studio album, Midnights, was released on October 21, 2022.[237] Characterized by a restrained electropop[238][239] and synth-pop[240] sound, the album was dubbed by Rolling Stone critics as an instant classic.[241][242] The album was her fifth to open atop the Billboard 200 with first-week sales of over one million copies and broke various sales and streaming records,[243] including the most single-day streams and most single-week streams on Spotify.[244] Its tracks, led by single \"Anti-Hero\", monopolized the top 10 of the Hot 100, making Swift the first artist to do so.[245] Two other singles, \"Lavender Haze\" and \"Karma\", peaked at number two on the Hot 100.[246] Swift won nine awards at the 2023 MTV Video Music Awards, including Video of the Year (\"Anti-Hero\") for a record fourth time.[247] At the 66th Annual Grammy Awards, she received Best Pop Vocal Album, and her fourth Album of the Year\\xe2\\x80\\x94the most for any artist.[248]\\nSwift released the third re-recorded album, Speak Now (Taylor\\'s Version), on July 7, 2023, becoming the woman with the most number-one albums (12) in Billboard 200 history, surpassing Barbra Streisand.[249] 1989 (Taylor\\'s Version), released on October 27, 2023, became Swift\\'s record-extending sixth album to sell one million copies in a single week in the US and surpassed Midnights for her career\\'s largest album sales week.[250] Its single \"Is It Over Now?\" debuted atop the Billboard Hot 100. Swift was 2023\\'s most streamed artist on Spotify,[251] Apple Music,[252] and Amazon Music;[253] the first act to place number one on the year-end Billboard top artists list in three different decades (2009, 2015 and 2023);[254] and the first living artist to simultaneously chart five albums in the top 10 of the Billboard 200.[255] She had five out of the 10 best-selling albums of 2023 in the United States, a record since Luminate began tracking US music sales in 1991.[256][257]\\nBeyond her albums, Swift featured on five songs from 2021 to 2023: \"Renegade\" and \"Birch\" by Big Red Machine,[258] a remix of \"Gasoline\" by Haim,[259] \"The Joker and the Queen\" by Ed Sheeran,[260] and \"The Alcott\" by the National.[261] For the soundtrack of the 2022 film Where the Crawdads Sing, she recorded \"Carolina\", which received nominations for Best Original Song at the Golden Globes and Best Song Written for Visual Media at the Grammy Awards.[262] Outside of music, Swift had a supporting role in the 2022 period comedy film Amsterdam and has signed to direct an upcoming feature film for Searchlight Pictures.[263][264]\\n2023\\xe2\\x80\\x93present: The Eras Tour and The Tortured Poets Department\\nSwift singing into a mic\\nSwift on the Eras Tour in 2023\\nIn March 2023, Swift embarked on the Eras Tour, a retrospective tour covering all her studio albums. Media outlets extensively covered the tour\\'s cultural and economic impact,[265] and its US leg broke the record for the most tickets sold in a day.[245] Ticketmaster received public and political criticisms for mishandling the tour\\'s ticket sales.[266] The Eras Tour became the highest-grossing tour in history, collecting over $1 billion.[267][268] Its concert film, released to theaters worldwide on October 13, 2023, grossed over $250 million to become the highest-grossing concert film, and was nominated for the Golden Globe Award for Cinematic and Box Office Achievement.[269][270] Swift\\'s music releases, touring, and related activities culminated in an unprecedented height of popularity post-pandemic.[271] Music Business Worldwide remarked that she entered a \"new stratosphere of global career success\" in 2023.[272]\\nSwift began dating Kansas City Chiefs\\' tight end Travis Kelce in 2023.[273] In January 2024, AI-generated fake pornographic images portraying Swift were posted to X (formerly Twitter) and spread to other social media platforms, spurring criticism and demands for legal reform.[274][275] At the 66th Grammy Awards, Swift announced her eleventh studio album, The Tortured Poets Department, set for release on April 19, 2024.[248][276]\\nArtistry\\nInfluences\\nOne of Swift\\'s earliest memories of music is listening to her maternal grandmother, Marjorie Finlay, sing in church.[5] As a child, she enjoyed Disney film soundtracks: \"My parents noticed that, once I had run out of words, I would just make up my own.\"[277] Swift said she owes her confidence and \"fascination with writing and storytelling\" to her mother, who helped her prepare for class presentations as a child.[278][279]\\nSwift was drawn to the storytelling aspect of country music,[280] which was introduced to the genre by female country artists of the 1990s: Shania Twain, Faith Hill, and the Dixie Chicks.[281][282] Twain, both as a songwriter and performer, was her biggest musical influence.[283] Hill was Swift\\'s childhood role model, and she would often imitate her.[284] She admired the Chicks\\' defiant attitude and the way they played their instruments,[285] and was also influenced by older country stars like Patsy Cline, Loretta Lynn, Tammy Wynette, and Dolly Parton,[22] the last of whom she believes is exemplary to female songwriters.[114] As a songwriter, Swift was influenced by Joni Mitchell\\'s emotional and autobiographical lyrics, highlighting Mitchell\\'s 1971 album Blue as a favorite \"because it explores somebody\\'s soul so deeply\".[286] She also spoke of influence from 1990s songwriters such as Melissa Etheridge, Sarah McLachlan, and Alanis Morissette,[287][288] and alt-country artists like Patty Griffin[289] and Lori McKenna.[290]\\nVarious pop and rock artists have also influenced Swift. She lists Paul McCartney, Bruce Springsteen, Emmylou Harris, and Kris Kristofferson as her career role models.[12][291] 1989 was influenced by some of her favorite 1980s pop acts, including Peter Gabriel, Annie Lennox, Phil Collins, and Madonna.[292][293] She also cited Keith Urban\\'s musical style and Fall Out Boy\\'s lyrics as major influences.[294][295]\\nGenres\\n\"If there\\'s one thing that Swift has proven throughout her career, it\\'s that she refuses to be put in a box. Her ever-evolving sound took her from country darling to pop phenom to folk\\'s newest raconteur.\"\\n\\xe2\\x80\\x94The Recording Academy, 2021[296]\\nSwift is known for venturing into various music genres and undergoing artistic reinventions,[297][264] having been described as a \"music chameleon\".[298][299] She self-identified as a country musician until 2012, when she released her fourth studio album, Red.[300] Her albums were promoted to country radio, but music critics noted wide-ranging styles of pop and rock[301][302] and said that the melodies of her songs were rooted in pop, and the country music elements were limited to instruments such as banjo, mandolin, and fiddle, and her slight twang.[303][304] Some commented that her country music identity was an indicator of her narrative songwriting rather than musical style.[305][306] Although the Nashville music industry was receptive of Swift\\'s status as a country musician, critics accused her of abandoning her roots in favor of crossover success in mainstream pop.[307][308] Red\\'s eclectic pop, rock, and electronic styles intensified the critical debate, to which Swift responded, \"I leave the genre labeling to other people.\"[309]\\nMusic journalist Jody Rosen commented that by originating her musical career in Nashville, Swift made a \"bait-and-switch maneuver, planting roots in loamy country soil, then pivoting to pop\".[310] She abandoned her country music identity in 2014 with the release of her synth-pop fifth studio album, 1989. Swift described it as her first \"documented, official pop album\".[311] Her subsequent albums Reputation (2017) and Lover (2019) have an upbeat pop production; the former incorporates hip hop, trap, and EDM elements.[312][313][314] Midnights (2022), on the other hand, is distinguished by a more experimental, \"subdued and amorphous pop sound\".[315][316] Although reviews of Swift\\'s pop albums were generally positive, some critics lamented that the pop music production indicated Swift\\'s pursuit of mainstream success, eroding her authenticity as a songwriter nurtured by her country music background\\xe2\\x80\\x94a criticism that has been retrospectively described as rockist.[317][318] Musicologist Nate Sloan remarked that Swift\\'s pop music transition was rather motivated by her need to expand her artistry.[319] Swift eschewed mainstream pop in favor of alternative, folk and indie rock styles with her 2020 studio albums Folklore and Evermore.[320][321] Clash said her career \"has always been one of transcendence and covert boundary-pushing\", reaching a point at which \"Taylor Swift is just Taylor Swift\", not defined by any genre.[322]\\nVoice\\n\\n\"Cardigan\"\\nDuration: 22 seconds.0:22\\nSwift uses her lower register in \"Cardigan\" (2020).[323]\\n\"Lavender Haze\"\\nDuration: 18 seconds.0:18\\n\"Lavender Haze\" (2022) features Swift\\'s falsetto vocals in the refrain.[324]\\nProblems playing these files? See media help.\\nSwift possesses a mezzo-soprano vocal range,[325] and a generally soft but versatile timbre.[326][327] As a country singer, her vocals were criticized by some as weak and strained compared to those of her contemporaries.[328] Swift admitted her vocal ability often concerned her in her early career and has worked hard to improve.[329] Reviews of her vocals remained mixed after she transitioned to pop music with 1989; critics complained that she lacked proper technique but appreciated her usage of her voice to communicate her feelings to the audience, prioritizing \"intimacy over power and nuance\".[330] They also praised her for refraining from correcting her pitch with Auto-Tune.[331]\\nThe Los Angeles Times remarked that Swift\\'s defining vocal feature is her attention to detail to convey an exact feeling\\xe2\\x80\\x94\"the line that slides down like a contented sigh or up like a raised eyebrow\".[332] With Reputation, critics noted she was \"learning how to use her voice as a percussion instrument of its own\",[333] swapping her \"signature\" expressive vocals for \"cool, conversational, detached\" cadences and rhythms similar to hip hop and R&B styles.[334][335][336] Alternative Press stated that her \"evocative\" vocal stylings are more reminiscent of pop-punk and emo genres.[337]\\nReviews of Swift\\'s later albums and performances were more appreciative of her vocals, finding them less nasal, richer, more resonant, and more powerful.[304][338][339] With Folklore and Evermore, Swift received praise for her sharp and agile yet translucent and controlled voice.[340][341][342] Pitchfork described it as \"versatile and expressive\".[343] With her 2021 re-recorded albums, critics began to praise the mature, deeper and \"fuller\" tone of her voice.[344][345][346] An i review said Swift\\'s voice is \"leagues better now\".[347] The Guardian highlighted \"yo-yoing vocal yelps\" and passionate climaxes as the trademarks of Swift\\'s voice,[348] and that her country twang faded away.[349] Midnights received acclaim for Swift\\'s nuanced vocal delivery.[350] She ranked 102nd on the 2023 Rolling Stone list of the 200 Greatest Singers of All Time.[327] In a review of the Eras Tour, The New Yorker critic Amanda Petrusich praised the clarity and tone of Swift\\'s live vocals.[351] Musicologist Alyssa Barna said that Swift\\'s timbre is \"breathy and bright\" in her upper register and \"full and dark\" in the lower.[222]\\nSongwriting\\nFurther information: List of songs by Taylor Swift\\nSwift has been referred to as one of the greatest songwriters ever by several publications.[352][353][354] Literature scholars like Jonathan Bate and Stephanie Burt have noted that her literary and melodic sensibility and writing style are rare amongst her peers.[355][356] Swift\\'s bridges are often noted as one of the best aspects of her songs,[357][358] earning her the title \"Queen of Bridges\" from Time.[359] Mojo described her as \"a sharp narrator with a gift for the extended metaphor\".[360]\\nIn The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\".[12] Her personal experiences were a common inspiration for her early songs, which helped her navigate life.[361][362] Her \"diaristic\" technique began with identifying an emotion, followed by a corresponding melody.[363][364] On her first three studio albums, love, heartbreak, and insecurities, from an adolescent perspective, were dominant themes.[365][366] She delved into the tumult of toxic relationships on Red,[367] and embraced nostalgia and post-romance positivity on 1989.[292] Reputation was inspired by the downsides of Swift\\'s fame,[368] and Lover detailed her realization of the \"full spectrum of love\".[369] Other themes in Swift\\'s music include family dynamics, friendship,[370][371] alienation, self-awareness, and tackling vitriol, especially sexism.[279][372]\\nHer confessional lyrics received positive reviews from critics,[373][12][374] who highlighted their vivid details and emotional engagement, which they found uncommon in pop music.[375][376][377] Critics also praised her melodic compositions; Rolling Stone described Swift as \"a songwriting savant with an intuitive gift for verse-chorus-bridge architecture\".[378][379] NPR dubbed Swift \"a master of the vernacular in her lyrics\",[335] remarking that her songs offer emotional engagement because \"the wit and clarity of her arrangements turn them from standard fare to heartfelt disclosures\".[379] Despite the positive reception, The New Yorker stated she was generally portrayed \"more as a skilled technician than as a Dylanesque visionary\".[12] Tabloid media often speculated and linked the subjects of her songs with her ex-lovers, a practice reviewers and Swift herself criticized as sexist.[380][381][382] Aside from clues in album liner notes, Swift avoided talking about the subjects of her songs.[383]\\nOn her 2020 albums Folklore and Evermore, Swift was inspired by escapism and romanticism to explore fictional narratives.[384] She imposed emotions onto imagined characters and story arcs, which liberated her from tabloid attention and suggested new paths for her artistry.[363] Swift explained that she welcomed the new songwriting direction after she stopped worrying about commercial success.[384] According to Spin, she explored complex emotions with \"precision and devastation\" on Evermore.[385] Consequence stated her 2020 albums convinced skeptics of her songwriting prowess, noting her transformation from \"teenage wunderkind to a confident and careful adult\".[358]\\nSwift divides her writing into three types: \"quill lyrics\", songs rooted in antiquated poeticism; \"fountain pen lyrics\", based on modern and vivid storylines; and \"glitter gel pen lyrics\", which are lively and frivolous.[386] Critics note the fifth track of every Swift album as the most \"emotionally vulnerable\" of the album.[387] Awarding her with the Songwriter Icon Award in 2021, the National Music Publishers\\' Association remarked that \"no one is more influential when it comes to writing music today\".[388] The Week deemed her the foremost female songwriter of modern times,[389] and the Nashville Songwriters Association International named her Songwriter-Artist of the Decade in 2022.[245] Swift has also published two original poems: \"Why She Disappeared\" and \"If You\\'re Anything Like Me\".[390]\\nPerformances\\nFurther information: List of Taylor Swift live performances\\n\\nSwift performing on the Reputation Stadium Tour in Seattle in May 2018\\nJournalists have described Swift as one of the best live performers. Often praised for her showmanship and stage presence,[391][392][393][394][395] Swift commands large audiences,[396][397][398] without having to rely on dance like her contemporaries do.[399] According to V magazine\\'s Greg Krelenstein, she possesses \"a rare gift of turning a stadium spectacle into an intimate setting\", irrespective of whether she is \"plucking a guitar or leading an army of dancers\".[400] In a 2008 review of Swift\\'s early performances, Sasha Frere-Jones of The New Yorker called Swift a \"preternaturally skilled\" entertainer with a vibrant stage presence, adding \"she returned the crowd\\'s energy with the professionalism she has shown since the age of fourteen.\"[401] In 2023, Adrian Horton of The Guardian noted her \"seemingly endless stamina\" on the Eras Tour,[402] and i critic Ilana Kaplan called her showmanship \"unparalleled\".[403]\\nCritics have highlighted Swift\\'s versatility as an entertainer, praising her ability to switch onstage personas and performance styles depending on the varying themes and aesthetics of her albums.[404][405] Her concert productions have been characterized by elaborate Broadway theatricality and high technology,[406] and her performances frequently incorporate a live band, with whom she has played and toured since 2007.[407] Swift also often accompanies herself with musical instruments such as electric guitar;[408] acoustic guitar; piano;[409] and sometimes twelve-string guitar,[410][411] banjo,[412] or ukulele.[413] Interacting frequently with the audience, her solo acoustic performances are considered intimate and emotionally resonant, complementing her story-based lyrics and fan connection.[351][414] Lydia Burgham of The Spinoff opined that this intimacy remains \"integral to her singer-songwriter origins\".[415][409] Chris Willman of Variety called Swift \"pop\\'s most approachable superstar\",[416] and the 21st century\\'s most popular performer.[417]\\nVideo and film\\nFurther information: Taylor Swift videography\\nSwift emphasizes visuals as a key creative component of her music-making process.[418] She has collaborated with different directors to produce her music videos, and over time she has become more involved with writing and directing. She developed the concept and treatment for \"Mean\" in 2011[419] and co-directed the music video for \"Mine\" with Roman White the year before.[420] In an interview, White said that Swift \"was keenly involved in writing the treatment, casting and wardrobe. And she stayed for both the 15-hour shooting days, even when she wasn\\'t in the scenes.\"[421]\\nFrom 2014 to 2018, Swift collaborated with director Joseph Kahn on eight music videos\\xe2\\x80\\x94four each from her albums 1989 and Reputation. Kahn has praised Swift\\'s involvement.[422] She worked with American Express for the \"Blank Space\" music video (which Kahn directed), and served as an executive producer for the interactive app AMEX Unstaged: Taylor Swift Experience, for which she won a Primetime Emmy Award for Outstanding Interactive Program in 2015.[423] Swift produced the music video for \"Bad Blood\" and won a Grammy Award for Best Music Video in 2016.[424]\\nHer production company, Taylor Swift Productions, is credited with producing all of her visual media starting with the 2018 concert documentary Reputation Stadium Tour.[425] She continued to co-direct music videos for the Lover singles \"Me!\" with Dave Meyers, and \"You Need to Calm Down\" (also serving as a co-executive producer) and \"Lover\" with Drew Kirsch,[426] but first ventured into sole direction with the video for \"The Man\" (which won her the MTV Video Music Award for Best Direction).[427] After Folklore: The Long Pond Studio Sessions, Swift debuted as a filmmaker with All Too Well: The Short Film,[245] which made her the first artist to win the Grammy Award for Best Music Video as a sole director.[428] Swift has cited Chlo\\xc3\\xa9 Zhao, Greta Gerwig, Nora Ephron, Guillermo del Toro, John Cassavetes, and Noah Baumbach as filmmaking influences.[418]\\nAccolades and achievements\\nFurther information: List of awards and nominations received by Taylor Swift\\n\\nIn 2009, Swift became the first country singer to win an MTV Video Music Award.\\nSwift\\'s discography is a \"critically hailed songbook\", as per Time\\'s Sam Lansky.[429] She has won 14 Grammy Awards (including four for Album of the Year\\xe2\\x80\\x94the most won by an artist),[430] an Emmy Award,[431] 40 American Music Awards (the most won by an artist),[432] 39 Billboard Music Awards (the most won by an artist\\xe2\\x80\\x94tying with Drake),[433] 118 Guinness World Records,[434] 23 MTV Video Music Awards (including four Video of the Year wins\\xe2\\x80\\x94the most by an act),[247] 12 Country Music Association Awards (including the Pinnacle Award),[435] eight Academy of Country Music Awards,[436] and two Brit Awards.[168] As a songwriter, she has been honored by the Nashville Songwriters Association,[63][437] the Songwriters Hall of Fame, and the National Music Publishers\\' Association and was the youngest person on Rolling Stone\\'s list of the 100 Greatest Songwriters of All Time in 2015.[438][439] At the 64th BMI Awards in 2016, Swift was the first woman to be honored with an award named after its recipient.[440]\\nCommercially, from available data, Swift has amassed over 50 million album sales and 150 million single sales as of 2019,[441][442][443] and 114 million units globally, including 78 billion streams as of 2021.[444][445] The International Federation of the Phonographic Industry ranked her as the Global Recording Artist of the Year for a record three times (2014, 2019 and 2022).[446] Swift has the most number-one albums in the United Kingdom and Ireland for a female artist this millennium,[447][448] earned the highest income for an artist on Chinese digital music platforms (RMB 159,000,000 as of 2021),[449] and is the first artist to replace themselves at the top spot and occupy the entire top five[note 3] of the Australian albums chart.[452][453] Swift remains the world\\'s highest-grossing female touring act ever, with cumulative ticket sales at $1.96 billion as of November 2023 according to Pollstar.[454] The Eras Tour is the highest-grossing tour of all time as of December 2023, and the first to surpass $1 billion in revenue.[455] Beginning with Fearless, each of her studio albums have opened with over one million global units.[456][457] Swift is the most streamed female act on Spotify and Apple Music.[458][459] On Spotify, she is the only artist to have received more than 200 and 250 million streams in one day (260 million on October 27, 2023),[460] and the only female act to reach 100 million monthly listeners.[234] The most entries and the most simultaneous entries for an artist on the Billboard Global 200, with 143 and 31 songs, respectively, are among her feats.[461][462]\\nIn the US, Swift has sold over 37.3 million albums as of 2019,[443] when Billboard placed her eighth on its Greatest of All Time Artists Chart.[463] Eleven of her songs have topped the Billboard Hot 100.[257] She is the longest-reigning act of the Billboard Artist 100 (97 weeks);[464] the soloist with the most cumulative weeks atop the Billboard 200 (68);[465] the woman with the most Billboard 200 number-ones (13),[234] Hot 100 entries (232),[234][156] number-one debuts (6),[note 4] top-ten songs (49),[257] and weeks atop the Top Country Albums chart (101);[467] and the act with the most number-one songs on Pop Airplay (12) and Digital Songs (28).[468][469] Swift is the first woman to simultaneously chart five albums in the top 10 and eleven albums on the entire Billboard 200;[470][471] and the first act to occupy the top four spots and chart seven albums[note 5] in the top 10 on the Top Album Sales chart.[473][474] She is the second highest-certified female digital singles artist (and fifth overall) in the US, with 137.5 million total units certified by the Recording Industry Association of America (RIAA),[475] and the first woman to have both an album (Fearless) and a song (\"Shake It Off\") certified Diamond.[476] Swift is the only artist in Luminate history to have six albums sell over a million copies in a week.[477]\\nSwift has appeared in various power listings. Time included her on its annual list of the 100 most influential people in 2010, 2015, and 2019.[478] She was one of the \"Silence Breakers\" that the magazine spotlighted as Person of the Year in 2017 for speaking up about sexual assault,[479] and received the honor again in 2023 for her cultural domination that year.[429] Time described Swift as the first Person of the Year to be recognized for \"achievement in the arts\", as well as the first woman to be recognized and appear on a Person of the Year cover more than once.[480][481] In 2014, she was named to Forbes\\' 30 Under 30 list in the music category[482] and again in 2017 in its \"All-Star Alumni\" category.[483] Swift became the youngest woman to be included on Forbes\\' list of the 100 most powerful women in 2015, ranked at number 64.[484] In 2023, she was ranked by Forbes as the fifth-most powerful woman in the world, the first entertainer to place in the top five.[485] Swift received an honorary Doctor of Fine Arts degree from New York University and served as its commencement speaker on May 18, 2022.[245]\\nCultural status\\nMain articles: Cultural impact of Taylor Swift and Public image of Taylor Swift\\n\\nSwift at the 2010 Time 100 Gala\\nSwift has been credited with making a profound impact on the music industry, popular culture and the economy.[486][487] She dominates cultural conversations,[488][489] which has led publications to describe her as a cultural \"vitality\" or zeitgeist.[490][491][492] Her music, life and public image are points of attention in global celebrity culture.[297] Initially a teen idol,[493] she has been referred to as a pop icon;[312][494] publications describe her immense popularity and longevity as unwitnessed since the 20th century.[495][496] In 2013, New York magazine\\'s Jody Rosen dubbed Swift the \"world\\'s biggest pop star\" and opined that the trajectory of her stardom has defied established patterns. Rosen added that Swift \"falls between genres, eras, demographics, paradigms, trends\", leaving her contemporaries \"vying for second place\".[310] Critics regard Swift as a rare yet successful combination of the pop star and singer-songwriter archetypes.[497]\\nHer fans are known as Swifties.[266] Billboard noted only few artists have had her chart success, critical acclaim, and fan support.[498] Swift\\'s million-selling albums are considered an anomaly in the streaming-dominated industry following the end of the album era in the 2010s.[499][500] Economist Alan Krueger described Swift as an \"economic genius\".[501]\\nAlthough labeled by the media in her early career as \"America\\'s Sweetheart\" for her girl next door persona,[502][503] Swift has been accused by detractors of being \"calculated\" and manipulative of her image, a narrative bolstered by her 2016 dispute with West.[173][174] Critics have also noted that her personal life and career have been subject to intense misogyny and \"slut-shaming\",[504][505] as well as rampant media scrutiny and tabloid speculation.[506]\\nSwift\\'s private jet use has drawn scrutiny for its carbon emissions.[507][508] In 2023, a spokesperson for Swift stated that she had purchased more than double the required carbon credits to offset all tour travel and personal flights.[509][510] In December 2023, Swift\\'s lawyers sent a cease and desist letter to American programmer Jack Sweeney over tracking her private jet, citing safety concerns and stalking.[511][512] Swift has been a victim of numerous house break-ins and stalkers, some of whom were armed.[513][514]\\nLegacy\\n\"You have different artists dominating different sectors of the industry: Some are huge at streaming, some are big draws on the road. But we\\'re at this moment where there\\'s no one better than Taylor Swift, whether that\\'s on the radio, with streaming, ticket sales or just cultural impact.\"\\n\\xe2\\x80\\x93 Jason Lipshutz, Billboard executive director, 2023[515]\\nSwift helped shape the modern country music scene,[516] having extended her success beyond the Anglosphere,[310][516] pioneered the use of internet (Myspace) as a marketing tool,[33][53] and introduced the genre to a younger generation.[517][310] Country labels have since become interested in signing young singers who write their own music;[518] her guitar performances contributed to the \"Taylor Swift factor\", a phenomenon to which an upsurge in guitar sales to women, a previously ignored demographic, is attributed.[519][520]\\nAccording to publications, Swift changed the music landscape with her genre transitions, a discography that accommodates cultural shifts,[521] and her ability to popularize any sound in mainstream music.[522] Lyrically, in being personal and vulnerable in her songs, music journalist Nick Catucci opined Swift helped make space for later singers like Billie Eilish, Ariana Grande, and Halsey to do the same.[523] Scholars have highlighted the literary sensibility and poptimist implications of Swift.[355][524] She has been credited with legitimizing and popularizing the concept of album \"eras\".[525][526] Swift is a subject of academic study and scholarly media research.[297] Various educational institutions offer courses on Swift in literary, cultural and sociopolitical contexts.[527][297]\\nSwift has influenced numerous music artists, and her albums have inspired a generation of singer-songwriters.[517][320][528] Journalists praise her ability to reform industry practices, noting how her actions changed streaming policies, prompted awareness of intellectual property in new musicians,[529][530] and reshaped ticketing models.[531] Various sources deem Swift\\'s music a paradigm representing the millennial generation;[532] Vox called her the \"millennial Bruce Springsteen\",[533] and The Times named her \"the Bob Dylan of our age\".[534] Swift earned the title Woman of the Decade (2010s) from Billboard,[535] Artist of the Decade (2010s) at the American Music Awards,[536] and Global Icon at the Brit Awards for her impact.[445] Senior artists such as Paul McCartney,[537] Mick Jagger,[538] Madonna,[539] and Dolly Parton have praised her musicianship.[540] Carole King regards Swift her \"professional grand daughter\" and thanked Swift for \"carrying the torch forward\".[541] Springsteen called her a \"tremendous\" writer,[542] while Ringo Starr and Billy Joel considered Swift the Beatles\\' successor.[543][544] Britney Spears labeled Swift \"the most iconic pop woman of our generation\".[545]\\nEntrepreneurship\\nMedia outlets describe Swift as a savvy businesswoman;[546][547] in 2024, she topped Billboard\\'s annual Power 100 ranking of the top music industry executives.[548] Swift is known for her traditional album rollouts, consisting of a variety of promotional activities that Rolling Stone termed as an inescapable \"multimedia bonanza\".[549][550] Easter eggs and cryptic teasers became a common practice in contemporary pop music because of Swift.[551] Publications describe her discography as a music \"universe\" subject to analyses by fans, critics and journalists.[552][553][554] Swift maintains an active presence on social media and a close relationship with fans, to which many journalists attribute her success.[555][487][556] Her in-house management team is called 13 Management.[557]\\nSwift has endorsed many brands and businesses, having launched clothing lines with L.E.I. and Stella McCartney,[558][559] designed American Greetings cards and Jakks Pacific dolls,[560][561] released a number of fragrances with Elizabeth Arden,[562] and signed multi-year deals with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League\\'s Nashville Predators and Sony Cyber-shot digital cameras,[565][566] and became the global ambassador for New York City in 2014 and Record Store Day in 2022.[567][568]\\nSocial activism\\nFurther information: Political impact of Taylor Swift\\nSwift identifies as a pro-choice feminist,[569] and is a founding signatory of the Time\\'s Up movement against sexual harassment.[570] Specifically, she criticized the US Supreme Court\\'s decision to end federal abortion rights in 2022.[571] Swift also advocates for LGBT rights,[572] and has called for the passing of the Equality Act, which prohibits discrimination based on sex, sexual orientation, and gender identity.[573][574] She performed during WorldPride NYC 2019 at the Stonewall Inn, a gay rights monument, and has donated to the LGBT organizations Tennessee Equality Project and GLAAD.[575][576][577]\\nA supporter of the March for Our Lives movement and gun control reform in the US,[578] Swift is a vocal critic of white supremacy, racism, and police brutality.[579][569] Following the George Floyd protests, she donated to the NAACP Legal Defense and Educational Fund and the Black Lives Matter movement,[580] called for the removal of Confederate monuments in Tennessee,[581] and advocated for Juneteenth to become a national holiday.[582] In 2020, Swift urged her fans to check their voter registration ahead of elections, which resulted in 65,000 people registering to vote within one day of her post,[583] and endorsed Joe Biden and Kamala Harris in the US presidential election.[584] She has openly criticized former president Donald Trump.[585]\\nWealth\\nSwift\\'s net worth is estimated by Forbes and Bloomberg News at $1.1 billion as of October 2023, making her the first musician to achieve billionaire status \"solely based on her songs and performances\".[586][587] Forbes named her the annual top-earning female musician in 2016, 2019, 2021, and 2022.[588] She was the highest-paid celebrity of 2016 with $170 million\\xe2\\x80\\x94a feat recognized by the Guinness World Records as the highest annual earnings ever for a female musician,[589] which she herself surpassed with $185 million in 2019.[590] Overall, Forbes listed Swift as the highest-paid female artist of the 2010s, earning $825 million.[591] She has also developed a real estate portfolio worth $150 million as of 2023, with properties in Nashville; Tribeca, Manhattan; Los Angeles (Samuel Goldwyn Estate); and Rhode Island (High Watch).[592]\\nPhilanthropy\\nSwift is known for her philanthropic efforts.[593] She ranked first on DoSomething\\'s 2015 \"Gone Good\" list,[594] having received the Star of Compassion from the Tennessee Disaster Services and the Big Help Award from the Nickelodeon Kids\\' Choice Awards for her \"dedication to helping others\" and \"inspiring others through action\".[595][596] She donated $100,000 to the Red Cross to help the victims of the Iowa flood of 2008.[597] In 2009, she sang at BBC\\'s Children in Need concert and raised \\xc2\\xa313,000 for the cause.[598] Swift has performed at charity relief events, including Sydney\\'s Sound Relief concert.[599] In response to the May 2010 Tennessee floods, Swift donated $500,000.[600] In 2011, Swift used a dress rehearsal of her Speak Now tour as a benefit concert for victims of recent tornadoes in the US, raising more than $750,000.[601] In 2016, she donated $1 million to Louisiana flood relief efforts and $100,000 to the Dolly Parton Fire Fund.[602][603] Swift donated to food banks after Hurricane Harvey struck Houston in 2017 and at every stop of the Eras Tour in 2023;[604][605] she also directly employed local businesses throughout the tour and gave $55 million in bonus payments to her entire crew.[606][607] Swift donated $1 million for Tennessee tornado relief in 2020 and again in 2023.[608][609]\\nShe is a supporter of the arts. A benefactor of the Nashville Songwriters Hall of Fame,[610] Swift has donated $75,000 to Nashville\\'s Hendersonville High School to help refurbish the school auditorium,[611] $4 million to build a new education center at the Country Music Hall of Fame and Museum in Nashville,[612] $60,000 to the music departments of six US colleges,[613] and $100,000 to the Nashville Symphony.[614] Also a promoter of children\\'s literacy, she has donated money and books to schools around the country.[615][616] In 2007, Swift partnered with the Tennessee Association of Chiefs of Police to launch a campaign to protect children from online predators.[617] She has donated items to several charities for auction, including the UNICEF Tap Project and MusiCares.[618] As recipient of the Academy of Country Music\\'s Entertainer of the Year in 2011, Swift donated $25,000 to St. Jude Children\\'s Research Hospital, Tennessee.[619] In 2012, Swift participated in the Stand Up to Cancer telethon, performing the charity single \"Ronan\", which she wrote in memory of a four-year-old boy who died of neuroblastoma.[620] She has also donated $100,000 to the V Foundation for Cancer Research[621] and $50,000 to the Children\\'s Hospital of Philadelphia.[622] Swift has encouraged young people to volunteer in their local communities as part of Global Youth Service Day.[623]\\nSwift donated to fellow singer-songwriter Kesha to help with her legal battles against Dr. Luke and to actress Mariska Hargitay\\'s Joyful Heart Foundation.[593][624] During the COVID-19 pandemic, Swift donated to the World Health Organization and Feeding America,[625] and supported independent record stores.[626][627] Swift performed \"Soon You\\'ll Get Better\" on the One World: Together At Home television special, a benefit concert curated by Lady Gaga for Global Citizen to raise funds for the World Health Organization\\'s COVID-19 Solidarity Response Fund.[628] In 2018 and 2021, Swift donated to the Rape, Abuse & Incest National Network in honor of Sexual Assault Awareness and Prevention Month.[593][629] She has made donations to her fans several times for their medical or academic expenses.[630] In December 2023, Swift attended Ramy Youssef\\'s fundraiser for the Gaza Strip.[631]\\nDiscography\\nMain articles: Taylor Swift albums discography, Taylor Swift singles discography, and List of songs by Taylor Swift\\nStudio albums\\nTaylor Swift (2006)\\nFearless (2008)\\nSpeak Now (2010)\\nRed (2012)\\n1989 (2014)\\nReputation (2017)\\nLover (2019)\\nFolklore (2020)\\nEvermore (2020)\\nMidnights (2022)\\nThe Tortured Poets Department (2024)\\nRe-recorded albums\\nFearless (Taylor\\'s Version) (2021)\\nRed (Taylor\\'s Version) (2021)\\nSpeak Now (Taylor\\'s Version) (2023)\\n1989 (Taylor\\'s Version) (2023)\\nFilmography\\nMain article: Taylor Swift videography\\nThis section lists select works only. Refer to the main article for further information.\\nValentine\\'s Day (2010)\\nThe Lorax (2012)\\nThe Giver (2014)\\nCats (2019)\\nAll Too Well: The Short Film (also director) (2021)\\nAmsterdam (2022)\\nDocumentary and concert films\\nJourney to Fearless (2010)\\nSpeak Now World Tour \\xe2\\x80\\x93 Live (2011)\\nThe 1989 World Tour Live (2015)\\nTaylor Swift: Reputation Stadium Tour (2018)\\nMiss Americana (2020)\\nTaylor Swift: City of Lover (2020)\\nFolklore: The Long Pond Studio Sessions (also director) (2020)\\nTaylor Swift: The Eras Tour (2023)\\nTours\\nMain article: List of Taylor Swift live performances\\nFearless Tour (2009\\xe2\\x80\\x932010)\\nSpeak Now World Tour (2011\\xe2\\x80\\x932012)\\nThe Red Tour (2013\\xe2\\x80\\x932014)\\nThe 1989 World Tour (2015)\\nReputation Stadium Tour (2018)\\nThe Eras Tour (2023\\xe2\\x80\\x932024)\\nSee also\\nList of American Grammy Award winners and nominees\\nList of highest-certified music artists in the United States\\nList of most-followed Instagram accounts\\nList of most-followed Twitter accounts\\nList of most-subscribed YouTube channels\\nFootnotes\\n Swift held the record until the 62nd Annual Grammy Awards in 2020.[91][92]\\n Though Swift has properties throughout the US, she identifies Nashville as her home.[149][150]\\n Swift has occupied the top five of the ARIA Albums Chart twice. She achieved this feat first on the issue published on July 7, 2023,[450] followed by a second time on the issue published on February 9, 2024.[451]\\n In a tie with Ariana Grande.[466]\\n Swift has charted seven titles in the top 10 of the Top Album Sales chart twice\\xe2\\x80\\x94on the issues dated January 6, 2024, and January 20, 2024.[472]\\nReferences\\n \"Taylor Swift: The record-breaking artist in numbers\". Newsround. March 2, 2020. Archived from the original on March 8, 2020. Retrieved April 20, 2020.\\n Sutherland, Mark (May 23, 2015). \"Taylor Swift interview: \\'A relationship? No one\\'s going to sign up for this\\'\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved April 20, 2020.\\n Scott, Walter (June 11, 2015). \"What Famous Pop Star Is Named After James Taylor?\". Parade. Archived from the original on October 15, 2016. Retrieved December 12, 2018.\\n \"Taylor Swift is not an \"underdog\": The real story about her 1 percent upbringing that the New York Times won\\'t tell you\". Salon.com. May 23, 2015. Archived from the original on May 25, 2022. Retrieved December 26, 2020.\\n Jepson 2013, p. 1.\\n Roth, Madeline (May 19, 2015). \"Taylor Swift\\'s Brother Had The Most Epic Graduation Weekend Ever\". MTV News. Archived from the original on July 23, 2016. Retrieved July 25, 2016.\\n McKay, Gabriel (July 6, 2023). \"Taylor Swift Edinburgh: Is star the real queen of Scotland?\". The Herald. Archived from the original on February 1, 2024. Retrieved February 4, 2024.\\n Eleftheriou-Smith, Loulla-Mae (June 24, 2015). \"Taylor Swift tells Scotland: \\'I am one of you\\'\". The Independent. Archived from the original on May 26, 2022. Retrieved July 10, 2019.\\n Vadala, Nick (July 14, 2017). \"Taylor Swift ancestor\\'s home added to Philly Register of Historic Places\". The Philadelphia Inquirer. Archived from the original on May 25, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift\\'s Great-Great-Grandfather\\'s Philly Home Gets Historic Landmark Status\". AP NEWS. July 25, 2017. Archived from the original on May 22, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift stammt aus dem Freistaat\" (in German). BR24. September 17, 2015. Archived from the original on December 31, 2021. Retrieved July 23, 2023.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Raab, Scott (October 20, 2014). \"Taylor Swift Interview\". Esquire. Archived from the original on February 16, 2015. Retrieved April 11, 2015.\\n \"Taylor Swift on Politicians Co-opting Faith: \\'I\\'m a Christian. That\\'s Not What We Stand For\\'\". Relevant. January 31, 2020. Archived from the original on November 25, 2022. Retrieved April 2, 2020.\\n Uhrich, Bill (February 13, 2010). \"Photos Students at Alvernia Montessori School sending Taylor Swift a valentine\". Reading Eagle. Archived from the original on October 16, 2013. Retrieved February 25, 2013.\\n Hatza, George (December 8, 2008). \"Taylor Swift: Growing into superstardom\". Reading Eagle. Archived from the original on April 1, 2012. Retrieved April 17, 2012.\\n Mennen, Lauren (November 12, 2014). \"Taylor Swift\\'s Wyomissing childhood home on the market for $799,500\". Philadelphia Daily News. Archived from the original on October 17, 2016. Retrieved October 13, 2016.\\n Chang, David (February 22, 2016). \"Taylor Swift Returns to Reading Pennsylvania as Maid of Honor in Friend\\'s Wedding\". WCAU. Archived from the original on September 16, 2016. Retrieved August 26, 2016.\\n \"Taylor Swift, Age 12\". New York Daily News. Archived from the original on August 27, 2016. Retrieved August 26, 2016.\\n Cooper, Brittany Joy (April 15, 2012). \"Taylor Swift Opens Up About a Future in Acting and Admiration for Emma Stone\". Taste of Country. Archived from the original on April 17, 2012. Retrieved April 17, 2012.\\n MacPherson, Alex (October 18, 2012). \"Taylor Swift: \\'I want to believe in pretty lies\\'\". The Guardian. Archived from the original on August 26, 2016. Retrieved August 3, 2016.\\n Rolling Stone Interview: The Unabridged Taylor Swift, December 2, 2008\\n Morris, Edward (December 1, 2006). \"When She Thinks \\'Tim McGraw\\', Taylor Swift Savors Payoff: Hardworking Teen to Open for George Strait Next Year\". CMT. Archived from the original on June 26, 2015. Retrieved March 11, 2010.\\n Diu, Nisha Lilia (April 3, 2011). \"Taylor Swift: \\'I won\\'t do sexy shoots\\'\". The Daily Telegraph. Archived from the original on May 6, 2013. Retrieved April 17, 2012.\\n \"News : CMT Insider Interview: Taylor Swift (Part 1 of 2)\". CMT. November 26, 2008. Archived from the original on January 23, 2015. Retrieved July 1, 2012.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. Archived from the original on May 10, 2012. Retrieved May 21, 2012.\\n Qureshi, Hira. \"Visit this Stone Harbor caf\\xc3\\xa9 where Taylor Swift was \\'always coming in to play\\' as a child\". Courier-Post. Archived from the original on October 26, 2023. Retrieved December 12, 2022.\\n Kuperinsky, Amy (July 28, 2020). \"Taylor Swift shouts out Jersey Shore town in video for surprise album\". NJ.com. Archived from the original on December 12, 2022. Retrieved December 12, 2022.\\n Martino, Andy (January 10, 2015). \"EXCLUSIVE: The real story of Taylor Swift\\'s guitar \\'legend\\'\". New York Daily News. Archived from the original on November 22, 2015. Retrieved August 28, 2017.\\n \"Dymtrow v. Swift et al: Federal Civil LawsuitNew York Southern District Court, Case No. 1:07-cv-11277-RJS\" (PDF). American Bar Association. Archived from the original (PDF) on October 11, 2012. Retrieved April 18, 2012.\\n \"On tour with Taylor Swift\". NBC News. May 31, 2009. Archived from the original on October 5, 2013. Retrieved July 1, 2012.\\n Castro, Vicky (February 6, 2015). \"How to Succeed as an Entrepreneur, Taylor Swift Style\". Inc. Archived from the original on June 7, 2016. Retrieved February 9, 2015.\\n Willman, Chris (July 25, 2007). \"Getting to know Taylor Swift\". Entertainment Weekly. Archived from the original on May 24, 2022. Retrieved January 25, 2022.\\n Jo, Nancy (January 2, 2014). \"Taylor Swift and the Growing of a Superstar: Her Men, Her Moods, Her Music\". Vanity Fair. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n \"News : Taylor Swift\\'s High School Names Auditorium in Her Honor\". CMT. September 23, 2010. Archived from the original on November 21, 2014. Retrieved April 18, 2012.\\n Grigoriadis, Vanessa (March 5, 2009). \"The Very Pink, Very Perfect Life of Taylor Swift\". Rolling Stone. Archived from the original on May 3, 2019. Retrieved July 28, 2019.\\n \"Taylor Swift receives her high school diploma\". Houston Chronicle. July 27, 2008. Archived from the original on January 1, 2024. Retrieved January 2, 2024.\\n \"Taylor Swift: The Garden In The Machine\". American Songwriter. May 2, 2011. Archived from the original on August 7, 2013. Retrieved May 21, 2012.\\n \"Songwriter Taylor Swift Signs Publishing Deal With Sony/ATV\". Broadcast Music, Inc. May 12, 2005. Archived from the original on December 4, 2012. Retrieved April 20, 2012.\\n Kosser, Michael (June 3, 2010). \"Liz Rose: Co-Writer to the Stars\". American Songwriter. Archived from the original on December 24, 2011. Retrieved April 19, 2012.\\n Leahey, Andrew (October 24, 2014). \"Songwriter Spotlight: Liz Rose\". Rolling Stone. Archived from the original on September 26, 2016. Retrieved September 24, 2016.\\n DeLuca, Dan (November 11, 2008). \"Focused on \\'great songs\\' Taylor Swift isn\\'t thinking about \\'the next level\\' or Joe Jon as gossip\". Philadelphia Daily News. p. 1. Archived from the original on November 18, 2012. Retrieved April 17, 2012.\\n Preston, John (April 26, 2009). \"Taylor Swift: the 19-year-old country music star conquering America \\xe2\\x80\\x93 and now Britain\". The Daily Telegraph. Archived from the original on January 5, 2012. Retrieved August 30, 2012.\\n Rosa, Christopher (March 24, 2015). \"Opening Acts Who Became Bigger Than The Headliner\". VH1. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n Rapkin, Mickey (July 27, 2017). \"Oral History of Nashville\\'s Bluebird Cafe: Taylor Swift, Maren Morris, Dierks Bentley & More on the Legendary Venue\". Billboard. Archived from the original on July 29, 2017. Retrieved July 28, 2017.\\n Hiatt, Brian (October 25, 2012). \"Taylor Swift in Wonderland\". Rolling Stone. Archived from the original on July 31, 2016. Retrieved August 1, 2016.\\n Greenburg, Zack O\\'Malley (June 26, 2013). \"Toby Keith, Cowboy Capitalist: Country\\'s $500 Million Man\". Forbes. Archived from the original on August 27, 2016. Retrieved August 1, 2016.\\n Taylor Swift (CD). Big Machine Records. 2006. BMR120702.\\n Tamarkin, Jeff. \"Taylor Swift \\xe2\\x80\\x93 Taylor Swift\". AllMusic. Archived from the original on October 20, 2015. Retrieved February 14, 2021.\\n Neal, Chris (December 4, 2006). \"Taylor Swift Review\". Country Weekly. Archived from the original on July 22, 2012. Retrieved March 31, 2010.\\n Trust, Gary (October 29, 2009). \"Chart Beat Thursday: Taylor Swift, Tim McGraw Linked Again\". Billboard. Archived from the original on March 7, 2013. Retrieved November 8, 2016.\\n \"Taylor Swift\". Songwriters\\' Hall of Fame. Archived from the original on February 12, 2021. Retrieved September 21, 2022.\\n Willman, Chris (February 5, 2008). \"Taylor Swift\\'s Road to Fame\". Entertainment Weekly. p. 3. Archived from the original on February 21, 2015. Retrieved April 22, 2012.\\n \"Taylor Swift Joins Rascal Flatts Tour\". CMT. October 18, 2006. Archived from the original on January 7, 2015. Retrieved March 11, 2010.\\n Whitaker, Sterling; Hammar, Ania (May 27, 2019). \"How Eric Church\\'s Rascal Flatts Feud Helped Launch Taylor Swift\\'s Career\". Taste of Country. Townsquare Media. Archived from the original on June 6, 2019. Retrieved June 10, 2019.\\n \"Taylor Swift No. 1 on iTunes\". Great American Country. December 19, 2007. Archived from the original on March 3, 2012. Retrieved July 5, 2010.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart history\". Billboard. Archived from the original on August 9, 2016. Retrieved July 26, 2016.\\n \"Taylor Swift owns top of country chart\". Country Standard Time. July 23, 2008. Archived from the original on July 31, 2008. Retrieved December 26, 2008.\\n \"Wal-Mart \"Eyes\" New Taylor Swift Project\". Great American Country. Archived from the original on July 23, 2008. Retrieved July 24, 2008.\\n \"Taylor Swift Joins George Strait\\'s 2007 Tour\". CMT. November 17, 2006. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Brad Paisley Plans Tour With Three Opening Acts\". CMT. January 9, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Joins Tim McGraw, Faith Hill on Tour\". CMT. June 1, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Youngest Winner of Songwriter/Artist Award\". Great American Country. October 16, 2007. Archived from the original on January 11, 2015. Retrieved February 2, 2015.\\n \"Photos : All Taylor Swift Pictures : Horizon Award Winner Poses in the Pressroom\". CMT. September 7, 2007. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Photos : 43rd Annual ACM Awards \\xe2\\x80\\x93 Onstage: Winners : Acceptance Speech\". CMT. May 18, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Taylor Swift, Rascal Flatts, Carrie Underwood Score at 2008 AMA Awards\" (Blog). Roughstock.com. November 24, 2008. Archived from the original on July 10, 2014. Retrieved May 21, 2012.\\n \"Amy Winehouse Wins Best New Artist, Kanye West Pays Tribute to Mom \\xe2\\x80\\x93 Grammy Awards 2008, Grammy Awards\". People. October 2, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Rascal Flatts Announce Summer Tour With Taylor Swift\". CMT. May 5, 2008. Archived from the original on April 18, 2022. Retrieved June 5, 2019.\\n Caplan, David (September 8, 2008). \"Scoop\". People. Archived from the original on February 2, 2016. Retrieved March 6, 2012.\\n Rizzo, Monica (November 24, 2008). \"Scoop \\xe2\\x80\\x93 Couples, Camilla Belle, Joe Jonas\". People. Archived from the original on March 3, 2016. Retrieved March 6, 2012.\\n Akers, Shelley (June 9, 2008). \"Taylor Swift to Appear in Hannah Montana Movie\". People. Archived from the original on October 27, 2017. Retrieved October 27, 2017.\\n \"Hannah Montana: The Movie (Original Motion Picture Soundtrack) by Hannah Montana\". iTunes Store. January 2009. Archived from the original on May 2, 2016. Retrieved August 2, 2016.\\n \"CD Taylor Swift \\xe2\\x80\\x93 Fearless\" (in Portuguese). Universal Music Group. Archived from the original on January 18, 2021. Retrieved February 14, 2021.\\n Raphael, Amy (February 1, 2009). \"First, she conquered Nashville. Now she\\'s set for world domination\". The Observer. ProQuest 250507223. Archived from the original on December 9, 2022. Retrieved December 9, 2022.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong with Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved July 24, 2014.\\n \"Discography Taylor Swift\". ARIA Charts. Archived from the original on March 21, 2012. Retrieved January 2, 2010.\\n Trust, Gary (December 15, 2009). \"Best of 2009: Part 1\". Billboard. Archived from the original on March 3, 2013. Retrieved September 20, 2022.\\n Ben-Yehuda, Ayala (August 13, 2009). \"Black Eyed Peas, Jason Mraz Tie Records on Billboard Hot 100\". Billboard. Archived from the original on May 8, 2013. Retrieved March 13, 2010.\\n Trust, Gary (September 24, 2009). \"Taylor Swift Climbs Hot 100, Black Eyed Peas Still No. 1\". Billboard. Archived from the original on February 1, 2013. Retrieved September 20, 2022.\\n \"Taylor Swift Chart History (Hot Country Songs)\". Billboard. Archived from the original on January 31, 2021. Retrieved February 14, 2021.\\n Grein, Paul (March 16, 2012). \"Chart Watch Extra: Top Albums Of Last 10 Years\" (Blog). Yahoo! Music. Archived from the original on April 2, 2015. Retrieved June 10, 2011.\\n Mapes, Jillian (November 23, 2010). \"Taylor Swift Announces \\'Speak Now\\' World Tour\". Billboard. Archived from the original on May 8, 2013. Retrieved May 15, 2012.\\n Weiss, Dan (December 12, 2011). \"Taylor Swift: Journey To Fearless DVD\". American Songwriter. Archived from the original on August 15, 2016. Retrieved August 2, 2016.\\n Ryan, Sarah (August 10, 2009). \"Taylor Swift Pranks Keith Urban\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved November 11, 2015.\\n \"Kanye calls Taylor Swift after \\'View\\' appearance\". MSNBC. September 15, 2009. Archived from the original on October 6, 2013. Retrieved September 16, 2009.\\n \"Taylor Swift Thanks \"Gracious\" Beyonce for Inviting Her Onstage After Kanye Stunt at VMAs\". Rolling Stone. September 14, 2009. Archived from the original on June 15, 2012. Retrieved May 15, 2012.\\n Anderson, Kyle (September 16, 2009). \"Kanye West\\'s VMA Interruption Gives Birth To Internet Photo Meme\". MTV. Archived from the original on January 16, 2016. Retrieved October 3, 2009.\\n Ditzian, Eric (2009). \"Taylor Swift, Michael Jackson Big Winners at American Music Awards\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"2009 Artists of the Year\". Billboard. December 10, 2009. Archived from the original on January 6, 2010. Retrieved May 21, 2012.\\n \"Taylor Swift Raps \\'Thug Story\\' With T-Pain On CMT Awards\". MTV. June 17, 2009. Archived from the original on May 25, 2022. Retrieved November 11, 2015.\\n Kreps, Daniel (February 1, 2010). \"Beyonce, Taylor Swift Dominate 2010 Grammy Awards\". Rolling Stone. Archived from the original on February 15, 2012. Retrieved February 13, 2012.\\n \"Billie Eilish replaces Taylor Swift as youngest artist to win a Grammy for Album of the Year\". MSN. January 27, 2020. Archived from the original on November 20, 2020. Retrieved September 1, 2020.\\n Kaufman, Gil (November 12, 2009). \"Taylor Swift Dominates CMA Awards\". MTV News. Archived from the original on March 6, 2016. Retrieved September 13, 2016.\\n Vena, Jocelyn (November 6, 2009). \"John Mayer Talks Taylor Swift Collaboration \\'Half of My Heart\\'\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"Boys Like Girls featuring Taylor Swift, \\'Two Is Better Than One\\'\". Billboard. December 2, 2009. Archived from the original on August 18, 2021. Retrieved December 4, 2020.\\n \"Kellie Pickler Has Her \\'Best Days\\' Thanks To Taylor Swift\". MTV. Archived from the original on January 1, 2016. Retrieved November 11, 2015.\\n Vena, Jocelyn (December 28, 2009). \"New Taylor Swift Song Included In \\'Valentine\\'s Day\\' Featurette\". MTV. Archived from the original on January 16, 2016. Retrieved November 11, 2015.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart history on Canadian Hot 100\". Billboard. Archived from the original on August 9, 2016. Retrieved August 3, 2016.\\n Park, Michael Y.; Sia, Nicole (December 29, 2009). \"Taylor & Taylor Romance Was Overblown, Says Source\". People. Archived from the original on November 13, 2012. Retrieved March 6, 2012.\\n Caramanica, Jon (March 6, 2009). \"OMG! Taylor Swift Does \\'CSI\\'!\". The New York Times (Blog). Archived from the original on August 14, 2011. Retrieved May 7, 2012.\\n Strecker, Erin (January 2, 2015). \"Remember When Taylor Swift Shined as \\'Saturday Night Live\\' Host?\". Billboard. Archived from the original on January 24, 2015. Retrieved January 15, 2015.\\n Dukes, Billy (October 22, 2012). \"10 Things You Didn\\'t Know About Taylor Swift\". Taste of Country. Townsquare Media. Archived from the original on May 22, 2022. Retrieved July 26, 2020.\\n Pietroluongo, Silvio (August 11, 2010). \"Taylor Swift Makes Sparkling Hot 100 Entrance\". Billboard. Archived from the original on January 31, 2016. Retrieved July 25, 2016.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift, Angry on \\'Speak Now\\'\". The New York Times. Archived from the original on October 21, 2010. Retrieved October 23, 2010.\\n \"Taylor Swift\\'s New Album, Speak Now, Set for Oct. 25 Release\". CMT. July 20, 2010. Archived from the original on June 21, 2019. Retrieved February 14, 2020.\\n Kaufman, Gil (November 3, 2010). \"Taylor Swift\\'s Speak Now Tops 1 Million in First Week\". MTV. Archived from the original on August 10, 2016. Retrieved August 8, 2016.\\n \"Fastest-selling digital album in the US by a female artist\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Knopper, Steve (November 25, 2010). \"Taylor Swift\\'s Speak Now Tops the Charts\". Rolling Stone. Archived from the original on March 3, 2021. Retrieved November 25, 2010.\\n Sheffield, Rob (October 26, 2010). \"Speak Now (2010)\". Rolling Stone. Archived from the original on September 11, 2013. Retrieved December 20, 2019.\\n Hammel, Sara (January 4, 2011). \"Taylor Swift & Jake Gyllenhaal Break Up: Source\". People. Archived from the original on May 9, 2012. Retrieved March 6, 2012.\\n Wyland, Sarah (February 12, 2012). \"Taylor Swift Takes Home Two GRAMMYs at Tribute-Filled Show\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved February 13, 2012.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on January 16, 2016. Retrieved November 21, 2015.\\n Smith, Hazel (October 24, 2011). \"News : Hot Dish: Taylor Swift Sings Alan Jackson\\'s Masterpiece at Nashville Songwriters Celebration\". CMT. Archived from the original on November 29, 2014. Retrieved April 22, 2012.\\n Roland, Tom (December 2, 2011). \"Taylor Swift: Billboard\\'s Woman of the Year\". Billboard. Archived from the original on February 3, 2013. Retrieved May 15, 2012.\\n Talbott, Chris; Silva, Cristina (April 2, 2012). \"Taylor Swift wins ACM entertainer of the year\". Yahoo!. Associated Press. Archived from the original on August 23, 2016. Retrieved May 15, 2012.\\n \"CMA Awards 2011: Taylor Swift wins entertainer of the year\". CBS News. November 9, 2011. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n Kellogg, Jane (November 20, 2011). \"AMAs 2011: Winners and Nominees Complete List\". The Hollywood Reporter. Archived from the original on June 27, 2015. Retrieved November 21, 2015.\\n Sheffield, Rob (June 23, 2012). \"Women Who Rock: The 50 Greatest Albums of All Time\". Rolling Stone. Archived from the original on December 10, 2016. Retrieved July 15, 2017.\\n Allen, Bob (March 29, 2012). \"Hot Tours: Taylor Swift, George Strait, Cirque Du Soleil\". Billboard. Archived from the original on February 21, 2013. Retrieved May 10, 2012.\\n \"Taylor Swift News and Blog\". taylorswift.com. September 21, 2011. Archived from the original on October 11, 2011. Retrieved September 21, 2011.\\n Herrera, Monica (March 15, 2012). \"Taylor Swift, Arcade Fire Talk \\'Hunger Games\\'\". Rolling Stone. Archived from the original on June 27, 2015. Retrieved May 10, 2012.\\n \"Nominations 2013 \\xe2\\x80\\x94 Golden Globe Awards\". goldenglobes.org. December 13, 2012. Archived from the original on December 14, 2012. Retrieved December 13, 2012.\\n Horowitz, Steven J. (April 20, 2012). \"B.o.B Explains Origins of Taylor Swift Collaboration \\'Both of Us\\'\". HipHopDX. Archived from the original on September 10, 2015. Retrieved May 15, 2012.\\n Toomedy, Alyssa (October 25, 2012). \"Taylor Swift and Conor Kennedy Breakup: Anatomy of a Split\". E! News. Archived from the original on December 4, 2015. Retrieved November 10, 2015.\\n Trust, Gary (August 22, 2012). \"Taylor Swift Scores First Hot 100 No. 1\". Billboard. Archived from the original on February 13, 2013. Retrieved August 22, 2012.\\n \"Discography Taylor Swift\". New Zealand Charts. Archived from the original on April 20, 2017. Retrieved July 26, 2016.\\n Lynch, Kevin (September 4, 2013). \"Calvin Harris trumps Michael Jackson feat to join Taylor Swift, Rihanna and One Direction in Guinness World Records\\xe2\\x84\\xa2 2014 book\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift \\xe2\\x80\\x93 I Knew You Were Trouble\". ARIA Charts. Archived from the original on May 27, 2016. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Official Singles Charts Top 100\". Official Charts Company. Archived from the original on July 13, 2021. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift Leads Record Breaking Digital Sales Week\". Billboard. January 3, 2013. Archived from the original on April 8, 2017. Retrieved February 14, 2021.\\n Lewis, Randy (October 30, 2012). \"Taylor Swift raises the bar with a savvy \\'Red\\' marketing campaign\". Los Angeles Times. Archived from the original on December 28, 2020. Retrieved December 28, 2020.\\n Mansfield, Brian (October 17, 2012). \"Taylor Swift sees \\'Red\\' all over\". USA Today. Archived from the original on December 21, 2012.\\n English, J. (August 28, 2017). \"Taylor Swift\\'s \\'Red\\': A Canonical Coming-Of-Age Album\". NPR. Archived from the original on April 12, 2021. Retrieved February 14, 2021.\\n Roberts, Randall (October 31, 2012). \"\\'The Last Time\\' connects Taylor Swift with Arcade Fire\". Los Angeles Times. Archived from the original on March 6, 2016. Retrieved December 27, 2020.\\n Greenwald, David (September 6, 2013). \"Taylor Swift, Rihanna, Justin Bieber Among 2014 Guinness Record-Setters\". Billboard. Archived from the original on June 8, 2015. Retrieved July 27, 2016.\\n Sexton, Paul (August 31, 2019). \"Taylor Swift Scores Fourth U.K. No. 1 With \\'Lover\\' Album\". Billboard. Archived from the original on August 16, 2021. Retrieved October 15, 2019.\\n \"Grammys 2014: The complete list of nominees and winners\". Los Angeles Times. January 26, 2014. Archived from the original on March 4, 2015. Retrieved January 25, 2015.\\n Gregoire, Carolyn (November 19, 2012). \"Taylor Swift AMA Awards 2012: Pop Star Performs \\'I Knew You Were Trouble\\' (Video)\". HuffPost. Archived from the original on May 26, 2013. Retrieved June 10, 2013.\\n Payne, Chris (November 25, 2013). \"Taylor Swift & Justin Timberlake Win Big at American Music Awards\". Billboard. Archived from the original on November 24, 2015. Retrieved November 21, 2015.\\n \"NSAI Songwriter/Artists of the Year\". Nashville Songwriters Association International. Archived from the original on August 19, 2016. Retrieved August 2, 2016.\\n Allen, Bob (July 3, 2014). \"Taylor Swift\\'s Red Wraps as All-Time Country Tour\". Billboard. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Caramanica, Jon (November 7, 2013). \"Country Awards Hold Swift Close\". The New York Times. Archived from the original on November 11, 2013. Retrieved April 3, 2014.\\n \"Taylor Swift\\'s Boyfriend Timeline: 10 Relationships & Their Songs\". Billboard. December 30, 2014. Archived from the original on August 26, 2016. Retrieved August 26, 2016.\\n Labrecque, Jeff (December 12, 2013). \"\\'12 Years a Slave\\' and \\'American Hustle\\' lead Golden Globe nominees\". Entertainment Weekly. Archived from the original on March 23, 2015. Retrieved December 12, 2013.\\n Bonaguro, Alison (January 25, 2013). \"News : Offstage: Tim McGraw Wanted to Make Taylor Swift Duet an Event\". CMT. Archived from the original on July 12, 2015. Retrieved February 25, 2013.\\n Blistein, Doyle (June 4, 2013). \"Taylor Swift Joins Rolling Stones for \\'As Tears Go By\\'\". Rolling Stone. Archived from the original on June 9, 2013. Retrieved June 10, 2013.\\n \"Taylor Swift Joins Florida Georgia Line Onstage for \\'Cruise\\'\". Taste of Country. Townsquare Media. March 2, 2013. Archived from the original on March 16, 2013. Retrieved March 29, 2013.\\n Collin, Robbie (July 26, 2012). \"The Lorax, review\". The Daily Telegraph. Archived from the original on September 17, 2016. Retrieved August 3, 2016.\\n Beard, Lanford (May 1, 2013). \"Taylor Swift says \\'I do\\' to \\'New Girl\\'\". Entertainment Weekly. Archived from the original on November 8, 2016. Retrieved August 4, 2016.\\n Busis, Hillary (September 27, 2013). \"Taylor Swift will co-star in long-awaited adaptation of \\'The Giver\\'\". Entertainment Weekly. Archived from the original on December 21, 2016. Retrieved August 2, 2016.\\n Peterson, Price (March 31, 2014). \"Taylor Swift Moves into NYC Apartment Built Over Mysterious River of Pink Slime\". The Atlantic. Archived from the original on September 27, 2016. Retrieved July 31, 2016.\\n Rogers, Alex (March 7, 2014). \"Why Taylor Swift Thinks Nashville Is the Best Place on Earth\". Time. Archived from the original on May 24, 2022. Retrieved September 27, 2016.\\n Milzoff, Rebecca; Brown, Eric Renner; Denis, Kyle (August 24, 2023). \"Taylor Swift and Beyonc\\xc3\\xa9 Are So Big, Even Their Publicists Have Fans\". Billboard. Archived from the original on August 24, 2023. Retrieved August 24, 2023.\\n Zollo, Paul (February 17, 2016). \"The Oral History of Taylor Swift\\'s 1989\". The Recording Academy. Archived from the original on June 3, 2021. Retrieved February 14, 2021.\\n Stutz, Colin (October 16, 2014). \"Watch Taylor Swift\\'s \\'1989\\' Secret Sessions Behind The Scenes Video\". Billboard. Archived from the original on June 5, 2016. Retrieved August 2, 2016.\\n Caulfield, Keith (November 4, 2014). \"Taylor Swift\\'s \"1989\" debuts with 1.287 million copies sold\". Billboard. Archived from the original on November 5, 2014. Retrieved November 4, 2014.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Discography Taylor Swift\". ARIA Charts. Archived from the original on August 26, 2019. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift Chart History (Canadian Hot 100)\". Billboard. Archived from the original on August 9, 2016. Retrieved February 14, 2021.\\n \\xe2\\x80\\xa2 \"Taylor Swift\\'s \\'Bad Blood\\' Blasts to No. 1 on Hot 100\". Billboard. May 28, 2015. Archived from the original on September 6, 2015. Retrieved May 28, 2015.\\n \"Taylor Swift \\xe2\\x80\\x93 Chart History: Hot 100\". Billboard. Archived from the original on March 8, 2018. Retrieved September 18, 2016.\\n \"Live Music\\'s $20 Billion Year: The Grateful Dead\\'s Fare Thee Well Reunion, Taylor Swift, One Direction Top Boxscore\\'s Year-End\". Billboard. Archived from the original on December 14, 2015.\\n Weissmann, Jordan (July 7, 2014). \"Taylor Swift Has Written an Op-Ed in the Wall Street Journal\". Slate (Blog). Archived from the original on January 23, 2015. Retrieved January 23, 2015.\\n Knopper, Steve (November 8, 2014). \"Taylor Swift\\'s Label Head Explains Spotify Removal\". Rolling Stone. Archived from the original on April 21, 2015. Retrieved April 11, 2015.\\n Peters, Mitchell (June 21, 2015). \"Taylor Swift Pens Open Letter Explaining Why \\'1989\\' Won\\'t Be on Apple Music\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Halperin, Shirley (June 21, 2015). \"Apple Changes Course After Taylor Swift Open Letter: Will Pay Labels During Free Trial\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Rosen, Christopher (June 25, 2015). \"Taylor Swift is putting 1989 on Apple Music\". Entertainment Weekly. Archived from the original on June 26, 2015. Retrieved June 25, 2015.\\n \"Taylor Swift returns to Spotify on the day Katy Perry\\'s album comes out\". BBC News. June 9, 2017. Archived from the original on June 9, 2017.\\n \"Taylor Swift: 2014 Billboard Woman of the Year\". Billboard. October 10, 2014. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Payne, Chris (November 23, 2014). \"Taylor Swift Wins Dick Clark Award of Excellence at 2014, Presented by Diana Ross\". Billboard. Archived from the original on July 4, 2015. Retrieved April 11, 2015.\\n \"The Taylor Swift Experience\". GRAMMY Museum. Archived from the original on November 25, 2022. Retrieved April 22, 2022.\\n Boehrer, Kat (January 7, 2016). \"Watch Taylor Swift\\'s Stunning Acoustic Performance of \\'Blank Space\\' at the Grammy Museum\". Complex. Archived from the original on April 22, 2022. Retrieved April 22, 2022.\\n Jonze, Tim (February 25, 2015). \"Taylor Swift wins international female solo artist at Brit awards 2015\". The Guardian. Archived from the original on January 1, 2016. Retrieved April 11, 2015.\\n Stutz, Colin (July 21, 2015). \"2015 MTV Video Music Awards Nominees Revealed: Taylor Swift, Kendrick Lamar, Ed Sheeran & More\". Billboard. Archived from the original on July 24, 2015. Retrieved August 15, 2016.\\n Lynch, Joe (February 19, 2016). \"Taylor Swift Joins Elite Club to Win Grammy Album of the Year More Than Once: See the Rest\". Billboard. Archived from the original on March 1, 2016. Retrieved July 31, 2016.\\n Chiu, Melody (June 1, 2016). \"Taylor Swift and Calvin Harris Split After 15 Months Together\". People. Archived from the original on June 22, 2016. Retrieved June 1, 2016.\\n Spanos, Brittany (July 13, 2016). \"Taylor Swift Co-Wrote Calvin Harris\\' Smash Hit \\'This Is What You Came For\\'\". Rolling Stone. Archived from the original on July 30, 2016. Retrieved July 31, 2016.\\n Grady, Constance (August 26, 2019). \"How the Taylor Swift-Kanye West VMAs scandal became a perfect American morality tale\". Vox. Archived from the original on December 2, 2022. Retrieved October 12, 2023.\\n Grady, Constance (March 21, 2020). \"Newly leaked footage shows Taylor Swift and Kanye West talking \"Famous\"\". Vox. Archived from the original on March 24, 2022. Retrieved October 12, 2023.\\n Lewis, Anna (July 15, 2016). \"Tom Hiddleston finally tells us the truth about his relationship with Taylor Swift\". Cosmopolitan. Archived from the original on July 17, 2016. Retrieved January 8, 2019.\\n \"Taylor Swift Finally Reveals When She Started Dating Joe Alwyn in Lover Album\". Yahoo!. August 23, 2019. Archived from the original on June 11, 2020. Retrieved June 11, 2020.\\n \"Paul McCartney & Taylor Swift\". Rolling Stone. November 13, 2020. Archived from the original on November 30, 2020. Retrieved September 15, 2021. McCartney: So how does that go? Does your partner sympathize with that and understand? Swift: Oh, absolutely.\\n Irvin, Jack (April 10, 2023). \"Inside Taylor Swift and Joe Alwyn\\'s \\'Differences\\' That Led to Their Breakup: Sources (Exclusive)\". People. Archived from the original on September 25, 2023. Retrieved September 25, 2023.\\n \"Taylor Swift, pop princess, wins song of the year at the CMA Awards\". USA Today. Archived from the original on November 9, 2017. Retrieved November 9, 2017.\\n Trust, Gary (February 21, 2017). \"Ed Sheeran Tops Hot 100, Katy Perry Debuts at No. 4 & Bruno Mars, Rihanna & The Weeknd All Hit Top 10\". Billboard. Archived from the original on February 22, 2017. Retrieved February 22, 2017.\\n Grady, Constance (August 11, 2017). \"Taylor Swift won her day in court. Here\\'s what you need to know\". Vox. Archived from the original on October 17, 2022. Retrieved October 4, 2023.\\n \"Taylor Swift wipes social media profiles, fuelling expectations of new album\". The Daily Telegraph. August 18, 2017. Archived from the original on January 8, 2019. Retrieved August 19, 2019.\\n Aswad, Jem (August 24, 2017). \"Taylor Swift\\'s New Single, \\'Look What You Made Me Do,\\' Arrives (Listen)\". Variety. Archived from the original on August 28, 2017. Retrieved August 29, 2017.\\n White, Jack (September 1, 2017). \"Taylor Swift scores first Number 1 on the Official Singles Chart with \\'LWYMMD\\'\". Official Charts Company. Archived from the original on September 2, 2017. Retrieved September 1, 2017.\\n Peak positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift Scores Fifth No. 1 Single\". Australian Recording Industry Association. September 2, 2017. Archived from the original on September 2, 2017. Retrieved September 2, 2017.\\n \\xe2\\x80\\xa2 \"IRMA \\xe2\\x80\\x93 Irish Charts\". Irish Recorded Music Association. Archived from the original on June 14, 2017. Retrieved September 2, 2017.\\n \\xe2\\x80\\xa2 \"NZ Top 40 Singles Chart\". Recorded Music NZ. September 4, 2017. Archived from the original on September 1, 2017. Retrieved September 1, 2017.\\n \\xe2\\x80\\xa2 \"Taylor Swift at Nos. 1 & 4 on Billboard Hot 100, as Cardi B Moves Up to No. 2\". Billboard. September 11, 2017. Archived from the original on September 21, 2017. Retrieved October 30, 2017.\\n Shaw, Lucas (November 7, 2017). \"Taylor Swift Will Keep New Album From Streaming for a Week\". Bloomberg. Bloomberg News. Archived from the original on November 8, 2017. Retrieved November 7, 2017.\\n Powers, Ann (November 10, 2019). \"The Old Taylor\\'s Not Dead\". NPR. Archived from the original on June 9, 2020. Retrieved June 29, 2020.\\n McDermott, Maeve (October 11, 2017). \"Taylor Swift \\'Reputation\\': Here\\'s what critics are saying\". USA Today. Archived from the original on March 4, 2020. Retrieved October 15, 2020.\\n \"Official: Taylor Swift\\'s \\'Reputation\\' Album Sells 1.2M Copies in US During First Week\". Billboard. Archived from the original on November 30, 2017. Retrieved May 29, 2018.\\n Chart positions:\\n \\xe2\\x80\\xa2 \"Taylor Swift\\'s \\'Reputation\\' Rules Australia\\'s Albums Chart\". Billboard. November 20, 2017. Archived from the original on November 20, 2017. Retrieved December 2, 2017.\\n \\xe2\\x80\\xa2 \"Taylor Swift Chart History\". Billboard. Archived from the original on November 22, 2021. Retrieved November 19, 2021.\\n Unterberger, Andrew (July 6, 2018). \"Taylor Swift\\'s \\'Delicate\\' Became the Biggest Reputation Radio Hit While You Weren\\'t Looking\". Billboard. Archived from the original on December 13, 2020. Retrieved April 13, 2020.\\n \"61st Grammy Nominees\". The Recording Academy. December 7, 2018. Archived from the original on December 7, 2018. Retrieved December 7, 2018.\\n Hudak, Joseph (April 12, 2018). \"Sugarland Announce New Album Bigger, Taylor Swift Collaboration\". Rolling Stone. Archived from the original on April 14, 2018. Retrieved April 13, 2018.\\n Havens, Lyndsey (October 9, 2018). \"Taylor Swift Breaks an All-Time AMA Record \\xe2\\x80\\x93 And Urges People to Vote in Midterm Elections\". Billboard. Archived from the original on October 10, 2018. Retrieved October 10, 2018.\\n Stubblebine, Allison (November 13, 2017). \"Taylor Swift Announces First Round of Reputation Stadium Tour Dates\". Billboard. Archived from the original on November 14, 2017. Retrieved November 18, 2017.\\n Frankenberg, Eric (December 6, 2018). \"Taylor Swift Closes Reputation Stadium Tour with $345 Million\". Billboard. Archived from the original on December 9, 2018. Retrieved December 22, 2018.\\n Wang, Amy X. (November 19, 2018). \"Taylor Swift\\'s New Record Deal Affects Thousands of Other Musicians\". Rolling Stone. Archived from the original on November 26, 2018. Retrieved November 26, 2018.\\nWillman, Chris (August 27, 2018). \"Taylor Swift Stands to Make Music Business History as a Free Agent\". Variety. Archived from the original on August 29, 2018. Retrieved August 29, 2018.\\nAswad, Jem; Willman, Chris (November 19, 2018). \"Taylor Swift Signs New Deal With Universal Music Group\". Variety. Archived from the original on November 19, 2018. Retrieved November 19, 2018.\\n Grady, Constance (November 19, 2018). \"What Taylor Swift\\'s new record deal means for the music industry \\xe2\\x80\\x94 and for her image\". Vox. Archived from the original on December 20, 2021. Retrieved December 20, 2021.\\n McKenna, Lyndsey (August 23, 2019). \"Stream Taylor Swift\\'s New Album, \\'Lover\\'\". NPR. Archived from the original on February 19, 2020. Retrieved September 10, 2019.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Archived from the original on August 23, 2019. Retrieved February 15, 2021.\\n Caulfield, Keith (September 1, 2019). \"Official: Taylor Swift\\'s \\'Lover\\' Debuts at No. 1 on Billboard 200 Chart With 867,000 Units Earned in First Week in U.S.\" Billboard. Archived from the original on September 1, 2019. Retrieved September 2, 2019.\\n White, Adam (August 23, 2019). \"Taylor Swift Lover Review Round-Up: Critics Say Album Feels \\'Evolutionary Rather Than Revolutionary\\'\". The Independent. Archived from the original on June 13, 2022. Retrieved June 13, 2022.\\n Moniuszko, Sara M. (August 23, 2019). \"Taylor Swift Lover Reviews: Critics Are Enamored by the \\'Earnest,\\' \\'Romantic\\' New Album\". USA Today. Archived from the original on August 23, 2019. Retrieved November 5, 2020.\\n Trust, Gary (May 6, 2019). \"Lil Nas X\\'s \\'Old Town Road\\' Tops Billboard Hot 100 For Fifth Week, Taylor Swift\\'s \\'Me!\\' Vaults to No. 2\". Billboard. Archived from the original on May 6, 2019. Retrieved June 14, 2019.\\n Trust, Gary (October 23, 2023). \"Taylor Swift\\'s \\'Cruel Summer\\' Hits No. 1 on Billboard Hot 100, Becoming Her 10th Leader\". Billboard. Archived from the original on October 24, 2023. Retrieved October 24, 2023.\\n \"Arashi Best-Of Tops Taylor Swift for IFPI\\'s Best-Selling Album of 2019\". Billboard. March 19, 2020. Archived from the original on March 19, 2020. Retrieved March 21, 2020.\\n \"2020 Grammy Awards: Complete Winners List\". The Recording Academy. November 20, 2019. Archived from the original on May 22, 2020. Retrieved February 15, 2021.\\n Grein, Paul (August 26, 2019). \"12 Records That Were Set at the 2019 VMAs\". Billboard. Archived from the original on January 30, 2020. Retrieved January 11, 2020.\\n Grady, Constance (September 1, 2019). \"The Taylor Swift/Scooter Braun controversy, explained\". Vox. Archived from the original on February 11, 2020. Retrieved August 23, 2019.\\n Beth, John (January 2, 2024). \"Taylor Swift\\'s Chart Triumph\". Square News.\\n \"The Taylor Swift, Scooter Braun, Justin Bieber row explained\". BBC News. July 1, 2019. Archived from the original on December 11, 2021. Retrieved July 18, 2021.\\n Willman, Chris (November 16, 2020). \"Taylor Swift Confirms Sale of Her Masters, Says She Is Already Re-Recording Her Catalog\". Variety. Archived from the original on December 3, 2022. Retrieved November 18, 2020.\\n Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases \\'Beautiful Ghosts,\\' Co-Written With Andrew Lloyd Webber for \\'Cats\\' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.\\n \"Golden Globes 2020: full list of nominations\". The Guardian. December 9, 2019. Archived from the original on December 10, 2019. Retrieved December 20, 2019.\\n Rooney, David (December 18, 2019). \"\\'Cats\\': Film Review\". The Hollywood Reporter. Archived from the original on December 20, 2019. Retrieved December 21, 2019.\\n Mamo, Heran (January 15, 2020). \"Taylor Swift Miss Americana Netflix Doc Has a Release Date & We\\'re So Ready for It\". Billboard. Archived from the original on April 24, 2020. Retrieved January 19, 2020.\\n Willman, Chris (February 6, 2020). \"Taylor Swift Moves to Universal Music Publishing Group with New Pact\". Variety. Archived from the original on February 12, 2020. Retrieved February 6, 2020.\\n Opperman, Jeff (March 12, 2021). \"Taylor Swift Is Singing Us Back to Nature\". The New York Times. Archived from the original on December 28, 2021. Retrieved May 24, 2021.\\n \"Taylor Swift to release surprise ninth album \\'Evermore\\' tonight\". NME. December 10, 2020. Archived from the original on December 10, 2020. Retrieved December 10, 2020.\\n Atkinson, Katie (December 15, 2020). \"Taylor Swift Isn\\'t So Sure She & Joe Alwyn Would Have Made Music Together If It Weren\\'t for Lockdown\". Billboard. Archived from the original on August 11, 2021. Retrieved February 18, 2021.\\n Schaffer, Claire (December 18, 2020). \"Aaron Dessner on How His Collaborative Chemistry With Taylor Swift Led to Evermore\". Rolling Stone. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Barna, Alyssa (December 16, 2020). \"These are the musicological reasons Taylor Swift\\'s new album sounds dull\". The Washington Post. Archived from the original on February 25, 2021. Retrieved November 3, 2021.\\n Snapes, Laura (October 14, 2022). \"\\'Genuine\\': why Taylor Swift can celebrate more than an album release\". The Guardian. Archived from the original on October 14, 2022. Retrieved October 14, 2022.\\n McGrath 2023, p. 79; Fogarty & Arnold 2021, p. 5.\\n Trust, Gary (January 28, 2021). \"Taylor Swift\\'s \\'Coney Island\\' and \\'No Body, No Crime\\' Debut on Airplay Charts, Joining \\'Willow\\'\". Billboard. Archived from the original on August 16, 2021. Retrieved February 2, 2021.\\n Willman, Chris (March 14, 2021). \"Taylor Swift Becomes First Woman to Win Album of the Year Grammy Three Times\". Variety. Archived from the original on December 2, 2021. Retrieved March 15, 2021.\\n Caulfield, Keith (January 7, 2021). \"Lil Baby\\'s My Turn Is MRC Data\\'s Top Album of 2020, Roddy Ricch\\'s \\'The Box\\' Most-Streamed Song\". Billboard. Archived from the original on January 7, 2021. Retrieved January 7, 2021.\\n Trust, Gary (December 21, 2020). \"Taylor Swift\\'s \\'Willow\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Willman, Chris (November 23, 2020). \"Taylor Swift Wins Three American Music Awards, Says She\\'s MIA Because of \\'Recording All of My Old Music\\'\". Variety. Archived from the original on December 2, 2021. Retrieved November 25, 2020.\\n Christman, Ed (July 19, 2021). \"Billboard\\'s U.S. Money Makers: The Top Paid Musicians of 2020\". Billboard. Archived from the original on July 24, 2021. Retrieved July 19, 2021.\\n \\xe2\\x80\\xa2 Christman, Ed (July 19, 2021). \"Billboard\\'s 2020 Global Money Makers: The 5 Top Highest Paid Musicians\". Billboard. Archived from the original on July 23, 2021. Retrieved July 19, 2021.\\n Caulfield, Keith (July 11, 2023). \"Taylor Swift\\'s Re-Recorded Speak Now Already Has 2023\\'s Biggest Week After 4 Days of Release\". Billboard. Archived from the original on July 11, 2023. Retrieved July 11, 2023.\\n Caulfield, Keith (April 18, 2021). \"Taylor Swift\\'s Re-Recorded Fearless Album Debuts at No. 1 on Billboard 200 Chart With Year\\'s Biggest Week\". Billboard. Archived from the original on April 18, 2021. Retrieved April 19, 2021.\\n Asker, Jim; Trust, Gary (February 22, 2021). \"Taylor Swift\\'s \\'Love Story (Taylor\\'s Version)\\' Debuts at No. 1 on Hot Country Songs Chart: \\'I\\'m So Grateful to the Fans\\'\". Billboard. Archived from the original on April 22, 2021. Retrieved February 22, 2021.\\n McCluskey, Megan (December 8, 2023). \"Breaking Down Taylor Swift\\'s 2023 Impact By the Numbers\". Time. Archived from the original on December 26, 2023. Retrieved December 26, 2023.\\n Horton, Adrian; Lee, Benjamin (February 6, 2023). \"Grammy awards 2023: list of winners\". The Guardian. Archived from the original on February 6, 2023. Retrieved February 6, 2023.\\n Corcoran, Nina (August 28, 2022). \"Taylor Swift Announces New Album Midnights, Breaks Record for Most Video of the Year Wins at 2022 VMAs\". Pitchfork. Archived from the original on August 29, 2022. Retrieved August 28, 2022.\\n \"Taylor Swift\\'s new album breaks Spotify streaming record\". The Guardian. October 22, 2022. Archived from the original on October 22, 2022. Retrieved October 22, 2022.\\n Harvilla, Rob (October 25, 2022). \"The Anti-Hero We Deserve: Taylor Swift and Her Polarizing \\'Midnights\\'\". The Ringer. Archived from the original on November 3, 2022. Retrieved November 3, 2022.\\n Light, Alan (October 24, 2022). \"Taylor Swift\\'s Midnights Does Something Astonishing. Even For Her\". Esquire. Archived from the original on January 17, 2023. Retrieved January 17, 2023.\\n Petridis, Alexis (October 21, 2022). \"Taylor Swift: Midnights Review \\xe2\\x80\\x93 Small-Hours Pop Rich with Self-Loathing and Stereotype-Smashing\". The Guardian. Archived from the original on October 22, 2022. Retrieved October 21, 2022.\\n Spanos, Brittany (October 21, 2022). \"Taylor Swift Lets Us Into Her Darkest Dreams On Midnights\". Rolling Stone. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Sheffield, Rob (October 21, 2022). \"Welcome to the Lavender Labyrinth: Taylor Swift\\'s Midnights Is the Mastermind\\'s Ultimate Power Move\". Rolling Stone. Archived from the original on October 22, 2022. Retrieved October 23, 2022.\\n Balasaygun, Kaitlin (November 1, 2022). \"How Taylor Swift went back to the past and turned Midnights into her biggest album success yet\". CNBC. Archived from the original on December 30, 2022. Retrieved December 30, 2022.\\n Shafer, Ellise (October 21, 2022). \"Taylor Swift\\'s Midnights Breaks Spotify Record for Most-Streamed Album in a Single Day\". Variety. Archived from the original on October 21, 2022. Retrieved October 22, 2022.\\n Dailey, Hannah (December 6, 2022). \"Here Are All of Taylor Swift\\'s Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022.\\n Trust, Gary (June 5, 2023). \"Morgan Wallen\\'s \\'Last Night\\' No. 1 on Hot 100 for Ninth Week, Taylor Swift & Ice Spice\\'s \\'Karma\\' Blasts to No. 2\". Billboard. Archived from the original on October 26, 2023. Retrieved August 21, 2023.\\n Blistein, Jon; Guglielmi, Jodi (September 13, 2023). \"Taylor Swift Makes History at 2023 VMAs\". Rolling Stone. Archived from the original on September 13, 2023. Retrieved September 13, 2023.\\n West, Bryan (February 5, 2024). \"Taylor Swift makes Grammys history with fourth album of the year win for Midnights\". USA Today. Archived from the original on February 5, 2024. Retrieved February 5, 2024.\\n Caulfield, Keith (July 16, 2023). \"Taylor Swift\\'s Re-Recorded \\'Speak Now\\' Debuts at No. 1 on Billboard 200 With 2023\\'s Biggest Week\". Billboard. Archived from the original on July 19, 2023. Retrieved July 16, 2023.\\n Caulfield, Keith (November 5, 2023). \"Taylor Swift\\'s 1989 (Taylor\\'s Version) Debuts at No. 1 on Billboard 200 With Biggest Week in Nearly a Decade\". Billboard. Archived from the original on November 5, 2023. Retrieved November 5, 2023.\\n Sherman, Maria (November 29, 2023). \"Taylor Swift is Spotify\\'s most-streamed artist of 2023, ending Bad Bunny\\'s 3-year reign\". ABC News. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n Garcia, Thania (November 28, 2023). \"Taylor Swift Named Apple Music\\'s Artist of the Year; Morgan Wallen Tops Global Songs Chart\". Variety. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n \"Best of 2023 (Taylor\\'s Version) Playlist on Amazon Music\". Curated by Amazon\\'s Music Experts. Amazon Music. Retrieved December 3, 2023. Our most streamed artist of 2023 globally. It\\'s Taylor\\'s world and we are just living for it.\\n Caulfield, Keith (November 21, 2023). \"Taylor Swift Is Billboard\\'s Top Artist of 2023\". Billboard. Archived from the original on November 21, 2023. Retrieved November 21, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Returns to No. 1 on Billboard 200\". Billboard. Retrieved December 3, 2023.\\n Caulfield, Keith (January 10, 2024). \"Morgan Wallen\\'s One Thing at a Time Is Luminate\\'s Top Album of 2023 in U.S.\" Billboard. Archived from the original on January 10, 2024. Retrieved January 12, 2024.\\n Trust, Gary (November 6, 2023). \"Taylor Swift\\'s \\'Is It Over Now? (Taylor\\'s Version)\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved November 6, 2023.\\n Cohen, Jonathan (June 29, 2021). \"Aaron Dessner, Justin Vernon Rev Up Big Red Machine With Help From Taylor Swift\". Variety. Archived from the original on May 23, 2022. Retrieved February 10, 2022.\\n Strauss, Matthew (February 19, 2021). \"HAIM Enlist Taylor Swift for New \"Gasoline\" Remix\". Pitchfork. Archived from the original on February 19, 2021. Retrieved February 10, 2022.\\n Dailey, Hannah (February 11, 2022). \"Ed Sheeran & Taylor Swift Release \\'The Joker and the Queen\\' Remix: Watch the Video\". Billboard. Archived from the original on February 11, 2022. Retrieved February 11, 2022.\\n Rowley, Glenn (January 18, 2023). \"The National Unveils \\'First Two Pages of Frankenstein\\' Tracklist With Taylor Swift, Phoebe Bridgers & Sufjan Stevens\". Billboard. Archived from the original on January 18, 2023. Retrieved January 18, 2023.\\n Davis, Clayton (December 21, 2022). \"Taylor Swift Doesn\\'t Make Oscar Shortlist for All Too Well Short Film, but Advances for \\'Carolina\\' Original Song\". Variety. Archived from the original on December 24, 2022. Retrieved December 24, 2022.\\n Utley, Riley (October 13, 2022). \"Every Taylor Swift Movie Performance, Ranked\". CinemaBlend. Archived from the original on October 13, 2022. Retrieved October 14, 2022.\\n Lang, Brent (December 9, 2022). \"Taylor Swift Making Feature Directing Debut for Searchlight Pictures\". Variety. Archived from the original on December 10, 2022. Retrieved December 9, 2022.\\n Wood, Mikael; Brown, August (August 1, 2023). \"It\\'s a love story, L.A. just says yes: How Taylormania took over the world\". Los Angeles Times. Archived from the original on August 1, 2023. Retrieved August 1, 2023.\\n Mahdawi, Arwa (November 20, 2022). \"Swifties know: the Ticketmaster fiasco shows America has a monopoly problem\". The Guardian. Archived from the original on December 10, 2022. Retrieved November 20, 2022.\\n Murray, Conor (December 8, 2023). \"Taylor Swift\\'s Eras Tour Is First In History To Gross Over $1 Billion, Report Says\". Forbes. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Tapp, Tom (December 8, 2023). \"Taylor Swift\\'s \\'The Eras Tour\\' Grosses Over $1 Billion In 2023, The Biggest Haul For Any Act Ever\". Deadline. Archived from the original on February 6, 2024. Retrieved February 10, 2024.\\n Kaufman, Gil (November 28, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Concert Movie Passes $250 Million in Worldwide Grosses\". Billboard. Archived from the original on December 1, 2023. Retrieved December 1, 2023.\\n Nordyke, Kimberly (January 7, 2024). \"Golden Globes 2024 Winners List\". The Hollywood Reporter. Archived from the original on January 8, 2024. Retrieved January 8, 2024.\\n Adamczyk, Alicia; Abrams, Joseph (July 25, 2023). \"The brilliant marketing synergy of Taylor Swift\\'s Eras Tour and her rerecorded albums\". Fortune. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\\n Ingham, Tim (June 14, 2023). \"Reliving the Taylor Swift Catalog Sale Saga (And Following the Money...)\". Music Business Worldwide. Archived from the original on June 14, 2023. Retrieved June 15, 2023.\\n Blanchet, Brenton (November 20, 2023). \"Travis Kelce Shares the Real Story of How Taylor Swift Romance Began in Wide-Ranging Interview\". People. Archived from the original on November 20, 2023. Retrieved November 20, 2023.\\n Kelly, Samantha Murphy (January 25, 2024). \"Explicit, AI-generated Taylor Swift images spread quickly on social media\". CNN Business. Archived from the original on January 25, 2024. Retrieved January 25, 2024.\\n Phillips, Zoe G. (January 27, 2024). \"SAG-AFTRA and White House Issue Statements on Taylor Swift AI Nudes: \"We Have It in Our Power to Control These Technologies\"\". The Hollywood Reporter. Archived from the original on January 27, 2024. Retrieved January 27, 2024.\\n \"Taylor Swift Announces \\'Brand New Album\\' \\'The Tortured Poets Department\\' with 13th Grammy Win\". Peoplemag. Archived from the original on February 10, 2024. Retrieved February 10, 2024.\\n Cairns, Dan (March 5, 2009). \"Swift rise of the anti-diva\". The Australian. Archived from the original on December 24, 2014. Retrieved July 2, 2012.\\n Bream, Jon (December 7, 2007). \"Music: OMG! Taylor\\'s senior year\". Star Tribune. Archived from the original on July 12, 2015. Retrieved July 1, 2012.\\n Newman, Melinda (December 19, 2008). \"Taylor Swift Sessions Interview\". AOL. Archived from the original on October 9, 2012. Retrieved March 25, 2011.\\n \"Swift starts world tour in Asia, pushes \"Speak Now\\' in NY\". Country Standard Time. October 23, 2007. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n \"News : 20 Questions With Taylor Swift\". Country Music Television. November 12, 2007. Archived from the original on November 17, 2014. Retrieved April 18, 2012.\\n McCafferty, Dennis (April 13, 2008). \"Taylor\\'s Swift rise\". USA Weekend. Archived from the original on November 14, 2012. Retrieved April 17, 2012.\\n \"Interview with Taylor Swift\". Time. April 23, 2009. Archived from the original on October 23, 2013. Retrieved July 1, 2012.\\n \"Taylor Swift Style: Singer Won\\'t Take Her Clothes Off, Wants People To Focus On Music\". HuffPost. October 23, 2012. Archived from the original on January 4, 2015. Retrieved January 4, 2015.\\n \"InStyle meets country singing sensation Taylor Swift\". InStyle UK. October 26, 2010. Archived from the original on May 27, 2013. Retrieved May 29, 2012.\\n \"Joni Mitchell: 15 Great Artists Influenced by the \\'Blue\\' Singer\". Rolling Stone. June 22, 2016. Archived from the original on December 26, 2021. Retrieved December 26, 2020.\\n Jenkins, Sally (September 28, 2023). \"You thought you knew the NFL. Now meet Taylor\\'s Version\". The Washington Post. Archived from the original on October 26, 2023. Retrieved September 29, 2023.\\n Bonaguro, Alison (November 8, 2012). \"Offstage: Taylor Swift Inspired by Female Singer-Songwriters of the \\'90s\". CMT. Archived from the original on October 4, 2023. Retrieved September 29, 2023.\\n \"Taylor Swift\\'s Favorite Music\". The Oprah Winfrey Show. Archived from the original on January 16, 2016. Retrieved October 23, 2012.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Mansfield, Brian (October 23, 2010). \"Taylor Swift learns to \\'Speak Now,\\' reveal her maturity\". USA Today. Archived from the original on November 4, 2012. Retrieved July 1, 2012.\\n Block, Melissa (October 31, 2014). \"\\'Anything That Connects\\': A Conversation With Taylor Swift\". NPR Music. Archived from the original on February 6, 2015. Retrieved October 26, 2019.\\n Eells, Josh (September 8, 2014). \"The Reinvention of Taylor Swift\". Rolling Stone. Archived from the original on June 4, 2016. Retrieved June 8, 2016.\\n Reid, Poppy (November 2, 2021). \"The Curious Case of Keith Urban\". Rolling Stone. Archived from the original on November 19, 2021. Retrieved November 3, 2021.\\n Hiatt, Brian (June 18, 2019). \"Taylor Swift: The Rolling Stone Interview\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved April 14, 2022.\\n Weatherby, Taylor (March 10, 2021). \"Taylor Swift\\'s Road to Folklore\". The Recording Academy. Archived from the original on November 25, 2021. Retrieved November 24, 2021.\\n Franssen, Gaston (January 2, 2022). \"Policing the celebrity of Taylor Swift: introduction\". Celebrity Studies. 13 (1): 90\\xe2\\x80\\x9392. doi:10.1080/19392397.2022.2026148. S2CID 246997248.\\n Savage, Mark (October 19, 2022). \"Midnights: What we know about Taylor Swift\\'s songwriting\". BBC News. Archived from the original on October 19, 2022. Retrieved October 20, 2022.\\n Bruner, Raisa (August 24, 2020). \"Let\\'s Break Down Taylor Swift\\'s Tender New Album Folklore\". Time. Archived from the original on July 31, 2020. Retrieved October 20, 2022.\\n McNutt 2020, p. 77.\\n Hughes 2017, p. 206; Perone 2017, p. 33.\\n \"Taylor Swift: Album Guide\". Rolling Stone. Archived from the original on December 5, 2012. Retrieved December 5, 2012.\\n \"Pop and Rock Listings July 22 \\xe2\\x80\\x93 28\". The New York Times. July 21, 2011. Archived from the original on January 28, 2012. Retrieved July 12, 2012.\\n \"Taylor Swift Remade Fearless as Taylor\\'s Version. Let\\'s Discuss\". The New York Times. April 9, 2021. Archived from the original on April 9, 2021. Retrieved April 21, 2021.\\n Petridis, Alexis (March 6, 2009). \"Taylor Swift: Fearless\". The Guardian. Archived from the original on October 16, 2013. Retrieved August 13, 2022.\\n Jones, Sasha-Frere (November 11, 2008). \"Prodigy\". The New Yorker. Archived from the original on October 21, 2016. Retrieved August 14, 2022.\\n Hughes 2017, p. 206.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. p. 5. Archived from the original on November 20, 2022. Retrieved August 12, 2022.\\n McNutt 2020, p. 78.\\n Rosen, Jody (November 17, 2013). \"Why Taylor Swift Is the Reigning Queen of Pop\". Vulture. Archived from the original on November 19, 2013. Retrieved November 9, 2020.\\n McNutt 2020, p. 79.\\n Levine, Nick (August 21, 2019). \"Taylor Swift\\'s Lover: The struggle to maintain superstardom\". BBC. Archived from the original on March 1, 2021. Retrieved October 29, 2021.\\n da Silva, Michelle (November 13, 2017). \"Taylor Swift Has Changed for the Worse on Reputation\". Now. Archived from the original on July 26, 2020. Retrieved July 27, 2020.\\n Tucker, Ken (November 13, 2017). \"Taylor Swift Pushes Further Into Electro-Pop With \\'Reputation\\'\". NPR. Archived from the original on October 27, 2021. Retrieved April 8, 2023.\\n Moreland, Quinn (October 24, 2022). \"Taylor Swift: Midnights\". Pitchfork. Archived from the original on October 24, 2022. Retrieved April 8, 2023.\\n Ryan, Elise (October 21, 2022). \"Review: Taylor Swift gets dark, electric on \\'Midnights\\'\". Associated Press News. Archived from the original on October 21, 2022. Retrieved April 8, 2023.\\n Winter, Velvet (November 12, 2022). \"Like The Beatles, Madonna and Kylie Minogue before her, Taylor Swift is masterful at pivoting\". ABC News. Archived from the original on November 13, 2022. Retrieved November 13, 2022.\\n McNutt 2020, p. 79; Sloan 2021, p. 17.\\n Sloan 2021, p. 17.\\n Hyden, Steven (March 10, 2021). \"Taylor Swift, Indie-rock star? Long, Long ago, this might have felt strange\". The New York Times. Archived from the original on April 13, 2021. Retrieved April 13, 2021.\\n Caramanica, Jon (July 26, 2020). \"Taylor Swift, A Pop Star Done with Pop\". The New York Times. Archived from the original on September 10, 2020. Retrieved August 14, 2022.\\n Harbron, Lucy (November 11, 2021). \"Why Taylor Swift\\'s \\'Red\\' Is Her Turning Point\". Clash. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Gerber, Brady (July 27, 2020). \"The Story Behind Every Song on Taylor Swift\\'s folklore\". Vulture. Retrieved December 12, 2023.\\n Willman, Chris (October 21, 2022). \"Taylor Swift\\'s Midnights Marks a Return to Electronic, Confessional Pop That\\'s Worth Losing Sleep Over: Album Review\". Variety. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Fulford 2014, p. 192.\\n \"Taylor Swift Deepens Her Goth-Folk Vision on the Excellent \\'Evermore\\'\". Rolling Stone. December 11, 2020. Archived from the original on December 11, 2020. Retrieved August 23, 2022.\\n \"The 200 Greatest Singers of All Time\". Rolling Stone. January 1, 2023. Archived from the original on January 1, 2023. Retrieved January 1, 2023.\\n Provenzano 2018, p. 173.\\n Roland, Tom (October 15, 2010). \"Taylor Swift: The Billboard Cover Story\". Billboard. Archived from the original on October 18, 2010. Retrieved July 3, 2012.\\n Provenzano 2018, pp. 173\\xe2\\x80\\x93174.\\n Provenzano 2018, p. 174.\\n Powers, Ann (October 25, 2010). \"Album review: Taylor Swift\\'s Speak Now\". Los Angeles Times. Archived from the original on October 28, 2010. Retrieved October 25, 2010.\\n Willman, Chris (November 10, 2017). \"Album Review: Taylor Swift\\'s \\'Reputation\\'\". Variety. Retrieved April 8, 2023.\\n Cox, Jamieson (November 13, 2017). \"Taylor Swift: Reputation\". Pitchfork. Retrieved April 8, 2023.\\n Powers, Ann (November 10, 2017). \"The Old Taylor\\'s Not Dead\". NPR. Retrieved April 8, 2023.\\n Wilson, Carl (November 13, 2017). \"On Reputation, the \"Old Taylor\" Is Dead, but the New One Isn\\'t Quite Ready to Come to the Phone\". Slate. Retrieved April 8, 2023.\\n DeCaro, Alessandro (October 19, 2022). \"10 best Taylor Swift scene covers\". Alternative Press. Retrieved July 1, 2023.\\n Barker, Andrew (November 27, 2020). \"Folklore: The Long Pond Studio Sessions Review\". Variety. Retrieved May 31, 2021.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved April 8, 2023.\\n Kornhaber, Spencer (July 28, 2020). \"Taylor Swift Is No Longer Living in the Present\". The Atlantic. Retrieved August 23, 2022.\\n Brehian, Tom (July 24, 2020). \"Review: Taylor Swift\\'s \\'folklore\\' Is An Indie Record Unconcerned With Being Cool\". Stereogum. Retrieved August 23, 2022.\\n Willman, Chris (December 11, 2020). \"Taylor Swift Has Her Second Great Album of 2020 With \\'Evermore\\': Album Review\". Variety. Retrieved August 23, 2022.\\n Sodomsky, Sam (December 15, 2020). \"Taylor Swift: evermore\". Pitchfork. Archived from the original on December 15, 2020. Retrieved December 14, 2020.\\n McCormick, Neil (April 9, 2021). \"Taylor Swift copies her younger self \\xe2\\x80\\x93 and she sounds even more Fearless today\". The Daily Telegraph. Retrieved August 23, 2022.\\n \"Taylor Swift forges ahead with a dreamy throwback in Fearless (Taylor\\'s Version)\". The A.V. Club. April 9, 2021. Archived from the original on April 9, 2021. Retrieved August 23, 2022.\\n Bernstein, Jonathan (April 9, 2021). \"Taylor Swift Carefully Reimagines Her Past on \\'Fearless: Taylor\\'s Version\\'\". Rolling Stone. Retrieved August 23, 2022.\\n Solomon, Kate (November 12, 2021). \"Taylor Swift, Red (Taylor\\'s Version), Review: How Brilliant She Is When Her Heart Is in Tatters\". i. Archived from the original on November 12, 2021. Retrieved November 13, 2021.\\n Snapes, Laura (August 23, 2020). \"Taylor Swift: Folklore review \\xe2\\x80\\x93 bombastic pop makes way for emotional acuity\". The Guardian. Retrieved August 23, 2022.\\n Snapes, Laura (November 12, 2021). \"Taylor Swift: Red (Taylor\\'s Version) Review \\xe2\\x80\\x93 Getting Back Together with a Classic\". The Guardian. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Kelly, Fred (October 21, 2022). \"Taylor Swift\\'s Midnights: what the critics are saying\". The Week. Retrieved October 23, 2022.\\n Petrusich, Amanda (June 12, 2023). \"The Startling Intimacy of Taylor Swift\\'s Eras Tour\". The New Yorker. Retrieved June 12, 2023.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC Music. August 22, 2019. Retrieved November 10, 2021.\\n Emily, Lee (November 5, 2021). \"Here Are Taylor Swift\\'s Best Bridges On \\'Red\\' Ranked\". iHeartRadio. Retrieved November 10, 2021.\\n Eggertsen, Chris (September 20, 2022). \"Taylor Swift\\'s Iconic Songwriting Credits Amplified By Spotify With Dedicated Page\". Billboard. Retrieved September 20, 2022.\\n Bate, Jonathan (April 10, 2023). \"Why Taylor Swift is a literary giant \\xe2\\x80\\x94 by a Shakespeare professor\". The Times. Retrieved April 10, 2023.\\n Pazzanese, Christina (August 2, 2023). \"So what exactly makes Taylor Swift so great?\". Harvard Gazette. Retrieved August 7, 2023.\\n Murphy, Sam (November 10, 2021). \"How \\'Red\\' Became The Most Pivotal Record In Taylor Swift\\'s Career\". Junkee. Retrieved November 10, 2021.\\n Siroky, Mary (November 9, 2021). \"Every Taylor Swift Album Ranked from Worst to Best\". Consequence. Retrieved November 10, 2021.\\n Bruner, Raisa (May 24, 2021). \"How Olivia Rodrigo Become America\\'s Biggest New Pop Star\". Time. Retrieved November 10, 2021.\\n Mulvey, John (September 16, 2023). \"Arctic Monkeys, Taylor Swift, Kendrick Lamar, Lana Del Rey And The 30 Artists Who Will Shape The Next 30 Years\". Mojo. Retrieved September 19, 2023.\\n Farley, Christopher John (October 22, 2010). \"Taylor Swift\\'s Solo Act\". The Wall Street Journal. Archived from the original on February 1, 2015. Retrieved May 24, 2012.\\n Jo Sales, Nancy; Diehl, Jessica (April 2013). \"Taylor Swift\\'s Telltale Heart\". Vanity Fair. Archived from the original on January 30, 2017. Retrieved February 4, 2017.\\n Daly, Rhian (December 13, 2020). \"Taylor Swift says her diaristic songwriting style was \\'unsustainable\\' for her future\". NME. Retrieved February 17, 2021.\\n Gallo, Phil (October 22, 2012). \"Taylor Swift\\'s Red: The Billboard Cover Story\". Billboard. Archived from the original on June 14, 2013.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift Is Angry, Darn It\". The New York Times. Archived from the original on September 11, 2012. Retrieved July 2, 2012.\\n Kelly, James (August 26, 2009). \"Taylor Swift writing her own songs and rules\". The Atlanta Journal-Constitution. Archived from the original on September 8, 2014. Retrieved July 30, 2012.\\n Lansky, Sam (November 8, 2017). \"Why Taylor Swift\\'s Red Is Her Best Album\". Billboard. Retrieved December 27, 2020.\\n Hiatt, Brian (September 30, 2019). \"9 Taylor Swift Moments That Didn\\'t Fit in Our Cover Story\". Rolling Stone. Archived from the original on October 1, 2019. Retrieved December 9, 2019.\\n \"Taylor Swift Talks Newfound \\'Freedom,\\' \\'Lover\\' Tour Plans and So Much More\". On Air with Ryan Seacrest. August 27, 2019. Retrieved March 22, 2020.\\n Yuan, Jada (December 30, 2009). \"Microwaving a tragedy: The marriage of romance and romanticism in \\'00s pop\". Las Vegas Weekly. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n Rotman, Natalie (January 9, 2009). \"Colbie Caillat has \\'Breakthrough\\' with sophomore CD\". Reading Eagle. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC News. Retrieved February 2, 2021.\\n Knibbs, Kate (August 21, 2019). \"Ten Years of Taylor Swift: How the Pop Star Went From Sweetheart to Snake (and Back Again?)\". The Ringer. Retrieved December 13, 2021.\\n Stubbs, Dan (October 9, 2015). \"Taylor Swift: Power, Fame And The Future \\xe2\\x80\\x93 The Full NME Cover Interview\". NME. Retrieved February 17, 2021.\\n Weber, Theon (November 3, 2010). \"The Iceberg Songs of Taylor Swift\". The Village Voice. Archived from the original on November 4, 2015. Retrieved July 30, 2012.\\n Beck, Julia (October 27, 2014). \"Taylor Swift Is So Much More Fun Now That She\\'s Jaded\". The Atlantic. Archived from the original on September 27, 2016. Retrieved October 30, 2021.\\n Willman, Chris (October 10, 2010). \"Princess Crossover\". New York. Archived from the original on July 27, 2013. Retrieved July 1, 2012.\\n Rosen, Jody (November 13, 2008). \"Fearless\". Rolling Stone. Archived from the original on August 15, 2012. Retrieved July 1, 2012.\\n Powers, Ann (October 30, 2014). \"The Many New Voices of Taylor Swift\". NPR. Retrieved June 2, 2022.\\n Stoeffel, Kat (November 16, 2012). \"Stop Asking Taylor Swift to Apologize for Writing Songs About Ex-Boyfriends \\xe2\\x80\\x93 The Cut\". New York. Archived from the original on November 27, 2012. Retrieved February 25, 2013.\\n Raven, Robin (March 16, 2022). \"10 Artists Who Have Stood Up For Women In Music: Taylor Swift, Lizzo & More\". The Recording Academy.\\n \"Cover Preview: Taylor Swift Fights Back About Her Love Life, the Hyannis Port House\\xe2\\x80\\x94and Has Words for Tina Fey and Amy Poehler\". Vanity Fair. March 5, 2013. Archived from the original on August 8, 2016. Retrieved August 3, 2016.\\n Dominus, Susan (November 16, 2012). \"The Many Insecurities of Taylor Swift\". The New York Times. Archived from the original on June 17, 2016.\\n Doyle, Patrick (November 13, 2020). \"Musicians on Musicians: Paul McCartney and Taylor Swift\". Rolling Stone. Archived from the original on November 30, 2020. Retrieved February 7, 2021.\\n Olivier, Bobby (December 11, 2020). \"Taylor Swift\\'s \\'Evermore\\' Is an Undeniable Folk-Pop Masterpiece\". Spin. Retrieved February 1, 2021.\\n Shutler, Ali (October 9, 2022). \"Taylor Swift organises her lyrics into three \\'dorky\\' pen-themed categories\". NME. Retrieved October 18, 2022.\\n Browne, Erin (October 21, 2022). \"All of Taylor Swift\\'s Famously Devastating Track 5\\'s, Ranked\". Vulture. Retrieved October 27, 2022.\\n \"NMPA to Honor Taylor Swift with Songwriter Icon Award Among Other 2021 Annual Meeting Honorees\". National Music Publishers\\' Association. May 24, 2021. Retrieved May 24, 2021.\\n Linker, Damon (November 26, 2021). \"Taylor Swift, Phoebe Bridgers, and Rihanna: How women took over songwriting\". The Week. Retrieved November 28, 2021.\\n Greco, Patti (November 13, 2017). \"A Harvard Professor Critiques Taylor Swift\\'s New Poems\". Cosmopolitan. Retrieved December 21, 2021.\\n Sheffield, Rob (October 13, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Movie Will Make You Sing, Scream, and Sob\". Rolling Stone. Retrieved December 8, 2023.\\n Zacharek, Stephanie (December 14, 2023). \"The Eras Tour Movie Is Irresistible No Matter How Much You Think You Like Taylor Swift\". TIME. Retrieved December 8, 2023.\\n \"Ticketing Shmicketing: Taylor Swift\\'s \\'Eras Tour\\' Debut Slays (And Could Break All-Time Touring Record)\". Pollstar. March 18, 2023. Archived from the original on March 20, 2023. Retrieved June 30, 2023.\\n Aramesh, Waiss David (March 18, 2023). \"Taylor Swift\\'s The Eras Tour Is a 3-Hour Career-Spanning Victory Lap\". Rolling Stone. OCLC 1787396. Archived from the original on March 18, 2023. Retrieved June 30, 2023.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved July 23, 2023.\\n McCormick, Neil (March 18, 2023). \"Taylor Swift: The Eras Tour, review: a roaring spectacle of a comeback\". The Daily Telegraph. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kornhaber, Spencer (March 18, 2023). \"What Made Taylor Swift\\'s Concert Unbelievable\". The Atlantic. Retrieved June 30, 2023.\\n Foggatt, Tyler (June 3, 2023). \"Look What Taylor Made Us Do\". The New Yorker. Retrieved June 30, 2023.\\n Seibert, Brian (August 9, 2023). \"How to Command a Stage Without Great Dance Moves (Taylor\\'s Version)\". The New York Times. Retrieved August 11, 2023.\\n Krelenstein, Greg (May 21, 2018). \"TAYLOR SWIFT\\'S REPUTATION TOUR IS A POP TRIUMPH\". V. Archived from the original on May 22, 2018. Retrieved May 22, 2018.\\n Frere-Jones, Sasha (November 3, 2008). \"Prodigy\". The New Yorker. Retrieved June 30, 2023.\\n Horton, Adrian (March 18, 2023). \"Taylor Swift review \\xe2\\x80\\x93 pop\\'s hardest-working star gives Eras tour her all\". The Guardian. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kaplan, Ilana (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' tour is a thrilling spectacle from a pop mastermind\". i. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Young, Alex (March 27, 2023). \"Taylor Swift\\'s \"The Eras Tour\" Is a Triumph of Spectacle and Stamina: Review\". Consequence. Retrieved June 30, 2023.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved July 1, 2023.\\n O\\'Connor, Roisin (June 8, 2018). \"Taylor Swift \\'reputation\\' stadium tour review: Dazzling pop spectacle from the star who doesn\\'t stand still\". The Independent. Archived from the original on June 30, 2023. Retrieved December 22, 2019.Savage, Mark (March 18, 2023). \"Taylor Swift launches Eras tour with three-hour, 44-song set\". BBC News. Archived from the original on March 18, 2023. Retrieved March 18, 2023. Sisario, Ben (November 5, 2023). \"How Taylor Swift\\'s Eras Tour Conquered the World\". The New York Times. Retrieved August 12, 2023. DeVille, Chris (July 12, 2018). \"Big Reputation: A Trip To Taylor Swift\\'s Hyper-Maximalist Stadium Tour\". Stereogum. Retrieved June 30, 2023.\\n Procell, Carlie; Padilla, Ramon (April 28, 2023). \"Taylor Swift tour has many \\'eras.\\' We tracked her movements to give you the look and feel\". USA Today. Retrieved June 30, 2023.\\n \"Taylor Swift Shares Stunning \\'Wildest Dreams\\' Performance from Grammy Museum\". Billboard. January 5, 2016. Retrieved October 29, 2023.\\n Burgham, Lydia (November 10, 2018). \"Taylor Swift in Auckland, reviewed: Despite the snakes, her Reputation shines on\". The Spinoff. Retrieved December 10, 2019.\\n Dodd, Sophie (November 15, 2023). \"All About Taylor Swift\\'s Parents, Scott and Andrea Swift\". People. Retrieved December 3, 2023.\\n Swift, Taylor (March 15, 2013). \"\"Sparks Fly\" (acoustic) Live on the RED Tour!\" \\xe2\\x80\\x93 via YouTube.\\n Lewis, Randy (April 3, 2011). \"Academy of Country Music Awards: Las Vegas welcomes Miranda Lambert, Taylor Swift with open arms\". Los Angeles Times. Retrieved August 20, 2023.\\n Ritchie, Mike (March 8, 2020). \"Why Taylor Swift is making the ukulele cool again\". The Herald. Retrieved July 1, 2023.\\n Gensler, Andy (August 17, 2023). \"The Showgoer: The Greatest Show On Earth \\xe2\\x80\\x94 Taylor Swift\\'s \\'Eras Tour\\' \\xe2\\x80\\x94 Is All That And Far More\". Pollstar. Retrieved August 19, 2023.\\n Sheffield, Rob (May 9, 2018). \"Why Taylor Swift\\'s \\'Reputation\\' Tour Is Her Finest Yet\". Rolling Stone. Retrieved June 30, 2023.\\n Willman, Chris (May 16, 2018). \"Taylor Swift\\'s \\'Reputation\\' Tour Has Bad Blood, Good Will, Sex Appeal and Serpents\". Variety. Retrieved December 22, 2019.\\n Willman, Chris (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' Show Is a Three-Hour, 44-Song Epic That Leaves \\'Em Wanting More: Concert Review\". Variety. Archived from the original on March 18, 2023. Retrieved June 30, 2018.\\n Ordo\\xc3\\xb1a, Michael (September 9, 2022). \"Taylor Swift wants an Oscar. So she took \\'All Too Well\\' to TIFF\". Los Angeles Times. Retrieved September 10, 2022.\\n CMT.com Staff (May 4, 2011). \"Taylor Swift\\'s \"Mean\" Video Debuts Friday\". CMT. Archived from the original on June 19, 2019. Retrieved June 19, 2019.\\n Anitai, Tamar (August 27, 2010). \"Video Premiere: Taylor Swift, \\'Mine\\'\". MTV News. Archived from the original on April 29, 2019. Retrieved June 19, 2019.\\n Bonaguro, Alison (May 6, 2011). \"OFFSTAGE: Taylor Swift Isn\\'t \\'Mean\\' at All, Director Says\". CMT News. Archived from the original on June 19, 2019. Retrieved June 20, 2019.\\n Tailor, Leena (September 1, 2017). \"Exclusive: Taylor Swift\\'s Director Joseph Kahn on How Her Image Invokes a Double Standard: \\'She\\'s a Genius\\'\". Entertainment Tonight. Archived from the original on June 19, 2019.\\n O\\'Connell, Michael (October 9, 2015). \"Taylor Swift and Jimmy Fallon Among Early Emmy Winners\". The Hollywood Reporter. Archived from the original on June 19, 2019.\\n Forbes, Jihan (May 14, 2015). \"Peep Taylor Swift\\'s Star-Studded Cast for Her \\'Bad Blood\\' Music Video\". The Fashion Spot. Retrieved May 13, 2020.\\n \"9 Things You Might Have Missed in Taylor Swift\\'s Netflix Concert Film\". E! News. December 31, 2018. Retrieved September 10, 2022.\\n Spanos, Brittany (April 25, 2019). \"Watch Taylor Swift, Brendon Urie\\'s Colorful \\'ME!\\' Video\". Rolling Stone. Archived from the original on April 26, 2019. Retrieved April 25, 2019.\\nMoore, Sam (August 23, 2019). \"Watch Taylor Swift\\'s colourful new video for \\'Lover\\'\". NME. Archived from the original on August 27, 2019. Retrieved August 27, 2019.\\nZemler, Emily (June 17, 2019). \"Watch Taylor Swift Reunite With Katy Perry in \\'You Need to Calm Down\\' Video\". Rolling Stone. Archived from the original on June 17, 2019. Retrieved June 17, 2019.\\n Mylrea, Hannah (February 28, 2020). \"Every incredible Easter Egg in Taylor Swift\\'s \\'The Man\\' video\". NME. Retrieved March 9, 2020.\\nSpanos, Brittany; Legaspi, Althea (July 24, 2020). \"Taylor Swift Blends Fantastical With Personal in \\'Cardigan\\' Video\". Rolling Stone. Retrieved July 27, 2020.\\n\"Justin Bieber & Megan Thee Stallion Lead 2021 MTV VMA Nominations\". Billboard. Retrieved August 17, 2021.\\n Weatherby, Taylor (February 5, 2023). \"Taylor Swift Makes GRAMMY History (Again) With Best Music Video Win For \"All Too Well: The Short Film\"\". Grammy Awards. Retrieved February 6, 2023.\\n Lansky, Sam (December 6, 2023). \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift\". The Recording Academy. Archived from the original on August 12, 2016. Retrieved August 3, 2016.\\n Friedlander, Whitney (September 10, 2015). \"Taylor Swift, Jimmy Fallon Among Juried Emmy Award Winners\". Variety. Archived from the original on September 15, 2015. Retrieved August 3, 2016.\\n \"Taylor Swift dominates AMAs with 6 wins, extending lead as show\\'s most-decorated artist\". KTRK-TV. November 21, 2022. Retrieved November 21, 2022.\\n Grein, Paul (November 19, 2023). \"After the 2023 Billboard Music Awards, Who Is the All-Time Biggest Winner?\". Billboard. Retrieved November 19, 2023.\\n See Guinness World Records by Taylor Swift\\n Lewis, Randy (November 4, 2013). \"Taylor Swift to receive rare Pinnacle Award at CMA Awards Nov. 6\". Los Angeles Times. Retrieved May 13, 2020.\\n \"Taylor Swift Nashville Tickets\". Excite. Archived from the original on February 3, 2015. Retrieved February 2, 2015.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on March 14, 2014. Retrieved February 2, 2015.\\n \"Songwriters Hall of Fame\". Songwriters Hall of Fame. Archived from the original on November 29, 2014. Retrieved February 2, 2015.\\n \"The 100 Greatest Songwriters of All Time\". Rolling Stone. Archived from the original on September 2, 2017. Retrieved August 28, 2017.\\n Polanco, Luis (April 5, 2016). \"Taylor Swift to Receive First-Ever Taylor Swift Award From BMI\". Billboard. Retrieved October 21, 2020.\\n Jolly, Nathan (November 17, 2019). \"Why Taylor Swift is to blame for latest twist in music rights drama\". News.com.au. Retrieved November 17, 2019.\\n \"10 Life mantras by Taylor Swift to live by\". India Today. December 13, 2016. Archived from the original on February 9, 2019. Retrieved July 16, 2020.\\n Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Decade Taylor Swift: \\'I Do Want My Music to Live On\\'\". Billboard. Retrieved December 11, 2019.\\n \"Taylor Swift to receive BRITs Global Icon award\". Official Charts Company. May 9, 2021. Retrieved May 10, 2021.\\n \"Taylor Swift to receive Global Icon Award!\". Brit Awards. May 9, 2021. Retrieved May 10, 2021.\\n Paine, Andre (February 22, 2023). \"Taylor Swift wins IFPI\\'s 2022 Global Recording Artist Of The Year Award\". Music Week. Retrieved February 22, 2023.\\n Brandle, Lars (November 5, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Debuts at U.K. No. 1 With \\'Massive\\' Sales\". Billboard. Retrieved November 5, 2023.\\n \"Female artists with the most Irish Number 1 albums since 2000\". Official Charts Company. November 5, 2020. Retrieved February 23, 2021.\\n Wang, Dennis (April 16, 2021). \"Taylor Swift\\'s Fearless hits the right note in China, again\". People\\'s Daily. Retrieved June 26, 2021.\\n Brandle, Lars (July 7, 2023). \"Taylor Swift Sets Chart Record In Australia With Top-Five Sweep\". Billboard. Retrieved February 13, 2024.\\n Brandle, Lars (February 9, 2024). \"Ahead of \\'The Eras Tour\\' of Australia, Taylor Swift Sweeps Top 5\". Billboard. Retrieved February 13, 2024.\\n \"Taylor beats Swift\". Australian Recording Industry Association. July 14, 2023. Archived from the original on July 14, 2023. Retrieved July 19, 2023.\\n \"Taylor sweeps the record\". Australian Recording Industry Association. July 7, 2023. Retrieved July 7, 2023.\\n Cumulative touring gross:\\n\"Top Touring Artist of the Pollstar Era\" (PDF). Pollstar. June 10, 2022. Archived (PDF) from the original on August 5, 2022. Retrieved August 4, 2022.\\nAllen, Bob (September 26, 2023). \"What A Friggin\\' Year! 2023 Boxoffice Results Remain At Record Highs\". Pollstar. Retrieved October 18, 2023.\\n Gensler, Andy (December 8, 2023). \"Taylor Swift Sets All-Time Touring Record With $1 Billion Gross\". Pollstar. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Willman, Chris (December 21, 2020). \"Taylor Swift\\'s \\'Evermore\\' Sells a Million Worldwide in First Week\". Variety. Archived from the original on January 11, 2021. Retrieved December 21, 2020.\\n Paine, Andre (December 22, 2022). \"Taylor Swift Achieves More Than 6 Million Global Units for Midnights and 37 Billion Total Streams in 2022\". Music Week. Archived from the original on December 22, 2022. Retrieved July 19, 2023.\\n Young, Alex (October 31, 2022). \"Taylor Swift broke 73 records with release of new album Midnights\". Consequence. Archived from the original on October 31, 2022. Retrieved July 19, 2023.\\n Grein, Paul (November 8, 2023). \"Taylor Swift Is Apple Music\\'s 2023 Artist of the Year\". Billboard. Retrieved November 8, 2023.\\n Willman, Chris (October 28, 2023). \"Taylor Swift Beats Her Own Spotify Record for Most Single-Day Streams for an Artist With \\'1989 (Taylor\\'s Version)\\' Release\". Variety. Archived from the original on October 29, 2023. Retrieved October 29, 2023.\\n @billboardcharts (January 19, 2022). \"Most entries on the #Global200 chart in a single week\" (Tweet) \\xe2\\x80\\x93 via Twitter.\\n \"Taylor Swift Chart History (Billboard Global 200)\". Billboard. Retrieved July 17, 2023.\\n \"Greatest of All Time Artists\". Billboard. Archived from the original on November 14, 2019. Retrieved November 15, 2019.\\n Zellner, Xander (January 25, 2024). \"Taylor Swift Tallies Record-Extending 95th Week at No. 1 on Artist 100 Chart\". Billboard. Retrieved January 26, 2024.\\n Caulfield, Keith (December 31, 2023). \"Taylor Swift Surpasses Elvis Presley for Most Weeks at No. 1 on Billboard 200 Among Soloists\". Billboard. Retrieved January 3, 2024.\\n Trust, Gary (January 22, 2024). \"Ariana Grande\\'s \\'Yes, And?\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved January 22, 2024.\\n \"Taylor Swift Chart History (Top Country Albums)\". Billboard. Retrieved July 17, 2023.\\n \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved January 14, 2021.\\n Trust, Gary (July 28, 2023). \"Taylor Swift Breaks Record for Most No. 1s on Pop Airplay Chart As \\'Cruel Summer\\' Becomes Her 12th\". Billboard. Archived from the original on September 29, 2023. Retrieved October 19, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift Makes History With Five of the Top 10 Albums on the Billboard 200\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (July 17, 2023). \"Taylor Swift Has 11 Albums on the Billboard 200 Chart for the First Time\". Billboard. Retrieved July 19, 2023.\\n McIntyre, Hugh (January 22, 2024). \"Taylor Swift Made Billboard History\\xe2\\x80\\x93Now Only She Can Match Her Own Feat\". Forbes. Retrieved January 22, 2024.\\n Caulfield, Keith (December 29, 2023). \"Taylor Swift Has the Top Four on the Album Sales Chart for the First Time\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (January 3, 2024). \"Taylor Swift Has 7 of the Top 10 on Billboard\\'s Album Sales Chart\". Billboard. Retrieved January 23, 2024.\\n \"RIAA \\xe2\\x80\\x93 Top Artists (Digital Singles)\". Recording Industry Association of America. Retrieved July 17, 2023.\\n Ahlgrim, Callie (July 12, 2023). \"There are 116 songs in history that have been certified diamond \\xe2\\x80\\x94 here they all are\". Insider Inc. Retrieved July 17, 2023.\\n Caulfield, Keith (October 30, 2022). \"Taylor Swift\\'s Midnights Blasts in at No. 1 on Billboard 200 Chart With Biggest Week for an Album in 7 Years\". Billboard. Retrieved October 30, 2022.\\n Nicks, Stevie (April 29, 2010). \"The 2010 Time 100: Taylor Swift\". Time. Archived from the original on November 29, 2013. Retrieved April 22, 2012.\\nVena, Jocelyn (April 16, 2015). \"Taylor Swift, Kanye West, Kim Kardashian Make Time\\'s 100 Most Influential People List\". Billboard. Archived from the original on September 25, 2016. Retrieved September 1, 2016.\\nJensen, Erin (April 17, 2019). \"Dwayne Johnson, Taylor Swift, Gayle King, more cover Time\\'s 100 most influential people issue\". USA Today. Archived from the original on April 17, 2019. Retrieved April 17, 2019.\\n Dockterman, Eliana (December 6, 2017). \"\\'I Was Angry.\\' Taylor Swift on What Powered Her Sexual Assault Testimony\". Time. Archived from the original on December 6, 2017. Retrieved December 7, 2017.\\n Shah, Simmone (December 6, 2023). \"Taylor Swift Makes History as Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. December 6, 2023. Retrieved December 11, 2023.\\n \"Taylor Swift\". Forbes. Retrieved December 1, 2020.\\n \"30 Under 30 2017: All-Star Alumni\". Forbes. Retrieved December 1, 2020.\\n \"#8 Taylor Swift\". Forbes. Archived from the original on July 28, 2015. Retrieved July 27, 2015.\\n \"Forbes Unveils Its 20th Annual Ranking Of The World\\'s Most Powerful Women\". Forbes. December 5, 2023. Retrieved December 5, 2023.\\n Schneider, Marc (July 24, 2023). \"8 Ways Taylor Swift Has Changed the Music Business\". Billboard. Retrieved July 24, 2023.\\n Khan, Fawzia (June 18, 2021). \"The Might Of Taylor Swift\". Elle. Archived from the original on June 28, 2021. Retrieved October 20, 2021.\\n Pattison, Kate (September 19, 2023). \"How did Taylor Swift get so popular? She never goes out of style\". The Conversation. Retrieved September 22, 2023.\\n Grady, Constance (October 12, 2023). \"The Eras concert movie is Taylor Swift leveling up\". Vox. Retrieved October 13, 2023.\\n Jericho, Greg (October 28, 2022). \"Taylor Swift\\'s incredible success in graphs \\xe2\\x80\\x93 who can blame me for being a Swiftie as a 50-year-old man?\". The Guardian. Retrieved October 29, 2022.\\n Weatherhead, Shaina (April 20, 2023). \"Taylor Swift\\'s \\'Miss Americana\\' Is Required Viewing Right Now\". Collider. Retrieved April 24, 2023.\\n Bartsch, Kayla (September 24, 2023). \"Taylor Swift: Empress of the Zeitgeist\". National Review. Retrieved September 24, 2023.\\n \"Taylor Swift: \\'My Confidence Is Easy To Shake\\'\". NPR. November 2, 2012. Retrieved May 20, 2021.\\nThanki, Juli (September 24, 2015). \"Taylor Swift: Teen idol to \\'biggest pop artist in the world\\'\". The Tennessean. Retrieved May 13, 2021.\\nYahr, Emily (June 16, 2016). \"Taylor Swift\\'s first song came out 10 years ago. Here\\'s what she was like as a teen songwriter\". Arts and Entertainment. The Washington Post. Archived from the original on May 13, 2021. Retrieved May 13, 2021.\\n El Mosselli, Sabrina (December 24, 2022). \"How Taylor Swift rose from \\'Miss Americana\\' to global megastar\". Le Monde. Retrieved December 28, 2022.\\n Cragg, Michael. \"Is Taylor Swift our last remaining real popstar?\". i-D. Archived from the original on May 6, 2023. Retrieved December 3, 2022.\\n Seisdedos, Iker (December 27, 2022). \"Pop music in the era of Taylor Swift: Behind the success of today\\'s biggest star\". El Pa\\xc3\\xads. Retrieved December 28, 2022.\\n Sanders, Sam (October 20, 2022). \"Taylor Swift Is the Greatest Self-Portraitist of Our Time\". Vulture. Retrieved October 21, 2022.\\n \"Taylor Swift\\'s 40 Biggest Hot 100 Hits\". Billboard. March 23, 2022. Retrieved April 12, 2022.\\n Leonard, Devin (November 12, 2014). \"Taylor Swift Is the Music Industry\". Bloomberg News. Archived from the original on March 13, 2016. Retrieved August 11, 2020.\\n Zoladz, Lindsay (December 30, 2019). \"What Were the 2010s?\". New York. Retrieved November 22, 2020.\\n Savaira, Augusta (November 23, 2022). \"Welcome to \\'Swiftonomics\\': What Taylor Swift reveals about the U.S. economy\". Los Angeles Times. Retrieved November 23, 2022.\\n Vincent, Alice (November 3, 2017). \"Taylor Swift: the rise, fall and re-invention of America\\'s sweetheart\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved July 5, 2020.\\n Fischer & Seidman 2016, p. 529.\\n Wahi, Sukriti (March 3, 2021). \"Every Time Taylor Swift Perfectly Shut Down A Sexist Interview Question\". Elle. Retrieved November 4, 2021.\\n Davis, Allison P. (June 28, 2018). \"The Taylor Swift Slut-Shaming Continues\". The Cut. Retrieved November 4, 2021.\\n Meddings, Jacqui (October 31, 2014). \"Taylor Swift is our new cover goddess\". Cosmopolitan. Archived from the original on July 14, 2016. Retrieved August 2, 2016.\\n Chiu, Allyson (August 3, 2022). \"Celebrities use private jets excessively. It\\'s a climate nightmare\". Washington Post. ISSN 0190-8286. Retrieved February 14, 2024.\\n Mendez, Lola (February 14, 2024). \"Taylor Swift claims she offsets her travel carbon footprint - how does that work?\". BBC. Retrieved February 14, 2024.\\n O\\'Malley, Isabella (February 2, 2024). \"Why Taylor Swift\\'s globe-trotting in private jets is getting scrutinized\". Associated Press. Retrieved February 6, 2024.\\n \"Taylor Swift and Travis Kelce romance is bad for the planet. Couple burns a whopping $70,779 jet fuel in the last three months\". The Economic Times. December 21, 2023. Retrieved February 6, 2024.\\n Blistein, Jon (February 6, 2024). \"Taylor Swift\\'s Lawyers Threaten Private Jet Flight Tracker Over \\'Stalking and Harassing Behavior\\'\". Rolling Stone. Retrieved February 6, 2024.\\n Matza, Max (February 7, 2024). \"Taylor Swift threatens to sue student who tracks her private jet\". BBC News. Retrieved February 7, 2024.\\n Benitez-Eves, Tina (July 2, 2022). \"Man Arrested for Harassing Taylor Swift, Attempting to Break Into Her New York City Apartment\". American Songwriter. Retrieved June 10, 2023.\\n Kreps, Daniel (July 2, 2022). \"Brooklyn Man Arrested on Charges of Stalking Taylor Swift\". Rolling Stone. Retrieved June 10, 2023.\\n Della Cava, Marco; Ryan, Patrick (August 31, 2023). \"Taylor Swift is \\'in a class of her own right now,\\' as Eras tour gives way to Eras movie\". USA Today. Archived from the original on September 1, 2023. Retrieved September 1, 2023.\\n Crummy, Colin (March 13, 2014). \"Detwanging country music: how Nashville took the UK\". The Guardian. Retrieved September 23, 2021.\\n Roland, Tom (July 7, 2016). \"Love Story: The Impact of Taylor Swift\\'s First Decade in Music\". Billboard. Archived from the original on July 14, 2016. Retrieved July 7, 2016.\\n Tannembaum, Rob (September 12, 2014). \"4 Ways Billboard Woman of the Year Taylor Swift Changed Country Music\". Billboard. Retrieved December 31, 2019.\\n O\\'Connor, Roisin (October 18, 2018). \"Half of all new guitar players are women, finds study\". The Independent. Archived from the original on May 26, 2022. Retrieved October 20, 2020.\\n Wang, Amy X. (May 22, 2018). \"Guitars Are Getting More Popular. So Why Do We Think They\\'re Dying?\". Rolling Stone. Retrieved October 20, 2020.\\n \"The 200 Most Important Artists of Pitchfork\\'s First 25 Years\". Pitchfork. October 4, 2021. Retrieved October 4, 2021.\\n Lipshutz, Jason (December 16, 2021). \"Billboard\\'s Greatest Pop Stars of 2021: No. 1 \\xe2\\x80\\x94 Taylor Swift\". Billboard. Retrieved December 16, 2021.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Retrieved August 18, 2021.\\n Gormely, Ian (December 3, 2014). \"Taylor Swift leads poptimism\\'s rebirth\". The Guardian. Retrieved August 2, 2021.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved September 7, 2023.\\n Friedman, Vanessa (April 24, 2023). \"Taylor Swift and the Sparkling Trap of Constant Reinvention\". The New York Times. Retrieved September 7, 2023.\\n Rowley, Glenn (March 9, 2023). \"Taylor Swift\\'s \\'All Too Well (10 Minute Version)\\' Is Now a Class at Stanford University\". Billboard. Retrieved March 10, 2023.\\n Ahlgrim, Callie (March 18, 2022). \"How Taylor Swift inspired a new generation of singer-songwriters, in their own words\". Business Insider. Retrieved March 18, 2022.\\n Sutherland, Mark (December 13, 2019). \"\\'I come with opinions about how we can better our industry\\': Taylor Swift\". Music Week. Retrieved April 27, 2020.\\nSisario, Ben (November 6, 2014). \"Sales of Taylor Swift\\'s \\'1989\\' Intensify Streaming Debate\". The New York Times. Retrieved August 11, 2020.\\nRosseinsky, Katie (October 29, 2021). \"How Taylor Swift is changing the music industry a re-record at a time\". Evening Standard. Archived from the original on April 9, 2021. Retrieved April 23, 2021.\\n Smith, Neil (June 22, 2015). \"Five ways Taylor Swift is changing the world\". BBC News. Retrieved December 31, 2019.\\n Brooks, Dave (November 3, 2022). \"How Taylor Swift Changed the Course of the Concert Ticketing Businesses\". Billboard. Retrieved November 4, 2022.\\n Sources on Swift and millennial era\\n \\xe2\\x80\\xa2 McDuling, John (November 3, 2014). \"Why Taylor Swift is the most important artist of the millennial era\". Quartz. Archived from the original on June 2, 2021. Retrieved April 2, 2021.\\n \\xe2\\x80\\xa2 \"What Taylor Swift tells us about millennials\". National Post. October 6, 2015. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Kidd, Joel (March 2, 2020). \"Taylor Swift Is the Surprising Face of Millennial Anxiety\". The Walrus. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Winograd, Morley; D. Hais, Michael (January 29, 2010). \"Move over Kanye West, Taylor Swift and the Millennial generation are taking over music\". The Christian Science Monitor. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 \"The 17 Musicians Gen Z & Millennials Say Represent Their Generations\". YPulse. October 8, 2019. Retrieved October 30, 2021.\\n \\xe2\\x80\\xa2 Nicolau, Elena (November 29, 2022). \"How Taylor Swift is transforming millennial weddings\". Today. Retrieved November 30, 2022.\\n \\xe2\\x80\\xa2 \"Taylor Swift is peak millennial vibes\". NPR. October 21, 2022. Retrieved December 19, 2022.\\n VanDerWerff, Emily (July 31, 2020). \"Taylor Swift is the millennial Bruce Springsteen\". Vox. Retrieved October 30, 2021.\\n Ditum, Sarah (October 8, 2022). \"Taylor Swift: the Bob Dylan of our age\". The Times. Archived from the original on December 24, 2022. Retrieved October 9, 2022.\\n \"Taylor Swift Will Receive First-Ever Woman of the Decade Honor at Billboard\\'s Women in Music\". Billboard. November 18, 2019. Retrieved December 31, 2019.\\n Elizabeth, De (November 24, 2019). \"Taylor Swift Is Named Artist of the Decade at AMAs 2019\". Teen Vogue. Retrieved December 31, 2019.\\n Kemp, Sam (January 27, 2022). \"The surprising Paul McCartney song inspired by Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Golsen, Tyler (May 14, 2023). \"Mick Jagger discusses his performance with Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Brandle, Lars (January 28, 2015). \"Taylor Swift on Madonna Praise: \\'Now I\\'m Dead\\'\". Billboard. Retrieved July 5, 2023.\\n Aniftos, Rania (January 27, 2021). \"Dolly Parton Says Taylor Swift Is \\'Magnificent\\' Following Damon Albarn Feud: \\'She Knows Who She Is\\'\". Billboard. Retrieved July 5, 2023.\\n Graff, Gary (October 31, 2021). \"Taylor Swift Helps Induct Carole King, Sings \"Will You Love Me Tomorrow\" at Rock Hall Ceremony\". The Hollywood Reporter. Retrieved September 20, 2022.\\n Skinner, Tom (November 2, 2022). \"Bruce Springsteen praises Taylor Swift\\'s new album \\'Midnights\\': \"She\\'s a tremendous writer\"\". NME. Retrieved July 5, 2023.\\n Ruggieri, Melissa (November 3, 2021). \"Billy Joel says Taylor Swift is like the Beatles of her generation: \\'She knows music\\'\". USA Today. Retrieved July 5, 2023.\\n Williams, Joe (July 5, 2023). \"Ringo Starr says Taylor Swift is \"biggest star in the world\"\". Far Out. Retrieved July 5, 2023.\\n Iasimone, Ashley (November 12, 2023). \"Britney Spears Shares Throwback Photo With Taylor Swift From 20 Years Ago: \\'Most Iconic Pop Woman of Our Generation\\'\". Billboard. Retrieved November 12, 2023.\\n Lutz, Ashley (October 8, 2022). \"Taylor Swift\\'s new album proves she is a business genius\". Fortune. Retrieved October 8, 2022.\\n Steele, Anne (July 1, 2023). \"How to Succeed in Business Like Taylor Swift\". The Wall Street Journal. Retrieved July 2, 2023.\\n Lynch, Joe (January 31, 2024). \"Taylor Swift Tops Billboard\\'s Annual Power 100 List for the First Time\". Billboard. Retrieved January 31, 2024.\\n Curto, Justin (December 22, 2020). \"Did 2020 Kill the Long, Fancy Pop-Album Rollout for Good?\". Vulture. Archived from the original on December 23, 2020. Retrieved December 27, 2020.\\n Leight, Elias (July 23, 2020). \"Taylor Swift Finally Abandoned the Traditional Album Rollout\". Rolling Stone. Retrieved September 15, 2022.\\n Kheraj, Alim (November 9, 2022). \"The curate\\'s Easter egg: how Taylor Swift turned pop into a multiplayer puzzle\". The Guardian. Retrieved November 9, 2022.\\n Tannenbaum, Emily. \"Taylor Swift Hid a Bunch of Easter Eggs in the All Too Well Short Film\". Glamour. Retrieved May 13, 2022.\\n Suskind, Alex (May 9, 2019). \"New Reputation: Taylor Swift shares intel on TS7, fan theories, and her next era\". Entertainment Weekly. Archived from the original on August 12, 2019. Retrieved May 13, 2022.\\n Yahr, Emily (December 26, 2022). \"2022: The year in review (Taylor\\'s version)\". The Washington Post. Retrieved December 26, 2022.\\n Stein, Jeannine (November 18, 2011). \"Taylor Swift weighs in on being a role model\". Los Angeles Times. Archived from the original on July 15, 2012. Retrieved April 18, 2012.\\n Collins, Katie (June 8, 2018). \"How Taylor Swift flipped online fandom on its head for the better\". CNET. Retrieved March 19, 2023.\\n King, Ashley (July 21, 2023). \"Taylor Swift\\'s 13 Management is Rarely Discussed, Changes Abound\". Digital Music News. Retrieved September 23, 2023.\\n Yotka, Steff (August 20, 2019). \"Taylor Swift and Stella McCartney Reveal Their Lover Fashion Collaboration\". Vogue. Retrieved October 20, 2020.\\n Kaplan, Julee (January 29, 2009). \"Taylor Swift Launches Sundress Line \\xe2\\x80\\x93 Ready-to-Wear and Sportswear \\xe2\\x80\\x93 Media\". Women\\'s Wear Daily. Archived from the original on July 29, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift to have greeting card line\". USA Today. November 18, 2009. Archived from the original on December 14, 2009. Retrieved August 7, 2012.\\n Serpe, Gina (October 29, 2008). \"Taylor Swift Gets All Dolled Up\". E! News (Blog). Archived from the original on March 5, 2011. Retrieved September 20, 2010.\\n Mychaskiw, Marianne (October 1, 2014). \"Taylor Swift Is Launching Her Fourth Fragrance!\". InStyle. Archived from the original on January 7, 2015. Retrieved December 3, 2015.\\n McIntyre, Hugh. \"Taylor Swift Has Signed A Multi-Year Deal With AT&T\". Forbes. Archived from the original on September 11, 2018. Retrieved September 11, 2018.\\n Chiu, Melody (July 26, 2019). \"Watch Taylor Swift Hilariously Moonlight \\xe2\\x80\\x94 and Fail \\xe2\\x80\\x94 as a Bartender and Server in New Commercial\". People. Retrieved May 7, 2022.\\n \"Taylor Swift: NHL\\'s New Spokesperson\". AOL. February 3, 2009. Archived from the original on February 7, 2009. Retrieved September 20, 2010.\\n \"Sony Electronics News and Information\". Sony. April 26, 2010. Retrieved August 7, 2012.\\n Lee, Ashley (October 27, 2014). \"Taylor Swift Named New York City Welcome Ambassador\". Billboard. Retrieved September 26, 2023.\\n Kaufman, Gil (January 20, 2022). \"Taylor Swift Tapped as First Record Store Day Global Ambassador\". Billboard. Retrieved January 20, 2022.\\n Snapes, Laura (August 24, 2019). \"Taylor Swift: \\'Pop music can feel like it\\'s The Hunger Games, and like we\\'re gladiators\\'\". The Guardian. Archived from the original on August 24, 2019. Retrieved August 24, 2019.\\n \"The Original 300+ Time\\'s Up Signatories\". Time\\'s Up. October 21, 2019. Archived from the original on May 3, 2020. Retrieved May 20, 2020.\\n Mier, Tom\\xc3\\xa1s (June 24, 2022). \"Taylor Swift Says She\\'s \\'Absolutely Terrified\\' After Roe v. Wade Decision\". Rolling Stone. Retrieved June 24, 2022.\\n \"Taylor Swift sends \\'love\\' and \\'respect\\' to LGBT fans\". BBC News. June 4, 2018. Archived from the original on November 14, 2018. Retrieved December 15, 2018.\\n Kreps, Daniel (June 1, 2019). \"Taylor Swift Urges Senate to Pass Equality Act in Letter to Tennessee Senator\". Rolling Stone. Retrieved August 12, 2020.\\n Lynch, Joe (June 26, 2020). \"Taylor Swift Calls Out U.S. Census for Transgender Erasure\". Billboard. Retrieved June 29, 2020.\\n Lynch, Joe (June 14, 2019). \"Taylor Swift Wishes NYC\\'s Stonewall Inn \\'Happy Pride\\' at Surprise Set\". Billboard. Retrieved August 7, 2022.\\n Aniftos, Rania (April 8, 2019). \"Taylor Swift Donates $113,000 to Tennessee Equality Project to Fight Anti-LGBTQ Bills\". Billboard. Archived from the original on April 9, 2019. Retrieved April 9, 2019.\\n \"Taylor Swift Makes a Generous Donation to GLAAD in Support of the LGBTQ Community\" (Press release). GLAAD. June 1, 2019. Archived from the original on June 1, 2019. Retrieved June 1, 2019.\\n Lynch, John (March 23, 2018). \"Taylor Swift Shares Support For Gun a Control Reform In Instagram Post\". Business Insider. Retrieved May 20, 2020.\\n Hiatt, Brian (September 18, 2019). \"The Rolling Stone Interview Taylor Swift\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved September 19, 2019.\\n Bailey, Alyssa (June 9, 2020). \"Taylor Swift Tweeted About the Black Lives Matter Movement and Urged People to Vote\". Elle. Retrieved July 27, 2020.\\n Kreps, Daniel (June 12, 2020). \"Taylor Swift Calls for Removal of Monuments That \\'Celebrate Racist Historical Figures\\'\". Rolling Stone. Retrieved August 12, 2020.\\n France, Lisa Respers (June 19, 2020). \"Taylor Swift got educated on Juneteenth and wants you to be, too\". CNN. Retrieved July 27, 2020.\\n Haag, Matthew (October 9, 2018). \"Voter Registrations Spike as Deadlines Loom. Taylor Swift Had Something to Do With It\". The New York Times. Archived from the original on October 9, 2018. Retrieved October 10, 2018.\\n Stevens, Matt (October 7, 2020). \"Taylor Swift, leaning further into politics, endorses Joe Biden and Kamala Harris\". The New York Times. Archived from the original on October 7, 2020. Retrieved October 8, 2020.\\n Schuster, Steve (May 19, 2023). \"Taylor Swift comes out against Trump, again\". Wisconsin Law Journal. Retrieved November 4, 2023.\\n Dellatto, Marisa (October 27, 2023). \"Taylor Swift\\'s New Era: The Pop Star Becomes A Billionaire\". Forbes. Retrieved October 29, 2023.\\n Pendleton, Devon; Ballentine, Claire; Patino, Marie; Whiteaker, Chloe; Li, Diana (October 26, 2023). \"Taylor Swift Vaults Into Billionaire Ranks With Blockbuster Eras Tour\". Bloomberg News. Retrieved October 26, 2023.\\n Greenburg, Zack O\\'Malley (November 2, 2016). \"The World\\'s Highest-Paid Women in Music 2016\". Forbes. Archived from the original on May 25, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (December 6, 2019). \"The World\\'s Top-Earning Musicians Of 2019\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 9, 2022). \"The Highest-Paid Entertainers 2022\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 13, 2023). \"The World\\'s 10 Highest-Paid Entertainers\". Forbes. Retrieved July 29, 2022.\\n \"Highest annual earnings ever for a female pop star\". Guinness World Records. Archived from the original on November 12, 2017. Retrieved November 15, 2017.\\n Mercuri, Monica (July 10, 2019). \"Taylor Swift Is The World\\'s Highest-Paid Celebrity With $185 Million in 2019\". Forbes. Archived from the original on July 12, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (July 11, 2016). \"Taylor Swift Is The World\\'s Top-Earning Celebrity With $170 Million in 2016\". Forbes. Archived from the original on August 3, 2016. Retrieved August 3, 2016.\\n Greenburg, Zack O\\'Malley (December 23, 2019). \"From Taylor Swift To Dr. Dre: The 10 Top-Earning Musicians Of The Decade\". Forbes. Archived from the original on December 30, 2019. Retrieved December 31, 2019.\\n Silva, Rachel; Tonelli, Lucia (October 3, 2023). \"Inside Taylor Swift\\'s 8 Incredible Homes\". Elle Decoration. Retrieved October 12, 2023.\\n Dailey, Hannah; Aniftos, Rania (December 21, 2023). \"A Timeline of Taylor Swift\\'s Generosity\". Billboard. Retrieved December 24, 2023.\\n Couch, Robbie (March 2, 2015). \"Taylor Swift Named Most Charitable Celeb For Supporting Feminist Causes, Education, More\". HuffPost. Archived from the original on October 21, 2016. Retrieved August 2, 2016.\\n Pacella, Megan (June 13, 2012). \"Taylor Swift Receives Star of Compassion Award\". Taste of Country. Archived from the original on July 13, 2015. Retrieved November 27, 2015.\\n Cooper, Brittany Joy (March 27, 2012). \"Taylor Swift to Receive \\'Big\\' Honor From Michelle Obama at Kids\\' Choice Awards\". Taste of Country. Archived from the original on July 12, 2012. Retrieved August 30, 2012.\\n Lee, Ken (August 9, 2009). \"Taylor Swift donates $100,000 to victims of Iowa Flood\". People. Archived from the original on June 7, 2009. Retrieved September 9, 2009.\\n \"How Taylor Swift helps people worldwide\". Borgen. February 2, 2023. Retrieved June 17, 2023.\\n Moran, Jonathon (March 8, 2009). \"Kylie to play at Sound Relief with Coldplay, Midnight Oil\". The Daily Telegraph. Archived from the original on October 10, 2016. Retrieved December 30, 2009.\\n \"Taylor Swift donates $500,000 after Nashville floods\". BBC News. May 7, 2010. Retrieved May 7, 2022.\\n Lewis, Randy (May 23, 2011). \"Taylor Swift benefit concert raises more than $750,000 for tornado victims\". Los Angeles Times (Blog). Archived from the original on April 6, 2012. Retrieved April 18, 2012.\\n \"Taylor Swift gives $1m to help Louisiana flood relief efforts\". The Guardian. August 17, 2016. Archived from the original on August 17, 2016. Retrieved August 17, 2016.\\n Muhammad, Latifah (December 9, 2016). \"Taylor Swift Donates $100,000 to Dolly Parton Fire Fund\". Entertainment Tonight. Archived from the original on January 13, 2017. Retrieved January 11, 2017.\\n Blair, Olivia (September 4, 2019). \"Taylor Swift Makes \\'Very Sizeable Donation\\' To A Food Bank in Houston After Hurricane Harvey\". Elle. Archived from the original on July 2, 2019. Retrieved July 3, 2019.\\n Rowley, Glenn (April 13, 2023). \"Taylor Swift Donated Enough to Feed \\'Over 125,000\\' at Tampa Food Bank Ahead of The Eras Tour Stop\". Billboard. Archived from the original on April 21, 2023. Retrieved May 11, 2023.\\n Dementri, Nikki (May 16, 2023). \"\\'It\\'s a blessing\\': South Philly pizza, cheesesteak shop thankful for T. Swift\\'s business\". CBS News. Archived from the original on May 17, 2023. Retrieved May 17, 2023.\\n Grow, Kory (August 3, 2023). \"Taylor Swift Is Helping Truck Drivers Buy First Homes With $100,000 Bonuses\". Rolling Stone. Retrieved August 3, 2023.\\n Paulson, Dave (March 5, 2020). \"Taylor Swift donates $1 million for Tennessee tornado relief\". The Tennessean. Retrieved March 5, 2020.\\n Ruggieri, Melissa. \"Taylor Swift donates $1 million to help communities ravaged by Tennessee tornadoes\". The Tennessean. Retrieved December 11, 2023.\\n \"Nashville Songwriters Hall of Fame\". Nashville Songwriters Foundation. Retrieved July 2, 2020.\\n \"Taylor Swift becomes namesake of Hendersonville High School auditorium\". The Tennessean. September 23, 2010. Archived from the original on July 12, 2015. Retrieved May 29, 2011.\\n McKinley, James C. (May 17, 2012). \"Taylor Swift to Donate $4 Million to the Country Hall of Fame\". The New York Times. Archived from the original on June 5, 2012. Retrieved July 1, 2012.\\n Golden, Zara (October 1, 2012). \"And The Taylor Swift VH1 Storytellers Contest Winner Is ... Harvey Mudd College!\". VH1. Archived from the original on July 12, 2015. Retrieved April 21, 2014.\\n \"$100,000 Gift from Taylor Swift Provides Vital Support to Nashville Symphony\". Nashville Symphony. December 16, 2013. Retrieved April 7, 2020.\\n Sanz, Cynthia (December 14, 2009). \"Taylor Swift Gives Big as She Turns 20 \\xe2\\x80\\x93 Good Deeds, Taylor Swift\". People. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\nUrban, Mike (October 14, 2011). \"Taylor Swift donates 6,000 books to Reading Library\". Reading Eagle. Archived from the original on May 12, 2012. Retrieved May 29, 2012.\\nZhu, Danielle (November 17, 2015). \"Taylor Swift partners with Scholastic to donate 25,000 books\". Entertainment Weekly. Archived from the original on August 14, 2016. Retrieved August 3, 2016.\\n Cooper, Brittany Joy (February 2, 2012). \"Taylor Swift Donates 14,000 Books to Nashville Public Library\". Taste of Country. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\n \"Taylor Swift Helps With \\'Delete Online Predators\\' Campaign\". Country Weekly. September 18, 2007. Archived from the original on January 16, 2016. Retrieved November 29, 2015.\\n \"Taylor Swift Teams Up With UNICEF Tap Project Initiative\". Taste of Country. March 17, 2011. Archived from the original on May 1, 2013. Retrieved August 7, 2012.\\n\"Taylor Swift Among Participants in MusiCares Back To School Charity Auction\". RTTNews. July 28, 2011. Archived from the original on May 17, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift and ACM Lifting Lives present $50,000 donation to St. Jude\". The Tennessean. June 13, 2011. Archived from the original on July 12, 2015. Retrieved May 29, 2012.\\n \"New and Hot Video: Taylor Swift Debuts \\'Ronan\\' at Stand Up To Cancer Benefit\". Rolling Stone. September 8, 2012. Archived from the original on September 13, 2012. Retrieved September 14, 2012.\\n Knight, Joey (May 17, 2014). \"Dick Vitale holds annual fundraiser for pediatric cancer\". Tampa Bay Times. Archived from the original on July 14, 2014. Retrieved July 30, 2014.\\n George, John. \"Taylor Swift donates $50K to CHOP to help teens with cancer\". The Business Journals (Blog). Archived from the original on July 12, 2015. Retrieved June 5, 2014.\\n Hardiman, Tim (April 20, 2007). \"Taylor Swift Encourages Teen Volunteers\". CMT. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n Vagianos, Alanna (August 17, 2017). \"This Is How Taylor Swift\\'s Donations Will Impact Sexual Assault Survivors\". HuffPost. Retrieved April 7, 2020.\\n Peppin, Hayley (March 24, 2020). \"Taylor Swift responded to the leaked \\'Famous\\' phone call with Kanye West by urging fans to instead focus on coronavirus aid\". Insider Inc. Retrieved May 13, 2020.\\n Grow, Kory (April 1, 2020). \"How Taylor Swift Is Helping a Small Nashville Record Store Survive\". Rolling Stone. Retrieved August 1, 2023.\\n Hissong, Samantha (August 20, 2020). \"Taylor Swift Starts Frenzy at Indie Record Stores With Surprise Signed \\'Folklore\\' CDs\". Rolling Stone. Retrieved August 1, 2023.\\n Havens, Lyndsey (April 18, 2020). \"Taylor Swift Sings Heart-Rending \\'Soon You\\'ll Get Better\\' During \\'One World\\' Concert\". Billboard. Retrieved April 19, 2020.\\n @RAINN (April 8, 2021). \"In 2017, @taylorswift13 fearlessly shared her story of sexual assault in court. In honor of Sexual Assault Awareness & Prevention Month & #RAINNDay, Taylor Swift has made a generous gift to RAINN. Thank you for helping us ensure all survivors are believed and supported! https://t.co/oE7P1BVnNJ\" (Tweet). Retrieved April 21, 2021 \\xe2\\x80\\x93 via Twitter.\\n \"Taylor Swift\\'s cash gift helps student take up degree\". BBC News. August 20, 2020. Retrieved November 9, 2020.\\n M Holloway, Sophie (December 14, 2023). \"Taylor Swift and Selena Gomez among celebrities attending Gaza aid fundraiser\". The Independent.\\nCited literature\\nFischer, Nancy; Seidman, Steven (2016). Introducing the New Sexuality Studies (3rd ed.). Routledge. ISBN 978-1-317-44918-8. OCLC 953030187.\\nFogarty, Mary; Arnold, Gina (2021). \"Are You Ready for It? Re-Evaluating Taylor Swift\". Contemporary Music Review. 40 (1): 1\\xe2\\x80\\x9310. doi:10.1080/07494467.2021.1976586.\\nFulford, Phyllis (2014). An Idiots Guide: Singing Second Edition. Penguin Publishing. ISBN 978-1-61564-622-7.\\nHughes, Charles (2017). \"Country Music and the Recording Industry\". In Stimeling, Travis D. (ed.). The Oxford Handbook of Country Music. Oxford University Press. pp. 205\\xe2\\x80\\x93228. ISBN 978-0-19-024817-8.\\nJepson, Louisa (2013). Taylor Swift. Simon & Schuster. ISBN 978-1-4711-3087-8.\\nMcNutt, Myles (2020). \"From \\'Mine\\' to \\'Ours\\': Gendered Hierarchies of Authorship and the Limits of Taylor Swift\\'s Paratextual Feminism\". Communication, Culture and Critique. 13 (1): 72\\xe2\\x80\\x9391. doi:10.1093/ccc/tcz042.\\nMcGrath, John (2023). \"The Return to Craft: Taylor Swift, Nostalgia, and Covid-19\". Popular Music and Society. 46 (1): 70\\xe2\\x80\\x9384. doi:10.1080/03007766.2022.2156761.\\nPerone, James E. (2017). The Words and Music of Taylor Swift. The Praeger Singer-Songwriter Collection. ABC-Clio. ISBN 978-1-4408-5294-7.\\nProvenzano, Catherine (2018). \"Auto-Tune, Labor, and the Pop-Music Voice\". In Fink, Robert; Latour, Melinda; Wallmark, Zachary (eds.). The Relentless Pursuit of Tone: Timbre in Popular Music. Oxford University Press. pp. 159\\xe2\\x80\\x93182. ISBN 978-0-19-998522-7.\\nSloan, Nate (2021). \"Taylor Swift and the Work of Songwriting\". Contemporary Music Review. Routledge. 40 (1): 11\\xe2\\x80\\x9326. doi:10.1080/07494467.2021.1945226. S2CID 237695045.\\nExternal links\\nTaylor Swift\\nat Wikipedia\\'s sister projects\\nMedia from Commons\\nNews from Wikinews\\nQuotations from Wikiquote\\nData from Wikidata\\nOfficial website Edit this at Wikidata\\nTaylor Swift at AllMusic Edit this at Wikidata\\nTaylor Swift discography at Discogs Edit this at Wikidata\\nTaylor Swift at IMDb Edit this at Wikidata\\nTaylor Swift discography at MusicBrainz Edit this at Wikidata\\nvte\\nTaylor Swift\\nvte\\nTaylor Swift songs\\nAwards for Taylor Swift\\nvte\\nIFPI Global Year-End Charts\\nvte\\nBillboard Year-End number one albums\\nvte\\nBillboard Year-End Top Artist\\nPortals:\\n Biography\\nicon Pop music\\nflag Pennsylvania\\nflag United States\\nAuthority control databases Edit this at Wikidata\\nCategories: Taylor Swift1989 birthsLiving people21st-century American actresses21st-century American philanthropists21st-century American songwriters21st-century American women guitarists21st-century American women pianists21st-century American women singers21st-century women philanthropistsActresses from Nashville, TennesseeActresses from PennsylvaniaAmerican acoustic guitaristsAmerican billionairesAmerican country banjoistsAmerican country guitaristsAmerican country pianistsAmerican country record producersAmerican country singer-songwritersAmerican feministsAmerican film actressesAmerican folk guitaristsAmerican folk musiciansAmerican folk singersAmerican mezzo-sopranosAmerican multi-instrumentalistsAmerican music video directorsAmerican people of English descentAmerican people of German descentAmerican people of Italian descentAmerican people of Scottish descentAmerican pop guitaristsAmerican pop pianistsAmerican synth-pop musiciansAmerican television actressesAmerican voice actressesAmerican women country singersAmerican women guitaristsAmerican women philanthropistsAmerican women pop singersAmerican women record producersAmerican women rock singersAmerican women singer-songwritersAmerican women songwritersBig Machine Records artistsBrit Award winnersChristians from TennesseeCountry musicians from PennsylvaniaCountry musicians from TennesseeCountry pop musiciansFemale billionairesFemale music video directorsFeminist musiciansFilm directors from PennsylvaniaFilm directors from TennesseeGrammy Award winnersGuitarists from PennsylvaniaGuitarists from TennesseeMTV Europe Music Award winnersMTV Video Music Award winnersNME Awards winnersPeople from Bucks County, PennsylvaniaPhilanthropists from PennsylvaniaPhilanthropists from TennesseePrimetime Emmy Award winnersRCA Records artistsRecord producers from PennsylvaniaRecord producers from TennesseeRepublic Records artistsSinger-songwriters from PennsylvaniaSinger-songwriters from TennesseeSingers from Nashville, TennesseeSony Music Publishing artistsSynth-pop singersTime Person of the YearUniversal Music Group artistsBaldi family\\nThis page was last edited on 16 February 2024, at 11:33 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\\xc2\\xae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\\nWikimedia FoundationPowered by MediaWiki'\n185767\n53311\ncompression ratio: 3.48\nmerges: {(101, 32): 256, (44, 32): 257, (100, 32): 258, (46, 32): 259, (114, 32): 260, (50, 48): 261, (115, 32): 262, (105, 110): 263, (111, 110): 264, (114, 105): 265, (116, 32): 266, (116, 104): 267, (101, 258): 268, (257, 261): 269, (97, 110): 270, (97, 114): 271, (101, 260): 272, (121, 32): 273, (97, 108): 274, (267, 256): 275, (118, 268): 276, (119, 105): 277, (101, 114): 278, (264, 32): 279, (277, 102): 280, (82, 101): 281, (83, 280): 282, (111, 260): 283, (99, 104): 284, (269, 49): 285, (111, 109): 286, (98, 272): 287, (32, 275): 288, (97, 121): 289, (101, 110): 290, (111, 114): 291, (274, 32): 292, (101, 109): 293, (46, 10): 294, (265, 101): 295, (263, 103): 296, (269, 50): 297, (116, 105): 298, (289, 108): 299, (34, 259): 300, (108, 108): 301, (84, 299): 302, (116, 295): 303, (294, 32): 304, (116, 111): 305, (259, 281): 306, (306, 303): 307, (307, 276): 308, (302, 283): 309, (101, 115): 310, (309, 282): 311, (117, 115): 312, (114, 286): 313, (293, 287): 314, (41, 259): 315, (65, 114): 316, (102, 313): 317, (315, 34): 318, (270, 258): 319, (114, 101): 320, (111, 117): 321, (111, 265): 322, (111, 102): 323, (103, 263): 324, (296, 32): 325, (284, 105): 326, (93, 32): 327, (324, 292): 328, (317, 288): 329, (322, 328): 330, (104, 256): 331, (316, 326): 332, (332, 276): 333, (329, 330): 334, (333, 334): 335, (335, 279): 336, (259, 336): 337, (97, 32): 338, (115, 116): 339, (105, 99): 340, (46, 91): 341, (101, 99): 342, (105, 301): 343, (39, 262): 344, (311, 266): 345, (111, 118): 346, (97, 116): 347, (97, 262): 348, (101, 262): 349, (74, 117): 350, (323, 32): 351, (305, 32): 352, (117, 109): 353, (84, 331): 354, (271, 100): 355, (263, 32): 356, (270, 32): 357, (101, 108): 358, (297, 51): 359, (271, 273): 360, (267, 32): 361, (97, 109): 362, (108, 273): 363, (111, 112): 364, (311, 116): 365, (116, 114): 366, (105, 115): 367, (104, 272): 368, (111, 32): 369, (117, 360): 370, (78, 346): 371, (312, 340): 372, (371, 314): 373, (101, 119): 374, (97, 266): 375, (108, 32): 376, (58, 32): 377, (98, 111): 378, (282, 266): 379, (68, 342): 380, (105, 116): 381, (105, 103): 382, (66, 343): 383, (49, 48): 384, (97, 115): 385, (264, 103): 386, (79, 99): 387, (97, 298): 388, (83, 116): 389, (387, 305): 390, (390, 287): 391, (97, 99): 392, (111, 119): 393, (380, 314): 394, (383, 378): 395, (97, 100): 396, (108, 101): 397, (117, 114): 398, (102, 283): 399, (32, 40): 400, (297, 50): 401, (117, 103): 402, (284, 32): 403, (115, 266): 404, (321, 110): 405, (98, 353): 406, (111, 108): 407, (312, 266): 408, (101, 98): 409, (77, 97): 410, (350, 363): 411, (318, 345): 412, (107, 32): 413, (278, 115): 414, (93, 91): 415, (65, 402): 416, (416, 408): 417, (105, 100): 418, (297, 49): 419, (109, 101): 420, (101, 112): 421, (261, 49): 422, (50, 51): 423, (285, 50): 424, (101, 271): 425, (269, 261): 426, (73, 110): 427, (102, 105): 428, (110, 256): 429, (395, 355): 430, (265, 116): 431, (104, 105): 432, (372, 32): 433, (304, 34): 434, (78, 374): 435, (100, 105): 436, (65, 112): 437, (285, 57): 438, (114, 111): 439, (39, 32): 440, (115, 257): 441, (350, 429): 442, (323, 288): 443, (99, 291): 444, (50, 49): 445, (49, 57): 446, (105, 109): 447, (290, 32): 448, (409, 114): 449, (290, 116): 450, (111, 301): 451, (77, 271): 452, (265, 99): 453, (277, 361): 454, (44, 91): 455, (70, 449): 456, (456, 370): 457, (365, 344): 458, (300, 430): 459, (101, 97): 460, (285, 54): 461, (421, 116): 462, (410, 273): 463, (285, 53): 464, (437, 265): 465, (465, 376): 466, (108, 256): 467, (65, 119): 468, (388, 264): 469, (83, 462): 470, (470, 314): 471, (114, 97): 472, (274, 406): 473, (67, 104): 474, (118, 256): 475, (310, 266): 476, (74, 270): 477, (50, 50): 478, (477, 370): 479, (405, 366): 480, (382, 104): 481, (300, 354): 482, (359, 304): 483, (49, 51): 484, (65, 108): 485, (101, 116): 486, (310, 115): 487, (452, 403): 488, (117, 116): 489, (119, 431): 490, (108, 111): 491, (115, 386): 492, (226, 128): 493, (271, 258): 494, (48, 32): 495, (117, 108): 496, (50, 52): 497, (105, 262): 498, (298, 99): 499, (97, 103): 500, (34, 32): 501, (65, 110): 502, (49, 56): 503, (102, 291): 504, (480, 273): 505, (65, 420): 506, (506, 453): 507, (32, 84): 508, (115, 296): 509, (119, 348): 510, (49, 50): 511, (111, 99): 512, (364, 32): 513, (49, 55): 514, (108, 105): 515, (77, 433): 516, (101, 257): 517, (49, 52): 518, (117, 110): 519, (93, 10): 520, (435, 32): 521, (118, 105): 522, (468, 355): 523, (269, 48): 524, (83, 331): 525, (116, 278): 526, (98, 273): 527, (115, 331): 528, (97, 105): 529, (271, 116): 530, (101, 301): 531, (49, 49): 532, (111, 116): 533, (97, 98): 534, (444, 100): 535, (83, 112): 536, (121, 257): 537, (308, 411): 538, (308, 417): 539, (321, 114): 540, (270, 100): 541, (285, 55): 542, (507, 357): 543, (101, 101): 544, (115, 117): 545, (327, 319): 546, (49, 53): 547, (50, 54): 548, (111, 103): 549, (101, 118): 550, (50, 57): 551, (419, 304): 552, (87, 105): 553, (32, 77): 554, (320, 99): 555, (83, 386): 556, (50, 53): 557, (50, 55): 558, (261, 48): 559, (389, 264): 560, (101, 105): 561, (308, 394): 562, (290, 266): 563, (401, 304): 564, (321, 260): 565, (114, 404): 566, (111, 100): 567, (40, 373): 568, (105, 264): 569, (32, 319): 570, (115, 115): 571, (82, 451): 572, (459, 337): 573, (418, 101): 574, (290, 110): 575, (101, 120): 576, (278, 257): 577, (347, 268): 578, (504, 109): 579, (270, 99): 580, (572, 325): 581, (581, 560): 582, (51, 48): 583, (308, 373): 584, (104, 97): 585, (34, 257): 586, (99, 264): 587, (112, 32): 588, (97, 263): 589, (101, 100): 590, (485, 406): 591, (271, 298): 592, (71, 114): 593, (105, 279): 594, (341, 53): 595, (84, 447): 596, (359, 318): 597, (49, 54): 598, (388, 279): 599, (290, 100): 600, (105, 339): 601, (282, 116): 602, (112, 108): 603, (310, 116): 604, (308, 457): 605, (308, 391): 606, (99, 286): 607, (110, 32): 608, (264, 256): 609, (298, 264): 610, (117, 99): 611, (105, 270): 612, (278, 262): 613, (297, 52): 614, (395, 494): 615, (338, 40): 616, (341, 51): 617, (263, 288): 618, (50, 56): 619, (393, 32): 620, (341, 52): 621, (308, 466): 622, (493, 147): 623, (310, 262): 624, (73, 262): 625, (104, 348): 626, (109, 273): 627, (267, 375): 628, (257, 74): 629, (285, 51): 630, (111, 266): 631, (116, 262): 632, (443, 89): 633, (460, 260): 634, (582, 101): 635, (327, 525): 636, (327, 379): 637, (115, 112): 638, (108, 117): 639, (112, 112): 640, (87, 104): 641, (279, 275): 642, (427, 32): 643, (285, 52): 644, (273, 40): 645, (109, 433): 646, (428, 566): 647, (116, 272): 648, (83, 104): 649, (291, 108): 650, (256, 40): 651, (300, 635): 652, (76, 105): 653, (261, 423): 654, (97, 107): 655, (112, 104): 656, (112, 114): 657, (385, 104): 658, (522, 374): 659, (285, 56): 660, (109, 111): 661, (281, 535): 662, (317, 32): 663, (105, 114): 664, (84, 104): 665, (434, 345): 666, (300, 67): 667, (290, 99): 668, (308, 463): 669, (85, 83): 670, (72, 272): 671, (257, 319): 672, (397, 385): 673, (65, 301): 674, (109, 270): 675, (553, 361): 676, (426, 304): 677, (316, 298): 678, (99, 505): 679, (602, 344): 680, (109, 343): 681, (337, 373): 682, (281, 112): 683, (473, 32): 684, (109, 372): 685, (48, 48): 686, (424, 304): 687, (574, 369): 688, (97, 260): 689, (268, 275): 690, (98, 101): 691, (346, 272): 692, (262, 40): 693, (115, 104): 694, (521, 89): 695, (695, 291): 696, (99, 265): 697, (285, 48): 698, (337, 394): 699, (105, 108): 700, (118, 343): 701, (119, 432): 702, (267, 272): 703, (112, 278): 704, (415, 51): 705, (308, 471): 706, (72, 631): 707, (102, 102): 708, (32, 83): 709, (544, 107): 710, (113, 117): 711, (308, 442): 712, (401, 318): 713, (41, 10): 714, (492, 490): 715, (98, 108): 716, (98, 342): 717, (320, 673): 718, (89, 321): 719, (291, 256): 720, (69, 114): 721, (119, 104): 722, (556, 490): 723, (278, 256): 724, (300, 86): 725, (56, 57): 726, (268, 527): 727, (271, 266): 728, (111, 115): 729, (67, 505): 730, (524, 57): 731, (67, 264): 732, (77, 372): 733, (320, 362): 734, (110, 353): 735, (98, 278): 736, (261, 261): 737, (71, 320): 738, (263, 99): 739, (702, 403): 740, (262, 351): 741, (77, 84): 742, (49, 32): 743, (97, 339): 744, (78, 658): 745, (745, 701): 746, (268, 368): 747, (112, 513): 748, (257, 379): 749, (593, 362): 750, (97, 118): 751, (274, 108): 752, (367, 266): 753, (337, 391): 754, (69, 110): 755, (65, 32): 756, (111, 111): 757, (302, 291): 758, (362, 256): 759, (377, 354): 760, (298, 109): 761, (97, 273): 762, (101, 273): 763, (105, 118): 764, (71, 117): 765, (51, 49): 766, (435, 115): 767, (116, 398): 768, (446, 726): 769, (274, 256): 770, (87, 650): 771, (750, 627): 772, (444, 258): 773, (348, 338): 774, (474, 265): 775, (321, 266): 776, (100, 257): 777, (66, 476): 778, (461, 304): 779, (337, 442): 780, (472, 656): 781, (97, 257): 782, (375, 275): 783, (270, 105): 784, (312, 366): 785, (69, 118): 786, (107, 256): 787, (721, 348): 788, (59, 32): 789, (74, 111): 790, (285, 49): 791, (101, 300): 792, (112, 101): 793, (121, 108): 794, (66, 382): 795, (116, 257): 796, (84, 565): 797, (109, 32): 798, (604, 45): 799, (274, 376): 800, (491, 98): 801, (633, 634): 802, (105, 420): 803, (117, 292): 804, (367, 104): 805, (384, 48): 806, (101, 266): 807, (263, 110): 808, (101, 103): 809, (458, 39): 810, (365, 257): 811, (347, 256): 812, (65, 99): 813, (310, 257): 814, (392, 413): 815, (683, 489): 816, (758, 344): 817, (99, 278): 818, (256, 351): 819, (268, 352): 820, (78, 111): 821, (415, 53): 822, (512, 105): 823, (112, 439): 824, (516, 523): 825, (119, 286): 826, (592, 339): 827, (87, 343): 828, (117, 260): 829, (293, 770): 830, (70, 291): 831, (591, 32): 832, (111, 312): 833, (271, 256): 834, (101, 344): 835, (296, 288): 836, (270, 110): 837, (308, 479): 838, (86, 688): 839, (117, 716): 840, (396, 105): 841, (77, 101): 842, (262, 319): 843, (681, 594): 844, (381, 273): 845, (704, 579): 846, (103, 104): 847, (108, 121): 848, (661, 404): 849, (44, 686): 850, (87, 710): 851, (116, 101): 852, (337, 411): 853, (337, 471): 854, (337, 417): 855, (115, 300): 856, (596, 310): 857, (270, 103): 858, (374, 32): 859, (340, 32): 860, (460, 413): 861, (396, 256): 862, (271, 107): 863, (305, 288): 864, (115, 369): 865, (73, 32): 866, (101, 102): 867, (392, 107): 868, (468, 494): 869, (286, 263): 870, (85, 110): 871, (414, 569): 872, (337, 479): 873, (359, 412): 874, (438, 304): 875, (575, 487): 876, (425, 108): 877, (381, 262): 878, (121, 110): 879, (97, 117): 880, (418, 110): 881, (678, 404): 882, (268, 279): 883, (696, 413): 884, (592, 404): 885, (697, 499): 886, (52, 327): 887, (65, 571): 888, (97, 104): 889, (337, 463): 890, (821, 259): 891, (675, 257): 892, (101, 347): 893, (382, 110): 894, (41, 32): 895, (111, 257): 896, (70, 877): 897, (319, 34): 898, (102, 648): 899, (104, 481): 900, (275, 647): 901, (53, 327): 902, (100, 264): 903, (103, 476): 904, (117, 266): 905, (103, 290): 906, (290, 298): 907, (481, 116): 908, (116, 121): 909, (86, 872): 910, (438, 318): 911, (419, 318): 912, (320, 256): 913, (115, 10): 914, (84, 540): 915, (100, 611): 916, (40, 422): 917, (509, 467): 918, (415, 52): 919, (80, 101): 920, (455, 53): 921, (70, 105): 922, (97, 496): 923, (97, 295): 924, (365, 377): 925, (436, 338): 926, (114, 310): 927, (585, 475): 928, (100, 310): 929, (473, 262): 930, (719, 32): 931, (97, 112): 932, (536, 861): 933, (933, 78): 934, (356, 422): 935, (831, 98): 936, (77, 111): 937, (639, 100): 938, (340, 107): 939, (327, 643): 940, (746, 256): 941, (111, 45): 942, (82, 111): 943, (405, 100): 944, (524, 56): 945, (399, 275): 946, (850, 495): 947, (84, 114): 948, (817, 910): 949, (77, 99): 950, (623, 32): 951, (549, 781): 952, (263, 100): 953, (112, 364): 954, (97, 258): 955, (108, 349): 956, (384, 495): 957, (116, 325): 958, (586, 34): 959, (66, 491): 960, (278, 109): 961, (320, 535): 962, (71, 801): 963, (739, 938): 964, (358, 112): 965, (455, 52): 966, (529, 258): 967, (119, 279): 968, (888, 823): 969, (502, 110): 970, (428, 358): 971, (291, 273): 972, (110, 859): 973, (385, 115): 974, (474, 728): 975, (32, 767): 976, (461, 539): 977, (365, 300): 978, (72, 620): 979, (83, 296): 980, (263, 256): 981, (339, 734): 982, (77, 881): 983, (97, 119): 984, (439, 571): 985, (291, 101): 986, (341, 54): 987, (67, 271): 988, (121, 425): 989, (262, 352): 990, (277, 267): 991, (493, 148): 992, (116, 589): 993, (84, 803): 994, (472, 367): 995, (97, 265): 996, (337, 488): 997, (40, 391): 998, (400, 391): 999, (40, 394): 1000, (971, 777): 1001, (115, 342): 1002, (367, 99): 1003, (291, 121): 1004, (115, 101): 1005, (10, 379): 1006, (80, 513): 1007, (103, 117): 1008, (117, 436): 1009, (76, 346): 1010, (386, 32): 1011, (108, 270): 1012, (407, 107): 1013, (428, 108): 1014, (717, 759): 1015, (119, 291): 1016, (56, 327): 1017, (274, 865): 1018, (32, 34): 1019, (116, 256): 1020, (559, 57): 1021, (75, 561): 1022, (308, 488): 1023, (464, 304): 1024, (99, 271): 1025, (278, 45): 1026, (824, 916): 1027, (436, 555): 1028, (286, 256): 1029, (261, 384): 1030, (261, 478): 1031, (674, 508): 1032, (305, 829): 1033, (261, 445): 1034, (261, 495): 1035, (351, 368): 1036, (114, 256): 1037, (102, 117): 1038, (115, 341): 1039, (100, 101): 1040, (116, 119): 1041, (991, 288): 1042, (393, 110): 1043, (57, 327): 1044, (707, 806): 1045, (305, 588): 1046, (1022, 361): 1047, (111, 339): 1048, (281, 659): 1049, (270, 107): 1050, (426, 412): 1051, (725, 924): 1052, (1052, 909): 1053, (553, 107): 1054, (545, 98): 1055, (69, 120): 1056, (500, 256): 1057, (84, 876): 1058, (1013, 108): 1059, (121, 259): 1060, (427, 526): 1061, (633, 425): 1062, (936, 310): 1063, (83, 99): 1064, (72, 271): 1065, (407, 100): 1066, (48, 327): 1067, (257, 528): 1068, (321, 262): 1069, (502, 103): 1070, (735, 287): 1071, (109, 720): 1072, (392, 266): 1073, (305, 112): 1074, (771, 258): 1075, (102, 32): 1076, (66, 111): 1077, (66, 114): 1078, (765, 355): 1079, (474, 530): 1080, (66, 67): 1081, (112, 259): 1082, (337, 457): 1083, (920, 364): 1084, (493, 162): 1085, (110, 533): 1086, (342, 116): 1087, (118, 101): 1088, (117, 320): 1089, (115, 105): 1090, (1009, 369): 1091, (519, 100): 1092, (587, 818): 1093, (102, 830): 1094, (97, 640): 1095, (274, 105): 1096, (325, 352): 1097, (66, 265): 1098, (580, 256): 1099, (55, 327): 1100, (34, 400): 1101, (112, 291): 1102, (555, 561): 1103, (65, 109): 1104, (80, 114): 1105, (257, 77): 1106, (614, 304): 1107, (667, 742): 1108, (424, 434): 1109, (891, 743): 1110, (426, 318): 1111, (414, 264): 1112, (101, 256): 1113, (86, 32): 1114, (263, 102): 1115, (262, 356): 1116, (422, 52): 1117, (70, 1059): 1118, (900, 799): 1119, (662, 325): 1120, (339, 45): 1121, (772, 523): 1122, (347, 116): 1123, (396, 293): 1124, (118, 512): 1125, (105, 256): 1126, (325, 368): 1127, (321, 847): 1128, (262, 399): 1129, (267, 357): 1130, (119, 724): 1131, (84, 513): 1132, (105, 122): 1133, (97, 122): 1134, (115, 256): 1135, (257, 65): 1136, (105, 292): 1137, (116, 291): 1138, (281, 773): 1139, (68, 529): 1140, (257, 76): 1141, (482, 1079): 1142, (1142, 612): 1143, (542, 304): 1144, (257, 83): 1145, (99, 108): 1146, (678, 339): 1147, (312, 263): 1148, (1008, 381): 1149, (795, 32): 1150, (897, 624): 1151, (934, 393): 1152, (82, 268): 1153, (103, 985): 1154, (723, 613): 1155, (87, 286): 1156, (320, 773): 1157, (49, 327): 1158, (105, 266): 1159, (323, 102): 1160, (38, 32): 1161, (492, 32): 1162, (262, 375): 1163, (109, 655): 1164, (66, 320): 1165, (50, 327): 1166, (268, 628): 1167, (54, 327): 1168, (116, 270): 1169, (67, 265): 1170, (289, 262): 1171, (481, 266): 1172, (339, 271): 1173, (76, 111): 1174, (39, 266): 1175, (816, 469): 1176, (318, 458): 1177, (337, 466): 1178, (300, 78): 1179, (40, 488): 1180, (482, 884): 1181, (293, 450): 1182, (257, 80): 1183, (89, 425): 1184, (267, 278): 1185, (115, 432): 1186, (268, 454): 1187, (509, 956): 1188, (736, 45): 1189, (640, 291): 1190, (79, 708): 1191, (641, 375): 1192, (481, 632): 1193, (788, 797): 1194, (550, 278): 1195, (264, 258): 1196, (119, 650): 1197, (362, 268): 1198, (469, 292): 1199, (80, 104): 1200, (274, 349): 1201, (398, 325): 1202, (536, 111): 1203, (607, 109): 1204, (270, 115): 1205, (65, 266): 1206, (268, 356): 1207, (273, 80): 1208, (107, 325): 1209, (561, 260): 1210, (109, 563): 1211, (340, 256): 1212, (1041, 369): 1213, (267, 114): 1214, (298, 475): 1215, (99, 942): 1216, (1070, 358): 1217, (347, 349): 1218, (274, 363): 1219, (393, 608): 1220, (97, 284): 1221, (298, 279): 1222, (291, 103): 1223, (384, 32): 1224, (755, 526): 1225, (1225, 993): 1226, (99, 99): 1227, (949, 41): 1228, (409, 489): 1229, (257, 1082): 1230, (400, 373): 1231, (828, 892): 1232, (1232, 775): 1233, (731, 318): 1234, (72, 601): 1235, (1110, 279): 1236, (1181, 857): 1237, (922, 566): 1238, (67, 923): 1239, (1239, 1001): 1240, (1240, 1047): 1241, (1025, 101): 1242, (117, 262): 1243, (117, 112): 1244, (301, 32): 1245, (353, 450): 1246, (410, 284): 1247, (267, 498): 1248, (1115, 639): 1249, (342, 266): 1250, (735, 1189): 1251, (284, 530): 1252, (492, 262): 1253, (73, 266): 1254, (816, 599): 1255, (786, 961): 1256, (270, 121): 1257, (691, 448): 1258, (963, 292): 1259, (364, 290): 1260, (1203, 298): 1261, (1261, 102): 1262, (414, 279): 1263, (771, 100): 1264, (69, 109): 1265, (757, 258): 1266, (99, 450): 1267, (108, 347): 1268, (100, 111): 1269, (114, 293): 1270, (696, 107): 1271, (51, 327): 1272, (455, 51): 1273, (268, 379): 1274, (670, 32): 1275}\n\n\n\nassert tokenizer.decode(tokenizer.encode(taylor_text))==taylor_text, 'enc/dec mismatch'\n\n\ntokenizer.decode(tokenizer.encode(taylor_text))\n\n'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaThe Free Encyclopedia\\n\\nSearch\\nCreate account\\nLog in\\n\\nPersonal tools\\nContents  hide\\n(Top)\\nLife and career\\nToggle Life and career subsection\\nArtistry\\nToggle Artistry subsection\\nAccolades and achievements\\nCultural status\\nToggle Cultural status subsection\\nWealth\\nToggle Wealth subsection\\nDiscography\\nFilmography\\nTours\\nSee also\\nFootnotes\\nReferences\\nToggle References subsection\\nExternal links\\nTaylor Swift\\n\\n136 languages\\nArticle\\nTalk\\nRead\\nView source\\nView history\\n\\nTools\\n Featured article\\nPage semi-protected\\nFrom Wikipedia, the free encyclopedia\\nFor the album, see Taylor Swift (album).\\nTaylor Swift\\nPortrait of Taylor Swift in a cocktail dress\\nSwift at the 2023 MTV Video Music Awards\\nBorn\\tTaylor Alison Swift\\nDecember 13, 1989 (age 34)\\nWest Reading, Pennsylvania, US\\nOccupations\\nSinger-songwriter producer director businesswoman actress\\nYears active\\t2004–present\\nWorks\\nAlbumssinglessongsvideosperformances\\nRelatives\\nAustin Swift (brother)\\nMarjorie Finlay (grandmother)\\nAwards\\tFull list\\nMusical career\\nOrigin\\tNashville, Tennessee, US\\nGenres\\nPop country folk rock alternative\\nInstruments\\nVocals guitar banjo piano ukulele\\nLabels\\nRCA Republic Big Machine\\nWebsite\\twww.taylorswift.com Edit this at Wikidata\\nSignature\\n\\nTaylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry, songwriting, and entrepreneurship have influenced the music industry, popular culture, and politics, and her life is a subject of widespread media coverage.\\nSwift began professional songwriting at 14 and signed with Big Machine Records in 2005 to become a country singer. She released six studio albums under the label, four of them to country radio, starting with Taylor Swift (2006). Her next, Fearless (2008), explored country pop, and its singles \"Love Story\" and \"You Belong with Me\" catapulted her to mainstream fame. Speak Now (2010) infused rock influences, while Red (2012) experimented with electronic elements and featured Swift\\'s first Billboard Hot 100 number-one song, \"We Are Never Ever Getting Back Together\". She departed from her country image with 1989 (2014), a synth-pop album supported by the chart-topping songs \"Shake It Off\", \"Blank Space\", and \"Bad Blood\". Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single \"Look What You Made Me Do\".\\nAfter signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylor\\'s Version after a dispute with Big Machine. These albums spawned the number-one songs \"Cruel Summer\", \"Cardigan\", \"Willow\", \"Anti-Hero\", \"All Too Well\", and \"Is It Over Now?\". Her Eras Tour (2023–2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed several music videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021).\\nOne of the world\\'s best-selling musicians, with over 200 million records sold as of 2019, Swift has been named Global Recording Artist of the Year three times by the International Federation of the Phonographic Industry, whereas six of her albums have opened with over a million sales in a week. She is the highest-grossing female touring act, the most-streamed woman on Spotify and Apple Music, and the first billionaire with music as the main source of income. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stone\\'s 100 Greatest Songwriters of All Time, Billboard\\'s Greatest of All Time Artists, and Forbes\\' World\\'s 100 Most Powerful Women. Her accolades include 14 Grammy Awards (featuring a record four Album of the Year wins), a Primetime Emmy Award, 40 American Music Awards, 40 Billboard Music Awards, and 23 MTV Video Music Awards.\\nLife and career\\nEarly life\\n\\nSwift\\'s childhood home in Wyomissing, Pennsylvania\\nTaylor Alison Swift was born on December 13, 1989,[1] in West Reading, Pennsylvania.[2] She is named after singer-songwriter James Taylor.[3] Her father, Scott Kingsley Swift, is a former stockbroker for Merrill Lynch[4] and her mother, Andrea Gardner Swift (née Finlay), worked for a time as a mutual fund marketing executive.[5] Taylor has a younger brother, actor Austin Swift.[6]\\nSwift\\'s mother is of Scottish and German descent, and her father is of Scottish and English descent with distant Italian ancestry.[7][8] Swift\\'s paternal great-great-grandfather, Charles Carmine Antonio Baldi, was an Italian immigrant entrepreneur and community leader who opened several businesses in Philadelphia in the 1800s.[9][10][8] Her maternal grandmother, Marjorie (née Moehlenkamp) Finlay, was an opera singer.[11]\\nSwift spent her early years on a Christmas tree farm that her father had purchased from one of his clients.[12][13] She is a Christian.[14] She attended preschool and kindergarten at Alvernia Montessori School, run by Bernardine Franciscan sisters,[15] before transferring to the Wyndcroft School.[16] The family moved to a rented house in Wyomissing, Pennsylvania,[17] where Swift attended Wyomissing Area Junior/Senior High School.[18]\\nAt age nine, Swift became interested in musical theater and performed in four Berks Youth Theatre Academy productions.[19] She also traveled regularly to New York City for vocal and acting lessons.[20] Swift later shifted her focus toward country music, inspired by Shania Twain\\'s songs, which made her \"want to just run around the block four times and daydream about everything\".[21] She spent weekends performing at local festivals and events.[22][23] After watching a documentary about Faith Hill, Swift felt she needed to move to Nashville, Tennessee, to pursue a career in music.[24] She traveled there with her mother at age eleven to visit record labels and submitted demo tapes of Dolly Parton and Dixie Chicks karaoke covers.[25] She was rejected, however, because \"everyone in that town wanted to do what I wanted to do. So, I kept thinking to myself, I need to figure out a way to be different.\"[26] She spent summers in Stone Harbor, New Jersey until she was 14 years old, performing in a local coffee shop.[27][28]\\nWhen Swift was around 12 years old, musician Ronnie Cremer taught her to play guitar. Cremer helped with her first efforts as a songwriter, leading her to write \"Lucky You\".[29] In 2003, Swift and her parents started working with New York–based talent manager Dan Dymtrow. With his help, Swift modeled for Abercrombie & Fitch as part of their \"Rising Stars\" campaign, had an original song included on a Maybelline compilation CD, and met with major record labels.[30] After performing original songs at an RCA Records showcase, Swift, then 13 years old, was given an artist development deal and began making frequent trips to Nashville with her mother.[31][32][33] To help Swift break into the country music scene, her father transferred to Merrill Lynch\\'s Nashville office when she was 14 years old, and the family relocated to Hendersonville, Tennessee.[12][34] Swift attended Hendersonville High School[35] before transferring to Aaron Academy after two years, which better accommodated her touring schedule through homeschooling. She graduated one year early.[36][37]\\n2004–2008: Career beginnings and first album\\nIn Nashville, Swift worked with experienced Music Row songwriters such as Troy Verges, Brett Beavers, Brett James, Mac McAnally, and the Warren Brothers[38][39] and formed a lasting working relationship with Liz Rose.[40] They began meeting for two-hour writing sessions every Tuesday afternoon after school.[41] Rose called the sessions \"some of the easiest I\\'ve ever done. Basically, I was just her editor. She\\'d write about what happened in school that day. She had such a clear vision of what she was trying to say. And she\\'d come in with the most incredible hooks.\" Swift became the youngest artist signed by the Sony/ATV Tree publishing house,[42] but left then BMG-owned RCA Records (later bought by Sony Music) at the age of 14 due to the label\\'s lack of care and them \"cut[ting] other people\\'s stuff\". She was also concerned that development deals can shelve artists[33][23] and recalled: \"I genuinely felt that I was running out of time. I wanted to capture these years of my life on an album while they still represented what I was going through.\"[43]\\nTaylor Swift singing on a microphone and playing a guitar\\nSwift opening for Brad Paisley in 2007. To promote her first album, she opened tours for other country musicians in 2007 and 2008.[44]\\nAt an industry showcase at Nashville\\'s Bluebird Cafe in 2005, Swift caught the attention of Scott Borchetta, a DreamWorks Records executive who was preparing to form an independent record label, Big Machine Records. She had first met Borchetta in 2004.[45] She was one of Big Machine\\'s first signings,[33] and her father purchased a three-percent stake in the company for an estimated $120,000.[46][47] She began working on her eponymous debut album with Nathan Chapman.[23] Swift wrote or co-wrote all album tracks, and co-writers included Rose, Robert Ellis Orrall, Brian Maher, and Angelo Petraglia.[48] Taylor Swift was released on October 24, 2006.[49] Country Weekly critic Chris Neal deemed Swift better than previous aspiring teenage country singers because of her \"honesty, intelligence and idealism\".[50] The album peaked at number five on the US Billboard 200, on which it spent 157 weeks—the longest stay on the chart by any release in the US in the 2000s decade.[51] Swift became the first female country music artist to write or co-write every track on a US platinum-certified debut album.[52]\\nBig Machine Records was still in its infancy during the June 2006 release of the lead single, \"Tim McGraw\", which Swift and her mother helped promote by packaging and sending copies of the CD single to country radio stations. As there was not enough furniture at the label yet, they would sit on the floor to do so.[53] She spent much of 2006 promoting Taylor Swift with a radio tour and television appearances; she opened for Rascal Flatts on select dates during their 2006 tour,[54] as a replacement for Eric Church.[55] Borchetta said that although record industry peers initially disapproved of his signing a 15-year-old singer-songwriter, Swift tapped into a previously unknown market—teenage girls who listen to country music.[53][12]\\nFollowing \"Tim McGraw\", four more singles were released throughout 2007 and 2008: \"Teardrops on My Guitar\", \"Our Song\", \"Picture to Burn\" and \"Should\\'ve Said No\". All appeared on Billboard\\'s Hot Country Songs, with \"Our Song\" and \"Should\\'ve Said No\" reaching number one. With \"Our Song\", Swift became the youngest person to single-handedly write and sing a number-one song on the chart.[56] \"Teardrops on My Guitar\" reached number thirteen on the US Billboard Hot 100.[57] Swift also released two EPs, The Taylor Swift Holiday Collection in October 2007 and Beautiful Eyes in July 2008.[58][59] She promoted her debut album extensively as the opening act for other country musicians\\' tours in 2006 and 2007, including those by George Strait,[60] Brad Paisley,[61] and Tim McGraw and Faith Hill.[62]\\nSwift won multiple accolades for Taylor Swift. She was one of the recipients of the Nashville Songwriters Association\\'s Songwriter/Artist of the Year in 2007, becoming the youngest person given the title.[63] She also won the Country Music Association\\'s Horizon Award for Best New Artist,[64] the Academy of Country Music Awards\\' Top New Female Vocalist,[65] and the American Music Awards\\' Favorite Country Female Artist honor.[66] She was also nominated for Best New Artist at the 50th Annual Grammy Awards.[67] In 2008, she opened for Rascal Flatts again[68] and briefly dated the singer Joe Jonas.[69][70]\\n2008–2010: Fearless\\nTaylor Swift in 2009\\nSwift at the 2009 premiere of Hannah Montana: The Movie. She had a cameo appearance in the film and wrote two songs for its soundtrack.[71][72]\\nSwift\\'s second studio album, Fearless, was released on November 11, 2008, in North America,[73] and in March 2009 in other markets.[74] Critics lauded Swift\\'s honest and vulnerable songwriting in contrast to other teenage singers.[75] Five singles were released in 2008–2009: \"Love Story\", \"White Horse\", \"You Belong with Me\", \"Fifteen\", and \"Fearless\". The first single peaked at number four on the Billboard Hot 100 and number one in Australia.[57][76] It was the first country song to top Billboard\\'s Pop Songs chart.[77] \"You Belong with Me\" was the album\\'s highest-charting single on the Billboard Hot 100, peaking at number two,[78] and was the first country song to top Billboard\\'s all-genre Radio Songs chart.[79] All five singles were Hot Country Songs top-10 entries, with \"Love Story\" and \"You Belong with Me\" topping the chart.[80] Fearless became her first number-one album on the Billboard 200 and 2009\\'s top-selling album in the US.[81] The Fearless Tour, Swift\\'s first headlining concert tour, grossed over $63 million.[82] Journey to Fearless, a documentary miniseries, aired on television and was later released on DVD and Blu-ray.[83] Swift performed as a supporting act for Keith Urban\\'s Escape Together World Tour in 2009.[84]\\nIn 2009, the music video for \"You Belong with Me\" was named Best Female Video at the 2009 MTV Video Music Awards.[85] Her acceptance speech was interrupted by rapper Kanye West,[86] an incident that became the subject of controversy, widespread media attention and Internet memes.[87] That year she won five American Music Awards, including Artist of the Year and Favorite Country Album.[88] Billboard named her 2009\\'s Artist of the Year.[89] She won Video of the Year and Female Video of the Year for \"Love Story\" at the 2009 CMT Music Awards, where she made a parody video of the song with rapper T-Pain called \"Thug Story\".[90] At the 52nd Annual Grammy Awards, Fearless was named Album of the Year and Best Country Album, and \"White Horse\" won Best Country Song and Best Female Country Vocal Performance. Swift was the youngest artist to win Album of the Year.[note 1] At the 2009 Country Music Association Awards, Swift won Album of the Year for Fearless and was named Entertainer of the Year, the youngest person to win the honor.[93]\\nSwift featured on John Mayer\\'s single \"Half of My Heart\" and Boys Like Girls\\' single \"Two Is Better Than One\", the latter of which she co-wrote.[94][95] She co-wrote and recorded \"Best Days of Your Life\" with Kellie Pickler,[96] and wrote two songs for the Hannah Montana: The Movie soundtrack—\"You\\'ll Always Find Your Way Back Home\" and \"Crazier\".[72] She contributed two songs to the Valentine\\'s Day soundtrack, including the single \"Today Was a Fairytale\", which was her first number-one on the Canadian Hot 100 and peaked at number two on the US Hot 100.[97][98] While shooting her film debut Valentine\\'s Day in October 2009, Swift dated co-star Taylor Lautner.[99] In 2009, she made her television debut as a rebellious teenager in an CSI: Crime Scene Investigation episode,[100] and she hosted and performed as the musical guest on Saturday Night Live; she was the first host ever to write their own opening monologue.[101][102]\\n2010–2014: Speak Now and Red\\nSwift singing into a mic while playing a banjo\\nSwift performing at the Speak Now World Tour in 2012\\nIn August 2010, Swift released \"Mine\", the lead single from her third studio album, Speak Now. The single entered the Hot 100 at number three.[103] Swift wrote the album alone and co-produced every track.[104] The album was released on October 25, 2010,[105] opening atop the Billboard 200 with over one million copies sold.[106] It became the fastest-selling digital album by a female artist, with 278,000 downloads in a week.[107] Critics appreciated Swift\\'s grown-up perspectives:[108] Rob Sheffield of Rolling Stone wrote, \"in a mere four years, the 20-year-old Nashville firecracker has put her name on three dozen or so of the smartest songs released by anyone in pop, rock or country.\"[109] \"Back to December\", \"Mean\", \"The Story of Us\", \"Sparks Fly\", and \"Ours\" became subsequent singles, with the latter two reaching number one on the Hot Country Songs[80] and the first two peaking in the top ten in Canada.[98] She dated actor Jake Gyllenhaal in 2010.[110]\\nAt the 54th Annual Grammy Awards in 2012, Swift won Best Country Song and Best Country Solo Performance for \"Mean\", which she performed during the ceremony.[111] Swift won other awards for Speak Now, including Songwriter/Artist of the Year by the Nashville Songwriters Association (2010 and 2011),[112][113] Woman of the Year by Billboard (2011),[114] and Entertainer of the Year by the Academy of Country Music (2011 and 2012)[115] and the Country Music Association in 2011.[116] At the American Music Awards of 2011, Swift won Artist of the Year and Favorite Country Album.[117] Rolling Stone named Speak Now amongst its \"50 Best Female Albums of All Time\" (2012), writing: \"She might get played on the country station, but she\\'s one of the few genuine rock stars we\\'ve got these days, with a flawless ear for what makes a song click.\"[118]\\nThe Speak Now World Tour ran from February 2011 to March 2012 and grossed over $123 million,[119] followed up by the live album, Speak Now World Tour: Live.[120] She contributed two original songs to The Hunger Games soundtrack album: \"Eyes Open\" and \"Safe & Sound\", co-written and recorded with the Civil Wars and T-Bone Burnett. \"Safe & Sound\" won the Grammy Award for Best Song Written for Visual Media and was nominated for the Golden Globe Award for Best Original Song.[121][122] Swift featured on B.o.B\\'s single \"Both of Us\", released in May 2012.[123] She dated Conor Kennedy that year.[124]\\nTaylor Swift on the Red Tour\\nSwift on the Red Tour (2013)\\nIn August 2012, Swift released \"We Are Never Ever Getting Back Together\", the lead single from her fourth studio album, Red. It became her first number one single in the US and New Zealand,[125][126] and became the fastest-selling single in digital history.[127] Other singles from the album were \"Begin Again\", \"I Knew You Were Trouble\", \"22\", \"Everything Has Changed\", \"The Last Time\", and \"Red\". \"I Knew You Were Trouble\" reached the top five on charts in Australia, Canada, Denmark, Ireland, New Zealand, the UK and the US.[128] \"Begin Again\", \"22\", and \"Red\" reached the top 20 in the US.[57] On Red, released on October 22, 2012,[129] Swift worked with Chapman and Rose, as well as the new producers Max Martin and Shellback.[130] It incorporated many pop and rock styles such as heartland rock, dubstep and dance-pop.[131] Randall Roberts of Los Angeles Times said Swift \"strives for something much more grand and accomplished\" with Red.[132] It opened at number one on the Billboard 200 with 1.21 million sales.[133] Red was Swift\\'s first number-one album in the UK.[134] It earned several accolades, including four nominations at the 56th Annual Grammy Awards (2014).[135] Swift received American Music Awards for Best Female Country Artist in 2012, Artist of the Year in 2013,[136][137] and the Nashville Songwriters Association\\'s Songwriter/Artist Award for the fifth and sixth consecutive years.[138] The Red Tour ran from March 2013 to June 2014 and grossed over $150 million, becoming the highest-grossing country tour ever.[139] Swift was honored with the Pinnacle Award, making her the second recipient of the accolade after Garth Brooks.[140] During this time, she briefly dated the English singer Harry Styles.[141]\\nIn 2013, Swift recorded \"Sweeter than Fiction\", a song she wrote and produced with Jack Antonoff for the One Chance soundtrack. The song received a Best Original Song nomination at the 71st Golden Globe Awards.[142] She provided guest vocals for Tim McGraw\\'s song \"Highway Don\\'t Care\", also featuring Keith Urban.[143] Swift performed \"As Tears Go By\" with the Rolling Stones in Chicago, Illinois, as part of the band\\'s 50 & Counting tour,[144] and joined Florida Georgia Line at their set at the 2013 Country Radio Seminar to sing \"Cruise\".[145] Swift voiced Audrey in the animated film The Lorax (2012),[146] made a cameo in the sitcom New Girl (2013),[147] and had a supporting role in the dystopian film The Giver (2014).[148]\\n2014–2018: 1989 and Reputation\\nSwift performing on a mic, dressed in a blue skirt\\nSwift at the 1989 World Tour, the highest-grossing tour of 2015\\nIn March 2014, Swift began living in New York City.[note 2] She hired Tree Paine as her publicist[151] and worked on her fifth studio album, 1989, with the producers Jack Antonoff, Max Martin, Shellback, Imogen Heap, Ryan Tedder, and Ali Payami.[152] She promoted the album extensively, including inviting fans to secret album-listening sessions.[153] 1989 was released on October 27, 2014, and opened atop the Billboard 200 with 1.28 million copies sold.[154] Its singles \"Shake It Off\", \"Blank Space\" and \"Bad Blood\" reached number one in Australia, Canada and the US, the first two making Swift the first woman to replace herself at the Hot 100 top spot;[155] other singles include \"Style\", \"Wildest Dreams\", \"Out of the Woods\" and \"New Romantics\".[156] The 1989 World Tour (2015) was the highest-grossing tour of the year with $250 million in total revenue.[157]\\nPrior to 1989\\'s release, Swift stressed the importance of albums to artists and fans.[158] In November 2014, she removed her entire catalog from Spotify, arguing that its ad-supported, free service undermined the premium service, which provides higher royalties for songwriters.[159] In a June 2015 open letter, Swift criticized Apple Music for not offering royalties to artists during the streaming service\\'s free three-month trial period and stated that she would pull 1989 from the catalog.[160] The following day, Apple Inc. announced that it would pay artists during the free trial period,[161] and Swift agreed to let 1989 on the streaming service.[162] She then returned her entire catalog plus 1989 to Spotify, Amazon Music and Google Play and other digital streaming platforms in June 2017.[163] Swift was named Billboard\\'s Woman of the Year in 2014, becoming the first artist to win the award twice.[164] At the 2014 American Music Awards, Swift received the inaugural Dick Clark Award for Excellence.[165] On her 25th birthday in 2014, the Grammy Museum at L.A. Live opened an exhibit in her honor in Los Angeles that ran until October 4, 2015, and broke museum attendance records.[166][167] In 2015, Swift won the Brit Award for International Female Solo Artist.[168] The video for \"Bad Blood\" won Video of the Year and Best Collaboration at the 2015 MTV Video Music Awards.[169] At the 58th Grammy Awards (2016), 1989 won Album of the Year and Best Pop Vocal Album, making Swift the first woman and fifth act overall to win Album of the Year twice.[170]\\nSwift wearing a sparkling blazer singing on a mic\\nSwift on her Reputation Stadium Tour (2018), the highest-grossing North American tour ever\\nSwift dated the Scottish DJ Calvin Harris from March 2015 to June 2016.[171] They co-wrote the song \"This Is What You Came For\", featuring vocals from the Barbadian singer Rihanna; Swift was initially credited under the pseudonym Nils Sjöberg.[172] In April 2016, Swift criticized the lyrics of Kanye West\\'s single \"Famous\", in which he sings \"I made that bitch famous\" in reference to his interruption of her acceptance speech at the 2009 MTV Video Music Awards. West claimed he had received her approval for the line, and his then-wife Kim Kardashian released video clips of Swift and West discussing the single amicably over the phone; a full recording leaked in 2020 established that West did not disclose that he would call her a \"bitch\".[173][174]\\nAfter briefly dating the English actor Tom Hiddleston,[175] Swift entered a six-year relationship with the English actor Joe Alwyn in September 2016.[176][177][178] She wrote the song \"Better Man\" for the band Little Big Town, which earned her the Song of the Year award at the 51st CMA Awards.[179] Swift and English singer Zayn Malik released the joint single \"I Don\\'t Wanna Live Forever\" for Fifty Shades Darker: Original Motion Picture Soundtrack (2017). The song reached number two in the US.[180]\\nIn August 2017, Swift successfully countersued David Mueller, a former radio jockey for KYGO-FM, who sued her for damages from loss of employment. Four years earlier, she informed Mueller\\'s bosses that he had sexually assaulted her by groping her at an event.[181] Also that month, after a one-year hiatus from the spotlight, Swift cleared her social media accounts and released \"Look What You Made Me Do\" as the lead single from her sixth album, Reputation.[182][183] The single was Swift\\'s first UK number-one single.[184] It topped charts in Australia, Ireland, New Zealand, and the US.[185] Reputation, released on November 10, 2017,[186] incorporated electropop, hip hop, R&B, and EDM.[187] Reviews praised Swift\\'s mature artistry, but some denounced the themes of fame and gossip.[188] The album opened atop the Billboard 200 with 1.21 million US sales[189] and topped the charts in the UK, Australia, and Canada.[190] Its singles \"...Ready for It?\", \"End Game\" (featuring Ed Sheeran and Future), and \"Delicate\" were released to pop radio.[191] Reputation was nominated for a Grammy Award for Best Pop Vocal Album.[192] Swift featured on the country duo Sugarland\\'s \"Babe\" (2018).[193]\\nAt the 2018 American Music Awards, Swift won four awards, which made her accumulate 23 trophies in total and become the AMAs\\' most awarded female musician, surpassing Whitney Houston.[194] The same year, she embarked on her Reputation Stadium Tour,[195] which became the highest-grossing North American concert tour in history and grossed $345.7 million worldwide.[196]\\n2018–2020: Lover, Folklore, and Evermore\\nIn November 2018, she signed a new deal with the Universal Music Group; her subsequent releases were promoted by Republic Records. Swift said the contract included a provision for her to maintain ownership of her masters. In addition, in the event that Universal sold any part of its stake in Spotify, it agreed to distribute a non-recoupable portion of the proceeds among its artists.[197] Vox called it a huge commitment from Universal, which was \"far from assured\" until Swift intervened.[198]\\nA portrait of Swift\\nSwift at the American Music Awards of 2019, where she was named Artist of the Decade\\nSwift\\'s first album with Republic Records, Lover, was released on August 23, 2019.[199] Besides Antonoff, she worked with Louis Bell, Frank Dukes, and Joel Little.[200] Lover was her sixth consecutive album to sell more than 500,000 US copies in one week.[201] Critics commended the album\\'s free-spirited mood and emotional intimacy.[202][203] The singles \"Me!\" and \"You Need to Calm Down\" both peaked at number two on the Hot 100,[204] and other singles were the top-10 single \"Lover\", the top-40 single \"The Man\",[57] and the 2023 resurgent success, chart topper \"Cruel Summer\".[205] Lover was the world\\'s best-selling album by a solo artist of 2019,[206] and along with its singles earned nominations at the 62nd Annual Grammy Awards in 2020.[207] At the 2019 MTV Video Music Awards, \"Me!\" won Best Visual Effects, and \"You Need to Calm Down\" won Video of the Year and Video for Good. Swift was the first female and second artist overall to win Video of the Year for a video that they directed.[208]\\nWhile promoting Lover, Swift became embroiled in a public dispute with the talent manager Scooter Braun and Big Machine over the purchase of the masters of her back catalog.[209][210] Swift said she had been trying to buy the masters, but Big Machine would only allow her to do so if she exchanged one new album for each older one under a new contract, which she refused to sign.[209][211] Swift began re-recording her back catalog in November 2020.[212] Besides music, she played Bombalurina in the film adaptation of Andrew Lloyd Webber\\'s musical Cats (2019), for which she co-wrote and recorded the Golden Globe-nominated original song \"Beautiful Ghosts\".[213][214] Critics panned the film but praised Swift\\'s performance.[215] The documentary Miss Americana, which chronicled parts of Swift\\'s life and career, premiered at the 2020 Sundance Film Festival.[216] Swift signed a global publishing deal with Universal Music Publishing Group in February 2020 after her 16-year contract with Sony/ATV expired.[217]\\nAmidst the COVID-19 pandemic in 2020, Swift surprise-released two \"sister albums\" that she recorded with Antonoff and Aaron Dessner: Folklore on July 24, and Evermore on December 11.[218][219] Alwyn co-wrote and co-produced a few songs under the pseudonym William Bowery.[220] Both explore indie folk with a more muted production compared to her previous upbeat pop songs[221][222] and earned Swift widespread critical acclaim and artistic recognition.[223][224] Each album was supported by three singles catering to US pop, country, and triple A radio formats. The singles were \"Cardigan\", \"Betty\", and \"Exile\" from Folklore, and \"Willow\", \"No Body, No Crime\", and \"Coney Island\" from Evermore.[225] Folklore made Swift the first woman to win the Grammy Award for Album of the Year three times at the 63rd Annual Grammy Awards[226] and was the best-selling album of 2020 in the US.[227] Swift became the first artist to debut a US number-one album and a number-one song at the same time with Folklore\\'s \"Cardigan\".[228] At the 2020 American Music Awards, she won three awards, including Artist of the Year for a record third consecutive time.[229] According to Billboard, she was 2020\\'s highest-paid musician in the US and highest-paid solo musician worldwide.[230]\\n2020–2023: Re-recordings and Midnights\\n\\nSwift performing in 2022\\nFollowing the masters dispute, Swift released re-recordings of her first six studio albums, beginning with Fearless (Taylor\\'s Version) and Red (Taylor\\'s Version) in April and November 2021, respectively. Both peaked atop the Billboard 200,[231] and the former was the first re-recorded album to do so.[232] Fearless (Taylor\\'s Version) was preceded by \"Love Story (Taylor\\'s Version)\", which made her the second artist after Dolly Parton to have both the original and re-recorded versions of a song reach number one on the Hot Country Songs chart.[233] Red (Taylor\\'s Version) was supported by \"All Too Well (10 Minute Version)\", which became the longest song in history to top the Hot 100.[234] The song was accompanied by a short film, which won a Grammy Award for Best Music Video[235] and Swift\\'s record third MTV Video Music Award for Video of the Year.[236]\\nSwift\\'s tenth studio album, Midnights, was released on October 21, 2022.[237] Characterized by a restrained electropop[238][239] and synth-pop[240] sound, the album was dubbed by Rolling Stone critics as an instant classic.[241][242] The album was her fifth to open atop the Billboard 200 with first-week sales of over one million copies and broke various sales and streaming records,[243] including the most single-day streams and most single-week streams on Spotify.[244] Its tracks, led by single \"Anti-Hero\", monopolized the top 10 of the Hot 100, making Swift the first artist to do so.[245] Two other singles, \"Lavender Haze\" and \"Karma\", peaked at number two on the Hot 100.[246] Swift won nine awards at the 2023 MTV Video Music Awards, including Video of the Year (\"Anti-Hero\") for a record fourth time.[247] At the 66th Annual Grammy Awards, she received Best Pop Vocal Album, and her fourth Album of the Year—the most for any artist.[248]\\nSwift released the third re-recorded album, Speak Now (Taylor\\'s Version), on July 7, 2023, becoming the woman with the most number-one albums (12) in Billboard 200 history, surpassing Barbra Streisand.[249] 1989 (Taylor\\'s Version), released on October 27, 2023, became Swift\\'s record-extending sixth album to sell one million copies in a single week in the US and surpassed Midnights for her career\\'s largest album sales week.[250] Its single \"Is It Over Now?\" debuted atop the Billboard Hot 100. Swift was 2023\\'s most streamed artist on Spotify,[251] Apple Music,[252] and Amazon Music;[253] the first act to place number one on the year-end Billboard top artists list in three different decades (2009, 2015 and 2023);[254] and the first living artist to simultaneously chart five albums in the top 10 of the Billboard 200.[255] She had five out of the 10 best-selling albums of 2023 in the United States, a record since Luminate began tracking US music sales in 1991.[256][257]\\nBeyond her albums, Swift featured on five songs from 2021 to 2023: \"Renegade\" and \"Birch\" by Big Red Machine,[258] a remix of \"Gasoline\" by Haim,[259] \"The Joker and the Queen\" by Ed Sheeran,[260] and \"The Alcott\" by the National.[261] For the soundtrack of the 2022 film Where the Crawdads Sing, she recorded \"Carolina\", which received nominations for Best Original Song at the Golden Globes and Best Song Written for Visual Media at the Grammy Awards.[262] Outside of music, Swift had a supporting role in the 2022 period comedy film Amsterdam and has signed to direct an upcoming feature film for Searchlight Pictures.[263][264]\\n2023–present: The Eras Tour and The Tortured Poets Department\\nSwift singing into a mic\\nSwift on the Eras Tour in 2023\\nIn March 2023, Swift embarked on the Eras Tour, a retrospective tour covering all her studio albums. Media outlets extensively covered the tour\\'s cultural and economic impact,[265] and its US leg broke the record for the most tickets sold in a day.[245] Ticketmaster received public and political criticisms for mishandling the tour\\'s ticket sales.[266] The Eras Tour became the highest-grossing tour in history, collecting over $1 billion.[267][268] Its concert film, released to theaters worldwide on October 13, 2023, grossed over $250 million to become the highest-grossing concert film, and was nominated for the Golden Globe Award for Cinematic and Box Office Achievement.[269][270] Swift\\'s music releases, touring, and related activities culminated in an unprecedented height of popularity post-pandemic.[271] Music Business Worldwide remarked that she entered a \"new stratosphere of global career success\" in 2023.[272]\\nSwift began dating Kansas City Chiefs\\' tight end Travis Kelce in 2023.[273] In January 2024, AI-generated fake pornographic images portraying Swift were posted to X (formerly Twitter) and spread to other social media platforms, spurring criticism and demands for legal reform.[274][275] At the 66th Grammy Awards, Swift announced her eleventh studio album, The Tortured Poets Department, set for release on April 19, 2024.[248][276]\\nArtistry\\nInfluences\\nOne of Swift\\'s earliest memories of music is listening to her maternal grandmother, Marjorie Finlay, sing in church.[5] As a child, she enjoyed Disney film soundtracks: \"My parents noticed that, once I had run out of words, I would just make up my own.\"[277] Swift said she owes her confidence and \"fascination with writing and storytelling\" to her mother, who helped her prepare for class presentations as a child.[278][279]\\nSwift was drawn to the storytelling aspect of country music,[280] which was introduced to the genre by female country artists of the 1990s: Shania Twain, Faith Hill, and the Dixie Chicks.[281][282] Twain, both as a songwriter and performer, was her biggest musical influence.[283] Hill was Swift\\'s childhood role model, and she would often imitate her.[284] She admired the Chicks\\' defiant attitude and the way they played their instruments,[285] and was also influenced by older country stars like Patsy Cline, Loretta Lynn, Tammy Wynette, and Dolly Parton,[22] the last of whom she believes is exemplary to female songwriters.[114] As a songwriter, Swift was influenced by Joni Mitchell\\'s emotional and autobiographical lyrics, highlighting Mitchell\\'s 1971 album Blue as a favorite \"because it explores somebody\\'s soul so deeply\".[286] She also spoke of influence from 1990s songwriters such as Melissa Etheridge, Sarah McLachlan, and Alanis Morissette,[287][288] and alt-country artists like Patty Griffin[289] and Lori McKenna.[290]\\nVarious pop and rock artists have also influenced Swift. She lists Paul McCartney, Bruce Springsteen, Emmylou Harris, and Kris Kristofferson as her career role models.[12][291] 1989 was influenced by some of her favorite 1980s pop acts, including Peter Gabriel, Annie Lennox, Phil Collins, and Madonna.[292][293] She also cited Keith Urban\\'s musical style and Fall Out Boy\\'s lyrics as major influences.[294][295]\\nGenres\\n\"If there\\'s one thing that Swift has proven throughout her career, it\\'s that she refuses to be put in a box. Her ever-evolving sound took her from country darling to pop phenom to folk\\'s newest raconteur.\"\\n—The Recording Academy, 2021[296]\\nSwift is known for venturing into various music genres and undergoing artistic reinventions,[297][264] having been described as a \"music chameleon\".[298][299] She self-identified as a country musician until 2012, when she released her fourth studio album, Red.[300] Her albums were promoted to country radio, but music critics noted wide-ranging styles of pop and rock[301][302] and said that the melodies of her songs were rooted in pop, and the country music elements were limited to instruments such as banjo, mandolin, and fiddle, and her slight twang.[303][304] Some commented that her country music identity was an indicator of her narrative songwriting rather than musical style.[305][306] Although the Nashville music industry was receptive of Swift\\'s status as a country musician, critics accused her of abandoning her roots in favor of crossover success in mainstream pop.[307][308] Red\\'s eclectic pop, rock, and electronic styles intensified the critical debate, to which Swift responded, \"I leave the genre labeling to other people.\"[309]\\nMusic journalist Jody Rosen commented that by originating her musical career in Nashville, Swift made a \"bait-and-switch maneuver, planting roots in loamy country soil, then pivoting to pop\".[310] She abandoned her country music identity in 2014 with the release of her synth-pop fifth studio album, 1989. Swift described it as her first \"documented, official pop album\".[311] Her subsequent albums Reputation (2017) and Lover (2019) have an upbeat pop production; the former incorporates hip hop, trap, and EDM elements.[312][313][314] Midnights (2022), on the other hand, is distinguished by a more experimental, \"subdued and amorphous pop sound\".[315][316] Although reviews of Swift\\'s pop albums were generally positive, some critics lamented that the pop music production indicated Swift\\'s pursuit of mainstream success, eroding her authenticity as a songwriter nurtured by her country music background—a criticism that has been retrospectively described as rockist.[317][318] Musicologist Nate Sloan remarked that Swift\\'s pop music transition was rather motivated by her need to expand her artistry.[319] Swift eschewed mainstream pop in favor of alternative, folk and indie rock styles with her 2020 studio albums Folklore and Evermore.[320][321] Clash said her career \"has always been one of transcendence and covert boundary-pushing\", reaching a point at which \"Taylor Swift is just Taylor Swift\", not defined by any genre.[322]\\nVoice\\n\\n\"Cardigan\"\\nDuration: 22 seconds.0:22\\nSwift uses her lower register in \"Cardigan\" (2020).[323]\\n\"Lavender Haze\"\\nDuration: 18 seconds.0:18\\n\"Lavender Haze\" (2022) features Swift\\'s falsetto vocals in the refrain.[324]\\nProblems playing these files? See media help.\\nSwift possesses a mezzo-soprano vocal range,[325] and a generally soft but versatile timbre.[326][327] As a country singer, her vocals were criticized by some as weak and strained compared to those of her contemporaries.[328] Swift admitted her vocal ability often concerned her in her early career and has worked hard to improve.[329] Reviews of her vocals remained mixed after she transitioned to pop music with 1989; critics complained that she lacked proper technique but appreciated her usage of her voice to communicate her feelings to the audience, prioritizing \"intimacy over power and nuance\".[330] They also praised her for refraining from correcting her pitch with Auto-Tune.[331]\\nThe Los Angeles Times remarked that Swift\\'s defining vocal feature is her attention to detail to convey an exact feeling—\"the line that slides down like a contented sigh or up like a raised eyebrow\".[332] With Reputation, critics noted she was \"learning how to use her voice as a percussion instrument of its own\",[333] swapping her \"signature\" expressive vocals for \"cool, conversational, detached\" cadences and rhythms similar to hip hop and R&B styles.[334][335][336] Alternative Press stated that her \"evocative\" vocal stylings are more reminiscent of pop-punk and emo genres.[337]\\nReviews of Swift\\'s later albums and performances were more appreciative of her vocals, finding them less nasal, richer, more resonant, and more powerful.[304][338][339] With Folklore and Evermore, Swift received praise for her sharp and agile yet translucent and controlled voice.[340][341][342] Pitchfork described it as \"versatile and expressive\".[343] With her 2021 re-recorded albums, critics began to praise the mature, deeper and \"fuller\" tone of her voice.[344][345][346] An i review said Swift\\'s voice is \"leagues better now\".[347] The Guardian highlighted \"yo-yoing vocal yelps\" and passionate climaxes as the trademarks of Swift\\'s voice,[348] and that her country twang faded away.[349] Midnights received acclaim for Swift\\'s nuanced vocal delivery.[350] She ranked 102nd on the 2023 Rolling Stone list of the 200 Greatest Singers of All Time.[327] In a review of the Eras Tour, The New Yorker critic Amanda Petrusich praised the clarity and tone of Swift\\'s live vocals.[351] Musicologist Alyssa Barna said that Swift\\'s timbre is \"breathy and bright\" in her upper register and \"full and dark\" in the lower.[222]\\nSongwriting\\nFurther information: List of songs by Taylor Swift\\nSwift has been referred to as one of the greatest songwriters ever by several publications.[352][353][354] Literature scholars like Jonathan Bate and Stephanie Burt have noted that her literary and melodic sensibility and writing style are rare amongst her peers.[355][356] Swift\\'s bridges are often noted as one of the best aspects of her songs,[357][358] earning her the title \"Queen of Bridges\" from Time.[359] Mojo described her as \"a sharp narrator with a gift for the extended metaphor\".[360]\\nIn The New Yorker in 2011, Swift said she identifies as a songwriter first: \"I write songs, and my voice is just a way to get those lyrics across\".[12] Her personal experiences were a common inspiration for her early songs, which helped her navigate life.[361][362] Her \"diaristic\" technique began with identifying an emotion, followed by a corresponding melody.[363][364] On her first three studio albums, love, heartbreak, and insecurities, from an adolescent perspective, were dominant themes.[365][366] She delved into the tumult of toxic relationships on Red,[367] and embraced nostalgia and post-romance positivity on 1989.[292] Reputation was inspired by the downsides of Swift\\'s fame,[368] and Lover detailed her realization of the \"full spectrum of love\".[369] Other themes in Swift\\'s music include family dynamics, friendship,[370][371] alienation, self-awareness, and tackling vitriol, especially sexism.[279][372]\\nHer confessional lyrics received positive reviews from critics,[373][12][374] who highlighted their vivid details and emotional engagement, which they found uncommon in pop music.[375][376][377] Critics also praised her melodic compositions; Rolling Stone described Swift as \"a songwriting savant with an intuitive gift for verse-chorus-bridge architecture\".[378][379] NPR dubbed Swift \"a master of the vernacular in her lyrics\",[335] remarking that her songs offer emotional engagement because \"the wit and clarity of her arrangements turn them from standard fare to heartfelt disclosures\".[379] Despite the positive reception, The New Yorker stated she was generally portrayed \"more as a skilled technician than as a Dylanesque visionary\".[12] Tabloid media often speculated and linked the subjects of her songs with her ex-lovers, a practice reviewers and Swift herself criticized as sexist.[380][381][382] Aside from clues in album liner notes, Swift avoided talking about the subjects of her songs.[383]\\nOn her 2020 albums Folklore and Evermore, Swift was inspired by escapism and romanticism to explore fictional narratives.[384] She imposed emotions onto imagined characters and story arcs, which liberated her from tabloid attention and suggested new paths for her artistry.[363] Swift explained that she welcomed the new songwriting direction after she stopped worrying about commercial success.[384] According to Spin, she explored complex emotions with \"precision and devastation\" on Evermore.[385] Consequence stated her 2020 albums convinced skeptics of her songwriting prowess, noting her transformation from \"teenage wunderkind to a confident and careful adult\".[358]\\nSwift divides her writing into three types: \"quill lyrics\", songs rooted in antiquated poeticism; \"fountain pen lyrics\", based on modern and vivid storylines; and \"glitter gel pen lyrics\", which are lively and frivolous.[386] Critics note the fifth track of every Swift album as the most \"emotionally vulnerable\" of the album.[387] Awarding her with the Songwriter Icon Award in 2021, the National Music Publishers\\' Association remarked that \"no one is more influential when it comes to writing music today\".[388] The Week deemed her the foremost female songwriter of modern times,[389] and the Nashville Songwriters Association International named her Songwriter-Artist of the Decade in 2022.[245] Swift has also published two original poems: \"Why She Disappeared\" and \"If You\\'re Anything Like Me\".[390]\\nPerformances\\nFurther information: List of Taylor Swift live performances\\n\\nSwift performing on the Reputation Stadium Tour in Seattle in May 2018\\nJournalists have described Swift as one of the best live performers. Often praised for her showmanship and stage presence,[391][392][393][394][395] Swift commands large audiences,[396][397][398] without having to rely on dance like her contemporaries do.[399] According to V magazine\\'s Greg Krelenstein, she possesses \"a rare gift of turning a stadium spectacle into an intimate setting\", irrespective of whether she is \"plucking a guitar or leading an army of dancers\".[400] In a 2008 review of Swift\\'s early performances, Sasha Frere-Jones of The New Yorker called Swift a \"preternaturally skilled\" entertainer with a vibrant stage presence, adding \"she returned the crowd\\'s energy with the professionalism she has shown since the age of fourteen.\"[401] In 2023, Adrian Horton of The Guardian noted her \"seemingly endless stamina\" on the Eras Tour,[402] and i critic Ilana Kaplan called her showmanship \"unparalleled\".[403]\\nCritics have highlighted Swift\\'s versatility as an entertainer, praising her ability to switch onstage personas and performance styles depending on the varying themes and aesthetics of her albums.[404][405] Her concert productions have been characterized by elaborate Broadway theatricality and high technology,[406] and her performances frequently incorporate a live band, with whom she has played and toured since 2007.[407] Swift also often accompanies herself with musical instruments such as electric guitar;[408] acoustic guitar; piano;[409] and sometimes twelve-string guitar,[410][411] banjo,[412] or ukulele.[413] Interacting frequently with the audience, her solo acoustic performances are considered intimate and emotionally resonant, complementing her story-based lyrics and fan connection.[351][414] Lydia Burgham of The Spinoff opined that this intimacy remains \"integral to her singer-songwriter origins\".[415][409] Chris Willman of Variety called Swift \"pop\\'s most approachable superstar\",[416] and the 21st century\\'s most popular performer.[417]\\nVideo and film\\nFurther information: Taylor Swift videography\\nSwift emphasizes visuals as a key creative component of her music-making process.[418] She has collaborated with different directors to produce her music videos, and over time she has become more involved with writing and directing. She developed the concept and treatment for \"Mean\" in 2011[419] and co-directed the music video for \"Mine\" with Roman White the year before.[420] In an interview, White said that Swift \"was keenly involved in writing the treatment, casting and wardrobe. And she stayed for both the 15-hour shooting days, even when she wasn\\'t in the scenes.\"[421]\\nFrom 2014 to 2018, Swift collaborated with director Joseph Kahn on eight music videos—four each from her albums 1989 and Reputation. Kahn has praised Swift\\'s involvement.[422] She worked with American Express for the \"Blank Space\" music video (which Kahn directed), and served as an executive producer for the interactive app AMEX Unstaged: Taylor Swift Experience, for which she won a Primetime Emmy Award for Outstanding Interactive Program in 2015.[423] Swift produced the music video for \"Bad Blood\" and won a Grammy Award for Best Music Video in 2016.[424]\\nHer production company, Taylor Swift Productions, is credited with producing all of her visual media starting with the 2018 concert documentary Reputation Stadium Tour.[425] She continued to co-direct music videos for the Lover singles \"Me!\" with Dave Meyers, and \"You Need to Calm Down\" (also serving as a co-executive producer) and \"Lover\" with Drew Kirsch,[426] but first ventured into sole direction with the video for \"The Man\" (which won her the MTV Video Music Award for Best Direction).[427] After Folklore: The Long Pond Studio Sessions, Swift debuted as a filmmaker with All Too Well: The Short Film,[245] which made her the first artist to win the Grammy Award for Best Music Video as a sole director.[428] Swift has cited Chloé Zhao, Greta Gerwig, Nora Ephron, Guillermo del Toro, John Cassavetes, and Noah Baumbach as filmmaking influences.[418]\\nAccolades and achievements\\nFurther information: List of awards and nominations received by Taylor Swift\\n\\nIn 2009, Swift became the first country singer to win an MTV Video Music Award.\\nSwift\\'s discography is a \"critically hailed songbook\", as per Time\\'s Sam Lansky.[429] She has won 14 Grammy Awards (including four for Album of the Year—the most won by an artist),[430] an Emmy Award,[431] 40 American Music Awards (the most won by an artist),[432] 39 Billboard Music Awards (the most won by an artist—tying with Drake),[433] 118 Guinness World Records,[434] 23 MTV Video Music Awards (including four Video of the Year wins—the most by an act),[247] 12 Country Music Association Awards (including the Pinnacle Award),[435] eight Academy of Country Music Awards,[436] and two Brit Awards.[168] As a songwriter, she has been honored by the Nashville Songwriters Association,[63][437] the Songwriters Hall of Fame, and the National Music Publishers\\' Association and was the youngest person on Rolling Stone\\'s list of the 100 Greatest Songwriters of All Time in 2015.[438][439] At the 64th BMI Awards in 2016, Swift was the first woman to be honored with an award named after its recipient.[440]\\nCommercially, from available data, Swift has amassed over 50 million album sales and 150 million single sales as of 2019,[441][442][443] and 114 million units globally, including 78 billion streams as of 2021.[444][445] The International Federation of the Phonographic Industry ranked her as the Global Recording Artist of the Year for a record three times (2014, 2019 and 2022).[446] Swift has the most number-one albums in the United Kingdom and Ireland for a female artist this millennium,[447][448] earned the highest income for an artist on Chinese digital music platforms (RMB 159,000,000 as of 2021),[449] and is the first artist to replace themselves at the top spot and occupy the entire top five[note 3] of the Australian albums chart.[452][453] Swift remains the world\\'s highest-grossing female touring act ever, with cumulative ticket sales at $1.96 billion as of November 2023 according to Pollstar.[454] The Eras Tour is the highest-grossing tour of all time as of December 2023, and the first to surpass $1 billion in revenue.[455] Beginning with Fearless, each of her studio albums have opened with over one million global units.[456][457] Swift is the most streamed female act on Spotify and Apple Music.[458][459] On Spotify, she is the only artist to have received more than 200 and 250 million streams in one day (260 million on October 27, 2023),[460] and the only female act to reach 100 million monthly listeners.[234] The most entries and the most simultaneous entries for an artist on the Billboard Global 200, with 143 and 31 songs, respectively, are among her feats.[461][462]\\nIn the US, Swift has sold over 37.3 million albums as of 2019,[443] when Billboard placed her eighth on its Greatest of All Time Artists Chart.[463] Eleven of her songs have topped the Billboard Hot 100.[257] She is the longest-reigning act of the Billboard Artist 100 (97 weeks);[464] the soloist with the most cumulative weeks atop the Billboard 200 (68);[465] the woman with the most Billboard 200 number-ones (13),[234] Hot 100 entries (232),[234][156] number-one debuts (6),[note 4] top-ten songs (49),[257] and weeks atop the Top Country Albums chart (101);[467] and the act with the most number-one songs on Pop Airplay (12) and Digital Songs (28).[468][469] Swift is the first woman to simultaneously chart five albums in the top 10 and eleven albums on the entire Billboard 200;[470][471] and the first act to occupy the top four spots and chart seven albums[note 5] in the top 10 on the Top Album Sales chart.[473][474] She is the second highest-certified female digital singles artist (and fifth overall) in the US, with 137.5 million total units certified by the Recording Industry Association of America (RIAA),[475] and the first woman to have both an album (Fearless) and a song (\"Shake It Off\") certified Diamond.[476] Swift is the only artist in Luminate history to have six albums sell over a million copies in a week.[477]\\nSwift has appeared in various power listings. Time included her on its annual list of the 100 most influential people in 2010, 2015, and 2019.[478] She was one of the \"Silence Breakers\" that the magazine spotlighted as Person of the Year in 2017 for speaking up about sexual assault,[479] and received the honor again in 2023 for her cultural domination that year.[429] Time described Swift as the first Person of the Year to be recognized for \"achievement in the arts\", as well as the first woman to be recognized and appear on a Person of the Year cover more than once.[480][481] In 2014, she was named to Forbes\\' 30 Under 30 list in the music category[482] and again in 2017 in its \"All-Star Alumni\" category.[483] Swift became the youngest woman to be included on Forbes\\' list of the 100 most powerful women in 2015, ranked at number 64.[484] In 2023, she was ranked by Forbes as the fifth-most powerful woman in the world, the first entertainer to place in the top five.[485] Swift received an honorary Doctor of Fine Arts degree from New York University and served as its commencement speaker on May 18, 2022.[245]\\nCultural status\\nMain articles: Cultural impact of Taylor Swift and Public image of Taylor Swift\\n\\nSwift at the 2010 Time 100 Gala\\nSwift has been credited with making a profound impact on the music industry, popular culture and the economy.[486][487] She dominates cultural conversations,[488][489] which has led publications to describe her as a cultural \"vitality\" or zeitgeist.[490][491][492] Her music, life and public image are points of attention in global celebrity culture.[297] Initially a teen idol,[493] she has been referred to as a pop icon;[312][494] publications describe her immense popularity and longevity as unwitnessed since the 20th century.[495][496] In 2013, New York magazine\\'s Jody Rosen dubbed Swift the \"world\\'s biggest pop star\" and opined that the trajectory of her stardom has defied established patterns. Rosen added that Swift \"falls between genres, eras, demographics, paradigms, trends\", leaving her contemporaries \"vying for second place\".[310] Critics regard Swift as a rare yet successful combination of the pop star and singer-songwriter archetypes.[497]\\nHer fans are known as Swifties.[266] Billboard noted only few artists have had her chart success, critical acclaim, and fan support.[498] Swift\\'s million-selling albums are considered an anomaly in the streaming-dominated industry following the end of the album era in the 2010s.[499][500] Economist Alan Krueger described Swift as an \"economic genius\".[501]\\nAlthough labeled by the media in her early career as \"America\\'s Sweetheart\" for her girl next door persona,[502][503] Swift has been accused by detractors of being \"calculated\" and manipulative of her image, a narrative bolstered by her 2016 dispute with West.[173][174] Critics have also noted that her personal life and career have been subject to intense misogyny and \"slut-shaming\",[504][505] as well as rampant media scrutiny and tabloid speculation.[506]\\nSwift\\'s private jet use has drawn scrutiny for its carbon emissions.[507][508] In 2023, a spokesperson for Swift stated that she had purchased more than double the required carbon credits to offset all tour travel and personal flights.[509][510] In December 2023, Swift\\'s lawyers sent a cease and desist letter to American programmer Jack Sweeney over tracking her private jet, citing safety concerns and stalking.[511][512] Swift has been a victim of numerous house break-ins and stalkers, some of whom were armed.[513][514]\\nLegacy\\n\"You have different artists dominating different sectors of the industry: Some are huge at streaming, some are big draws on the road. But we\\'re at this moment where there\\'s no one better than Taylor Swift, whether that\\'s on the radio, with streaming, ticket sales or just cultural impact.\"\\n– Jason Lipshutz, Billboard executive director, 2023[515]\\nSwift helped shape the modern country music scene,[516] having extended her success beyond the Anglosphere,[310][516] pioneered the use of internet (Myspace) as a marketing tool,[33][53] and introduced the genre to a younger generation.[517][310] Country labels have since become interested in signing young singers who write their own music;[518] her guitar performances contributed to the \"Taylor Swift factor\", a phenomenon to which an upsurge in guitar sales to women, a previously ignored demographic, is attributed.[519][520]\\nAccording to publications, Swift changed the music landscape with her genre transitions, a discography that accommodates cultural shifts,[521] and her ability to popularize any sound in mainstream music.[522] Lyrically, in being personal and vulnerable in her songs, music journalist Nick Catucci opined Swift helped make space for later singers like Billie Eilish, Ariana Grande, and Halsey to do the same.[523] Scholars have highlighted the literary sensibility and poptimist implications of Swift.[355][524] She has been credited with legitimizing and popularizing the concept of album \"eras\".[525][526] Swift is a subject of academic study and scholarly media research.[297] Various educational institutions offer courses on Swift in literary, cultural and sociopolitical contexts.[527][297]\\nSwift has influenced numerous music artists, and her albums have inspired a generation of singer-songwriters.[517][320][528] Journalists praise her ability to reform industry practices, noting how her actions changed streaming policies, prompted awareness of intellectual property in new musicians,[529][530] and reshaped ticketing models.[531] Various sources deem Swift\\'s music a paradigm representing the millennial generation;[532] Vox called her the \"millennial Bruce Springsteen\",[533] and The Times named her \"the Bob Dylan of our age\".[534] Swift earned the title Woman of the Decade (2010s) from Billboard,[535] Artist of the Decade (2010s) at the American Music Awards,[536] and Global Icon at the Brit Awards for her impact.[445] Senior artists such as Paul McCartney,[537] Mick Jagger,[538] Madonna,[539] and Dolly Parton have praised her musicianship.[540] Carole King regards Swift her \"professional grand daughter\" and thanked Swift for \"carrying the torch forward\".[541] Springsteen called her a \"tremendous\" writer,[542] while Ringo Starr and Billy Joel considered Swift the Beatles\\' successor.[543][544] Britney Spears labeled Swift \"the most iconic pop woman of our generation\".[545]\\nEntrepreneurship\\nMedia outlets describe Swift as a savvy businesswoman;[546][547] in 2024, she topped Billboard\\'s annual Power 100 ranking of the top music industry executives.[548] Swift is known for her traditional album rollouts, consisting of a variety of promotional activities that Rolling Stone termed as an inescapable \"multimedia bonanza\".[549][550] Easter eggs and cryptic teasers became a common practice in contemporary pop music because of Swift.[551] Publications describe her discography as a music \"universe\" subject to analyses by fans, critics and journalists.[552][553][554] Swift maintains an active presence on social media and a close relationship with fans, to which many journalists attribute her success.[555][487][556] Her in-house management team is called 13 Management.[557]\\nSwift has endorsed many brands and businesses, having launched clothing lines with L.E.I. and Stella McCartney,[558][559] designed American Greetings cards and Jakks Pacific dolls,[560][561] released a number of fragrances with Elizabeth Arden,[562] and signed multi-year deals with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League\\'s Nashville Predators and Sony Cyber-shot digital cameras,[565][566] and became the global ambassador for New York City in 2014 and Record Store Day in 2022.[567][568]\\nSocial activism\\nFurther information: Political impact of Taylor Swift\\nSwift identifies as a pro-choice feminist,[569] and is a founding signatory of the Time\\'s Up movement against sexual harassment.[570] Specifically, she criticized the US Supreme Court\\'s decision to end federal abortion rights in 2022.[571] Swift also advocates for LGBT rights,[572] and has called for the passing of the Equality Act, which prohibits discrimination based on sex, sexual orientation, and gender identity.[573][574] She performed during WorldPride NYC 2019 at the Stonewall Inn, a gay rights monument, and has donated to the LGBT organizations Tennessee Equality Project and GLAAD.[575][576][577]\\nA supporter of the March for Our Lives movement and gun control reform in the US,[578] Swift is a vocal critic of white supremacy, racism, and police brutality.[579][569] Following the George Floyd protests, she donated to the NAACP Legal Defense and Educational Fund and the Black Lives Matter movement,[580] called for the removal of Confederate monuments in Tennessee,[581] and advocated for Juneteenth to become a national holiday.[582] In 2020, Swift urged her fans to check their voter registration ahead of elections, which resulted in 65,000 people registering to vote within one day of her post,[583] and endorsed Joe Biden and Kamala Harris in the US presidential election.[584] She has openly criticized former president Donald Trump.[585]\\nWealth\\nSwift\\'s net worth is estimated by Forbes and Bloomberg News at $1.1 billion as of October 2023, making her the first musician to achieve billionaire status \"solely based on her songs and performances\".[586][587] Forbes named her the annual top-earning female musician in 2016, 2019, 2021, and 2022.[588] She was the highest-paid celebrity of 2016 with $170 million—a feat recognized by the Guinness World Records as the highest annual earnings ever for a female musician,[589] which she herself surpassed with $185 million in 2019.[590] Overall, Forbes listed Swift as the highest-paid female artist of the 2010s, earning $825 million.[591] She has also developed a real estate portfolio worth $150 million as of 2023, with properties in Nashville; Tribeca, Manhattan; Los Angeles (Samuel Goldwyn Estate); and Rhode Island (High Watch).[592]\\nPhilanthropy\\nSwift is known for her philanthropic efforts.[593] She ranked first on DoSomething\\'s 2015 \"Gone Good\" list,[594] having received the Star of Compassion from the Tennessee Disaster Services and the Big Help Award from the Nickelodeon Kids\\' Choice Awards for her \"dedication to helping others\" and \"inspiring others through action\".[595][596] She donated $100,000 to the Red Cross to help the victims of the Iowa flood of 2008.[597] In 2009, she sang at BBC\\'s Children in Need concert and raised £13,000 for the cause.[598] Swift has performed at charity relief events, including Sydney\\'s Sound Relief concert.[599] In response to the May 2010 Tennessee floods, Swift donated $500,000.[600] In 2011, Swift used a dress rehearsal of her Speak Now tour as a benefit concert for victims of recent tornadoes in the US, raising more than $750,000.[601] In 2016, she donated $1 million to Louisiana flood relief efforts and $100,000 to the Dolly Parton Fire Fund.[602][603] Swift donated to food banks after Hurricane Harvey struck Houston in 2017 and at every stop of the Eras Tour in 2023;[604][605] she also directly employed local businesses throughout the tour and gave $55 million in bonus payments to her entire crew.[606][607] Swift donated $1 million for Tennessee tornado relief in 2020 and again in 2023.[608][609]\\nShe is a supporter of the arts. A benefactor of the Nashville Songwriters Hall of Fame,[610] Swift has donated $75,000 to Nashville\\'s Hendersonville High School to help refurbish the school auditorium,[611] $4 million to build a new education center at the Country Music Hall of Fame and Museum in Nashville,[612] $60,000 to the music departments of six US colleges,[613] and $100,000 to the Nashville Symphony.[614] Also a promoter of children\\'s literacy, she has donated money and books to schools around the country.[615][616] In 2007, Swift partnered with the Tennessee Association of Chiefs of Police to launch a campaign to protect children from online predators.[617] She has donated items to several charities for auction, including the UNICEF Tap Project and MusiCares.[618] As recipient of the Academy of Country Music\\'s Entertainer of the Year in 2011, Swift donated $25,000 to St. Jude Children\\'s Research Hospital, Tennessee.[619] In 2012, Swift participated in the Stand Up to Cancer telethon, performing the charity single \"Ronan\", which she wrote in memory of a four-year-old boy who died of neuroblastoma.[620] She has also donated $100,000 to the V Foundation for Cancer Research[621] and $50,000 to the Children\\'s Hospital of Philadelphia.[622] Swift has encouraged young people to volunteer in their local communities as part of Global Youth Service Day.[623]\\nSwift donated to fellow singer-songwriter Kesha to help with her legal battles against Dr. Luke and to actress Mariska Hargitay\\'s Joyful Heart Foundation.[593][624] During the COVID-19 pandemic, Swift donated to the World Health Organization and Feeding America,[625] and supported independent record stores.[626][627] Swift performed \"Soon You\\'ll Get Better\" on the One World: Together At Home television special, a benefit concert curated by Lady Gaga for Global Citizen to raise funds for the World Health Organization\\'s COVID-19 Solidarity Response Fund.[628] In 2018 and 2021, Swift donated to the Rape, Abuse & Incest National Network in honor of Sexual Assault Awareness and Prevention Month.[593][629] She has made donations to her fans several times for their medical or academic expenses.[630] In December 2023, Swift attended Ramy Youssef\\'s fundraiser for the Gaza Strip.[631]\\nDiscography\\nMain articles: Taylor Swift albums discography, Taylor Swift singles discography, and List of songs by Taylor Swift\\nStudio albums\\nTaylor Swift (2006)\\nFearless (2008)\\nSpeak Now (2010)\\nRed (2012)\\n1989 (2014)\\nReputation (2017)\\nLover (2019)\\nFolklore (2020)\\nEvermore (2020)\\nMidnights (2022)\\nThe Tortured Poets Department (2024)\\nRe-recorded albums\\nFearless (Taylor\\'s Version) (2021)\\nRed (Taylor\\'s Version) (2021)\\nSpeak Now (Taylor\\'s Version) (2023)\\n1989 (Taylor\\'s Version) (2023)\\nFilmography\\nMain article: Taylor Swift videography\\nThis section lists select works only. Refer to the main article for further information.\\nValentine\\'s Day (2010)\\nThe Lorax (2012)\\nThe Giver (2014)\\nCats (2019)\\nAll Too Well: The Short Film (also director) (2021)\\nAmsterdam (2022)\\nDocumentary and concert films\\nJourney to Fearless (2010)\\nSpeak Now World Tour – Live (2011)\\nThe 1989 World Tour Live (2015)\\nTaylor Swift: Reputation Stadium Tour (2018)\\nMiss Americana (2020)\\nTaylor Swift: City of Lover (2020)\\nFolklore: The Long Pond Studio Sessions (also director) (2020)\\nTaylor Swift: The Eras Tour (2023)\\nTours\\nMain article: List of Taylor Swift live performances\\nFearless Tour (2009–2010)\\nSpeak Now World Tour (2011–2012)\\nThe Red Tour (2013–2014)\\nThe 1989 World Tour (2015)\\nReputation Stadium Tour (2018)\\nThe Eras Tour (2023–2024)\\nSee also\\nList of American Grammy Award winners and nominees\\nList of highest-certified music artists in the United States\\nList of most-followed Instagram accounts\\nList of most-followed Twitter accounts\\nList of most-subscribed YouTube channels\\nFootnotes\\n Swift held the record until the 62nd Annual Grammy Awards in 2020.[91][92]\\n Though Swift has properties throughout the US, she identifies Nashville as her home.[149][150]\\n Swift has occupied the top five of the ARIA Albums Chart twice. She achieved this feat first on the issue published on July 7, 2023,[450] followed by a second time on the issue published on February 9, 2024.[451]\\n In a tie with Ariana Grande.[466]\\n Swift has charted seven titles in the top 10 of the Top Album Sales chart twice—on the issues dated January 6, 2024, and January 20, 2024.[472]\\nReferences\\n \"Taylor Swift: The record-breaking artist in numbers\". Newsround. March 2, 2020. Archived from the original on March 8, 2020. Retrieved April 20, 2020.\\n Sutherland, Mark (May 23, 2015). \"Taylor Swift interview: \\'A relationship? No one\\'s going to sign up for this\\'\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved April 20, 2020.\\n Scott, Walter (June 11, 2015). \"What Famous Pop Star Is Named After James Taylor?\". Parade. Archived from the original on October 15, 2016. Retrieved December 12, 2018.\\n \"Taylor Swift is not an \"underdog\": The real story about her 1 percent upbringing that the New York Times won\\'t tell you\". Salon.com. May 23, 2015. Archived from the original on May 25, 2022. Retrieved December 26, 2020.\\n Jepson 2013, p. 1.\\n Roth, Madeline (May 19, 2015). \"Taylor Swift\\'s Brother Had The Most Epic Graduation Weekend Ever\". MTV News. Archived from the original on July 23, 2016. Retrieved July 25, 2016.\\n McKay, Gabriel (July 6, 2023). \"Taylor Swift Edinburgh: Is star the real queen of Scotland?\". The Herald. Archived from the original on February 1, 2024. Retrieved February 4, 2024.\\n Eleftheriou-Smith, Loulla-Mae (June 24, 2015). \"Taylor Swift tells Scotland: \\'I am one of you\\'\". The Independent. Archived from the original on May 26, 2022. Retrieved July 10, 2019.\\n Vadala, Nick (July 14, 2017). \"Taylor Swift ancestor\\'s home added to Philly Register of Historic Places\". The Philadelphia Inquirer. Archived from the original on May 25, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift\\'s Great-Great-Grandfather\\'s Philly Home Gets Historic Landmark Status\". AP NEWS. July 25, 2017. Archived from the original on May 22, 2022. Retrieved January 15, 2021.\\n \"Taylor Swift stammt aus dem Freistaat\" (in German). BR24. September 17, 2015. Archived from the original on December 31, 2021. Retrieved July 23, 2023.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Raab, Scott (October 20, 2014). \"Taylor Swift Interview\". Esquire. Archived from the original on February 16, 2015. Retrieved April 11, 2015.\\n \"Taylor Swift on Politicians Co-opting Faith: \\'I\\'m a Christian. That\\'s Not What We Stand For\\'\". Relevant. January 31, 2020. Archived from the original on November 25, 2022. Retrieved April 2, 2020.\\n Uhrich, Bill (February 13, 2010). \"Photos Students at Alvernia Montessori School sending Taylor Swift a valentine\". Reading Eagle. Archived from the original on October 16, 2013. Retrieved February 25, 2013.\\n Hatza, George (December 8, 2008). \"Taylor Swift: Growing into superstardom\". Reading Eagle. Archived from the original on April 1, 2012. Retrieved April 17, 2012.\\n Mennen, Lauren (November 12, 2014). \"Taylor Swift\\'s Wyomissing childhood home on the market for $799,500\". Philadelphia Daily News. Archived from the original on October 17, 2016. Retrieved October 13, 2016.\\n Chang, David (February 22, 2016). \"Taylor Swift Returns to Reading Pennsylvania as Maid of Honor in Friend\\'s Wedding\". WCAU. Archived from the original on September 16, 2016. Retrieved August 26, 2016.\\n \"Taylor Swift, Age 12\". New York Daily News. Archived from the original on August 27, 2016. Retrieved August 26, 2016.\\n Cooper, Brittany Joy (April 15, 2012). \"Taylor Swift Opens Up About a Future in Acting and Admiration for Emma Stone\". Taste of Country. Archived from the original on April 17, 2012. Retrieved April 17, 2012.\\n MacPherson, Alex (October 18, 2012). \"Taylor Swift: \\'I want to believe in pretty lies\\'\". The Guardian. Archived from the original on August 26, 2016. Retrieved August 3, 2016.\\n Rolling Stone Interview: The Unabridged Taylor Swift, December 2, 2008\\n Morris, Edward (December 1, 2006). \"When She Thinks \\'Tim McGraw\\', Taylor Swift Savors Payoff: Hardworking Teen to Open for George Strait Next Year\". CMT. Archived from the original on June 26, 2015. Retrieved March 11, 2010.\\n Diu, Nisha Lilia (April 3, 2011). \"Taylor Swift: \\'I won\\'t do sexy shoots\\'\". The Daily Telegraph. Archived from the original on May 6, 2013. Retrieved April 17, 2012.\\n \"News : CMT Insider Interview: Taylor Swift (Part 1 of 2)\". CMT. November 26, 2008. Archived from the original on January 23, 2015. Retrieved July 1, 2012.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. Archived from the original on May 10, 2012. Retrieved May 21, 2012.\\n Qureshi, Hira. \"Visit this Stone Harbor café where Taylor Swift was \\'always coming in to play\\' as a child\". Courier-Post. Archived from the original on October 26, 2023. Retrieved December 12, 2022.\\n Kuperinsky, Amy (July 28, 2020). \"Taylor Swift shouts out Jersey Shore town in video for surprise album\". NJ.com. Archived from the original on December 12, 2022. Retrieved December 12, 2022.\\n Martino, Andy (January 10, 2015). \"EXCLUSIVE: The real story of Taylor Swift\\'s guitar \\'legend\\'\". New York Daily News. Archived from the original on November 22, 2015. Retrieved August 28, 2017.\\n \"Dymtrow v. Swift et al: Federal Civil LawsuitNew York Southern District Court, Case No. 1:07-cv-11277-RJS\" (PDF). American Bar Association. Archived from the original (PDF) on October 11, 2012. Retrieved April 18, 2012.\\n \"On tour with Taylor Swift\". NBC News. May 31, 2009. Archived from the original on October 5, 2013. Retrieved July 1, 2012.\\n Castro, Vicky (February 6, 2015). \"How to Succeed as an Entrepreneur, Taylor Swift Style\". Inc. Archived from the original on June 7, 2016. Retrieved February 9, 2015.\\n Willman, Chris (July 25, 2007). \"Getting to know Taylor Swift\". Entertainment Weekly. Archived from the original on May 24, 2022. Retrieved January 25, 2022.\\n Jo, Nancy (January 2, 2014). \"Taylor Swift and the Growing of a Superstar: Her Men, Her Moods, Her Music\". Vanity Fair. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n \"News : Taylor Swift\\'s High School Names Auditorium in Her Honor\". CMT. September 23, 2010. Archived from the original on November 21, 2014. Retrieved April 18, 2012.\\n Grigoriadis, Vanessa (March 5, 2009). \"The Very Pink, Very Perfect Life of Taylor Swift\". Rolling Stone. Archived from the original on May 3, 2019. Retrieved July 28, 2019.\\n \"Taylor Swift receives her high school diploma\". Houston Chronicle. July 27, 2008. Archived from the original on January 1, 2024. Retrieved January 2, 2024.\\n \"Taylor Swift: The Garden In The Machine\". American Songwriter. May 2, 2011. Archived from the original on August 7, 2013. Retrieved May 21, 2012.\\n \"Songwriter Taylor Swift Signs Publishing Deal With Sony/ATV\". Broadcast Music, Inc. May 12, 2005. Archived from the original on December 4, 2012. Retrieved April 20, 2012.\\n Kosser, Michael (June 3, 2010). \"Liz Rose: Co-Writer to the Stars\". American Songwriter. Archived from the original on December 24, 2011. Retrieved April 19, 2012.\\n Leahey, Andrew (October 24, 2014). \"Songwriter Spotlight: Liz Rose\". Rolling Stone. Archived from the original on September 26, 2016. Retrieved September 24, 2016.\\n DeLuca, Dan (November 11, 2008). \"Focused on \\'great songs\\' Taylor Swift isn\\'t thinking about \\'the next level\\' or Joe Jon as gossip\". Philadelphia Daily News. p. 1. Archived from the original on November 18, 2012. Retrieved April 17, 2012.\\n Preston, John (April 26, 2009). \"Taylor Swift: the 19-year-old country music star conquering America – and now Britain\". The Daily Telegraph. Archived from the original on January 5, 2012. Retrieved August 30, 2012.\\n Rosa, Christopher (March 24, 2015). \"Opening Acts Who Became Bigger Than The Headliner\". VH1. Archived from the original on November 10, 2015. Retrieved November 11, 2015.\\n Rapkin, Mickey (July 27, 2017). \"Oral History of Nashville\\'s Bluebird Cafe: Taylor Swift, Maren Morris, Dierks Bentley & More on the Legendary Venue\". Billboard. Archived from the original on July 29, 2017. Retrieved July 28, 2017.\\n Hiatt, Brian (October 25, 2012). \"Taylor Swift in Wonderland\". Rolling Stone. Archived from the original on July 31, 2016. Retrieved August 1, 2016.\\n Greenburg, Zack O\\'Malley (June 26, 2013). \"Toby Keith, Cowboy Capitalist: Country\\'s $500 Million Man\". Forbes. Archived from the original on August 27, 2016. Retrieved August 1, 2016.\\n Taylor Swift (CD). Big Machine Records. 2006. BMR120702.\\n Tamarkin, Jeff. \"Taylor Swift – Taylor Swift\". AllMusic. Archived from the original on October 20, 2015. Retrieved February 14, 2021.\\n Neal, Chris (December 4, 2006). \"Taylor Swift Review\". Country Weekly. Archived from the original on July 22, 2012. Retrieved March 31, 2010.\\n Trust, Gary (October 29, 2009). \"Chart Beat Thursday: Taylor Swift, Tim McGraw Linked Again\". Billboard. Archived from the original on March 7, 2013. Retrieved November 8, 2016.\\n \"Taylor Swift\". Songwriters\\' Hall of Fame. Archived from the original on February 12, 2021. Retrieved September 21, 2022.\\n Willman, Chris (February 5, 2008). \"Taylor Swift\\'s Road to Fame\". Entertainment Weekly. p. 3. Archived from the original on February 21, 2015. Retrieved April 22, 2012.\\n \"Taylor Swift Joins Rascal Flatts Tour\". CMT. October 18, 2006. Archived from the original on January 7, 2015. Retrieved March 11, 2010.\\n Whitaker, Sterling; Hammar, Ania (May 27, 2019). \"How Eric Church\\'s Rascal Flatts Feud Helped Launch Taylor Swift\\'s Career\". Taste of Country. Townsquare Media. Archived from the original on June 6, 2019. Retrieved June 10, 2019.\\n \"Taylor Swift No. 1 on iTunes\". Great American Country. December 19, 2007. Archived from the original on March 3, 2012. Retrieved July 5, 2010.\\n \"Taylor Swift – Chart history\". Billboard. Archived from the original on August 9, 2016. Retrieved July 26, 2016.\\n \"Taylor Swift owns top of country chart\". Country Standard Time. July 23, 2008. Archived from the original on July 31, 2008. Retrieved December 26, 2008.\\n \"Wal-Mart \"Eyes\" New Taylor Swift Project\". Great American Country. Archived from the original on July 23, 2008. Retrieved July 24, 2008.\\n \"Taylor Swift Joins George Strait\\'s 2007 Tour\". CMT. November 17, 2006. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Brad Paisley Plans Tour With Three Opening Acts\". CMT. January 9, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Joins Tim McGraw, Faith Hill on Tour\". CMT. June 1, 2007. Archived from the original on August 11, 2017. Retrieved February 16, 2020.\\n \"Taylor Swift Youngest Winner of Songwriter/Artist Award\". Great American Country. October 16, 2007. Archived from the original on January 11, 2015. Retrieved February 2, 2015.\\n \"Photos : All Taylor Swift Pictures : Horizon Award Winner Poses in the Pressroom\". CMT. September 7, 2007. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Photos : 43rd Annual ACM Awards – Onstage: Winners : Acceptance Speech\". CMT. May 18, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Taylor Swift, Rascal Flatts, Carrie Underwood Score at 2008 AMA Awards\" (Blog). Roughstock.com. November 24, 2008. Archived from the original on July 10, 2014. Retrieved May 21, 2012.\\n \"Amy Winehouse Wins Best New Artist, Kanye West Pays Tribute to Mom – Grammy Awards 2008, Grammy Awards\". People. October 2, 2008. Archived from the original on November 13, 2012. Retrieved May 21, 2012.\\n \"Rascal Flatts Announce Summer Tour With Taylor Swift\". CMT. May 5, 2008. Archived from the original on April 18, 2022. Retrieved June 5, 2019.\\n Caplan, David (September 8, 2008). \"Scoop\". People. Archived from the original on February 2, 2016. Retrieved March 6, 2012.\\n Rizzo, Monica (November 24, 2008). \"Scoop – Couples, Camilla Belle, Joe Jonas\". People. Archived from the original on March 3, 2016. Retrieved March 6, 2012.\\n Akers, Shelley (June 9, 2008). \"Taylor Swift to Appear in Hannah Montana Movie\". People. Archived from the original on October 27, 2017. Retrieved October 27, 2017.\\n \"Hannah Montana: The Movie (Original Motion Picture Soundtrack) by Hannah Montana\". iTunes Store. January 2009. Archived from the original on May 2, 2016. Retrieved August 2, 2016.\\n \"CD Taylor Swift – Fearless\" (in Portuguese). Universal Music Group. Archived from the original on January 18, 2021. Retrieved February 14, 2021.\\n Raphael, Amy (February 1, 2009). \"First, she conquered Nashville. Now she\\'s set for world domination\". The Observer. ProQuest 250507223. Archived from the original on December 9, 2022. Retrieved December 9, 2022.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong with Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved July 24, 2014.\\n \"Discography Taylor Swift\". ARIA Charts. Archived from the original on March 21, 2012. Retrieved January 2, 2010.\\n Trust, Gary (December 15, 2009). \"Best of 2009: Part 1\". Billboard. Archived from the original on March 3, 2013. Retrieved September 20, 2022.\\n Ben-Yehuda, Ayala (August 13, 2009). \"Black Eyed Peas, Jason Mraz Tie Records on Billboard Hot 100\". Billboard. Archived from the original on May 8, 2013. Retrieved March 13, 2010.\\n Trust, Gary (September 24, 2009). \"Taylor Swift Climbs Hot 100, Black Eyed Peas Still No. 1\". Billboard. Archived from the original on February 1, 2013. Retrieved September 20, 2022.\\n \"Taylor Swift Chart History (Hot Country Songs)\". Billboard. Archived from the original on January 31, 2021. Retrieved February 14, 2021.\\n Grein, Paul (March 16, 2012). \"Chart Watch Extra: Top Albums Of Last 10 Years\" (Blog). Yahoo! Music. Archived from the original on April 2, 2015. Retrieved June 10, 2011.\\n Mapes, Jillian (November 23, 2010). \"Taylor Swift Announces \\'Speak Now\\' World Tour\". Billboard. Archived from the original on May 8, 2013. Retrieved May 15, 2012.\\n Weiss, Dan (December 12, 2011). \"Taylor Swift: Journey To Fearless DVD\". American Songwriter. Archived from the original on August 15, 2016. Retrieved August 2, 2016.\\n Ryan, Sarah (August 10, 2009). \"Taylor Swift Pranks Keith Urban\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved November 11, 2015.\\n \"Kanye calls Taylor Swift after \\'View\\' appearance\". MSNBC. September 15, 2009. Archived from the original on October 6, 2013. Retrieved September 16, 2009.\\n \"Taylor Swift Thanks \"Gracious\" Beyonce for Inviting Her Onstage After Kanye Stunt at VMAs\". Rolling Stone. September 14, 2009. Archived from the original on June 15, 2012. Retrieved May 15, 2012.\\n Anderson, Kyle (September 16, 2009). \"Kanye West\\'s VMA Interruption Gives Birth To Internet Photo Meme\". MTV. Archived from the original on January 16, 2016. Retrieved October 3, 2009.\\n Ditzian, Eric (2009). \"Taylor Swift, Michael Jackson Big Winners at American Music Awards\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"2009 Artists of the Year\". Billboard. December 10, 2009. Archived from the original on January 6, 2010. Retrieved May 21, 2012.\\n \"Taylor Swift Raps \\'Thug Story\\' With T-Pain On CMT Awards\". MTV. June 17, 2009. Archived from the original on May 25, 2022. Retrieved November 11, 2015.\\n Kreps, Daniel (February 1, 2010). \"Beyonce, Taylor Swift Dominate 2010 Grammy Awards\". Rolling Stone. Archived from the original on February 15, 2012. Retrieved February 13, 2012.\\n \"Billie Eilish replaces Taylor Swift as youngest artist to win a Grammy for Album of the Year\". MSN. January 27, 2020. Archived from the original on November 20, 2020. Retrieved September 1, 2020.\\n Kaufman, Gil (November 12, 2009). \"Taylor Swift Dominates CMA Awards\". MTV News. Archived from the original on March 6, 2016. Retrieved September 13, 2016.\\n Vena, Jocelyn (November 6, 2009). \"John Mayer Talks Taylor Swift Collaboration \\'Half of My Heart\\'\". MTV. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n \"Boys Like Girls featuring Taylor Swift, \\'Two Is Better Than One\\'\". Billboard. December 2, 2009. Archived from the original on August 18, 2021. Retrieved December 4, 2020.\\n \"Kellie Pickler Has Her \\'Best Days\\' Thanks To Taylor Swift\". MTV. Archived from the original on January 1, 2016. Retrieved November 11, 2015.\\n Vena, Jocelyn (December 28, 2009). \"New Taylor Swift Song Included In \\'Valentine\\'s Day\\' Featurette\". MTV. Archived from the original on January 16, 2016. Retrieved November 11, 2015.\\n \"Taylor Swift – Chart history on Canadian Hot 100\". Billboard. Archived from the original on August 9, 2016. Retrieved August 3, 2016.\\n Park, Michael Y.; Sia, Nicole (December 29, 2009). \"Taylor & Taylor Romance Was Overblown, Says Source\". People. Archived from the original on November 13, 2012. Retrieved March 6, 2012.\\n Caramanica, Jon (March 6, 2009). \"OMG! Taylor Swift Does \\'CSI\\'!\". The New York Times (Blog). Archived from the original on August 14, 2011. Retrieved May 7, 2012.\\n Strecker, Erin (January 2, 2015). \"Remember When Taylor Swift Shined as \\'Saturday Night Live\\' Host?\". Billboard. Archived from the original on January 24, 2015. Retrieved January 15, 2015.\\n Dukes, Billy (October 22, 2012). \"10 Things You Didn\\'t Know About Taylor Swift\". Taste of Country. Townsquare Media. Archived from the original on May 22, 2022. Retrieved July 26, 2020.\\n Pietroluongo, Silvio (August 11, 2010). \"Taylor Swift Makes Sparkling Hot 100 Entrance\". Billboard. Archived from the original on January 31, 2016. Retrieved July 25, 2016.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift, Angry on \\'Speak Now\\'\". The New York Times. Archived from the original on October 21, 2010. Retrieved October 23, 2010.\\n \"Taylor Swift\\'s New Album, Speak Now, Set for Oct. 25 Release\". CMT. July 20, 2010. Archived from the original on June 21, 2019. Retrieved February 14, 2020.\\n Kaufman, Gil (November 3, 2010). \"Taylor Swift\\'s Speak Now Tops 1 Million in First Week\". MTV. Archived from the original on August 10, 2016. Retrieved August 8, 2016.\\n \"Fastest-selling digital album in the US by a female artist\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Knopper, Steve (November 25, 2010). \"Taylor Swift\\'s Speak Now Tops the Charts\". Rolling Stone. Archived from the original on March 3, 2021. Retrieved November 25, 2010.\\n Sheffield, Rob (October 26, 2010). \"Speak Now (2010)\". Rolling Stone. Archived from the original on September 11, 2013. Retrieved December 20, 2019.\\n Hammel, Sara (January 4, 2011). \"Taylor Swift & Jake Gyllenhaal Break Up: Source\". People. Archived from the original on May 9, 2012. Retrieved March 6, 2012.\\n Wyland, Sarah (February 12, 2012). \"Taylor Swift Takes Home Two GRAMMYs at Tribute-Filled Show\" (Blog). Great American Country. Archived from the original on September 6, 2015. Retrieved February 13, 2012.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on January 16, 2016. Retrieved November 21, 2015.\\n Smith, Hazel (October 24, 2011). \"News : Hot Dish: Taylor Swift Sings Alan Jackson\\'s Masterpiece at Nashville Songwriters Celebration\". CMT. Archived from the original on November 29, 2014. Retrieved April 22, 2012.\\n Roland, Tom (December 2, 2011). \"Taylor Swift: Billboard\\'s Woman of the Year\". Billboard. Archived from the original on February 3, 2013. Retrieved May 15, 2012.\\n Talbott, Chris; Silva, Cristina (April 2, 2012). \"Taylor Swift wins ACM entertainer of the year\". Yahoo!. Associated Press. Archived from the original on August 23, 2016. Retrieved May 15, 2012.\\n \"CMA Awards 2011: Taylor Swift wins entertainer of the year\". CBS News. November 9, 2011. Archived from the original on September 8, 2014. Retrieved May 15, 2012.\\n Kellogg, Jane (November 20, 2011). \"AMAs 2011: Winners and Nominees Complete List\". The Hollywood Reporter. Archived from the original on June 27, 2015. Retrieved November 21, 2015.\\n Sheffield, Rob (June 23, 2012). \"Women Who Rock: The 50 Greatest Albums of All Time\". Rolling Stone. Archived from the original on December 10, 2016. Retrieved July 15, 2017.\\n Allen, Bob (March 29, 2012). \"Hot Tours: Taylor Swift, George Strait, Cirque Du Soleil\". Billboard. Archived from the original on February 21, 2013. Retrieved May 10, 2012.\\n \"Taylor Swift News and Blog\". taylorswift.com. September 21, 2011. Archived from the original on October 11, 2011. Retrieved September 21, 2011.\\n Herrera, Monica (March 15, 2012). \"Taylor Swift, Arcade Fire Talk \\'Hunger Games\\'\". Rolling Stone. Archived from the original on June 27, 2015. Retrieved May 10, 2012.\\n \"Nominations 2013 — Golden Globe Awards\". goldenglobes.org. December 13, 2012. Archived from the original on December 14, 2012. Retrieved December 13, 2012.\\n Horowitz, Steven J. (April 20, 2012). \"B.o.B Explains Origins of Taylor Swift Collaboration \\'Both of Us\\'\". HipHopDX. Archived from the original on September 10, 2015. Retrieved May 15, 2012.\\n Toomedy, Alyssa (October 25, 2012). \"Taylor Swift and Conor Kennedy Breakup: Anatomy of a Split\". E! News. Archived from the original on December 4, 2015. Retrieved November 10, 2015.\\n Trust, Gary (August 22, 2012). \"Taylor Swift Scores First Hot 100 No. 1\". Billboard. Archived from the original on February 13, 2013. Retrieved August 22, 2012.\\n \"Discography Taylor Swift\". New Zealand Charts. Archived from the original on April 20, 2017. Retrieved July 26, 2016.\\n Lynch, Kevin (September 4, 2013). \"Calvin Harris trumps Michael Jackson feat to join Taylor Swift, Rihanna and One Direction in Guinness World Records™ 2014 book\". Guinness World Records. Archived from the original on June 22, 2015. Retrieved June 16, 2015.\\n Chart positions:\\n • \"Taylor Swift – I Knew You Were Trouble\". ARIA Charts. Archived from the original on May 27, 2016. Retrieved February 14, 2021.\\n • \"Official Singles Charts Top 100\". Official Charts Company. Archived from the original on July 13, 2021. Retrieved February 14, 2021.\\n • \"Taylor Swift Leads Record Breaking Digital Sales Week\". Billboard. January 3, 2013. Archived from the original on April 8, 2017. Retrieved February 14, 2021.\\n Lewis, Randy (October 30, 2012). \"Taylor Swift raises the bar with a savvy \\'Red\\' marketing campaign\". Los Angeles Times. Archived from the original on December 28, 2020. Retrieved December 28, 2020.\\n Mansfield, Brian (October 17, 2012). \"Taylor Swift sees \\'Red\\' all over\". USA Today. Archived from the original on December 21, 2012.\\n English, J. (August 28, 2017). \"Taylor Swift\\'s \\'Red\\': A Canonical Coming-Of-Age Album\". NPR. Archived from the original on April 12, 2021. Retrieved February 14, 2021.\\n Roberts, Randall (October 31, 2012). \"\\'The Last Time\\' connects Taylor Swift with Arcade Fire\". Los Angeles Times. Archived from the original on March 6, 2016. Retrieved December 27, 2020.\\n Greenwald, David (September 6, 2013). \"Taylor Swift, Rihanna, Justin Bieber Among 2014 Guinness Record-Setters\". Billboard. Archived from the original on June 8, 2015. Retrieved July 27, 2016.\\n Sexton, Paul (August 31, 2019). \"Taylor Swift Scores Fourth U.K. No. 1 With \\'Lover\\' Album\". Billboard. Archived from the original on August 16, 2021. Retrieved October 15, 2019.\\n \"Grammys 2014: The complete list of nominees and winners\". Los Angeles Times. January 26, 2014. Archived from the original on March 4, 2015. Retrieved January 25, 2015.\\n Gregoire, Carolyn (November 19, 2012). \"Taylor Swift AMA Awards 2012: Pop Star Performs \\'I Knew You Were Trouble\\' (Video)\". HuffPost. Archived from the original on May 26, 2013. Retrieved June 10, 2013.\\n Payne, Chris (November 25, 2013). \"Taylor Swift & Justin Timberlake Win Big at American Music Awards\". Billboard. Archived from the original on November 24, 2015. Retrieved November 21, 2015.\\n \"NSAI Songwriter/Artists of the Year\". Nashville Songwriters Association International. Archived from the original on August 19, 2016. Retrieved August 2, 2016.\\n Allen, Bob (July 3, 2014). \"Taylor Swift\\'s Red Wraps as All-Time Country Tour\". Billboard. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Caramanica, Jon (November 7, 2013). \"Country Awards Hold Swift Close\". The New York Times. Archived from the original on November 11, 2013. Retrieved April 3, 2014.\\n \"Taylor Swift\\'s Boyfriend Timeline: 10 Relationships & Their Songs\". Billboard. December 30, 2014. Archived from the original on August 26, 2016. Retrieved August 26, 2016.\\n Labrecque, Jeff (December 12, 2013). \"\\'12 Years a Slave\\' and \\'American Hustle\\' lead Golden Globe nominees\". Entertainment Weekly. Archived from the original on March 23, 2015. Retrieved December 12, 2013.\\n Bonaguro, Alison (January 25, 2013). \"News : Offstage: Tim McGraw Wanted to Make Taylor Swift Duet an Event\". CMT. Archived from the original on July 12, 2015. Retrieved February 25, 2013.\\n Blistein, Doyle (June 4, 2013). \"Taylor Swift Joins Rolling Stones for \\'As Tears Go By\\'\". Rolling Stone. Archived from the original on June 9, 2013. Retrieved June 10, 2013.\\n \"Taylor Swift Joins Florida Georgia Line Onstage for \\'Cruise\\'\". Taste of Country. Townsquare Media. March 2, 2013. Archived from the original on March 16, 2013. Retrieved March 29, 2013.\\n Collin, Robbie (July 26, 2012). \"The Lorax, review\". The Daily Telegraph. Archived from the original on September 17, 2016. Retrieved August 3, 2016.\\n Beard, Lanford (May 1, 2013). \"Taylor Swift says \\'I do\\' to \\'New Girl\\'\". Entertainment Weekly. Archived from the original on November 8, 2016. Retrieved August 4, 2016.\\n Busis, Hillary (September 27, 2013). \"Taylor Swift will co-star in long-awaited adaptation of \\'The Giver\\'\". Entertainment Weekly. Archived from the original on December 21, 2016. Retrieved August 2, 2016.\\n Peterson, Price (March 31, 2014). \"Taylor Swift Moves into NYC Apartment Built Over Mysterious River of Pink Slime\". The Atlantic. Archived from the original on September 27, 2016. Retrieved July 31, 2016.\\n Rogers, Alex (March 7, 2014). \"Why Taylor Swift Thinks Nashville Is the Best Place on Earth\". Time. Archived from the original on May 24, 2022. Retrieved September 27, 2016.\\n Milzoff, Rebecca; Brown, Eric Renner; Denis, Kyle (August 24, 2023). \"Taylor Swift and Beyoncé Are So Big, Even Their Publicists Have Fans\". Billboard. Archived from the original on August 24, 2023. Retrieved August 24, 2023.\\n Zollo, Paul (February 17, 2016). \"The Oral History of Taylor Swift\\'s 1989\". The Recording Academy. Archived from the original on June 3, 2021. Retrieved February 14, 2021.\\n Stutz, Colin (October 16, 2014). \"Watch Taylor Swift\\'s \\'1989\\' Secret Sessions Behind The Scenes Video\". Billboard. Archived from the original on June 5, 2016. Retrieved August 2, 2016.\\n Caulfield, Keith (November 4, 2014). \"Taylor Swift\\'s \"1989\" debuts with 1.287 million copies sold\". Billboard. Archived from the original on November 5, 2014. Retrieved November 4, 2014.\\n Chart positions:\\n • \"Discography Taylor Swift\". ARIA Charts. Archived from the original on August 26, 2019. Retrieved February 14, 2021.\\n • \"Taylor Swift Chart History (Canadian Hot 100)\". Billboard. Archived from the original on August 9, 2016. Retrieved February 14, 2021.\\n • \"Taylor Swift\\'s \\'Bad Blood\\' Blasts to No. 1 on Hot 100\". Billboard. May 28, 2015. Archived from the original on September 6, 2015. Retrieved May 28, 2015.\\n \"Taylor Swift – Chart History: Hot 100\". Billboard. Archived from the original on March 8, 2018. Retrieved September 18, 2016.\\n \"Live Music\\'s $20 Billion Year: The Grateful Dead\\'s Fare Thee Well Reunion, Taylor Swift, One Direction Top Boxscore\\'s Year-End\". Billboard. Archived from the original on December 14, 2015.\\n Weissmann, Jordan (July 7, 2014). \"Taylor Swift Has Written an Op-Ed in the Wall Street Journal\". Slate (Blog). Archived from the original on January 23, 2015. Retrieved January 23, 2015.\\n Knopper, Steve (November 8, 2014). \"Taylor Swift\\'s Label Head Explains Spotify Removal\". Rolling Stone. Archived from the original on April 21, 2015. Retrieved April 11, 2015.\\n Peters, Mitchell (June 21, 2015). \"Taylor Swift Pens Open Letter Explaining Why \\'1989\\' Won\\'t Be on Apple Music\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Halperin, Shirley (June 21, 2015). \"Apple Changes Course After Taylor Swift Open Letter: Will Pay Labels During Free Trial\". Billboard. Archived from the original on June 22, 2015. Retrieved June 22, 2015.\\n Rosen, Christopher (June 25, 2015). \"Taylor Swift is putting 1989 on Apple Music\". Entertainment Weekly. Archived from the original on June 26, 2015. Retrieved June 25, 2015.\\n \"Taylor Swift returns to Spotify on the day Katy Perry\\'s album comes out\". BBC News. June 9, 2017. Archived from the original on June 9, 2017.\\n \"Taylor Swift: 2014 Billboard Woman of the Year\". Billboard. October 10, 2014. Archived from the original on June 1, 2015. Retrieved April 11, 2015.\\n Payne, Chris (November 23, 2014). \"Taylor Swift Wins Dick Clark Award of Excellence at 2014, Presented by Diana Ross\". Billboard. Archived from the original on July 4, 2015. Retrieved April 11, 2015.\\n \"The Taylor Swift Experience\". GRAMMY Museum. Archived from the original on November 25, 2022. Retrieved April 22, 2022.\\n Boehrer, Kat (January 7, 2016). \"Watch Taylor Swift\\'s Stunning Acoustic Performance of \\'Blank Space\\' at the Grammy Museum\". Complex. Archived from the original on April 22, 2022. Retrieved April 22, 2022.\\n Jonze, Tim (February 25, 2015). \"Taylor Swift wins international female solo artist at Brit awards 2015\". The Guardian. Archived from the original on January 1, 2016. Retrieved April 11, 2015.\\n Stutz, Colin (July 21, 2015). \"2015 MTV Video Music Awards Nominees Revealed: Taylor Swift, Kendrick Lamar, Ed Sheeran & More\". Billboard. Archived from the original on July 24, 2015. Retrieved August 15, 2016.\\n Lynch, Joe (February 19, 2016). \"Taylor Swift Joins Elite Club to Win Grammy Album of the Year More Than Once: See the Rest\". Billboard. Archived from the original on March 1, 2016. Retrieved July 31, 2016.\\n Chiu, Melody (June 1, 2016). \"Taylor Swift and Calvin Harris Split After 15 Months Together\". People. Archived from the original on June 22, 2016. Retrieved June 1, 2016.\\n Spanos, Brittany (July 13, 2016). \"Taylor Swift Co-Wrote Calvin Harris\\' Smash Hit \\'This Is What You Came For\\'\". Rolling Stone. Archived from the original on July 30, 2016. Retrieved July 31, 2016.\\n Grady, Constance (August 26, 2019). \"How the Taylor Swift-Kanye West VMAs scandal became a perfect American morality tale\". Vox. Archived from the original on December 2, 2022. Retrieved October 12, 2023.\\n Grady, Constance (March 21, 2020). \"Newly leaked footage shows Taylor Swift and Kanye West talking \"Famous\"\". Vox. Archived from the original on March 24, 2022. Retrieved October 12, 2023.\\n Lewis, Anna (July 15, 2016). \"Tom Hiddleston finally tells us the truth about his relationship with Taylor Swift\". Cosmopolitan. Archived from the original on July 17, 2016. Retrieved January 8, 2019.\\n \"Taylor Swift Finally Reveals When She Started Dating Joe Alwyn in Lover Album\". Yahoo!. August 23, 2019. Archived from the original on June 11, 2020. Retrieved June 11, 2020.\\n \"Paul McCartney & Taylor Swift\". Rolling Stone. November 13, 2020. Archived from the original on November 30, 2020. Retrieved September 15, 2021. McCartney: So how does that go? Does your partner sympathize with that and understand? Swift: Oh, absolutely.\\n Irvin, Jack (April 10, 2023). \"Inside Taylor Swift and Joe Alwyn\\'s \\'Differences\\' That Led to Their Breakup: Sources (Exclusive)\". People. Archived from the original on September 25, 2023. Retrieved September 25, 2023.\\n \"Taylor Swift, pop princess, wins song of the year at the CMA Awards\". USA Today. Archived from the original on November 9, 2017. Retrieved November 9, 2017.\\n Trust, Gary (February 21, 2017). \"Ed Sheeran Tops Hot 100, Katy Perry Debuts at No. 4 & Bruno Mars, Rihanna & The Weeknd All Hit Top 10\". Billboard. Archived from the original on February 22, 2017. Retrieved February 22, 2017.\\n Grady, Constance (August 11, 2017). \"Taylor Swift won her day in court. Here\\'s what you need to know\". Vox. Archived from the original on October 17, 2022. Retrieved October 4, 2023.\\n \"Taylor Swift wipes social media profiles, fuelling expectations of new album\". The Daily Telegraph. August 18, 2017. Archived from the original on January 8, 2019. Retrieved August 19, 2019.\\n Aswad, Jem (August 24, 2017). \"Taylor Swift\\'s New Single, \\'Look What You Made Me Do,\\' Arrives (Listen)\". Variety. Archived from the original on August 28, 2017. Retrieved August 29, 2017.\\n White, Jack (September 1, 2017). \"Taylor Swift scores first Number 1 on the Official Singles Chart with \\'LWYMMD\\'\". Official Charts Company. Archived from the original on September 2, 2017. Retrieved September 1, 2017.\\n Peak positions:\\n • \"Taylor Swift Scores Fifth No. 1 Single\". Australian Recording Industry Association. September 2, 2017. Archived from the original on September 2, 2017. Retrieved September 2, 2017.\\n • \"IRMA – Irish Charts\". Irish Recorded Music Association. Archived from the original on June 14, 2017. Retrieved September 2, 2017.\\n • \"NZ Top 40 Singles Chart\". Recorded Music NZ. September 4, 2017. Archived from the original on September 1, 2017. Retrieved September 1, 2017.\\n • \"Taylor Swift at Nos. 1 & 4 on Billboard Hot 100, as Cardi B Moves Up to No. 2\". Billboard. September 11, 2017. Archived from the original on September 21, 2017. Retrieved October 30, 2017.\\n Shaw, Lucas (November 7, 2017). \"Taylor Swift Will Keep New Album From Streaming for a Week\". Bloomberg. Bloomberg News. Archived from the original on November 8, 2017. Retrieved November 7, 2017.\\n Powers, Ann (November 10, 2019). \"The Old Taylor\\'s Not Dead\". NPR. Archived from the original on June 9, 2020. Retrieved June 29, 2020.\\n McDermott, Maeve (October 11, 2017). \"Taylor Swift \\'Reputation\\': Here\\'s what critics are saying\". USA Today. Archived from the original on March 4, 2020. Retrieved October 15, 2020.\\n \"Official: Taylor Swift\\'s \\'Reputation\\' Album Sells 1.2M Copies in US During First Week\". Billboard. Archived from the original on November 30, 2017. Retrieved May 29, 2018.\\n Chart positions:\\n • \"Taylor Swift\\'s \\'Reputation\\' Rules Australia\\'s Albums Chart\". Billboard. November 20, 2017. Archived from the original on November 20, 2017. Retrieved December 2, 2017.\\n • \"Taylor Swift Chart History\". Billboard. Archived from the original on November 22, 2021. Retrieved November 19, 2021.\\n Unterberger, Andrew (July 6, 2018). \"Taylor Swift\\'s \\'Delicate\\' Became the Biggest Reputation Radio Hit While You Weren\\'t Looking\". Billboard. Archived from the original on December 13, 2020. Retrieved April 13, 2020.\\n \"61st Grammy Nominees\". The Recording Academy. December 7, 2018. Archived from the original on December 7, 2018. Retrieved December 7, 2018.\\n Hudak, Joseph (April 12, 2018). \"Sugarland Announce New Album Bigger, Taylor Swift Collaboration\". Rolling Stone. Archived from the original on April 14, 2018. Retrieved April 13, 2018.\\n Havens, Lyndsey (October 9, 2018). \"Taylor Swift Breaks an All-Time AMA Record – And Urges People to Vote in Midterm Elections\". Billboard. Archived from the original on October 10, 2018. Retrieved October 10, 2018.\\n Stubblebine, Allison (November 13, 2017). \"Taylor Swift Announces First Round of Reputation Stadium Tour Dates\". Billboard. Archived from the original on November 14, 2017. Retrieved November 18, 2017.\\n Frankenberg, Eric (December 6, 2018). \"Taylor Swift Closes Reputation Stadium Tour with $345 Million\". Billboard. Archived from the original on December 9, 2018. Retrieved December 22, 2018.\\n Wang, Amy X. (November 19, 2018). \"Taylor Swift\\'s New Record Deal Affects Thousands of Other Musicians\". Rolling Stone. Archived from the original on November 26, 2018. Retrieved November 26, 2018.\\nWillman, Chris (August 27, 2018). \"Taylor Swift Stands to Make Music Business History as a Free Agent\". Variety. Archived from the original on August 29, 2018. Retrieved August 29, 2018.\\nAswad, Jem; Willman, Chris (November 19, 2018). \"Taylor Swift Signs New Deal With Universal Music Group\". Variety. Archived from the original on November 19, 2018. Retrieved November 19, 2018.\\n Grady, Constance (November 19, 2018). \"What Taylor Swift\\'s new record deal means for the music industry — and for her image\". Vox. Archived from the original on December 20, 2021. Retrieved December 20, 2021.\\n McKenna, Lyndsey (August 23, 2019). \"Stream Taylor Swift\\'s New Album, \\'Lover\\'\". NPR. Archived from the original on February 19, 2020. Retrieved September 10, 2019.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Archived from the original on August 23, 2019. Retrieved February 15, 2021.\\n Caulfield, Keith (September 1, 2019). \"Official: Taylor Swift\\'s \\'Lover\\' Debuts at No. 1 on Billboard 200 Chart With 867,000 Units Earned in First Week in U.S.\" Billboard. Archived from the original on September 1, 2019. Retrieved September 2, 2019.\\n White, Adam (August 23, 2019). \"Taylor Swift Lover Review Round-Up: Critics Say Album Feels \\'Evolutionary Rather Than Revolutionary\\'\". The Independent. Archived from the original on June 13, 2022. Retrieved June 13, 2022.\\n Moniuszko, Sara M. (August 23, 2019). \"Taylor Swift Lover Reviews: Critics Are Enamored by the \\'Earnest,\\' \\'Romantic\\' New Album\". USA Today. Archived from the original on August 23, 2019. Retrieved November 5, 2020.\\n Trust, Gary (May 6, 2019). \"Lil Nas X\\'s \\'Old Town Road\\' Tops Billboard Hot 100 For Fifth Week, Taylor Swift\\'s \\'Me!\\' Vaults to No. 2\". Billboard. Archived from the original on May 6, 2019. Retrieved June 14, 2019.\\n Trust, Gary (October 23, 2023). \"Taylor Swift\\'s \\'Cruel Summer\\' Hits No. 1 on Billboard Hot 100, Becoming Her 10th Leader\". Billboard. Archived from the original on October 24, 2023. Retrieved October 24, 2023.\\n \"Arashi Best-Of Tops Taylor Swift for IFPI\\'s Best-Selling Album of 2019\". Billboard. March 19, 2020. Archived from the original on March 19, 2020. Retrieved March 21, 2020.\\n \"2020 Grammy Awards: Complete Winners List\". The Recording Academy. November 20, 2019. Archived from the original on May 22, 2020. Retrieved February 15, 2021.\\n Grein, Paul (August 26, 2019). \"12 Records That Were Set at the 2019 VMAs\". Billboard. Archived from the original on January 30, 2020. Retrieved January 11, 2020.\\n Grady, Constance (September 1, 2019). \"The Taylor Swift/Scooter Braun controversy, explained\". Vox. Archived from the original on February 11, 2020. Retrieved August 23, 2019.\\n Beth, John (January 2, 2024). \"Taylor Swift\\'s Chart Triumph\". Square News.\\n \"The Taylor Swift, Scooter Braun, Justin Bieber row explained\". BBC News. July 1, 2019. Archived from the original on December 11, 2021. Retrieved July 18, 2021.\\n Willman, Chris (November 16, 2020). \"Taylor Swift Confirms Sale of Her Masters, Says She Is Already Re-Recording Her Catalog\". Variety. Archived from the original on December 3, 2022. Retrieved November 18, 2020.\\n Aniftos, Rania (November 15, 2019). \"Taylor Swift Releases \\'Beautiful Ghosts,\\' Co-Written With Andrew Lloyd Webber for \\'Cats\\' Film\". Billboard. Archived from the original on November 19, 2019. Retrieved November 15, 2019.\\n \"Golden Globes 2020: full list of nominations\". The Guardian. December 9, 2019. Archived from the original on December 10, 2019. Retrieved December 20, 2019.\\n Rooney, David (December 18, 2019). \"\\'Cats\\': Film Review\". The Hollywood Reporter. Archived from the original on December 20, 2019. Retrieved December 21, 2019.\\n Mamo, Heran (January 15, 2020). \"Taylor Swift Miss Americana Netflix Doc Has a Release Date & We\\'re So Ready for It\". Billboard. Archived from the original on April 24, 2020. Retrieved January 19, 2020.\\n Willman, Chris (February 6, 2020). \"Taylor Swift Moves to Universal Music Publishing Group with New Pact\". Variety. Archived from the original on February 12, 2020. Retrieved February 6, 2020.\\n Opperman, Jeff (March 12, 2021). \"Taylor Swift Is Singing Us Back to Nature\". The New York Times. Archived from the original on December 28, 2021. Retrieved May 24, 2021.\\n \"Taylor Swift to release surprise ninth album \\'Evermore\\' tonight\". NME. December 10, 2020. Archived from the original on December 10, 2020. Retrieved December 10, 2020.\\n Atkinson, Katie (December 15, 2020). \"Taylor Swift Isn\\'t So Sure She & Joe Alwyn Would Have Made Music Together If It Weren\\'t for Lockdown\". Billboard. Archived from the original on August 11, 2021. Retrieved February 18, 2021.\\n Schaffer, Claire (December 18, 2020). \"Aaron Dessner on How His Collaborative Chemistry With Taylor Swift Led to Evermore\". Rolling Stone. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Barna, Alyssa (December 16, 2020). \"These are the musicological reasons Taylor Swift\\'s new album sounds dull\". The Washington Post. Archived from the original on February 25, 2021. Retrieved November 3, 2021.\\n Snapes, Laura (October 14, 2022). \"\\'Genuine\\': why Taylor Swift can celebrate more than an album release\". The Guardian. Archived from the original on October 14, 2022. Retrieved October 14, 2022.\\n McGrath 2023, p. 79; Fogarty & Arnold 2021, p. 5.\\n Trust, Gary (January 28, 2021). \"Taylor Swift\\'s \\'Coney Island\\' and \\'No Body, No Crime\\' Debut on Airplay Charts, Joining \\'Willow\\'\". Billboard. Archived from the original on August 16, 2021. Retrieved February 2, 2021.\\n Willman, Chris (March 14, 2021). \"Taylor Swift Becomes First Woman to Win Album of the Year Grammy Three Times\". Variety. Archived from the original on December 2, 2021. Retrieved March 15, 2021.\\n Caulfield, Keith (January 7, 2021). \"Lil Baby\\'s My Turn Is MRC Data\\'s Top Album of 2020, Roddy Ricch\\'s \\'The Box\\' Most-Streamed Song\". Billboard. Archived from the original on January 7, 2021. Retrieved January 7, 2021.\\n Trust, Gary (December 21, 2020). \"Taylor Swift\\'s \\'Willow\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Archived from the original on December 22, 2020. Retrieved February 18, 2021.\\n Willman, Chris (November 23, 2020). \"Taylor Swift Wins Three American Music Awards, Says She\\'s MIA Because of \\'Recording All of My Old Music\\'\". Variety. Archived from the original on December 2, 2021. Retrieved November 25, 2020.\\n Christman, Ed (July 19, 2021). \"Billboard\\'s U.S. Money Makers: The Top Paid Musicians of 2020\". Billboard. Archived from the original on July 24, 2021. Retrieved July 19, 2021.\\n • Christman, Ed (July 19, 2021). \"Billboard\\'s 2020 Global Money Makers: The 5 Top Highest Paid Musicians\". Billboard. Archived from the original on July 23, 2021. Retrieved July 19, 2021.\\n Caulfield, Keith (July 11, 2023). \"Taylor Swift\\'s Re-Recorded Speak Now Already Has 2023\\'s Biggest Week After 4 Days of Release\". Billboard. Archived from the original on July 11, 2023. Retrieved July 11, 2023.\\n Caulfield, Keith (April 18, 2021). \"Taylor Swift\\'s Re-Recorded Fearless Album Debuts at No. 1 on Billboard 200 Chart With Year\\'s Biggest Week\". Billboard. Archived from the original on April 18, 2021. Retrieved April 19, 2021.\\n Asker, Jim; Trust, Gary (February 22, 2021). \"Taylor Swift\\'s \\'Love Story (Taylor\\'s Version)\\' Debuts at No. 1 on Hot Country Songs Chart: \\'I\\'m So Grateful to the Fans\\'\". Billboard. Archived from the original on April 22, 2021. Retrieved February 22, 2021.\\n McCluskey, Megan (December 8, 2023). \"Breaking Down Taylor Swift\\'s 2023 Impact By the Numbers\". Time. Archived from the original on December 26, 2023. Retrieved December 26, 2023.\\n Horton, Adrian; Lee, Benjamin (February 6, 2023). \"Grammy awards 2023: list of winners\". The Guardian. Archived from the original on February 6, 2023. Retrieved February 6, 2023.\\n Corcoran, Nina (August 28, 2022). \"Taylor Swift Announces New Album Midnights, Breaks Record for Most Video of the Year Wins at 2022 VMAs\". Pitchfork. Archived from the original on August 29, 2022. Retrieved August 28, 2022.\\n \"Taylor Swift\\'s new album breaks Spotify streaming record\". The Guardian. October 22, 2022. Archived from the original on October 22, 2022. Retrieved October 22, 2022.\\n Harvilla, Rob (October 25, 2022). \"The Anti-Hero We Deserve: Taylor Swift and Her Polarizing \\'Midnights\\'\". The Ringer. Archived from the original on November 3, 2022. Retrieved November 3, 2022.\\n Light, Alan (October 24, 2022). \"Taylor Swift\\'s Midnights Does Something Astonishing. Even For Her\". Esquire. Archived from the original on January 17, 2023. Retrieved January 17, 2023.\\n Petridis, Alexis (October 21, 2022). \"Taylor Swift: Midnights Review – Small-Hours Pop Rich with Self-Loathing and Stereotype-Smashing\". The Guardian. Archived from the original on October 22, 2022. Retrieved October 21, 2022.\\n Spanos, Brittany (October 21, 2022). \"Taylor Swift Lets Us Into Her Darkest Dreams On Midnights\". Rolling Stone. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Sheffield, Rob (October 21, 2022). \"Welcome to the Lavender Labyrinth: Taylor Swift\\'s Midnights Is the Mastermind\\'s Ultimate Power Move\". Rolling Stone. Archived from the original on October 22, 2022. Retrieved October 23, 2022.\\n Balasaygun, Kaitlin (November 1, 2022). \"How Taylor Swift went back to the past and turned Midnights into her biggest album success yet\". CNBC. Archived from the original on December 30, 2022. Retrieved December 30, 2022.\\n Shafer, Ellise (October 21, 2022). \"Taylor Swift\\'s Midnights Breaks Spotify Record for Most-Streamed Album in a Single Day\". Variety. Archived from the original on October 21, 2022. Retrieved October 22, 2022.\\n Dailey, Hannah (December 6, 2022). \"Here Are All of Taylor Swift\\'s Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022.\\n Trust, Gary (June 5, 2023). \"Morgan Wallen\\'s \\'Last Night\\' No. 1 on Hot 100 for Ninth Week, Taylor Swift & Ice Spice\\'s \\'Karma\\' Blasts to No. 2\". Billboard. Archived from the original on October 26, 2023. Retrieved August 21, 2023.\\n Blistein, Jon; Guglielmi, Jodi (September 13, 2023). \"Taylor Swift Makes History at 2023 VMAs\". Rolling Stone. Archived from the original on September 13, 2023. Retrieved September 13, 2023.\\n West, Bryan (February 5, 2024). \"Taylor Swift makes Grammys history with fourth album of the year win for Midnights\". USA Today. Archived from the original on February 5, 2024. Retrieved February 5, 2024.\\n Caulfield, Keith (July 16, 2023). \"Taylor Swift\\'s Re-Recorded \\'Speak Now\\' Debuts at No. 1 on Billboard 200 With 2023\\'s Biggest Week\". Billboard. Archived from the original on July 19, 2023. Retrieved July 16, 2023.\\n Caulfield, Keith (November 5, 2023). \"Taylor Swift\\'s 1989 (Taylor\\'s Version) Debuts at No. 1 on Billboard 200 With Biggest Week in Nearly a Decade\". Billboard. Archived from the original on November 5, 2023. Retrieved November 5, 2023.\\n Sherman, Maria (November 29, 2023). \"Taylor Swift is Spotify\\'s most-streamed artist of 2023, ending Bad Bunny\\'s 3-year reign\". ABC News. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n Garcia, Thania (November 28, 2023). \"Taylor Swift Named Apple Music\\'s Artist of the Year; Morgan Wallen Tops Global Songs Chart\". Variety. Archived from the original on November 29, 2023. Retrieved November 29, 2023.\\n \"Best of 2023 (Taylor\\'s Version) Playlist on Amazon Music\". Curated by Amazon\\'s Music Experts. Amazon Music. Retrieved December 3, 2023. Our most streamed artist of 2023 globally. It\\'s Taylor\\'s world and we are just living for it.\\n Caulfield, Keith (November 21, 2023). \"Taylor Swift Is Billboard\\'s Top Artist of 2023\". Billboard. Archived from the original on November 21, 2023. Retrieved November 21, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Returns to No. 1 on Billboard 200\". Billboard. Retrieved December 3, 2023.\\n Caulfield, Keith (January 10, 2024). \"Morgan Wallen\\'s One Thing at a Time Is Luminate\\'s Top Album of 2023 in U.S.\" Billboard. Archived from the original on January 10, 2024. Retrieved January 12, 2024.\\n Trust, Gary (November 6, 2023). \"Taylor Swift\\'s \\'Is It Over Now? (Taylor\\'s Version)\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved November 6, 2023.\\n Cohen, Jonathan (June 29, 2021). \"Aaron Dessner, Justin Vernon Rev Up Big Red Machine With Help From Taylor Swift\". Variety. Archived from the original on May 23, 2022. Retrieved February 10, 2022.\\n Strauss, Matthew (February 19, 2021). \"HAIM Enlist Taylor Swift for New \"Gasoline\" Remix\". Pitchfork. Archived from the original on February 19, 2021. Retrieved February 10, 2022.\\n Dailey, Hannah (February 11, 2022). \"Ed Sheeran & Taylor Swift Release \\'The Joker and the Queen\\' Remix: Watch the Video\". Billboard. Archived from the original on February 11, 2022. Retrieved February 11, 2022.\\n Rowley, Glenn (January 18, 2023). \"The National Unveils \\'First Two Pages of Frankenstein\\' Tracklist With Taylor Swift, Phoebe Bridgers & Sufjan Stevens\". Billboard. Archived from the original on January 18, 2023. Retrieved January 18, 2023.\\n Davis, Clayton (December 21, 2022). \"Taylor Swift Doesn\\'t Make Oscar Shortlist for All Too Well Short Film, but Advances for \\'Carolina\\' Original Song\". Variety. Archived from the original on December 24, 2022. Retrieved December 24, 2022.\\n Utley, Riley (October 13, 2022). \"Every Taylor Swift Movie Performance, Ranked\". CinemaBlend. Archived from the original on October 13, 2022. Retrieved October 14, 2022.\\n Lang, Brent (December 9, 2022). \"Taylor Swift Making Feature Directing Debut for Searchlight Pictures\". Variety. Archived from the original on December 10, 2022. Retrieved December 9, 2022.\\n Wood, Mikael; Brown, August (August 1, 2023). \"It\\'s a love story, L.A. just says yes: How Taylormania took over the world\". Los Angeles Times. Archived from the original on August 1, 2023. Retrieved August 1, 2023.\\n Mahdawi, Arwa (November 20, 2022). \"Swifties know: the Ticketmaster fiasco shows America has a monopoly problem\". The Guardian. Archived from the original on December 10, 2022. Retrieved November 20, 2022.\\n Murray, Conor (December 8, 2023). \"Taylor Swift\\'s Eras Tour Is First In History To Gross Over $1 Billion, Report Says\". Forbes. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Tapp, Tom (December 8, 2023). \"Taylor Swift\\'s \\'The Eras Tour\\' Grosses Over $1 Billion In 2023, The Biggest Haul For Any Act Ever\". Deadline. Archived from the original on February 6, 2024. Retrieved February 10, 2024.\\n Kaufman, Gil (November 28, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Concert Movie Passes $250 Million in Worldwide Grosses\". Billboard. Archived from the original on December 1, 2023. Retrieved December 1, 2023.\\n Nordyke, Kimberly (January 7, 2024). \"Golden Globes 2024 Winners List\". The Hollywood Reporter. Archived from the original on January 8, 2024. Retrieved January 8, 2024.\\n Adamczyk, Alicia; Abrams, Joseph (July 25, 2023). \"The brilliant marketing synergy of Taylor Swift\\'s Eras Tour and her rerecorded albums\". Fortune. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\\n Ingham, Tim (June 14, 2023). \"Reliving the Taylor Swift Catalog Sale Saga (And Following the Money...)\". Music Business Worldwide. Archived from the original on June 14, 2023. Retrieved June 15, 2023.\\n Blanchet, Brenton (November 20, 2023). \"Travis Kelce Shares the Real Story of How Taylor Swift Romance Began in Wide-Ranging Interview\". People. Archived from the original on November 20, 2023. Retrieved November 20, 2023.\\n Kelly, Samantha Murphy (January 25, 2024). \"Explicit, AI-generated Taylor Swift images spread quickly on social media\". CNN Business. Archived from the original on January 25, 2024. Retrieved January 25, 2024.\\n Phillips, Zoe G. (January 27, 2024). \"SAG-AFTRA and White House Issue Statements on Taylor Swift AI Nudes: \"We Have It in Our Power to Control These Technologies\"\". The Hollywood Reporter. Archived from the original on January 27, 2024. Retrieved January 27, 2024.\\n \"Taylor Swift Announces \\'Brand New Album\\' \\'The Tortured Poets Department\\' with 13th Grammy Win\". Peoplemag. Archived from the original on February 10, 2024. Retrieved February 10, 2024.\\n Cairns, Dan (March 5, 2009). \"Swift rise of the anti-diva\". The Australian. Archived from the original on December 24, 2014. Retrieved July 2, 2012.\\n Bream, Jon (December 7, 2007). \"Music: OMG! Taylor\\'s senior year\". Star Tribune. Archived from the original on July 12, 2015. Retrieved July 1, 2012.\\n Newman, Melinda (December 19, 2008). \"Taylor Swift Sessions Interview\". AOL. Archived from the original on October 9, 2012. Retrieved March 25, 2011.\\n \"Swift starts world tour in Asia, pushes \"Speak Now\\' in NY\". Country Standard Time. October 23, 2007. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n \"News : 20 Questions With Taylor Swift\". Country Music Television. November 12, 2007. Archived from the original on November 17, 2014. Retrieved April 18, 2012.\\n McCafferty, Dennis (April 13, 2008). \"Taylor\\'s Swift rise\". USA Weekend. Archived from the original on November 14, 2012. Retrieved April 17, 2012.\\n \"Interview with Taylor Swift\". Time. April 23, 2009. Archived from the original on October 23, 2013. Retrieved July 1, 2012.\\n \"Taylor Swift Style: Singer Won\\'t Take Her Clothes Off, Wants People To Focus On Music\". HuffPost. October 23, 2012. Archived from the original on January 4, 2015. Retrieved January 4, 2015.\\n \"InStyle meets country singing sensation Taylor Swift\". InStyle UK. October 26, 2010. Archived from the original on May 27, 2013. Retrieved May 29, 2012.\\n \"Joni Mitchell: 15 Great Artists Influenced by the \\'Blue\\' Singer\". Rolling Stone. June 22, 2016. Archived from the original on December 26, 2021. Retrieved December 26, 2020.\\n Jenkins, Sally (September 28, 2023). \"You thought you knew the NFL. Now meet Taylor\\'s Version\". The Washington Post. Archived from the original on October 26, 2023. Retrieved September 29, 2023.\\n Bonaguro, Alison (November 8, 2012). \"Offstage: Taylor Swift Inspired by Female Singer-Songwriters of the \\'90s\". CMT. Archived from the original on October 4, 2023. Retrieved September 29, 2023.\\n \"Taylor Swift\\'s Favorite Music\". The Oprah Winfrey Show. Archived from the original on January 16, 2016. Retrieved October 23, 2012.\\n Widdicombe, Lizzie (October 10, 2011). \"You Belong With Me\". The New Yorker. Archived from the original on July 24, 2014. Retrieved October 11, 2011.\\n Mansfield, Brian (October 23, 2010). \"Taylor Swift learns to \\'Speak Now,\\' reveal her maturity\". USA Today. Archived from the original on November 4, 2012. Retrieved July 1, 2012.\\n Block, Melissa (October 31, 2014). \"\\'Anything That Connects\\': A Conversation With Taylor Swift\". NPR Music. Archived from the original on February 6, 2015. Retrieved October 26, 2019.\\n Eells, Josh (September 8, 2014). \"The Reinvention of Taylor Swift\". Rolling Stone. Archived from the original on June 4, 2016. Retrieved June 8, 2016.\\n Reid, Poppy (November 2, 2021). \"The Curious Case of Keith Urban\". Rolling Stone. Archived from the original on November 19, 2021. Retrieved November 3, 2021.\\n Hiatt, Brian (June 18, 2019). \"Taylor Swift: The Rolling Stone Interview\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved April 14, 2022.\\n Weatherby, Taylor (March 10, 2021). \"Taylor Swift\\'s Road to Folklore\". The Recording Academy. Archived from the original on November 25, 2021. Retrieved November 24, 2021.\\n Franssen, Gaston (January 2, 2022). \"Policing the celebrity of Taylor Swift: introduction\". Celebrity Studies. 13 (1): 90–92. doi:10.1080/19392397.2022.2026148. S2CID 246997248.\\n Savage, Mark (October 19, 2022). \"Midnights: What we know about Taylor Swift\\'s songwriting\". BBC News. Archived from the original on October 19, 2022. Retrieved October 20, 2022.\\n Bruner, Raisa (August 24, 2020). \"Let\\'s Break Down Taylor Swift\\'s Tender New Album Folklore\". Time. Archived from the original on July 31, 2020. Retrieved October 20, 2022.\\n McNutt 2020, p. 77.\\n Hughes 2017, p. 206; Perone 2017, p. 33.\\n \"Taylor Swift: Album Guide\". Rolling Stone. Archived from the original on December 5, 2012. Retrieved December 5, 2012.\\n \"Pop and Rock Listings July 22 – 28\". The New York Times. July 21, 2011. Archived from the original on January 28, 2012. Retrieved July 12, 2012.\\n \"Taylor Swift Remade Fearless as Taylor\\'s Version. Let\\'s Discuss\". The New York Times. April 9, 2021. Archived from the original on April 9, 2021. Retrieved April 21, 2021.\\n Petridis, Alexis (March 6, 2009). \"Taylor Swift: Fearless\". The Guardian. Archived from the original on October 16, 2013. Retrieved August 13, 2022.\\n Jones, Sasha-Frere (November 11, 2008). \"Prodigy\". The New Yorker. Archived from the original on October 21, 2016. Retrieved August 14, 2022.\\n Hughes 2017, p. 206.\\n Malec, Jim (May 2, 2011). \"Taylor Swift: The Garden In The Machine\". American Songwriter. p. 5. Archived from the original on November 20, 2022. Retrieved August 12, 2022.\\n McNutt 2020, p. 78.\\n Rosen, Jody (November 17, 2013). \"Why Taylor Swift Is the Reigning Queen of Pop\". Vulture. Archived from the original on November 19, 2013. Retrieved November 9, 2020.\\n McNutt 2020, p. 79.\\n Levine, Nick (August 21, 2019). \"Taylor Swift\\'s Lover: The struggle to maintain superstardom\". BBC. Archived from the original on March 1, 2021. Retrieved October 29, 2021.\\n da Silva, Michelle (November 13, 2017). \"Taylor Swift Has Changed for the Worse on Reputation\". Now. Archived from the original on July 26, 2020. Retrieved July 27, 2020.\\n Tucker, Ken (November 13, 2017). \"Taylor Swift Pushes Further Into Electro-Pop With \\'Reputation\\'\". NPR. Archived from the original on October 27, 2021. Retrieved April 8, 2023.\\n Moreland, Quinn (October 24, 2022). \"Taylor Swift: Midnights\". Pitchfork. Archived from the original on October 24, 2022. Retrieved April 8, 2023.\\n Ryan, Elise (October 21, 2022). \"Review: Taylor Swift gets dark, electric on \\'Midnights\\'\". Associated Press News. Archived from the original on October 21, 2022. Retrieved April 8, 2023.\\n Winter, Velvet (November 12, 2022). \"Like The Beatles, Madonna and Kylie Minogue before her, Taylor Swift is masterful at pivoting\". ABC News. Archived from the original on November 13, 2022. Retrieved November 13, 2022.\\n McNutt 2020, p. 79; Sloan 2021, p. 17.\\n Sloan 2021, p. 17.\\n Hyden, Steven (March 10, 2021). \"Taylor Swift, Indie-rock star? Long, Long ago, this might have felt strange\". The New York Times. Archived from the original on April 13, 2021. Retrieved April 13, 2021.\\n Caramanica, Jon (July 26, 2020). \"Taylor Swift, A Pop Star Done with Pop\". The New York Times. Archived from the original on September 10, 2020. Retrieved August 14, 2022.\\n Harbron, Lucy (November 11, 2021). \"Why Taylor Swift\\'s \\'Red\\' Is Her Turning Point\". Clash. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Gerber, Brady (July 27, 2020). \"The Story Behind Every Song on Taylor Swift\\'s folklore\". Vulture. Retrieved December 12, 2023.\\n Willman, Chris (October 21, 2022). \"Taylor Swift\\'s Midnights Marks a Return to Electronic, Confessional Pop That\\'s Worth Losing Sleep Over: Album Review\". Variety. Archived from the original on October 21, 2022. Retrieved October 21, 2022.\\n Fulford 2014, p. 192.\\n \"Taylor Swift Deepens Her Goth-Folk Vision on the Excellent \\'Evermore\\'\". Rolling Stone. December 11, 2020. Archived from the original on December 11, 2020. Retrieved August 23, 2022.\\n \"The 200 Greatest Singers of All Time\". Rolling Stone. January 1, 2023. Archived from the original on January 1, 2023. Retrieved January 1, 2023.\\n Provenzano 2018, p. 173.\\n Roland, Tom (October 15, 2010). \"Taylor Swift: The Billboard Cover Story\". Billboard. Archived from the original on October 18, 2010. Retrieved July 3, 2012.\\n Provenzano 2018, pp. 173–174.\\n Provenzano 2018, p. 174.\\n Powers, Ann (October 25, 2010). \"Album review: Taylor Swift\\'s Speak Now\". Los Angeles Times. Archived from the original on October 28, 2010. Retrieved October 25, 2010.\\n Willman, Chris (November 10, 2017). \"Album Review: Taylor Swift\\'s \\'Reputation\\'\". Variety. Retrieved April 8, 2023.\\n Cox, Jamieson (November 13, 2017). \"Taylor Swift: Reputation\". Pitchfork. Retrieved April 8, 2023.\\n Powers, Ann (November 10, 2017). \"The Old Taylor\\'s Not Dead\". NPR. Retrieved April 8, 2023.\\n Wilson, Carl (November 13, 2017). \"On Reputation, the \"Old Taylor\" Is Dead, but the New One Isn\\'t Quite Ready to Come to the Phone\". Slate. Retrieved April 8, 2023.\\n DeCaro, Alessandro (October 19, 2022). \"10 best Taylor Swift scene covers\". Alternative Press. Retrieved July 1, 2023.\\n Barker, Andrew (November 27, 2020). \"Folklore: The Long Pond Studio Sessions Review\". Variety. Retrieved May 31, 2021.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved April 8, 2023.\\n Kornhaber, Spencer (July 28, 2020). \"Taylor Swift Is No Longer Living in the Present\". The Atlantic. Retrieved August 23, 2022.\\n Brehian, Tom (July 24, 2020). \"Review: Taylor Swift\\'s \\'folklore\\' Is An Indie Record Unconcerned With Being Cool\". Stereogum. Retrieved August 23, 2022.\\n Willman, Chris (December 11, 2020). \"Taylor Swift Has Her Second Great Album of 2020 With \\'Evermore\\': Album Review\". Variety. Retrieved August 23, 2022.\\n Sodomsky, Sam (December 15, 2020). \"Taylor Swift: evermore\". Pitchfork. Archived from the original on December 15, 2020. Retrieved December 14, 2020.\\n McCormick, Neil (April 9, 2021). \"Taylor Swift copies her younger self – and she sounds even more Fearless today\". The Daily Telegraph. Retrieved August 23, 2022.\\n \"Taylor Swift forges ahead with a dreamy throwback in Fearless (Taylor\\'s Version)\". The A.V. Club. April 9, 2021. Archived from the original on April 9, 2021. Retrieved August 23, 2022.\\n Bernstein, Jonathan (April 9, 2021). \"Taylor Swift Carefully Reimagines Her Past on \\'Fearless: Taylor\\'s Version\\'\". Rolling Stone. Retrieved August 23, 2022.\\n Solomon, Kate (November 12, 2021). \"Taylor Swift, Red (Taylor\\'s Version), Review: How Brilliant She Is When Her Heart Is in Tatters\". i. Archived from the original on November 12, 2021. Retrieved November 13, 2021.\\n Snapes, Laura (August 23, 2020). \"Taylor Swift: Folklore review – bombastic pop makes way for emotional acuity\". The Guardian. Retrieved August 23, 2022.\\n Snapes, Laura (November 12, 2021). \"Taylor Swift: Red (Taylor\\'s Version) Review – Getting Back Together with a Classic\". The Guardian. Archived from the original on November 12, 2021. Retrieved November 12, 2021.\\n Kelly, Fred (October 21, 2022). \"Taylor Swift\\'s Midnights: what the critics are saying\". The Week. Retrieved October 23, 2022.\\n Petrusich, Amanda (June 12, 2023). \"The Startling Intimacy of Taylor Swift\\'s Eras Tour\". The New Yorker. Retrieved June 12, 2023.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC Music. August 22, 2019. Retrieved November 10, 2021.\\n Emily, Lee (November 5, 2021). \"Here Are Taylor Swift\\'s Best Bridges On \\'Red\\' Ranked\". iHeartRadio. Retrieved November 10, 2021.\\n Eggertsen, Chris (September 20, 2022). \"Taylor Swift\\'s Iconic Songwriting Credits Amplified By Spotify With Dedicated Page\". Billboard. Retrieved September 20, 2022.\\n Bate, Jonathan (April 10, 2023). \"Why Taylor Swift is a literary giant — by a Shakespeare professor\". The Times. Retrieved April 10, 2023.\\n Pazzanese, Christina (August 2, 2023). \"So what exactly makes Taylor Swift so great?\". Harvard Gazette. Retrieved August 7, 2023.\\n Murphy, Sam (November 10, 2021). \"How \\'Red\\' Became The Most Pivotal Record In Taylor Swift\\'s Career\". Junkee. Retrieved November 10, 2021.\\n Siroky, Mary (November 9, 2021). \"Every Taylor Swift Album Ranked from Worst to Best\". Consequence. Retrieved November 10, 2021.\\n Bruner, Raisa (May 24, 2021). \"How Olivia Rodrigo Become America\\'s Biggest New Pop Star\". Time. Retrieved November 10, 2021.\\n Mulvey, John (September 16, 2023). \"Arctic Monkeys, Taylor Swift, Kendrick Lamar, Lana Del Rey And The 30 Artists Who Will Shape The Next 30 Years\". Mojo. Retrieved September 19, 2023.\\n Farley, Christopher John (October 22, 2010). \"Taylor Swift\\'s Solo Act\". The Wall Street Journal. Archived from the original on February 1, 2015. Retrieved May 24, 2012.\\n Jo Sales, Nancy; Diehl, Jessica (April 2013). \"Taylor Swift\\'s Telltale Heart\". Vanity Fair. Archived from the original on January 30, 2017. Retrieved February 4, 2017.\\n Daly, Rhian (December 13, 2020). \"Taylor Swift says her diaristic songwriting style was \\'unsustainable\\' for her future\". NME. Retrieved February 17, 2021.\\n Gallo, Phil (October 22, 2012). \"Taylor Swift\\'s Red: The Billboard Cover Story\". Billboard. Archived from the original on June 14, 2013.\\n Caramanica, Jon (October 20, 2010). \"Taylor Swift Is Angry, Darn It\". The New York Times. Archived from the original on September 11, 2012. Retrieved July 2, 2012.\\n Kelly, James (August 26, 2009). \"Taylor Swift writing her own songs and rules\". The Atlanta Journal-Constitution. Archived from the original on September 8, 2014. Retrieved July 30, 2012.\\n Lansky, Sam (November 8, 2017). \"Why Taylor Swift\\'s Red Is Her Best Album\". Billboard. Retrieved December 27, 2020.\\n Hiatt, Brian (September 30, 2019). \"9 Taylor Swift Moments That Didn\\'t Fit in Our Cover Story\". Rolling Stone. Archived from the original on October 1, 2019. Retrieved December 9, 2019.\\n \"Taylor Swift Talks Newfound \\'Freedom,\\' \\'Lover\\' Tour Plans and So Much More\". On Air with Ryan Seacrest. August 27, 2019. Retrieved March 22, 2020.\\n Yuan, Jada (December 30, 2009). \"Microwaving a tragedy: The marriage of romance and romanticism in \\'00s pop\". Las Vegas Weekly. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n Rotman, Natalie (January 9, 2009). \"Colbie Caillat has \\'Breakthrough\\' with sophomore CD\". Reading Eagle. Archived from the original on December 21, 2013. Retrieved August 17, 2012.\\n \"Taylor Swift\\'s songwriting: how the star\\'s music has changed, for better or worse\". CBC News. Retrieved February 2, 2021.\\n Knibbs, Kate (August 21, 2019). \"Ten Years of Taylor Swift: How the Pop Star Went From Sweetheart to Snake (and Back Again?)\". The Ringer. Retrieved December 13, 2021.\\n Stubbs, Dan (October 9, 2015). \"Taylor Swift: Power, Fame And The Future – The Full NME Cover Interview\". NME. Retrieved February 17, 2021.\\n Weber, Theon (November 3, 2010). \"The Iceberg Songs of Taylor Swift\". The Village Voice. Archived from the original on November 4, 2015. Retrieved July 30, 2012.\\n Beck, Julia (October 27, 2014). \"Taylor Swift Is So Much More Fun Now That She\\'s Jaded\". The Atlantic. Archived from the original on September 27, 2016. Retrieved October 30, 2021.\\n Willman, Chris (October 10, 2010). \"Princess Crossover\". New York. Archived from the original on July 27, 2013. Retrieved July 1, 2012.\\n Rosen, Jody (November 13, 2008). \"Fearless\". Rolling Stone. Archived from the original on August 15, 2012. Retrieved July 1, 2012.\\n Powers, Ann (October 30, 2014). \"The Many New Voices of Taylor Swift\". NPR. Retrieved June 2, 2022.\\n Stoeffel, Kat (November 16, 2012). \"Stop Asking Taylor Swift to Apologize for Writing Songs About Ex-Boyfriends – The Cut\". New York. Archived from the original on November 27, 2012. Retrieved February 25, 2013.\\n Raven, Robin (March 16, 2022). \"10 Artists Who Have Stood Up For Women In Music: Taylor Swift, Lizzo & More\". The Recording Academy.\\n \"Cover Preview: Taylor Swift Fights Back About Her Love Life, the Hyannis Port House—and Has Words for Tina Fey and Amy Poehler\". Vanity Fair. March 5, 2013. Archived from the original on August 8, 2016. Retrieved August 3, 2016.\\n Dominus, Susan (November 16, 2012). \"The Many Insecurities of Taylor Swift\". The New York Times. Archived from the original on June 17, 2016.\\n Doyle, Patrick (November 13, 2020). \"Musicians on Musicians: Paul McCartney and Taylor Swift\". Rolling Stone. Archived from the original on November 30, 2020. Retrieved February 7, 2021.\\n Olivier, Bobby (December 11, 2020). \"Taylor Swift\\'s \\'Evermore\\' Is an Undeniable Folk-Pop Masterpiece\". Spin. Retrieved February 1, 2021.\\n Shutler, Ali (October 9, 2022). \"Taylor Swift organises her lyrics into three \\'dorky\\' pen-themed categories\". NME. Retrieved October 18, 2022.\\n Browne, Erin (October 21, 2022). \"All of Taylor Swift\\'s Famously Devastating Track 5\\'s, Ranked\". Vulture. Retrieved October 27, 2022.\\n \"NMPA to Honor Taylor Swift with Songwriter Icon Award Among Other 2021 Annual Meeting Honorees\". National Music Publishers\\' Association. May 24, 2021. Retrieved May 24, 2021.\\n Linker, Damon (November 26, 2021). \"Taylor Swift, Phoebe Bridgers, and Rihanna: How women took over songwriting\". The Week. Retrieved November 28, 2021.\\n Greco, Patti (November 13, 2017). \"A Harvard Professor Critiques Taylor Swift\\'s New Poems\". Cosmopolitan. Retrieved December 21, 2021.\\n Sheffield, Rob (October 13, 2023). \"Taylor Swift\\'s \\'Eras Tour\\' Movie Will Make You Sing, Scream, and Sob\". Rolling Stone. Retrieved December 8, 2023.\\n Zacharek, Stephanie (December 14, 2023). \"The Eras Tour Movie Is Irresistible No Matter How Much You Think You Like Taylor Swift\". TIME. Retrieved December 8, 2023.\\n \"Ticketing Shmicketing: Taylor Swift\\'s \\'Eras Tour\\' Debut Slays (And Could Break All-Time Touring Record)\". Pollstar. March 18, 2023. Archived from the original on March 20, 2023. Retrieved June 30, 2023.\\n Aramesh, Waiss David (March 18, 2023). \"Taylor Swift\\'s The Eras Tour Is a 3-Hour Career-Spanning Victory Lap\". Rolling Stone. OCLC 1787396. Archived from the original on March 18, 2023. Retrieved June 30, 2023.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved July 23, 2023.\\n McCormick, Neil (March 18, 2023). \"Taylor Swift: The Eras Tour, review: a roaring spectacle of a comeback\". The Daily Telegraph. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kornhaber, Spencer (March 18, 2023). \"What Made Taylor Swift\\'s Concert Unbelievable\". The Atlantic. Retrieved June 30, 2023.\\n Foggatt, Tyler (June 3, 2023). \"Look What Taylor Made Us Do\". The New Yorker. Retrieved June 30, 2023.\\n Seibert, Brian (August 9, 2023). \"How to Command a Stage Without Great Dance Moves (Taylor\\'s Version)\". The New York Times. Retrieved August 11, 2023.\\n Krelenstein, Greg (May 21, 2018). \"TAYLOR SWIFT\\'S REPUTATION TOUR IS A POP TRIUMPH\". V. Archived from the original on May 22, 2018. Retrieved May 22, 2018.\\n Frere-Jones, Sasha (November 3, 2008). \"Prodigy\". The New Yorker. Retrieved June 30, 2023.\\n Horton, Adrian (March 18, 2023). \"Taylor Swift review – pop\\'s hardest-working star gives Eras tour her all\". The Guardian. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Kaplan, Ilana (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' tour is a thrilling spectacle from a pop mastermind\". i. Archived from the original on March 18, 2023. Retrieved March 18, 2023.\\n Young, Alex (March 27, 2023). \"Taylor Swift\\'s \"The Eras Tour\" Is a Triumph of Spectacle and Stamina: Review\". Consequence. Retrieved June 30, 2023.\\n Lipshutz, Jason (March 18, 2023). \"The 13 Best Moments From Taylor Swift\\'s Eras Tour Kickoff\". Billboard. Retrieved July 1, 2023.\\n O\\'Connor, Roisin (June 8, 2018). \"Taylor Swift \\'reputation\\' stadium tour review: Dazzling pop spectacle from the star who doesn\\'t stand still\". The Independent. Archived from the original on June 30, 2023. Retrieved December 22, 2019.Savage, Mark (March 18, 2023). \"Taylor Swift launches Eras tour with three-hour, 44-song set\". BBC News. Archived from the original on March 18, 2023. Retrieved March 18, 2023. Sisario, Ben (November 5, 2023). \"How Taylor Swift\\'s Eras Tour Conquered the World\". The New York Times. Retrieved August 12, 2023. DeVille, Chris (July 12, 2018). \"Big Reputation: A Trip To Taylor Swift\\'s Hyper-Maximalist Stadium Tour\". Stereogum. Retrieved June 30, 2023.\\n Procell, Carlie; Padilla, Ramon (April 28, 2023). \"Taylor Swift tour has many \\'eras.\\' We tracked her movements to give you the look and feel\". USA Today. Retrieved June 30, 2023.\\n \"Taylor Swift Shares Stunning \\'Wildest Dreams\\' Performance from Grammy Museum\". Billboard. January 5, 2016. Retrieved October 29, 2023.\\n Burgham, Lydia (November 10, 2018). \"Taylor Swift in Auckland, reviewed: Despite the snakes, her Reputation shines on\". The Spinoff. Retrieved December 10, 2019.\\n Dodd, Sophie (November 15, 2023). \"All About Taylor Swift\\'s Parents, Scott and Andrea Swift\". People. Retrieved December 3, 2023.\\n Swift, Taylor (March 15, 2013). \"\"Sparks Fly\" (acoustic) Live on the RED Tour!\" – via YouTube.\\n Lewis, Randy (April 3, 2011). \"Academy of Country Music Awards: Las Vegas welcomes Miranda Lambert, Taylor Swift with open arms\". Los Angeles Times. Retrieved August 20, 2023.\\n Ritchie, Mike (March 8, 2020). \"Why Taylor Swift is making the ukulele cool again\". The Herald. Retrieved July 1, 2023.\\n Gensler, Andy (August 17, 2023). \"The Showgoer: The Greatest Show On Earth — Taylor Swift\\'s \\'Eras Tour\\' — Is All That And Far More\". Pollstar. Retrieved August 19, 2023.\\n Sheffield, Rob (May 9, 2018). \"Why Taylor Swift\\'s \\'Reputation\\' Tour Is Her Finest Yet\". Rolling Stone. Retrieved June 30, 2023.\\n Willman, Chris (May 16, 2018). \"Taylor Swift\\'s \\'Reputation\\' Tour Has Bad Blood, Good Will, Sex Appeal and Serpents\". Variety. Retrieved December 22, 2019.\\n Willman, Chris (March 18, 2023). \"Taylor Swift\\'s \\'Eras\\' Show Is a Three-Hour, 44-Song Epic That Leaves \\'Em Wanting More: Concert Review\". Variety. Archived from the original on March 18, 2023. Retrieved June 30, 2018.\\n Ordoña, Michael (September 9, 2022). \"Taylor Swift wants an Oscar. So she took \\'All Too Well\\' to TIFF\". Los Angeles Times. Retrieved September 10, 2022.\\n CMT.com Staff (May 4, 2011). \"Taylor Swift\\'s \"Mean\" Video Debuts Friday\". CMT. Archived from the original on June 19, 2019. Retrieved June 19, 2019.\\n Anitai, Tamar (August 27, 2010). \"Video Premiere: Taylor Swift, \\'Mine\\'\". MTV News. Archived from the original on April 29, 2019. Retrieved June 19, 2019.\\n Bonaguro, Alison (May 6, 2011). \"OFFSTAGE: Taylor Swift Isn\\'t \\'Mean\\' at All, Director Says\". CMT News. Archived from the original on June 19, 2019. Retrieved June 20, 2019.\\n Tailor, Leena (September 1, 2017). \"Exclusive: Taylor Swift\\'s Director Joseph Kahn on How Her Image Invokes a Double Standard: \\'She\\'s a Genius\\'\". Entertainment Tonight. Archived from the original on June 19, 2019.\\n O\\'Connell, Michael (October 9, 2015). \"Taylor Swift and Jimmy Fallon Among Early Emmy Winners\". The Hollywood Reporter. Archived from the original on June 19, 2019.\\n Forbes, Jihan (May 14, 2015). \"Peep Taylor Swift\\'s Star-Studded Cast for Her \\'Bad Blood\\' Music Video\". The Fashion Spot. Retrieved May 13, 2020.\\n \"9 Things You Might Have Missed in Taylor Swift\\'s Netflix Concert Film\". E! News. December 31, 2018. Retrieved September 10, 2022.\\n Spanos, Brittany (April 25, 2019). \"Watch Taylor Swift, Brendon Urie\\'s Colorful \\'ME!\\' Video\". Rolling Stone. Archived from the original on April 26, 2019. Retrieved April 25, 2019.\\nMoore, Sam (August 23, 2019). \"Watch Taylor Swift\\'s colourful new video for \\'Lover\\'\". NME. Archived from the original on August 27, 2019. Retrieved August 27, 2019.\\nZemler, Emily (June 17, 2019). \"Watch Taylor Swift Reunite With Katy Perry in \\'You Need to Calm Down\\' Video\". Rolling Stone. Archived from the original on June 17, 2019. Retrieved June 17, 2019.\\n Mylrea, Hannah (February 28, 2020). \"Every incredible Easter Egg in Taylor Swift\\'s \\'The Man\\' video\". NME. Retrieved March 9, 2020.\\nSpanos, Brittany; Legaspi, Althea (July 24, 2020). \"Taylor Swift Blends Fantastical With Personal in \\'Cardigan\\' Video\". Rolling Stone. Retrieved July 27, 2020.\\n\"Justin Bieber & Megan Thee Stallion Lead 2021 MTV VMA Nominations\". Billboard. Retrieved August 17, 2021.\\n Weatherby, Taylor (February 5, 2023). \"Taylor Swift Makes GRAMMY History (Again) With Best Music Video Win For \"All Too Well: The Short Film\"\". Grammy Awards. Retrieved February 6, 2023.\\n Lansky, Sam (December 6, 2023). \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift\". The Recording Academy. Archived from the original on August 12, 2016. Retrieved August 3, 2016.\\n Friedlander, Whitney (September 10, 2015). \"Taylor Swift, Jimmy Fallon Among Juried Emmy Award Winners\". Variety. Archived from the original on September 15, 2015. Retrieved August 3, 2016.\\n \"Taylor Swift dominates AMAs with 6 wins, extending lead as show\\'s most-decorated artist\". KTRK-TV. November 21, 2022. Retrieved November 21, 2022.\\n Grein, Paul (November 19, 2023). \"After the 2023 Billboard Music Awards, Who Is the All-Time Biggest Winner?\". Billboard. Retrieved November 19, 2023.\\n See Guinness World Records by Taylor Swift\\n Lewis, Randy (November 4, 2013). \"Taylor Swift to receive rare Pinnacle Award at CMA Awards Nov. 6\". Los Angeles Times. Retrieved May 13, 2020.\\n \"Taylor Swift Nashville Tickets\". Excite. Archived from the original on February 3, 2015. Retrieved February 2, 2015.\\n Shelburne, Craig (October 18, 2010). \"Taylor Swift Named NSAI\\'s Songwriter-Artist of the Year\". CMT. Archived from the original on March 14, 2014. Retrieved February 2, 2015.\\n \"Songwriters Hall of Fame\". Songwriters Hall of Fame. Archived from the original on November 29, 2014. Retrieved February 2, 2015.\\n \"The 100 Greatest Songwriters of All Time\". Rolling Stone. Archived from the original on September 2, 2017. Retrieved August 28, 2017.\\n Polanco, Luis (April 5, 2016). \"Taylor Swift to Receive First-Ever Taylor Swift Award From BMI\". Billboard. Retrieved October 21, 2020.\\n Jolly, Nathan (November 17, 2019). \"Why Taylor Swift is to blame for latest twist in music rights drama\". News.com.au. Retrieved November 17, 2019.\\n \"10 Life mantras by Taylor Swift to live by\". India Today. December 13, 2016. Archived from the original on February 9, 2019. Retrieved July 16, 2020.\\n Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Decade Taylor Swift: \\'I Do Want My Music to Live On\\'\". Billboard. Retrieved December 11, 2019.\\n \"Taylor Swift to receive BRITs Global Icon award\". Official Charts Company. May 9, 2021. Retrieved May 10, 2021.\\n \"Taylor Swift to receive Global Icon Award!\". Brit Awards. May 9, 2021. Retrieved May 10, 2021.\\n Paine, Andre (February 22, 2023). \"Taylor Swift wins IFPI\\'s 2022 Global Recording Artist Of The Year Award\". Music Week. Retrieved February 22, 2023.\\n Brandle, Lars (November 5, 2023). \"Taylor Swift\\'s \\'1989 (Taylor\\'s Version)\\' Debuts at U.K. No. 1 With \\'Massive\\' Sales\". Billboard. Retrieved November 5, 2023.\\n \"Female artists with the most Irish Number 1 albums since 2000\". Official Charts Company. November 5, 2020. Retrieved February 23, 2021.\\n Wang, Dennis (April 16, 2021). \"Taylor Swift\\'s Fearless hits the right note in China, again\". People\\'s Daily. Retrieved June 26, 2021.\\n Brandle, Lars (July 7, 2023). \"Taylor Swift Sets Chart Record In Australia With Top-Five Sweep\". Billboard. Retrieved February 13, 2024.\\n Brandle, Lars (February 9, 2024). \"Ahead of \\'The Eras Tour\\' of Australia, Taylor Swift Sweeps Top 5\". Billboard. Retrieved February 13, 2024.\\n \"Taylor beats Swift\". Australian Recording Industry Association. July 14, 2023. Archived from the original on July 14, 2023. Retrieved July 19, 2023.\\n \"Taylor sweeps the record\". Australian Recording Industry Association. July 7, 2023. Retrieved July 7, 2023.\\n Cumulative touring gross:\\n\"Top Touring Artist of the Pollstar Era\" (PDF). Pollstar. June 10, 2022. Archived (PDF) from the original on August 5, 2022. Retrieved August 4, 2022.\\nAllen, Bob (September 26, 2023). \"What A Friggin\\' Year! 2023 Boxoffice Results Remain At Record Highs\". Pollstar. Retrieved October 18, 2023.\\n Gensler, Andy (December 8, 2023). \"Taylor Swift Sets All-Time Touring Record With $1 Billion Gross\". Pollstar. Archived from the original on December 8, 2023. Retrieved December 8, 2023.\\n Willman, Chris (December 21, 2020). \"Taylor Swift\\'s \\'Evermore\\' Sells a Million Worldwide in First Week\". Variety. Archived from the original on January 11, 2021. Retrieved December 21, 2020.\\n Paine, Andre (December 22, 2022). \"Taylor Swift Achieves More Than 6 Million Global Units for Midnights and 37 Billion Total Streams in 2022\". Music Week. Archived from the original on December 22, 2022. Retrieved July 19, 2023.\\n Young, Alex (October 31, 2022). \"Taylor Swift broke 73 records with release of new album Midnights\". Consequence. Archived from the original on October 31, 2022. Retrieved July 19, 2023.\\n Grein, Paul (November 8, 2023). \"Taylor Swift Is Apple Music\\'s 2023 Artist of the Year\". Billboard. Retrieved November 8, 2023.\\n Willman, Chris (October 28, 2023). \"Taylor Swift Beats Her Own Spotify Record for Most Single-Day Streams for an Artist With \\'1989 (Taylor\\'s Version)\\' Release\". Variety. Archived from the original on October 29, 2023. Retrieved October 29, 2023.\\n @billboardcharts (January 19, 2022). \"Most entries on the #Global200 chart in a single week\" (Tweet) – via Twitter.\\n \"Taylor Swift Chart History (Billboard Global 200)\". Billboard. Retrieved July 17, 2023.\\n \"Greatest of All Time Artists\". Billboard. Archived from the original on November 14, 2019. Retrieved November 15, 2019.\\n Zellner, Xander (January 25, 2024). \"Taylor Swift Tallies Record-Extending 95th Week at No. 1 on Artist 100 Chart\". Billboard. Retrieved January 26, 2024.\\n Caulfield, Keith (December 31, 2023). \"Taylor Swift Surpasses Elvis Presley for Most Weeks at No. 1 on Billboard 200 Among Soloists\". Billboard. Retrieved January 3, 2024.\\n Trust, Gary (January 22, 2024). \"Ariana Grande\\'s \\'Yes, And?\\' Debuts at No. 1 on Billboard Hot 100\". Billboard. Retrieved January 22, 2024.\\n \"Taylor Swift Chart History (Top Country Albums)\". Billboard. Retrieved July 17, 2023.\\n \"Taylor Swift Chart History (Digital Song Sales)\". Billboard. Retrieved January 14, 2021.\\n Trust, Gary (July 28, 2023). \"Taylor Swift Breaks Record for Most No. 1s on Pop Airplay Chart As \\'Cruel Summer\\' Becomes Her 12th\". Billboard. Archived from the original on September 29, 2023. Retrieved October 19, 2023.\\n Caulfield, Keith (December 3, 2023). \"Taylor Swift Makes History With Five of the Top 10 Albums on the Billboard 200\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (July 17, 2023). \"Taylor Swift Has 11 Albums on the Billboard 200 Chart for the First Time\". Billboard. Retrieved July 19, 2023.\\n McIntyre, Hugh (January 22, 2024). \"Taylor Swift Made Billboard History–Now Only She Can Match Her Own Feat\". Forbes. Retrieved January 22, 2024.\\n Caulfield, Keith (December 29, 2023). \"Taylor Swift Has the Top Four on the Album Sales Chart for the First Time\". Billboard. Retrieved January 3, 2024.\\n Caulfield, Keith (January 3, 2024). \"Taylor Swift Has 7 of the Top 10 on Billboard\\'s Album Sales Chart\". Billboard. Retrieved January 23, 2024.\\n \"RIAA – Top Artists (Digital Singles)\". Recording Industry Association of America. Retrieved July 17, 2023.\\n Ahlgrim, Callie (July 12, 2023). \"There are 116 songs in history that have been certified diamond — here they all are\". Insider Inc. Retrieved July 17, 2023.\\n Caulfield, Keith (October 30, 2022). \"Taylor Swift\\'s Midnights Blasts in at No. 1 on Billboard 200 Chart With Biggest Week for an Album in 7 Years\". Billboard. Retrieved October 30, 2022.\\n Nicks, Stevie (April 29, 2010). \"The 2010 Time 100: Taylor Swift\". Time. Archived from the original on November 29, 2013. Retrieved April 22, 2012.\\nVena, Jocelyn (April 16, 2015). \"Taylor Swift, Kanye West, Kim Kardashian Make Time\\'s 100 Most Influential People List\". Billboard. Archived from the original on September 25, 2016. Retrieved September 1, 2016.\\nJensen, Erin (April 17, 2019). \"Dwayne Johnson, Taylor Swift, Gayle King, more cover Time\\'s 100 most influential people issue\". USA Today. Archived from the original on April 17, 2019. Retrieved April 17, 2019.\\n Dockterman, Eliana (December 6, 2017). \"\\'I Was Angry.\\' Taylor Swift on What Powered Her Sexual Assault Testimony\". Time. Archived from the original on December 6, 2017. Retrieved December 7, 2017.\\n Shah, Simmone (December 6, 2023). \"Taylor Swift Makes History as Person of the Year\". Time. Retrieved December 6, 2023.\\n \"Taylor Swift Is TIME\\'s 2023 Person of the Year\". Time. December 6, 2023. Retrieved December 11, 2023.\\n \"Taylor Swift\". Forbes. Retrieved December 1, 2020.\\n \"30 Under 30 2017: All-Star Alumni\". Forbes. Retrieved December 1, 2020.\\n \"#8 Taylor Swift\". Forbes. Archived from the original on July 28, 2015. Retrieved July 27, 2015.\\n \"Forbes Unveils Its 20th Annual Ranking Of The World\\'s Most Powerful Women\". Forbes. December 5, 2023. Retrieved December 5, 2023.\\n Schneider, Marc (July 24, 2023). \"8 Ways Taylor Swift Has Changed the Music Business\". Billboard. Retrieved July 24, 2023.\\n Khan, Fawzia (June 18, 2021). \"The Might Of Taylor Swift\". Elle. Archived from the original on June 28, 2021. Retrieved October 20, 2021.\\n Pattison, Kate (September 19, 2023). \"How did Taylor Swift get so popular? She never goes out of style\". The Conversation. Retrieved September 22, 2023.\\n Grady, Constance (October 12, 2023). \"The Eras concert movie is Taylor Swift leveling up\". Vox. Retrieved October 13, 2023.\\n Jericho, Greg (October 28, 2022). \"Taylor Swift\\'s incredible success in graphs – who can blame me for being a Swiftie as a 50-year-old man?\". The Guardian. Retrieved October 29, 2022.\\n Weatherhead, Shaina (April 20, 2023). \"Taylor Swift\\'s \\'Miss Americana\\' Is Required Viewing Right Now\". Collider. Retrieved April 24, 2023.\\n Bartsch, Kayla (September 24, 2023). \"Taylor Swift: Empress of the Zeitgeist\". National Review. Retrieved September 24, 2023.\\n \"Taylor Swift: \\'My Confidence Is Easy To Shake\\'\". NPR. November 2, 2012. Retrieved May 20, 2021.\\nThanki, Juli (September 24, 2015). \"Taylor Swift: Teen idol to \\'biggest pop artist in the world\\'\". The Tennessean. Retrieved May 13, 2021.\\nYahr, Emily (June 16, 2016). \"Taylor Swift\\'s first song came out 10 years ago. Here\\'s what she was like as a teen songwriter\". Arts and Entertainment. The Washington Post. Archived from the original on May 13, 2021. Retrieved May 13, 2021.\\n El Mosselli, Sabrina (December 24, 2022). \"How Taylor Swift rose from \\'Miss Americana\\' to global megastar\". Le Monde. Retrieved December 28, 2022.\\n Cragg, Michael. \"Is Taylor Swift our last remaining real popstar?\". i-D. Archived from the original on May 6, 2023. Retrieved December 3, 2022.\\n Seisdedos, Iker (December 27, 2022). \"Pop music in the era of Taylor Swift: Behind the success of today\\'s biggest star\". El País. Retrieved December 28, 2022.\\n Sanders, Sam (October 20, 2022). \"Taylor Swift Is the Greatest Self-Portraitist of Our Time\". Vulture. Retrieved October 21, 2022.\\n \"Taylor Swift\\'s 40 Biggest Hot 100 Hits\". Billboard. March 23, 2022. Retrieved April 12, 2022.\\n Leonard, Devin (November 12, 2014). \"Taylor Swift Is the Music Industry\". Bloomberg News. Archived from the original on March 13, 2016. Retrieved August 11, 2020.\\n Zoladz, Lindsay (December 30, 2019). \"What Were the 2010s?\". New York. Retrieved November 22, 2020.\\n Savaira, Augusta (November 23, 2022). \"Welcome to \\'Swiftonomics\\': What Taylor Swift reveals about the U.S. economy\". Los Angeles Times. Retrieved November 23, 2022.\\n Vincent, Alice (November 3, 2017). \"Taylor Swift: the rise, fall and re-invention of America\\'s sweetheart\". The Daily Telegraph. Archived from the original on January 10, 2022. Retrieved July 5, 2020.\\n Fischer & Seidman 2016, p. 529.\\n Wahi, Sukriti (March 3, 2021). \"Every Time Taylor Swift Perfectly Shut Down A Sexist Interview Question\". Elle. Retrieved November 4, 2021.\\n Davis, Allison P. (June 28, 2018). \"The Taylor Swift Slut-Shaming Continues\". The Cut. Retrieved November 4, 2021.\\n Meddings, Jacqui (October 31, 2014). \"Taylor Swift is our new cover goddess\". Cosmopolitan. Archived from the original on July 14, 2016. Retrieved August 2, 2016.\\n Chiu, Allyson (August 3, 2022). \"Celebrities use private jets excessively. It\\'s a climate nightmare\". Washington Post. ISSN 0190-8286. Retrieved February 14, 2024.\\n Mendez, Lola (February 14, 2024). \"Taylor Swift claims she offsets her travel carbon footprint - how does that work?\". BBC. Retrieved February 14, 2024.\\n O\\'Malley, Isabella (February 2, 2024). \"Why Taylor Swift\\'s globe-trotting in private jets is getting scrutinized\". Associated Press. Retrieved February 6, 2024.\\n \"Taylor Swift and Travis Kelce romance is bad for the planet. Couple burns a whopping $70,779 jet fuel in the last three months\". The Economic Times. December 21, 2023. Retrieved February 6, 2024.\\n Blistein, Jon (February 6, 2024). \"Taylor Swift\\'s Lawyers Threaten Private Jet Flight Tracker Over \\'Stalking and Harassing Behavior\\'\". Rolling Stone. Retrieved February 6, 2024.\\n Matza, Max (February 7, 2024). \"Taylor Swift threatens to sue student who tracks her private jet\". BBC News. Retrieved February 7, 2024.\\n Benitez-Eves, Tina (July 2, 2022). \"Man Arrested for Harassing Taylor Swift, Attempting to Break Into Her New York City Apartment\". American Songwriter. Retrieved June 10, 2023.\\n Kreps, Daniel (July 2, 2022). \"Brooklyn Man Arrested on Charges of Stalking Taylor Swift\". Rolling Stone. Retrieved June 10, 2023.\\n Della Cava, Marco; Ryan, Patrick (August 31, 2023). \"Taylor Swift is \\'in a class of her own right now,\\' as Eras tour gives way to Eras movie\". USA Today. Archived from the original on September 1, 2023. Retrieved September 1, 2023.\\n Crummy, Colin (March 13, 2014). \"Detwanging country music: how Nashville took the UK\". The Guardian. Retrieved September 23, 2021.\\n Roland, Tom (July 7, 2016). \"Love Story: The Impact of Taylor Swift\\'s First Decade in Music\". Billboard. Archived from the original on July 14, 2016. Retrieved July 7, 2016.\\n Tannembaum, Rob (September 12, 2014). \"4 Ways Billboard Woman of the Year Taylor Swift Changed Country Music\". Billboard. Retrieved December 31, 2019.\\n O\\'Connor, Roisin (October 18, 2018). \"Half of all new guitar players are women, finds study\". The Independent. Archived from the original on May 26, 2022. Retrieved October 20, 2020.\\n Wang, Amy X. (May 22, 2018). \"Guitars Are Getting More Popular. So Why Do We Think They\\'re Dying?\". Rolling Stone. Retrieved October 20, 2020.\\n \"The 200 Most Important Artists of Pitchfork\\'s First 25 Years\". Pitchfork. October 4, 2021. Retrieved October 4, 2021.\\n Lipshutz, Jason (December 16, 2021). \"Billboard\\'s Greatest Pop Stars of 2021: No. 1 — Taylor Swift\". Billboard. Retrieved December 16, 2021.\\n Catucci, Nick (August 23, 2019). \"Taylor Swift Reaches For New Heights of Personal and Musical Liberation on \\'Lover\\'\". Rolling Stone. Retrieved August 18, 2021.\\n Gormely, Ian (December 3, 2014). \"Taylor Swift leads poptimism\\'s rebirth\". The Guardian. Retrieved August 2, 2021.\\n Gambles, Sarah (July 23, 2023). \"The ubiquitous power of Taylor Swift\". Deseret News. Retrieved September 7, 2023.\\n Friedman, Vanessa (April 24, 2023). \"Taylor Swift and the Sparkling Trap of Constant Reinvention\". The New York Times. Retrieved September 7, 2023.\\n Rowley, Glenn (March 9, 2023). \"Taylor Swift\\'s \\'All Too Well (10 Minute Version)\\' Is Now a Class at Stanford University\". Billboard. Retrieved March 10, 2023.\\n Ahlgrim, Callie (March 18, 2022). \"How Taylor Swift inspired a new generation of singer-songwriters, in their own words\". Business Insider. Retrieved March 18, 2022.\\n Sutherland, Mark (December 13, 2019). \"\\'I come with opinions about how we can better our industry\\': Taylor Swift\". Music Week. Retrieved April 27, 2020.\\nSisario, Ben (November 6, 2014). \"Sales of Taylor Swift\\'s \\'1989\\' Intensify Streaming Debate\". The New York Times. Retrieved August 11, 2020.\\nRosseinsky, Katie (October 29, 2021). \"How Taylor Swift is changing the music industry a re-record at a time\". Evening Standard. Archived from the original on April 9, 2021. Retrieved April 23, 2021.\\n Smith, Neil (June 22, 2015). \"Five ways Taylor Swift is changing the world\". BBC News. Retrieved December 31, 2019.\\n Brooks, Dave (November 3, 2022). \"How Taylor Swift Changed the Course of the Concert Ticketing Businesses\". Billboard. Retrieved November 4, 2022.\\n Sources on Swift and millennial era\\n • McDuling, John (November 3, 2014). \"Why Taylor Swift is the most important artist of the millennial era\". Quartz. Archived from the original on June 2, 2021. Retrieved April 2, 2021.\\n • \"What Taylor Swift tells us about millennials\". National Post. October 6, 2015. Retrieved October 30, 2021.\\n • Kidd, Joel (March 2, 2020). \"Taylor Swift Is the Surprising Face of Millennial Anxiety\". The Walrus. Retrieved October 30, 2021.\\n • Winograd, Morley; D. Hais, Michael (January 29, 2010). \"Move over Kanye West, Taylor Swift and the Millennial generation are taking over music\". The Christian Science Monitor. Retrieved October 30, 2021.\\n • \"The 17 Musicians Gen Z & Millennials Say Represent Their Generations\". YPulse. October 8, 2019. Retrieved October 30, 2021.\\n • Nicolau, Elena (November 29, 2022). \"How Taylor Swift is transforming millennial weddings\". Today. Retrieved November 30, 2022.\\n • \"Taylor Swift is peak millennial vibes\". NPR. October 21, 2022. Retrieved December 19, 2022.\\n VanDerWerff, Emily (July 31, 2020). \"Taylor Swift is the millennial Bruce Springsteen\". Vox. Retrieved October 30, 2021.\\n Ditum, Sarah (October 8, 2022). \"Taylor Swift: the Bob Dylan of our age\". The Times. Archived from the original on December 24, 2022. Retrieved October 9, 2022.\\n \"Taylor Swift Will Receive First-Ever Woman of the Decade Honor at Billboard\\'s Women in Music\". Billboard. November 18, 2019. Retrieved December 31, 2019.\\n Elizabeth, De (November 24, 2019). \"Taylor Swift Is Named Artist of the Decade at AMAs 2019\". Teen Vogue. Retrieved December 31, 2019.\\n Kemp, Sam (January 27, 2022). \"The surprising Paul McCartney song inspired by Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Golsen, Tyler (May 14, 2023). \"Mick Jagger discusses his performance with Taylor Swift\". Far Out. Retrieved July 5, 2023.\\n Brandle, Lars (January 28, 2015). \"Taylor Swift on Madonna Praise: \\'Now I\\'m Dead\\'\". Billboard. Retrieved July 5, 2023.\\n Aniftos, Rania (January 27, 2021). \"Dolly Parton Says Taylor Swift Is \\'Magnificent\\' Following Damon Albarn Feud: \\'She Knows Who She Is\\'\". Billboard. Retrieved July 5, 2023.\\n Graff, Gary (October 31, 2021). \"Taylor Swift Helps Induct Carole King, Sings \"Will You Love Me Tomorrow\" at Rock Hall Ceremony\". The Hollywood Reporter. Retrieved September 20, 2022.\\n Skinner, Tom (November 2, 2022). \"Bruce Springsteen praises Taylor Swift\\'s new album \\'Midnights\\': \"She\\'s a tremendous writer\"\". NME. Retrieved July 5, 2023.\\n Ruggieri, Melissa (November 3, 2021). \"Billy Joel says Taylor Swift is like the Beatles of her generation: \\'She knows music\\'\". USA Today. Retrieved July 5, 2023.\\n Williams, Joe (July 5, 2023). \"Ringo Starr says Taylor Swift is \"biggest star in the world\"\". Far Out. Retrieved July 5, 2023.\\n Iasimone, Ashley (November 12, 2023). \"Britney Spears Shares Throwback Photo With Taylor Swift From 20 Years Ago: \\'Most Iconic Pop Woman of Our Generation\\'\". Billboard. Retrieved November 12, 2023.\\n Lutz, Ashley (October 8, 2022). \"Taylor Swift\\'s new album proves she is a business genius\". Fortune. Retrieved October 8, 2022.\\n Steele, Anne (July 1, 2023). \"How to Succeed in Business Like Taylor Swift\". The Wall Street Journal. Retrieved July 2, 2023.\\n Lynch, Joe (January 31, 2024). \"Taylor Swift Tops Billboard\\'s Annual Power 100 List for the First Time\". Billboard. Retrieved January 31, 2024.\\n Curto, Justin (December 22, 2020). \"Did 2020 Kill the Long, Fancy Pop-Album Rollout for Good?\". Vulture. Archived from the original on December 23, 2020. Retrieved December 27, 2020.\\n Leight, Elias (July 23, 2020). \"Taylor Swift Finally Abandoned the Traditional Album Rollout\". Rolling Stone. Retrieved September 15, 2022.\\n Kheraj, Alim (November 9, 2022). \"The curate\\'s Easter egg: how Taylor Swift turned pop into a multiplayer puzzle\". The Guardian. Retrieved November 9, 2022.\\n Tannenbaum, Emily. \"Taylor Swift Hid a Bunch of Easter Eggs in the All Too Well Short Film\". Glamour. Retrieved May 13, 2022.\\n Suskind, Alex (May 9, 2019). \"New Reputation: Taylor Swift shares intel on TS7, fan theories, and her next era\". Entertainment Weekly. Archived from the original on August 12, 2019. Retrieved May 13, 2022.\\n Yahr, Emily (December 26, 2022). \"2022: The year in review (Taylor\\'s version)\". The Washington Post. Retrieved December 26, 2022.\\n Stein, Jeannine (November 18, 2011). \"Taylor Swift weighs in on being a role model\". Los Angeles Times. Archived from the original on July 15, 2012. Retrieved April 18, 2012.\\n Collins, Katie (June 8, 2018). \"How Taylor Swift flipped online fandom on its head for the better\". CNET. Retrieved March 19, 2023.\\n King, Ashley (July 21, 2023). \"Taylor Swift\\'s 13 Management is Rarely Discussed, Changes Abound\". Digital Music News. Retrieved September 23, 2023.\\n Yotka, Steff (August 20, 2019). \"Taylor Swift and Stella McCartney Reveal Their Lover Fashion Collaboration\". Vogue. Retrieved October 20, 2020.\\n Kaplan, Julee (January 29, 2009). \"Taylor Swift Launches Sundress Line – Ready-to-Wear and Sportswear – Media\". Women\\'s Wear Daily. Archived from the original on July 29, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift to have greeting card line\". USA Today. November 18, 2009. Archived from the original on December 14, 2009. Retrieved August 7, 2012.\\n Serpe, Gina (October 29, 2008). \"Taylor Swift Gets All Dolled Up\". E! News (Blog). Archived from the original on March 5, 2011. Retrieved September 20, 2010.\\n Mychaskiw, Marianne (October 1, 2014). \"Taylor Swift Is Launching Her Fourth Fragrance!\". InStyle. Archived from the original on January 7, 2015. Retrieved December 3, 2015.\\n McIntyre, Hugh. \"Taylor Swift Has Signed A Multi-Year Deal With AT&T\". Forbes. Archived from the original on September 11, 2018. Retrieved September 11, 2018.\\n Chiu, Melody (July 26, 2019). \"Watch Taylor Swift Hilariously Moonlight — and Fail — as a Bartender and Server in New Commercial\". People. Retrieved May 7, 2022.\\n \"Taylor Swift: NHL\\'s New Spokesperson\". AOL. February 3, 2009. Archived from the original on February 7, 2009. Retrieved September 20, 2010.\\n \"Sony Electronics News and Information\". Sony. April 26, 2010. Retrieved August 7, 2012.\\n Lee, Ashley (October 27, 2014). \"Taylor Swift Named New York City Welcome Ambassador\". Billboard. Retrieved September 26, 2023.\\n Kaufman, Gil (January 20, 2022). \"Taylor Swift Tapped as First Record Store Day Global Ambassador\". Billboard. Retrieved January 20, 2022.\\n Snapes, Laura (August 24, 2019). \"Taylor Swift: \\'Pop music can feel like it\\'s The Hunger Games, and like we\\'re gladiators\\'\". The Guardian. Archived from the original on August 24, 2019. Retrieved August 24, 2019.\\n \"The Original 300+ Time\\'s Up Signatories\". Time\\'s Up. October 21, 2019. Archived from the original on May 3, 2020. Retrieved May 20, 2020.\\n Mier, Tomás (June 24, 2022). \"Taylor Swift Says She\\'s \\'Absolutely Terrified\\' After Roe v. Wade Decision\". Rolling Stone. Retrieved June 24, 2022.\\n \"Taylor Swift sends \\'love\\' and \\'respect\\' to LGBT fans\". BBC News. June 4, 2018. Archived from the original on November 14, 2018. Retrieved December 15, 2018.\\n Kreps, Daniel (June 1, 2019). \"Taylor Swift Urges Senate to Pass Equality Act in Letter to Tennessee Senator\". Rolling Stone. Retrieved August 12, 2020.\\n Lynch, Joe (June 26, 2020). \"Taylor Swift Calls Out U.S. Census for Transgender Erasure\". Billboard. Retrieved June 29, 2020.\\n Lynch, Joe (June 14, 2019). \"Taylor Swift Wishes NYC\\'s Stonewall Inn \\'Happy Pride\\' at Surprise Set\". Billboard. Retrieved August 7, 2022.\\n Aniftos, Rania (April 8, 2019). \"Taylor Swift Donates $113,000 to Tennessee Equality Project to Fight Anti-LGBTQ Bills\". Billboard. Archived from the original on April 9, 2019. Retrieved April 9, 2019.\\n \"Taylor Swift Makes a Generous Donation to GLAAD in Support of the LGBTQ Community\" (Press release). GLAAD. June 1, 2019. Archived from the original on June 1, 2019. Retrieved June 1, 2019.\\n Lynch, John (March 23, 2018). \"Taylor Swift Shares Support For Gun a Control Reform In Instagram Post\". Business Insider. Retrieved May 20, 2020.\\n Hiatt, Brian (September 18, 2019). \"The Rolling Stone Interview Taylor Swift\". Rolling Stone. Archived from the original on September 18, 2019. Retrieved September 19, 2019.\\n Bailey, Alyssa (June 9, 2020). \"Taylor Swift Tweeted About the Black Lives Matter Movement and Urged People to Vote\". Elle. Retrieved July 27, 2020.\\n Kreps, Daniel (June 12, 2020). \"Taylor Swift Calls for Removal of Monuments That \\'Celebrate Racist Historical Figures\\'\". Rolling Stone. Retrieved August 12, 2020.\\n France, Lisa Respers (June 19, 2020). \"Taylor Swift got educated on Juneteenth and wants you to be, too\". CNN. Retrieved July 27, 2020.\\n Haag, Matthew (October 9, 2018). \"Voter Registrations Spike as Deadlines Loom. Taylor Swift Had Something to Do With It\". The New York Times. Archived from the original on October 9, 2018. Retrieved October 10, 2018.\\n Stevens, Matt (October 7, 2020). \"Taylor Swift, leaning further into politics, endorses Joe Biden and Kamala Harris\". The New York Times. Archived from the original on October 7, 2020. Retrieved October 8, 2020.\\n Schuster, Steve (May 19, 2023). \"Taylor Swift comes out against Trump, again\". Wisconsin Law Journal. Retrieved November 4, 2023.\\n Dellatto, Marisa (October 27, 2023). \"Taylor Swift\\'s New Era: The Pop Star Becomes A Billionaire\". Forbes. Retrieved October 29, 2023.\\n Pendleton, Devon; Ballentine, Claire; Patino, Marie; Whiteaker, Chloe; Li, Diana (October 26, 2023). \"Taylor Swift Vaults Into Billionaire Ranks With Blockbuster Eras Tour\". Bloomberg News. Retrieved October 26, 2023.\\n Greenburg, Zack O\\'Malley (November 2, 2016). \"The World\\'s Highest-Paid Women in Music 2016\". Forbes. Archived from the original on May 25, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (December 6, 2019). \"The World\\'s Top-Earning Musicians Of 2019\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 9, 2022). \"The Highest-Paid Entertainers 2022\". Forbes. Retrieved July 29, 2022.\\nVoytko, Lisette (February 13, 2023). \"The World\\'s 10 Highest-Paid Entertainers\". Forbes. Retrieved July 29, 2022.\\n \"Highest annual earnings ever for a female pop star\". Guinness World Records. Archived from the original on November 12, 2017. Retrieved November 15, 2017.\\n Mercuri, Monica (July 10, 2019). \"Taylor Swift Is The World\\'s Highest-Paid Celebrity With $185 Million in 2019\". Forbes. Archived from the original on July 12, 2019. Retrieved July 13, 2019.\\nGreenburg, Zack O\\'Malley (July 11, 2016). \"Taylor Swift Is The World\\'s Top-Earning Celebrity With $170 Million in 2016\". Forbes. Archived from the original on August 3, 2016. Retrieved August 3, 2016.\\n Greenburg, Zack O\\'Malley (December 23, 2019). \"From Taylor Swift To Dr. Dre: The 10 Top-Earning Musicians Of The Decade\". Forbes. Archived from the original on December 30, 2019. Retrieved December 31, 2019.\\n Silva, Rachel; Tonelli, Lucia (October 3, 2023). \"Inside Taylor Swift\\'s 8 Incredible Homes\". Elle Decoration. Retrieved October 12, 2023.\\n Dailey, Hannah; Aniftos, Rania (December 21, 2023). \"A Timeline of Taylor Swift\\'s Generosity\". Billboard. Retrieved December 24, 2023.\\n Couch, Robbie (March 2, 2015). \"Taylor Swift Named Most Charitable Celeb For Supporting Feminist Causes, Education, More\". HuffPost. Archived from the original on October 21, 2016. Retrieved August 2, 2016.\\n Pacella, Megan (June 13, 2012). \"Taylor Swift Receives Star of Compassion Award\". Taste of Country. Archived from the original on July 13, 2015. Retrieved November 27, 2015.\\n Cooper, Brittany Joy (March 27, 2012). \"Taylor Swift to Receive \\'Big\\' Honor From Michelle Obama at Kids\\' Choice Awards\". Taste of Country. Archived from the original on July 12, 2012. Retrieved August 30, 2012.\\n Lee, Ken (August 9, 2009). \"Taylor Swift donates $100,000 to victims of Iowa Flood\". People. Archived from the original on June 7, 2009. Retrieved September 9, 2009.\\n \"How Taylor Swift helps people worldwide\". Borgen. February 2, 2023. Retrieved June 17, 2023.\\n Moran, Jonathon (March 8, 2009). \"Kylie to play at Sound Relief with Coldplay, Midnight Oil\". The Daily Telegraph. Archived from the original on October 10, 2016. Retrieved December 30, 2009.\\n \"Taylor Swift donates $500,000 after Nashville floods\". BBC News. May 7, 2010. Retrieved May 7, 2022.\\n Lewis, Randy (May 23, 2011). \"Taylor Swift benefit concert raises more than $750,000 for tornado victims\". Los Angeles Times (Blog). Archived from the original on April 6, 2012. Retrieved April 18, 2012.\\n \"Taylor Swift gives $1m to help Louisiana flood relief efforts\". The Guardian. August 17, 2016. Archived from the original on August 17, 2016. Retrieved August 17, 2016.\\n Muhammad, Latifah (December 9, 2016). \"Taylor Swift Donates $100,000 to Dolly Parton Fire Fund\". Entertainment Tonight. Archived from the original on January 13, 2017. Retrieved January 11, 2017.\\n Blair, Olivia (September 4, 2019). \"Taylor Swift Makes \\'Very Sizeable Donation\\' To A Food Bank in Houston After Hurricane Harvey\". Elle. Archived from the original on July 2, 2019. Retrieved July 3, 2019.\\n Rowley, Glenn (April 13, 2023). \"Taylor Swift Donated Enough to Feed \\'Over 125,000\\' at Tampa Food Bank Ahead of The Eras Tour Stop\". Billboard. Archived from the original on April 21, 2023. Retrieved May 11, 2023.\\n Dementri, Nikki (May 16, 2023). \"\\'It\\'s a blessing\\': South Philly pizza, cheesesteak shop thankful for T. Swift\\'s business\". CBS News. Archived from the original on May 17, 2023. Retrieved May 17, 2023.\\n Grow, Kory (August 3, 2023). \"Taylor Swift Is Helping Truck Drivers Buy First Homes With $100,000 Bonuses\". Rolling Stone. Retrieved August 3, 2023.\\n Paulson, Dave (March 5, 2020). \"Taylor Swift donates $1 million for Tennessee tornado relief\". The Tennessean. Retrieved March 5, 2020.\\n Ruggieri, Melissa. \"Taylor Swift donates $1 million to help communities ravaged by Tennessee tornadoes\". The Tennessean. Retrieved December 11, 2023.\\n \"Nashville Songwriters Hall of Fame\". Nashville Songwriters Foundation. Retrieved July 2, 2020.\\n \"Taylor Swift becomes namesake of Hendersonville High School auditorium\". The Tennessean. September 23, 2010. Archived from the original on July 12, 2015. Retrieved May 29, 2011.\\n McKinley, James C. (May 17, 2012). \"Taylor Swift to Donate $4 Million to the Country Hall of Fame\". The New York Times. Archived from the original on June 5, 2012. Retrieved July 1, 2012.\\n Golden, Zara (October 1, 2012). \"And The Taylor Swift VH1 Storytellers Contest Winner Is ... Harvey Mudd College!\". VH1. Archived from the original on July 12, 2015. Retrieved April 21, 2014.\\n \"$100,000 Gift from Taylor Swift Provides Vital Support to Nashville Symphony\". Nashville Symphony. December 16, 2013. Retrieved April 7, 2020.\\n Sanz, Cynthia (December 14, 2009). \"Taylor Swift Gives Big as She Turns 20 – Good Deeds, Taylor Swift\". People. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\nUrban, Mike (October 14, 2011). \"Taylor Swift donates 6,000 books to Reading Library\". Reading Eagle. Archived from the original on May 12, 2012. Retrieved May 29, 2012.\\nZhu, Danielle (November 17, 2015). \"Taylor Swift partners with Scholastic to donate 25,000 books\". Entertainment Weekly. Archived from the original on August 14, 2016. Retrieved August 3, 2016.\\n Cooper, Brittany Joy (February 2, 2012). \"Taylor Swift Donates 14,000 Books to Nashville Public Library\". Taste of Country. Archived from the original on June 17, 2012. Retrieved May 29, 2012.\\n \"Taylor Swift Helps With \\'Delete Online Predators\\' Campaign\". Country Weekly. September 18, 2007. Archived from the original on January 16, 2016. Retrieved November 29, 2015.\\n \"Taylor Swift Teams Up With UNICEF Tap Project Initiative\". Taste of Country. March 17, 2011. Archived from the original on May 1, 2013. Retrieved August 7, 2012.\\n\"Taylor Swift Among Participants in MusiCares Back To School Charity Auction\". RTTNews. July 28, 2011. Archived from the original on May 17, 2013. Retrieved August 7, 2012.\\n \"Taylor Swift and ACM Lifting Lives present $50,000 donation to St. Jude\". The Tennessean. June 13, 2011. Archived from the original on July 12, 2015. Retrieved May 29, 2012.\\n \"New and Hot Video: Taylor Swift Debuts \\'Ronan\\' at Stand Up To Cancer Benefit\". Rolling Stone. September 8, 2012. Archived from the original on September 13, 2012. Retrieved September 14, 2012.\\n Knight, Joey (May 17, 2014). \"Dick Vitale holds annual fundraiser for pediatric cancer\". Tampa Bay Times. Archived from the original on July 14, 2014. Retrieved July 30, 2014.\\n George, John. \"Taylor Swift donates $50K to CHOP to help teens with cancer\". The Business Journals (Blog). Archived from the original on July 12, 2015. Retrieved June 5, 2014.\\n Hardiman, Tim (April 20, 2007). \"Taylor Swift Encourages Teen Volunteers\". CMT. Archived from the original on November 13, 2012. Retrieved July 1, 2012.\\n Vagianos, Alanna (August 17, 2017). \"This Is How Taylor Swift\\'s Donations Will Impact Sexual Assault Survivors\". HuffPost. Retrieved April 7, 2020.\\n Peppin, Hayley (March 24, 2020). \"Taylor Swift responded to the leaked \\'Famous\\' phone call with Kanye West by urging fans to instead focus on coronavirus aid\". Insider Inc. Retrieved May 13, 2020.\\n Grow, Kory (April 1, 2020). \"How Taylor Swift Is Helping a Small Nashville Record Store Survive\". Rolling Stone. Retrieved August 1, 2023.\\n Hissong, Samantha (August 20, 2020). \"Taylor Swift Starts Frenzy at Indie Record Stores With Surprise Signed \\'Folklore\\' CDs\". Rolling Stone. Retrieved August 1, 2023.\\n Havens, Lyndsey (April 18, 2020). \"Taylor Swift Sings Heart-Rending \\'Soon You\\'ll Get Better\\' During \\'One World\\' Concert\". Billboard. Retrieved April 19, 2020.\\n @RAINN (April 8, 2021). \"In 2017, @taylorswift13 fearlessly shared her story of sexual assault in court. In honor of Sexual Assault Awareness & Prevention Month & #RAINNDay, Taylor Swift has made a generous gift to RAINN. Thank you for helping us ensure all survivors are believed and supported! https://t.co/oE7P1BVnNJ\" (Tweet). Retrieved April 21, 2021 – via Twitter.\\n \"Taylor Swift\\'s cash gift helps student take up degree\". BBC News. August 20, 2020. Retrieved November 9, 2020.\\n M Holloway, Sophie (December 14, 2023). \"Taylor Swift and Selena Gomez among celebrities attending Gaza aid fundraiser\". The Independent.\\nCited literature\\nFischer, Nancy; Seidman, Steven (2016). Introducing the New Sexuality Studies (3rd ed.). Routledge. ISBN 978-1-317-44918-8. OCLC 953030187.\\nFogarty, Mary; Arnold, Gina (2021). \"Are You Ready for It? Re-Evaluating Taylor Swift\". Contemporary Music Review. 40 (1): 1–10. doi:10.1080/07494467.2021.1976586.\\nFulford, Phyllis (2014). An Idiots Guide: Singing Second Edition. Penguin Publishing. ISBN 978-1-61564-622-7.\\nHughes, Charles (2017). \"Country Music and the Recording Industry\". In Stimeling, Travis D. (ed.). The Oxford Handbook of Country Music. Oxford University Press. pp. 205–228. ISBN 978-0-19-024817-8.\\nJepson, Louisa (2013). Taylor Swift. Simon & Schuster. ISBN 978-1-4711-3087-8.\\nMcNutt, Myles (2020). \"From \\'Mine\\' to \\'Ours\\': Gendered Hierarchies of Authorship and the Limits of Taylor Swift\\'s Paratextual Feminism\". Communication, Culture and Critique. 13 (1): 72–91. doi:10.1093/ccc/tcz042.\\nMcGrath, John (2023). \"The Return to Craft: Taylor Swift, Nostalgia, and Covid-19\". Popular Music and Society. 46 (1): 70–84. doi:10.1080/03007766.2022.2156761.\\nPerone, James E. (2017). The Words and Music of Taylor Swift. The Praeger Singer-Songwriter Collection. ABC-Clio. ISBN 978-1-4408-5294-7.\\nProvenzano, Catherine (2018). \"Auto-Tune, Labor, and the Pop-Music Voice\". In Fink, Robert; Latour, Melinda; Wallmark, Zachary (eds.). The Relentless Pursuit of Tone: Timbre in Popular Music. Oxford University Press. pp. 159–182. ISBN 978-0-19-998522-7.\\nSloan, Nate (2021). \"Taylor Swift and the Work of Songwriting\". Contemporary Music Review. Routledge. 40 (1): 11–26. doi:10.1080/07494467.2021.1945226. S2CID 237695045.\\nExternal links\\nTaylor Swift\\nat Wikipedia\\'s sister projects\\nMedia from Commons\\nNews from Wikinews\\nQuotations from Wikiquote\\nData from Wikidata\\nOfficial website Edit this at Wikidata\\nTaylor Swift at AllMusic Edit this at Wikidata\\nTaylor Swift discography at Discogs Edit this at Wikidata\\nTaylor Swift at IMDb Edit this at Wikidata\\nTaylor Swift discography at MusicBrainz Edit this at Wikidata\\nvte\\nTaylor Swift\\nvte\\nTaylor Swift songs\\nAwards for Taylor Swift\\nvte\\nIFPI Global Year-End Charts\\nvte\\nBillboard Year-End number one albums\\nvte\\nBillboard Year-End Top Artist\\nPortals:\\n Biography\\nicon Pop music\\nflag Pennsylvania\\nflag United States\\nAuthority control databases Edit this at Wikidata\\nCategories: Taylor Swift1989 birthsLiving people21st-century American actresses21st-century American philanthropists21st-century American songwriters21st-century American women guitarists21st-century American women pianists21st-century American women singers21st-century women philanthropistsActresses from Nashville, TennesseeActresses from PennsylvaniaAmerican acoustic guitaristsAmerican billionairesAmerican country banjoistsAmerican country guitaristsAmerican country pianistsAmerican country record producersAmerican country singer-songwritersAmerican feministsAmerican film actressesAmerican folk guitaristsAmerican folk musiciansAmerican folk singersAmerican mezzo-sopranosAmerican multi-instrumentalistsAmerican music video directorsAmerican people of English descentAmerican people of German descentAmerican people of Italian descentAmerican people of Scottish descentAmerican pop guitaristsAmerican pop pianistsAmerican synth-pop musiciansAmerican television actressesAmerican voice actressesAmerican women country singersAmerican women guitaristsAmerican women philanthropistsAmerican women pop singersAmerican women record producersAmerican women rock singersAmerican women singer-songwritersAmerican women songwritersBig Machine Records artistsBrit Award winnersChristians from TennesseeCountry musicians from PennsylvaniaCountry musicians from TennesseeCountry pop musiciansFemale billionairesFemale music video directorsFeminist musiciansFilm directors from PennsylvaniaFilm directors from TennesseeGrammy Award winnersGuitarists from PennsylvaniaGuitarists from TennesseeMTV Europe Music Award winnersMTV Video Music Award winnersNME Awards winnersPeople from Bucks County, PennsylvaniaPhilanthropists from PennsylvaniaPhilanthropists from TennesseePrimetime Emmy Award winnersRCA Records artistsRecord producers from PennsylvaniaRecord producers from TennesseeRepublic Records artistsSinger-songwriters from PennsylvaniaSinger-songwriters from TennesseeSingers from Nashville, TennesseeSony Music Publishing artistsSynth-pop singersTime Person of the YearUniversal Music Group artistsBaldi family\\nThis page was last edited on 16 February 2024, at 11:33 (UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\\nWikimedia FoundationPowered by MediaWiki'"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#regex-tokenizer",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#regex-tokenizer",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "The ‘naive’ BPE implementation would merge common words occuring with different punctuation into single tokens: For example, ‘dog.’, ‘dog!’, and ‘dog?’ are likely to become individual tokens. This would combine semantics with punctuation, which is sub-optimal according to studies done by Radford et al. (2019).\nThe regex patterns used by GPT-2 and GPT-4 are below:\n\n\nimport regex\nGPT4_SPLIT_PATTERN = regex.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\nGPT2_SPLIT_PATTERN = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n\nLooking at the example below,\n\n\nprint(regex.findall(GPT2_SPLIT_PATTERN,\"Hello're,     world! Here are the cats.\"))\n\n['Hello', \"'re\", ',', '    ', ' world', '!', ' Here', ' are', ' the', ' cats', '.']\n\n\nwe can make the following observations: - By breaking up the text this way, we’ll not be merging ‘e’, for example, as they are now parts of separate elements and we’ll only perform merges within elements.\n- GPT-2 pattern instructs the tokenizer not to merge across letters, numbers, punctuation, and select apostrophes. - re.IGNORECASE was not used in the original GPT-2 tokenizer, which means tokenization will not be consistent for uppercase and lowercase words containing apostrophes! - +(?!) is a negative lookahead assertion. It’s matching white spaces up to but not including the last white space. This consistently tokenizes start-of-word tokens with white space prepended to them.\nThere are additional rules other than chunking and BPE that are not clear from the original OpenAI implementation. For example 4 spaces in a succession would be likely merged by a tokenizer trained on Python code, which was likely the case. However, these spaces are not merged by the default GPT-2 tokenizer:\n\n\n\nSource: https://tiktokenizer.vercel.app/?model=gpt2\n\n\n\nMy implementation of the BPE tokenizer with RegEx is below:\n\n\nfrom tqdm import tqdm\nimport regex\nclass RegexTokenizer(BasicTokenizer):\n    def __init__(self,pattern) -&gt; None:\n        super().__init__()\n        self.pattern = pattern\n    \n    def train(self, text, vocab_size, verbose=False):\n        num_merges = vocab_size - 256\n        #--- Start: main regex modification ---#\n        raw_tokens = [token for token in regex.findall(self.pattern, text)]\n        encoded_tokens = [s.encode('utf-8') for s in raw_tokens]\n        tokens = b\"\".join(encoded_tokens)\n        #---   End: main regex modification ---#\n\n        init_num_tokens = len(tokens)\n        for i in tqdm(range(num_merges)):\n            stats = RegexTokenizer.get_stats(tokens)\n            #top_pair = max(stats, key=stats.get)\n            max_freq = max(stats.values())\n            top_pairs = [pair for pair, freq in stats.items() if freq == max_freq]\n            top_pair = min(top_pairs)  # lexicographically smallest pair\n            tokens = RegexTokenizer.merge(tokens, top_pair, 256+i)\n            self.merges[top_pair] = 256+i\n        final_num_tokens = len(tokens)\n        if verbose:\n            print(init_num_tokens)\n            print(final_num_tokens)\n            print(f\"compression ratio: {init_num_tokens/final_num_tokens:.2f}\")\n            print(f\"merges: {self.merges}\")\n\n\nBreaking down the main new lines above, it appears that the encoding checks out:\n\n\nraw_tokens = [token for token in regex.findall(GPT2_SPLIT_PATTERN, text)]\nencoded_tokens = [s.encode('utf-8') for s in raw_tokens]\ntokens = b\"\".join(encoded_tokens) \n\n\ntext = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view. I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.\"\ntokenizer = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\ntokenizer.train(text,vocab_size=276,verbose=True)\n\n100%|██████████| 20/20 [00:00&lt;00:00, 8480.19it/s]\n\n\n1116\n847\ncompression ratio: 1.32\nmerges: {(101, 32): 256, (105, 110): 257, (115, 32): 258, (226, 128): 259, (32, 116): 260, (97, 110): 261, (240, 159): 262, (116, 32): 263, (97, 114): 264, (257, 103): 265, (100, 32): 266, (101, 114): 267, (44, 32): 268, (111, 114): 269, (105, 99): 270, (111, 110): 271, (260, 104): 272, (32, 115): 273, (46, 32): 274, (100, 101): 275}\n\n\n\n\n\n\nAndrej has a more sophisticated implementation with handling of special tokens, saving, and loading methods. His train method is below for reference:\n\n\n\nclass RegexTokenizerAK(Tokenizer):\n    ...\n    def train(self, text, vocab_size, verbose=False):\n        assert vocab_size &gt;= 256\n        num_merges = vocab_size - 256\n\n        # split the text up into text chunks\n        text_chunks = re.findall(self.compiled_pattern, text)\n\n        # input text preprocessing\n        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n\n        # iteratively merge the most common pairs to create new tokens\n        merges = {} # (int, int) -&gt; int\n        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -&gt; bytes\n        for i in range(num_merges):\n            # count the number of times every consecutive pair appears\n            stats = {}\n            for chunk_ids in ids:\n                # passing in stats will update it in place, adding up counts\n                get_stats(chunk_ids, stats)\n            # find the pair with the highest count\n            pair = max(stats, key=stats.get)\n            # mint a new token: assign it the next available id\n            idx = 256 + i\n            # replace all occurrences of pair in ids with idx\n            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n            # save the merge\n            merges[pair] = idx\n            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n            # prints\n            if verbose:\n                print(f\"merge {i+1}/{num_merges}: {pair} -&gt; {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n\n        # save class variables\n        self.merges = merges # used in encode()\n        self.vocab = vocab   # used in decode()\n    ...\n\n\nAs expected, Andrej’s tokenizer works flawlessly:\n\n\nfrom pathlib import Path\ntokenizer = RegexTokenizerAK(pattern=GPT2_SPLIT_PATTERN)\ntaylor_text = Path('taylorswift.txt').read_text().strip()\ntokenizer.train(taylor_text,vocab_size=1276,verbose=False)\nassert tokenizer.decode(tokenizer.encode(taylor_text))==taylor_text, 'enc/dec mismatch'\n\n\nAs a side note, I could be wrong, but I think the following line may help make the tokenizer deterministic: pair = max(stats.items(), key=lambda item: (item[1], -item[0][0], -item[0][1]))[0]"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#training-and-comparing-tokenizers-on-tinystories-data",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#training-and-comparing-tokenizers-on-tinystories-data",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "Now let’s train and compare the tokenizers on subsets on the TinyStories dataset.\n\n\nfrom datasets import load_dataset\ntiny_stories_ds=load_dataset(\"roneneldan/TinyStories\")\n\n/home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nsample_list = tiny_stories_ds['train'].shuffle(seed=1337).select(range(1000))['text']\n\n\nsample_list[:3]\n\n['Once upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to her mother.\\n\\nHer mother said, \"Wow, Lily! That\\'s a beautiful horse. Can you kneel down and show me how it runs?\"\\n\\nLily knelt down and made the horse run with her hands. Her mother smiled and said, \"You\\'re so creative, Lily. Keep making more things with clay.\" \\n\\nLily smiled and said, \"I will, Mommy. And maybe next time, I\\'ll make a good-looking dog for you.\"',\n 'Once upon a time, there was a mum and her two kids. The two kids were Joe and Sam. Joe and Sam did not always agree with each other. One day, the two boys had a big fight over a toy. Joe wanted to use the toy first, but Sam did not agree.\\n\\nSo, mum decided to take the boys to see a lawyer. The lawyer was very calm. The lawyer helped them talk with each other and figure out a solution. Eventually, the lawyer helped Joe and Sam agree on which one should use the toy first. \\n\\nThe boys were proud of themselves for sorting their problem out. Ever since then, they have become very good at using the law when they disagree, and they often keep calm when talking with each other.',\n 'One day, a curious little girl named Lucy wanted to have a picnic. She asked her mom, \"Can we have a picnic today?\" Her mom said, \"Yes, but we must be careful.\" Lucy was very happy and started to pack a big basket with food.\\n\\nThey went to the park and found a nice spot under a big tree. They put a blanket on the grass and started eating their sandwiches. Lucy saw a little bird and wanted to give it some bread. Her mom said, \"Be careful, Lucy. Don\\'t go too far.\"\\n\\nBut Lucy didn\\'t listen. She walked away from the picnic to find the little bird. She looked and looked but couldn\\'t find it. She was lost and didn\\'t know how to go back to her mom. Lucy started to cry.\\n\\nHer mom looked for her but couldn\\'t find her. She was very sad and scared. The picnic was not fun anymore. They didn\\'t know that going too far away from the picnic would make them feel so bad.\\n\\nIn the end, Lucy and her mom were both very sad. They wished they had listened to each other and stayed close during the picnic.']\n\n\n\ntraining_corpus='&lt;--&gt;'.join(sample_list) # choose to use this since my basic tokenizer does not handle the special tokens\n#training_corpus='&lt;|endoftext|&gt;'.join(sample_list)\n\n\nprint(training_corpus[:500])\n\nOnce upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to her mother.\n\nHer mother said, \"Wow, Lily! That's a beautiful horse. Can you kneel down and show me how it runs?\"\n\nLily knelt down and made the horse run with her hands. Her mother smiled and said, \"You're so creative, Lily. Keep making more things with clay.\" \n\nLily smiled and said, \"I will, Mommy. A\n\n\n\ntokenizer_dpv = RegexTokenizer(pattern=GPT2_SPLIT_PATTERN)\ntokenizer_dpv.train(training_corpus,vocab_size=6400,verbose=False)\n\n100%|██████████| 6144/6144 [03:09&lt;00:00, 32.38it/s]\n\n\n\nassert tokenizer_dpv.decode(tokenizer_dpv.encode(training_corpus[:200]))==training_corpus[:200], 'enc/dec mismatch'\n\n\ntokenizer_ak = RegexTokenizerAK(pattern=GPT2_SPLIT_PATTERN)\ntokenizer_ak.train(training_corpus,vocab_size=6400,verbose=False)\n\n\nassert tokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))==training_corpus[:200], 'enc/dec mismatch'\n\n\nassert tokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))==tokenizer_dpv.decode(tokenizer_dpv.encode(training_corpus[:200])), 'ak/dpv mismatch'\n\n\ntokenizer_ak.decode(tokenizer_ak.encode(training_corpus[:200]))\n\n'Once upon a time, there was a little girl named Lily. She loved to play with clay and make different shapes. One day, she made a good-looking horse with the clay. She was very happy and showed it to h'\n\n\n\ntext_sample = training_corpus[:1024]\n\n\nNow let’s train the GPT-2 tokenizer using HuggingFace tokenizers:\n\n\nfrom transformers import GPT2TokenizerFast\nfrom tokenizers import trainers, Tokenizer\n\n\n# Load the GPT-2 tokenizer\noriginal_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n\n# Get the settings from the original tokenizer\nmodel = original_tokenizer._tokenizer.model\npre_tokenizer = original_tokenizer._tokenizer.pre_tokenizer\npost_processor = original_tokenizer._tokenizer.post_processor\n\n# Initialize a new tokenizer with the same settings\ngpt2_custom = Tokenizer(model)\ngpt2_custom.pre_tokenizer = pre_tokenizer\ngpt2_custom.post_processor = post_processor\n\n# Train the new tokenizer\ntrainer = trainers.BpeTrainer(vocab_size=6400, show_progress=True)\ngpt2_custom.train_from_iterator(sample_list, trainer=trainer)\n\n/home/mainuser/anaconda3/envs/mintonano/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\nFor reference, let’s also train a ‘standard’ BPE tokenizer where all implementation details are known:\n\n\nfrom tokenizers import Tokenizer, trainers, models, pre_tokenizers, normalizers\n\n# Initialize a new ByteLevelBPETokenizer\nbpe_tokenizer = Tokenizer(models.BPE())\n\n# Normalize the text\nbpe_tokenizer.normalizer = normalizers.Sequence([\n    normalizers.NFKC(),\n    normalizers.Lowercase()\n])\n\n# Pre-tokenize the text\nbpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Train the tokenizer\ntrainer = trainers.BpeTrainer(vocab_size=6400, show_progress=True)\nbpe_tokenizer.train_from_iterator(sample_list, trainer=trainer)\n\n\n\n\n\n\n\nBefore the lexicographic adjustment:\n\n\ntokenizer_dpv.encode(text) \n#[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 435, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 435, 240, 159, 152, 132, 32, 389, 378, 2443, 5152, 3301, 3655, 274, 97, 669, 937, 655, 697, 452, 1481, 1022, 331, 109, 2477, 2632, 119, 1268, 866, 637, 1111, 669, 273, 567, 275, 226, 128, 156, 1656, 337, 288, 261, 85, 2159, 111, 732, 226, 128, 157, 32, 549, 6378, 3206, 270, 397, 40, 3598, 1831, 356, 1541, 115, 226, 128, 148, 534, 782, 316, 119, 99, 681, 95, 261, 343, 1320, 2267, 283, 859, 1183, 63, 41, 619, 85, 2159, 111, 557, 2090, 97, 98, 308, 1310, 367, 494, 805, 1467, 937, 719, 1859, 45, 552, 1013, 85, 2159, 111, 557, 83, 116, 935, 2041, 112, 708, 5540, 456, 122, 276, 262, 1890, 117, 337, 323, 4443, 282, 2440, 1226, 783, 294, 112, 1139, 1510, 820, 458, 2090, 821, 738, 4221, 269, 380, 298, 398, 283, 839, 100, 280, 226, 128, 153, 261, 2112, 1030, 1481, 1022, 331, 109, 2477, 343, 4582, 2604, 803, 3402, 589, 2636, 1071, 859, 1085, 51, 48, 32, 1303, 115, 3316, 85, 2159, 111, 732, 226, 128, 153, 262, 2761, 535, 4733, 2701, 2234, 1569, 258, 115, 259, 395, 2763, 676, 2658, 4462, 85, 2159, 111, 100, 477, 712, 4294, 641, 1077, 753, 316, 821, 2841, 313, 549, 732, 2053, 369, 1403, 2318, 1434, 116, 469, 323, 272, 73, 226, 128, 153, 904, 1204, 371, 1322, 412, 100, 902, 4627, 275, 313, 4569, 1481, 1022, 331, 4442, 226, 128, 153, 262, 3348, 261, 452, 5107, 119, 369, 226, 128, 153, 713, 2002, 1858, 957, 4459, 1447, 3213, 460, 281, 1640, 3598, 226, 128, 153, 262, 269, 118, 467, 709, 549, 896, 316, 333, 1573, 283, 421, 4717, 458, 452, 85, 2159, 111, 557, 116, 466, 116, 5154, 1277, 272, 549, 2318, 1434, 116, 469, 339, 73, 226, 128, 153, 713, 438, 2002, 1611, 961, 102, 280, 116, 859, 116, 466, 261, 307, 347, 116, 47, 2716, 283, 47, 114, 402, 264, 1498, 1036, 297, 528, 320, 122, 3732, 549, 732, 2053, 226, 128, 148, 5112, 1059, 367, 506, 1736, 455, 1656, 783, 338, 3599, 265, 420, 2669, 513, 256, 40, 304, 1061, 2379, 415, 41, 4573, 46]\nlen(tokenizer_dpv.encode(text)) \n#454\n\n\n454\n\n\n\ntokenizer_ak.encode(text) \n#[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 299, 396, 1253, 2342, 778, 2407, 264, 6081, 696, 262, 3703, 345, 282, 1312, 114, 333, 109, 667, 1371, 119, 433, 46, 750, 495, 713, 366, 32, 611, 266, 32, 226, 128, 156, 115, 117, 340, 811, 4089, 474, 4759, 226, 128, 157, 315, 1097, 1431, 119, 491, 32, 40, 119, 578, 1426, 353, 2900, 226, 128, 148, 108, 1636, 3290, 260, 382, 289, 95, 116, 365, 495, 262, 733, 566, 44, 957, 63, 41, 46, 552, 4089, 474, 4759, 483, 324, 689, 362, 114, 117, 387, 44, 264, 3897, 847, 696, 262, 305, 613, 787, 45, 112, 945, 4089, 474, 4759, 2341, 787, 462, 356, 585, 883, 417, 122, 2951, 345, 4497, 304, 1837, 1198, 414, 3062, 120, 310, 44, 393, 112, 3054, 44, 264, 5469, 483, 324, 677, 645, 258, 388, 630, 300, 292, 2508, 46, 335, 877, 226, 128, 153, 116, 595, 539, 282, 1312, 114, 333, 109, 667, 365, 901, 3602, 262, 2245, 1188, 2552, 44, 850, 2032, 48, 1390, 1149, 4089, 474, 4759, 226, 128, 153, 115, 315, 336, 885, 751, 46, 469, 1543, 5633, 535, 111, 44, 335, 587, 3903, 315, 4089, 474, 4759, 264, 642, 266, 2200, 499, 392, 2851, 677, 700, 302, 315, 278, 311, 2309, 46, 1395, 762, 3236, 474, 304, 44, 335, 226, 128, 153, 306, 1136, 414, 4042, 4077, 751, 266, 302, 583, 258, 282, 1312, 114, 333, 1195, 226, 128, 153, 115, 5903, 345, 3179, 46, 335, 226, 128, 153, 109, 925, 266, 1255, 99, 585, 349, 262, 5953, 419, 1263, 264, 563, 226, 128, 153, 115, 2284, 1189, 420, 315, 2645, 339, 733, 566, 264, 3009, 310, 345, 4089, 474, 4759, 1139, 976, 46, 3963, 44, 315, 762, 3236, 474, 304, 335, 226, 128, 153, 109, 400, 925, 266, 1594, 700, 272, 911, 115, 44, 1139, 976, 2634, 389, 47, 1367, 669, 287, 47, 270, 686, 287, 44, 716, 432, 99, 406, 735, 1577, 315, 278, 311, 2309, 226, 128, 148, 1014, 1752, 453, 5149, 402, 115, 117, 310, 44, 324, 121, 2387, 660, 532, 529, 101, 32, 40, 787, 4511, 41, 1324, 46]\nlen(tokenizer_ak.encode(text)) \n#454\n\n454\n\n\n\nAfter the lexicographic adjustment:\n\n\ntokenizer_ak.encode(text) #[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 303, 366, 1285, 357, 324, 1355, 2631, 264, 258, 119, 101, 980, 262, 2098, 115, 338, 1827, 2010, 327, 109, 711, 1172, 119, 429, 46, 1015, 477, 746, 384, 32, 634, 267, 32, 226, 2556, 115, 117, 345, 654, 1469, 986, 111, 100, 101, 226, 128, 157, 312, 1335, 1784, 119, 493, 32, 40, 119, 104, 726, 313, 360, 1835, 115, 226, 128, 148, 108, 2506, 2293, 260, 402, 293, 95, 116, 365, 477, 262, 357, 2037, 44, 1114, 63, 41, 46, 638, 1469, 986, 111, 100, 101, 507, 328, 722, 575, 114, 117, 361, 44, 264, 281, 105, 783, 980, 262, 305, 816, 1480, 45, 112, 905, 1469, 986, 111, 100, 101, 298, 116, 1480, 502, 359, 462, 790, 440, 122, 290, 115, 338, 259, 117, 345, 310, 2024, 293, 121, 428, 110, 101, 120, 306, 44, 446, 782, 115, 44, 264, 450, 306, 507, 328, 650, 764, 258, 390, 850, 304, 292, 291, 287, 46, 337, 998, 226, 128, 153, 116, 626, 553, 1827, 2010, 327, 109, 711, 365, 1266, 635, 287, 262, 714, 310, 508, 1856, 2018, 44, 820, 1971, 48, 1317, 990, 1469, 986, 111, 100, 101, 226, 128, 153, 115, 312, 1258, 755, 46, 524, 2891, 910, 1130, 115, 562, 111, 44, 337, 543, 2995, 575, 263, 312, 1469, 986, 111, 100, 101, 264, 682, 267, 1813, 469, 388, 2930, 650, 827, 301, 312, 281, 314, 1481, 46, 1153, 741, 435, 116, 480, 310, 44, 337, 226, 128, 153, 289, 885, 428, 2997, 116, 755, 267, 301, 578, 258, 1827, 2010, 327, 109, 276, 226, 128, 153, 115, 2104, 116, 338, 3395, 46, 337, 226, 128, 153, 109, 1164, 267, 273, 111, 2474, 347, 262, 607, 849, 99, 393, 1365, 264, 537, 226, 128, 153, 115, 312, 118, 806, 394, 312, 2878, 334, 357, 2037, 264, 273, 296, 306, 338, 1469, 986, 111, 100, 101, 3005, 46, 2139, 1487, 44, 312, 741, 435, 116, 480, 310, 337, 226, 128, 153, 109, 450, 1164, 267, 1447, 827, 273, 2716, 115, 44, 3005, 2644, 385, 47, 115, 104, 1038, 287, 47, 268, 1052, 287, 44, 788, 414, 99, 370, 620, 291, 755, 312, 281, 314, 1481, 226, 128, 148, 1130, 2033, 472, 259, 803, 293, 726, 415, 115, 117, 306, 44, 328, 121, 2525, 712, 515, 541, 101, 32, 40, 1480, 746, 1126, 594, 41, 1384, 46]\nlen(tokenizer_ak.encode(text)) #497\n\n454\n\n\n\ntokenizer_dpv.encode('&lt;--&gt;'),tokenizer_ak.encode('&lt;--&gt;') # Ok, encoded early on by both tokenizers\n\n([374], [375])\n\n\n\nMy tokenizer behaves similarly to Andrej’s, with most merges happening around the same time. Both appear to possess the same compression ratio (tokenized the sample text to 454 tokens to represent text of length 1024) on the chosen sample, and both successfully recover the original sequence. The main difference seems to come from Andrej’s tokenizer using get_stats on each chunk separately and merging pairs within the chunks separately, while mine merges pairs within the entire concatenated byte string. While this is slower (13:57 vs 03:18 on my machine), it is a more careful (and likely more ‘correct’) handling of the individual chunks. I would imagine the modern highly optimized tokenizers to perform tokenization on multiple chunks of text in parallel, aligning best with Andrej’s implementation.\nHowever, GPT-2 tokenizer appears to yield different tokens even with same vocabulary size. It also differs from the standard BPE tokenizer available from Hugging Face. Since not all details of GPT-2 tokenizer implementation are known (referring to 4-spaces not being merged for Python code, for example), this could be due to a custom implementation detail of GPT-2.\n\n\nprint(gpt2_custom.encode(text_sample).ids)\nprint(len(gpt2_custom.encode(text)))\n\n[248, 261, 81, 212, 4, 216, 103, 81, 209, 260, 321, 182, 6, 136, 309, 89, 195, 162, 4781, 87, 395, 1036, 1676, 6, 333, 178, 4, 155, 370, 81, 419, 5, 3323, 1709, 162, 85, 4781, 6, 136, 103, 215, 227, 87, 999, 125, 89, 135, 1062, 6, 72, 72, 1080, 1062, 145, 4, 154, 1188, 4, 182, 0, 1032, 190, 81, 610, 1709, 6, 1174, 175, 3679, 470, 87, 528, 354, 541, 125, 1703, 295, 72, 72, 164, 3973, 470, 87, 370, 85, 1709, 631, 162, 135, 799, 6, 677, 1062, 307, 87, 145, 4, 154, 341, 1207, 170, 3040, 4, 182, 6, 4921, 1011, 496, 464, 162, 4781, 235, 228, 72, 164, 307, 87, 145, 4, 154, 23, 500, 4, 944, 6, 792, 2025, 894, 212, 4, 158, 1466, 395, 81, 419, 5, 3323, 536, 188, 175, 235, 5155, 248, 261, 81, 212, 4, 216, 103, 81, 808, 87, 135, 729, 1049, 6, 122, 729, 1049, 253, 813, 87, 455, 6, 813, 87, 455, 287, 219, 452, 3404, 162, 628, 412, 6, 333, 178, 4, 85, 729, 1772, 186, 81, 220, 1515, 582, 81, 314, 6, 813, 267, 89, 1072, 85, 314, 1117, 4, 234, 455, 287, 219, 3404, 6, 72, 72, 762, 4, 808, 462, 89, 492, 85, 1772, 89, 343, 81, 2612, 6, 122, 2612, 103, 215, 2006, 6, 122, 2612, 1065, 278, 1428, 162, 628, 412, 87, 5314, 327, 264, 81, 4973, 6, 4359, 4, 85, 2612, 1065, 813, 87, 455, 3404, 172, 3178, 483, 828, 1072, 85]\n344\n\n\n\nprint(bpe_tokenizer.encode(text_sample).ids)\nprint(len(bpe_tokenizer.encode(text)))\n\n[211, 228, 56, 182, 4, 183, 79, 56, 177, 226, 282, 150, 6, 86, 277, 64, 162, 132, 4495, 63, 360, 971, 1574, 6, 220, 148, 4, 86, 334, 56, 376, 5, 3147, 1606, 132, 58, 4495, 6, 86, 79, 185, 196, 63, 934, 90, 64, 105, 966, 6, 47, 47, 217, 966, 119, 4, 125, 1110, 4, 150, 0, 145, 158, 56, 558, 1606, 6, 249, 126, 3479, 424, 63, 474, 313, 462, 90, 1601, 260, 47, 47, 393, 3752, 424, 63, 334, 58, 1606, 586, 132, 105, 742, 6, 105, 966, 275, 63, 119, 4, 125, 697, 1125, 131, 2871, 4, 150, 6, 755, 945, 448, 420, 132, 4495, 201, 197, 47, 393, 275, 63, 119, 4, 125, 23, 447, 4, 369, 6, 63, 1029, 811, 182, 4, 174, 1372, 360, 56, 376, 5, 3147, 489, 156, 126, 201, 4857, 3410, 228, 56, 182, 4, 183, 79, 56, 579, 63, 105, 666, 981, 6, 58, 666, 981, 222, 748, 63, 379, 6, 748, 63, 379, 253, 189, 407, 3230, 132, 575, 373, 6, 220, 148, 4, 58, 666, 1664, 153, 56, 188, 1400, 529, 56, 461, 6, 748, 233, 64, 1002, 58, 461, 1000, 4, 154, 379, 253, 189, 3230, 6, 47, 47, 468, 4, 579, 419, 64, 443, 58, 1664, 64, 303, 56, 2468, 6, 58, 2468, 79, 185, 1887, 6, 58, 2468, 995, 244, 1341, 132, 575, 373, 63, 4995, 291, 230, 56, 4690, 6, 1908, 4, 58, 2468, 995, 748, 63, 379, 3230, 103, 2658, 220, 766, 1002, 58]\n314\n\n\n\nIt is noteworthy that tokenizer trained directly from GPT-2 tokenizer using the Hugging Face library yielded a smaller compression ratio than the BPE tokenizer (344 vs 314 tokens to represent text of length 1024), and the compression ratio of these tokenizers was larger than that of our custom tokenizers.\nAlso, both GPT-2 and BPE tokenizer appear to be highly optimized, taking less than a second to train on our sample compared to minutes, implying that it’s the best move to understand the tokenization process and use an optimized off-the-shelf implementation."
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#additional-discussion",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#additional-discussion",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "Andrej makes a number or important points in the second half of the Let’s build the GPT Tokenizer video.\n\n\n\n\nGPT-4 tokenizer fixed the case sensitivity pointed out above by making the match pattern case insensitive.\nIn additional to the encoder and decoder, GPT-2 also has byte_encoder and byte_decoder methods that don’t add substantially to the implementation.\n\nGPT-2 tokenizer’s vocab size of 50257 comes from 256 raw byte tokens, 50000 merges, and the &lt;|endoftext|&gt; token.\n\nModern tokenizers, such as the gpt-3.5-turbo tokenizer, also have other special tokens such as &lt;|imstart|&gt;, &lt;|imend|&gt;, etc, where ‘im’ stands for ‘imaginary monologue’ to delimit chat conversations, for example. tiktoken library allows addition of these special tokens.\nAdding special tokens necessitates transformer model surgery to add the tokens in the embedding matrix and the classifier layer.\nAs noted above, tiktoken does not provide the train function, just the inference code.\n\n\n\n\n\nsentencepiece provices a train function and works on the level of code points, not bytes, falling back to UTF-8 if it encounters unknown tokens.\nIt also tries to split out sentences, which could be challenging since many syntactic units do not squarely fall within sentence/no-sentence category.\nBy default sentencepiece adds white spaces in front of strings that are not at the start of sentences for consistency of treating words occurring at the start and in the middle of sentences.\nIn Andrej’s experience, sentencepiece documentation is lacking a bit at the moment.\n\n\n\n\n\nWith larger vocabulary size, more tokens can be compressed and included within a given context length, but there would also be fewer of a given token for a language model to train on.\nWhile the greater amount of information included within a context window is generally beneficial, too much of it due to a large vocab size is likely not give the model enough time to ‘think’ as it’s attending to the tokens. Thus vocab size can be viewed as a hyperparameter of the model.\nIf we’re finetuning and want to add special tokens (eg, for using a browser), we would resize the embedding and the weights, then could freeze the original weights and finetune just the new parameters.\nThere is a gist tokens compression technique where new tokens are introduced and the model is trained by distillation, training just the embedding representations of the new tokens such that the behavior of this language model is identical to that of a model with a very long prompt. This is shown in the following diagram:\n\n\n\n\nSource: Mu, J., Li, X. L., & Goodman, N. (2023).\n\n\n\nIt is also possible to tokenize non-language data (eg, images), and use the transformer architecture for autoregressive or diffusion models (Sora is an example).\n\n\n\n\n\nAndrej’s answers to the following questions are below.\nWhy can’t LLM spell words?\n\nAK: Characters are chunked up into tokens, some of which are long (a model with a ‘:DefaultCellStyle’ token will not be able to answer spelling questions about it). Also, if ‘:DefaultCellSty’ is passed to the model, the model generation will likely stop since it has never seen just a chunk of the token in it’s vocabulary and such chunks are outside of the model’s distribution.\n\nWhy can’t LLM do super simple string processing tasks like reversing a string?\n\nAK: Similarly, reversing a single long token would be challenging to a model unless it’s first prompted to list out the individual characters.\n\n\nWhy is LLM worse at non-English languages (e.g. Japanese)?\n\nAK: Tokenizers are not sufficiently trained on non-English data and as a consequence do not compress it sufficiently.\n\nWhy is LLM bad at simple arithmetic?\n\nAK: Addition algorithm is character-level, yet integers are not split up consistently (sometimes into 1, or 2, or 3 digits).\n\nWhy did GPT-2 have more than necessary trouble coding in Python?\n\nAK: Encoding efficiency of the GPT-2 tokenizer was not great since tabs did not get encoded into single tokens, for example.\n\nWhy did my LLM abruptly halt when it sees the string “&lt;|endoftext|&gt;”?\n\nAK: Something is breaking as this token does not seem to be handled properly: questions to LLMs containing it end up unanswered as this token appears to be parsed when it shouldn’t be when coming from a user that could attack the system.\n\nWhat is this weird warning I get about a “trailing whitespace”?\n\nAK: Prompting ending in trailing space sent to instruct models cause sampling inefficiency since tokens at the beginning of words already start with a space and adding an extra space is out of distribution for the model.\n\n\nWhy did the LLM break if I ask it about “SolidGoldMagikarp”?\n\nAK: There’s an internet-famous blog examining clusters of tokens based on their embeddings. When these tokens are used in questions to an LLM that had these tokens in its tokenizer, the model may respond with evasion, hallucination, insults, etc. The likely cause of this is the discrepancy between tokenization and training datasets. In the tokenization dataset, tokens for popular Reddit users such as SolidGoldMagikarp got merged into their own tokens. However, the token likely never appeared in the training data, and the token never got sampled and ‘trained’. Then at test time, giving this token to the model leads to undefined behavior.\n\n\nWhy should I prefer to use YAML over JSON with LLMs?\n\nAK: YAMLs generally end up being tokenized into fewer tokens than JSONs and hence are more economical.\n\n\nWhy is LLM not actually end-to-end language modeling?\n\nAK: Tokenization.\n\nWhat is the real root of suffering?\n\nAK: Andrej really dislikes tokenization ( ͡° ͜ʖ ͡°)"
  },
  {
    "objectID": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#references",
    "href": "blog/posts/gpt2-from-from-scratch-to-tiny-stories/Part 4: Tokenization mechanics and challenges.html#references",
    "title": "Part 4 - Tokenization mechanics and challenges",
    "section": "",
    "text": "Mu, J., Li, X. L., & Goodman, N. (2023). Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467. doi: 10.48550/arXiv.2304.08467\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. Technical report, OpenAI."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Dmitriy’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nDec 1, 2024\n\n\nFavorite Books and Resources of 2024\n\n\n\n\nSep 19, 2024\n\n\nPart 4 - Tokenization mechanics and challenges\n\n\n\n\nSep 17, 2024\n\n\nPart 3 - Training on TinyStories\n\n\n\n\nSep 14, 2024\n\n\nPart 2 - Optimizing GPT-2\n\n\n\n\nSep 5, 2024\n\n\nPart 1 - Building GPT-2 Architecture\n\n\n\n\nAug 31, 2024\n\n\nHow to pass GCP PDE exam on the first attempt: Write-up in progress\n\n\n\n\nMay 31, 2024\n\n\narXiv LLMs Assistant: Write-up in progress\n\n\n\n\nApr 15, 2024\n\n\nKTO Optimization - Write-up forthcoming\n\n\n\n\nOct 18, 2022\n\n\nDetecting anomalies in a statewide housing market with alternative data\n\n\n\n\nAug 29, 2022\n\n\nInvesting in student housing\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist and machine learning engineer with expertise in Python, Google Cloud Platform (GCP), and large language models (LLMs). My passion lies in leveraging code and LLMs to build innovative solutions.\nCurrently, I’m developing several projects focused on language models. From November 2022 to August 2024, I served as a Data Analyst at LS Direct, where I successfully executed data analytics and engineering tasks for major clients, resulting in significant budget increases—up to five times the original amounts. I automated both internal and client-facing processes using Python, SQL, and GCP, saving our team valuable hours each week. I have a strong preference for programmatic solutions that streamline operations and enhance efficiency.\nPrior to this role, I taught AP Computer Science at Lake Mary High School for two years, where I rebuilt the AP Computer Science A program and founded the LMHS AI Club, introducing advanced students to deep learning through PyTorch and Linux. Earlier in my career, I taught algebra to underserved communities at Vanden High School.\nMy academic background includes an undergraduate degree in mathematics and three graduate programs—one in mathematics and two in economics (applied and mathematical), the latest from UC Davis. During my graduate studies, I worked on computer vision applications and taught statistical methods, econometrics, and graduate economic theory. This experience deepened my appreciation for the transformative power of deep learning in computer vision.\nI have always been fascinated by the intersection of computation and linguistics, having learned several languages and achieving fluency in around seven at one point. This linguistic curiosity drives my intrinsic interest in LLMs, particularly in creating practical projects that benefit others.\nI’m excited to engage with innovative projects and connect with like-minded professionals in the data science and machine learning fields."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Dmitriy Popov-Velasco",
    "section": "",
    "text": "I’m a data scientist and machine learning engineer with expertise in Python, Google Cloud Platform (GCP), and large language models (LLMs). My passion lies in leveraging code and LLMs to build innovative solutions.\nCurrently, I’m developing several projects focused on language models. From November 2022 to August 2024, I served as a Data Analyst at LS Direct, where I successfully executed data analytics and engineering tasks for major clients, resulting in significant budget increases—up to five times the original amounts. I automated both internal and client-facing processes using Python, SQL, and GCP, saving our team valuable hours each week. I have a strong preference for programmatic solutions that streamline operations and enhance efficiency.\nPrior to this role, I taught AP Computer Science at Lake Mary High School for two years, where I rebuilt the AP Computer Science A program and founded the LMHS AI Club, introducing advanced students to deep learning through PyTorch and Linux. Earlier in my career, I taught algebra to underserved communities at Vanden High School.\nMy academic background includes an undergraduate degree in mathematics and three graduate programs—one in mathematics and two in economics (applied and mathematical), the latest from UC Davis. During my graduate studies, I worked on computer vision applications and taught statistical methods, econometrics, and graduate economic theory. This experience deepened my appreciation for the transformative power of deep learning in computer vision.\nI have always been fascinated by the intersection of computation and linguistics, having learned several languages and achieving fluency in around seven at one point. This linguistic curiosity drives my intrinsic interest in LLMs, particularly in creating practical projects that benefit others.\nI’m excited to engage with innovative projects and connect with like-minded professionals in the data science and machine learning fields."
  }
]